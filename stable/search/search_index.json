{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Axion","text":"<p> Moving from passive, black-box observation to active, white-box evaluation\u2014Axion empowers builders with actionable signals, automated pipelines, and fully transparent metrics. See exactly why your agent succeeds or fails. </p> <p> White-Box Transparency | Modular by Design | Built for Scale </p>"},{"location":"#component-arsenal","title":"Component Arsenal","text":"Core Primitives Structured Handlers &amp; Tool Abstractions <p>Build composable toolchains with pre-defined base classes for structured LLMs, tools, and knowledge retrieval. Eliminate boilerplate, enforce consistency, and focus on your logic.</p> API Integrations Extensible Backend Access <p>Base API classes with built-in tracing and authentication support. Build your own API integrations with ease or extend the provided abstractions.</p> Evaluation Engine &amp; Metric Suite Built-in &amp; Open-Source Friendly <p>Define experiments, run batch evaluations, calibrate judges, and score using our native metrics\u2014or integrate with open libraries for broader experimentation coverage.</p> RAG Toolbox Everything Retrieval\u2014Chunking, Grounding, Response Assembly <p>End-to-end support for grounding pipelines with modular components you can reuse across use cases.</p> Observability at Its Core Trace, Log, Debug with Confidence <p>Native support for Logfire, structured logging, and run tracking gives you production-grade visibility across every step of your AI pipeline.</p> Designed for Scale Async-Native, Pydantic-Validated, Error Resilient <p>Async support everywhere. Predictable, structured I/O with Pydantic validation. Robust error handling out-of-the-box.</p>"},{"location":"#hierarchical-scoring","title":"Hierarchical Scoring","text":"What sets Axion apart: Our scoring framework is hierarchical by design\u2014moving from a single overall score down into layered sub-scores. This delivers a diagnostic map of quality, not just a number.  <pre><code>                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  Overall Score  \u2502\n                \u2502      0.82       \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u25bc                  \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Relevance \u2502      \u2502 Accuracy  \u2502      \u2502   Tone    \u2502\n\u2502   0.91    \u2502      \u2502   0.78    \u2502      \u2502   0.85    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>   **Instant Root Cause Diagnosis**  Drill down to pinpoint whether issues stem from relevance, accuracy, tone, or other dimensions\u2014no more guessing from flat scores.     **Strategic Prioritization**  Forces clarity on what really matters for your business by breaking quality into weighted layers.     **Actionable Feedback Loop**  Each layer translates directly into actions\u2014retraining, prompt adjustments, or alignment tuning.     **Customizable to Business Goals**  Weight and expand dimensions to match your unique KPIs. Define what \"good AI\" means for you.   <pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Define hierarchical scoring configuration\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n    },\n    'model': {\n        'ANSWER_QUALITY': {'Relevance': 1.0},\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=config,  # Or pass path to config.yaml\n)\n\n# Generate scorecard with hierarchical breakdown\nresults.to_scorecard()\n</code></pre> <p>Learn more about Hierarchical Scoring \u2192</p>"},{"location":"#think-of-axion-as-your-lego-set-for-ai-systems","title":"Think of Axion as your Lego set for AI systems:","text":""},{"location":"#why-axion","title":"Why \"Axion\"?","text":"<p>Agent X-Ray Inspection &amp; Optimization Network</p> <p>The name draws inspiration from the axion\u2014a hypothetical particle in physics proposed to solve the \"strong CP problem\" in quantum chromodynamics. Physicists Frank Wilczek and Steven Weinberg named it after a laundry detergent, hoping it would \"clean up\" their theoretical mess.</p> <p>Like its namesake, this toolkit provides lightweight, modular components that work together to solve complex problems. Axions in physics are characterized by being incredibly small yet potentially accounting for much of the universe's dark matter through sheer numbers. Similarly, Axion the toolkit offers small, focused tools that combine to tackle the substantial challenge of AI agent evaluation.</p>"},{"location":"agent_playbook/","title":"Agent Evaluation Playbook","text":"<p>This playbook outlines the core principles and strategy for effective AI agent evaluation\u2014going beyond surface-level metrics and dashboards to focus on what truly matters: continuously improving agents to perform in the real world.</p>"},{"location":"agent_playbook/#the-problem-with-generic-evaluation","title":"The Problem with Generic Evaluation","text":"<p>Evaluating AI agents isn't a traditional machine learning task. It's a context-driven problem\u2014requiring more than generic metrics or off-the-shelf tools. Generative agents are unpredictable and can be highly domain-specific, so success demands a tailored approach rooted in:</p> <ul> <li>Deep domain expertise</li> <li>Context-aware error analysis</li> <li>Metrics tied to real business outcomes</li> </ul> <p>There's no silver bullet. One-size-fits-all evaluation leads to mediocrity. The real question is: Is this agent improving at the job it was designed to do?</p> <p>A customer support agent and an onboarding agent may share similarities, but grading them with the same rubric is like judging a painter and a sculptor by identical standards.</p>"},{"location":"agent_playbook/#four-common-mistakes","title":"Four Common Mistakes","text":"<p>Before diving into the process, avoid these common pitfalls:</p>"},{"location":"agent_playbook/#1-not-looking-at-your-data","title":"1. Not Looking at Your Data","text":"<p>The most common\u2014and most damaging\u2014mistake is neglecting to examine your data. Despite being fundamental, this step is often skipped.</p> <p>Effective evaluation begins with frictionless, high-quality data exploration and labeling. Without a deep understanding of your agent's failures, any metrics or conclusions you draw are on shaky ground.</p> <p>If you haven't spent time reviewing raw examples, you're not truly evaluating.</p> <p>Start manually. Use spreadsheets. Iterate. Understand the process before automating.</p>"},{"location":"agent_playbook/#2-frameworks-before-fundamentals","title":"2. Frameworks Before Fundamentals","text":"<p>Jumping to tools or frameworks before establishing a process is a red flag.</p> <p>If your first question is \"What framework should I use?\", you're starting in the wrong place. Evaluation is not a plug-and-play task. You must define what success looks like for your agent. Frameworks are secondary\u2014they should support your workflow, not define it.</p>"},{"location":"agent_playbook/#3-overreliance-on-generic-evaluations","title":"3. Overreliance on Generic Evaluations","text":"<p>Generic metrics are not a shortcut\u2014they're a risk.</p> <p>If your evals aren't grounded in known errors, you're wasting time. Off-the-shelf metrics (e.g., hallucination, toxicity, relevancy) are easy to reach for\u2014but dangerous when misused. Agents are purpose-built. So should your evaluations be.</p> <p>Generic metrics may be useful after understanding your agent's unique failure modes, but using them prematurely becomes a crutch that masks real issues.</p>"},{"location":"agent_playbook/#4-misusing-llm-as-a-judge","title":"4. Misusing LLM-as-a-Judge","text":"<p>Large language models can assist in evaluation, but they must be used responsibly.</p> <p>Relying on LLMs without first validating them against domain expert feedback is a recipe for false confidence. The correct approach:</p> <ol> <li>Build a lightweight review pipeline</li> <li>Have experts label a few dozen examples</li> <li>Align your LLM judge to match that standard</li> </ol> <p>Until you achieve strong agreement between human and model judgment, LLM outputs should be treated as advisory\u2014not definitive.</p>"},{"location":"agent_playbook/#the-analyze-measure-improve-methodology","title":"The Analyze-Measure-Improve Methodology","text":"<p>The AMI cycle isn't new\u2014it's long been used in traditional ML. But with generative agents, the game has changed. We've moved from predictable single-token outputs to freeform systems with far greater complexity.</p> <p>If you approach evaluation as a simple side-by-side comparison or a table of results, you'll miss the deeper nuances in your data. True progress requires going beyond surface-level metrics to uncover root causes and meaningful insights.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                             \u2502\n\u2502    ANALYZE \u2500\u2500\u2500\u2500\u2500\u2500\u25ba MEASURE \u2500\u2500\u2500\u2500\u2500\u2500\u25ba IMPROVE \u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502        \u25b2                                          \u2502         \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Most AI agent problems aren't solved with a single tweak\u2014they're solved with a disciplined, repeatable process. The AMI lifecycle replaces one-off troubleshooting with a data-driven, continuous improvement loop that:</p> <ul> <li>Cuts failures</li> <li>Speeds time to value</li> <li>Manages risk</li> <li>Drives measurable gains</li> </ul>"},{"location":"agent_playbook/#phase-1-analyze","title":"Phase 1: ANALYZE","text":"<p>Outcome: A clear map of what's broken and why.</p> <ol> <li>Collect representative agent interactions from real-world or test environments</li> <li>Spot and categorize failure patterns (e.g., misunderstood prompts, inconsistent answers)</li> <li>Trace root causes: Specification Issues (prompt ambiguity) vs. Generalization Issues (context mismatch)</li> </ol>"},{"location":"agent_playbook/#phase-2-measure","title":"Phase 2: MEASURE","text":"<p>Outcome: Solid baselines and clear priorities grounded in data.</p> <ol> <li>Build evaluators targeted at each failure type</li> <li>Quantify how often and how badly issues occur across scenarios</li> <li>Prioritize fixes by business impact, surfaced in dashboards everyone can trust</li> </ol>"},{"location":"agent_playbook/#phase-3-improve","title":"Phase 3: IMPROVE","text":"<p>Outcome: Fixes that address root causes\u2014not just symptoms.</p> <ol> <li>Tighten prompts, definitions, and examples to fix Specification Issues</li> <li>Enrich data and refine model/prompt pipelines for Generalization Issues</li> <li>Roll out changes in controlled environments and validate improvements with metrics</li> </ol> <p>And do it again, and again, and again. Once you get the handle of this process, reach for frameworks that can help automate some of this work, but never stop looking at your data!</p>"},{"location":"agent_playbook/#dataset-best-practices","title":"Dataset Best Practices","text":"<p>A strong evaluation starts with the dataset. Your dataset should be grounded in your agent's errors.</p>"},{"location":"agent_playbook/#diversity-matters","title":"Diversity Matters","text":"<ul> <li>Comprehensive Testing: Covers a wide range of situations so the AI is evaluated fairly</li> <li>Realistic Interactions: Reflects actual user behavior for relevant evaluations</li> <li>Weakness Discovery: Surfaces areas where the AI struggles or produces errors</li> </ul>"},{"location":"agent_playbook/#how-many-examples-do-you-need","title":"How Many Examples Do You Need?","text":"<p>Start with ~30 examples. Continue adding until no new failure modes appear. Stop when additional examples stop revealing new insights.</p>"},{"location":"agent_playbook/#coverage-dimensions","title":"Coverage Dimensions","text":"<p>Your golden dataset must be a representative sample of the overall population of queries your end users actually ask:</p> Dimension What to Cover Intent Coverage All major intents + common \"out-of-scope\" questions Utterance Variation Formal, casual, slang, misspellings, abbreviations, verbose Complexity Multi-turn dialogues, corrections, partial information Sentiment &amp; Tone Frustrated, urgent, confused, polite interactions Edge Cases Rare but valid queries, known failure modes"},{"location":"agent_playbook/#dataset-lifecycle","title":"Dataset Lifecycle","text":"Phase Focus Formation Curate high-quality real-world utterances with expert validation Maintenance Continuous review cycles with clear governance Expansion Controlled generation and targeted mining of edge cases"},{"location":"agent_playbook/#defining-evaluation-criteria","title":"Defining Evaluation Criteria","text":"<p>Binary pass/fail judgments are important, but often not enough. Each judgment must be paired with a detailed critique.</p>"},{"location":"agent_playbook/#why-critiques-matter","title":"Why Critiques Matter","text":"<ul> <li>Capture Nuances: Note if something was mostly correct but had areas for improvement</li> <li>Guide Improvement: Provide specific insights into how the AI can be enhanced</li> <li>Balance Simplicity with Depth: Pass/fail offers a clear verdict; critique offers reasoning</li> </ul>"},{"location":"agent_playbook/#example-help-agent-evaluation","title":"Example: Help Agent Evaluation","text":"Interaction Judgment Critique User asks how to create a custom field. Agent provides accurate step-by-step navigation. Pass Technically correct and actionable. However, missed opportunities to check permissions first or ask about field type requirements. Met primary need\u2014passes with enhancement opportunities noted. User asks about lead scoring. Agent says \"Contact your administrator.\" Fail Made assumptions without checking org capabilities. Failed to explore alternatives or explain what lead scoring entails. Dismissive rather than helpful. Due to poor investigation and lack of solutions\u2014fails. <p>The critique should be detailed enough to use in a few-shot prompt for an LLM judge.</p>"},{"location":"agent_playbook/#error-classification","title":"Error Classification","text":"<p>Once you know where the errors are, perform an error analysis to identify root causes. The most effective approach is manually classifying examples using a spreadsheet.</p>"},{"location":"agent_playbook/#example-classification","title":"Example Classification","text":"Root Cause Count Percentage Insufficient Workflow Guidance 12 40% Ignoring User Context 8 27% Generic Documentation Links 6 20% Missing Permission Checks 4 13% <p>Now you know where to focus your efforts. This classification helps identify whether the problem lies in:</p> <ul> <li>Prompt engineering</li> <li>Knowledge retrieval</li> <li>Contextual reasoning</li> </ul>"},{"location":"agent_playbook/#metrics-philosophy","title":"Metrics Philosophy","text":""},{"location":"agent_playbook/#metrics-without-actionable-meaning-are-worthless","title":"Metrics Without Actionable Meaning are Worthless","text":"<p>Observability and evaluation are not the same thing. Observability tells you what happened; evaluation is about making agents better\u2014an iterative loop of analyzing, measuring, and improving.</p>"},{"location":"agent_playbook/#why-likert-scales-fail","title":"Why Likert Scales Fail","text":"Problem Explanation Numbers Aren't Actionable A \"3\" doesn't tell you how to improve Subjectivity Evaluators disagree on what qualifies as \"minor inaccuracies\" vs \"some factual errors\" Metrics Rarely Reflect Expert Judgment Domain experts make binary calls\u2014approve vs. reject"},{"location":"agent_playbook/#the-better-approach","title":"The Better Approach","text":"<p>Binary outcomes paired with clear critiques surface what actually matters and provide a direct path for improvement.</p> <p>Rule of Thumb: If you can't clearly explain what a score of 0.8 means\u2014or what action it implies\u2014it's not a useful metric.</p>"},{"location":"agent_playbook/#metric-selection-guide","title":"Metric Selection Guide","text":"<p>The most common failure mode is \"Metric Spaghetti\"\u2014throwing every metric at every test case. This burns tokens, increases latency, and creates noise.</p>"},{"location":"agent_playbook/#selection-by-dimension","title":"Selection by Dimension","text":""},{"location":"agent_playbook/#retrieval-context-did-we-find-the-right-data","title":"Retrieval &amp; Context (Did we find the right data?)","text":"Metric Type Use When <code>HitRateAtK</code> Math \"Did the right doc appear in the top 5?\" (Binary) <code>MeanReciprocalRank</code> Math You care about position <code>ContextualRelevancy</code> LLM No ground truth ranking\u2014ask \"Is this chunk garbage?\" <code>ContextualSufficiency</code> LLM Check if retrieved chunks actually contain the answer"},{"location":"agent_playbook/#response-quality-is-the-answer-correct","title":"Response Quality (Is the answer correct?)","text":"Metric Type Use When <code>FactualAccuracy</code> LLM You have a Golden Answer <code>Faithfulness</code> LLM RAG essential\u2014checks hallucination <code>AnswerCompleteness</code> LLM User asked 3 sub-questions\u2014did agent answer all 3? <code>AnswerCriteria</code> LLM You have business rules to verify"},{"location":"agent_playbook/#agent-behavior-is-the-interaction-working","title":"Agent Behavior (Is the interaction working?)","text":"Metric Type Use When <code>AnswerRelevancy</code> LLM \"Evasion Detector\"\u2014penalizes off-topic answers <code>ConversationEfficiency</code> LLM Penalizes loops (agent asks for same info twice) <code>GoalCompletion</code> LLM Did user achieve what they came for?"},{"location":"agent_playbook/#style-trust","title":"Style &amp; Trust","text":"Metric Type Use When <code>ToneStyleConsistency</code> LLM Does it match writing style of reference? <code>CitationPresence</code> Heuristic Compliance\u2014checks if citations exist <code>CitationRelevancy</code> LLM Do citations actually support the claim?"},{"location":"agent_playbook/#llm-as-a-judge-best-practices","title":"LLM-as-a-Judge Best Practices","text":""},{"location":"agent_playbook/#the-anti-pattern","title":"The Anti-Pattern","text":"<pre><code>You are an evaluation expert. Judge the correctness based on:\n1. Accuracy &amp; Factual Correctness\n2. Relevance &amp; Completeness\n3. Clarity &amp; Coherence\n\nGive me a score between 1 and 10 and a reason.\n</code></pre> <p>Why this fails:</p> <ul> <li>What's the difference between a 7 and an 8?</li> <li>Five dimensions collapsed into one number</li> <li>Score before reasoning (LLMs reason token-by-token)</li> <li>No examples to ground the model</li> </ul>"},{"location":"agent_playbook/#the-better-pattern","title":"The Better Pattern","text":"<p>Separation of Concerns: Split into two steps.</p> <ol> <li>Extraction: Parse what was said (no judgment yet)</li> <li>Verification: Boolean logic\u2014\"Does X exist in Y?\"</li> </ol> <p>Atomic Statements: Break compound sentences into individual facts.</p> <pre><code># Instead of scoring \"The product has feature A and feature B\"\nstatements = [\n    \"The product has feature A.\",      # Judge: 1 (True)\n    \"The product has feature B.\"       # Judge: 0 (False)\n]\n# Result: 50% accuracy (mathematical, not vibes)\n</code></pre> <p>Chain of Thought: Force analysis before the score.</p> <p>Few-Shot Examples: Always provide examples of good and bad responses with correct scores.</p>"},{"location":"agent_playbook/#judge-model-selection","title":"Judge Model Selection","text":"<p>The teacher must be smarter than the student.</p> <p>You cannot evaluate a GPT-4o agent using a GPT-3.5-Turbo judge. The judge must have reasoning capabilities equal to or greater than the agent being tested.</p> <p>Key principle: \"Cheap on generation, expensive on evaluation.\"</p>"},{"location":"agent_playbook/#summary","title":"Summary","text":"<p>The goal is not to automate evaluation away\u2014it's to build resilient, domain-aware evaluation systems that measure what truly matters and drive meaningful, measurable improvement.</p>"},{"location":"deep-dives/internals/environment/","title":"Axion Environment &amp; Settings","text":"<p>This guide details the configuration management system for Axion, powered by pydantic-settings.</p>"},{"location":"deep-dives/internals/environment/#core-concepts","title":"Core Concepts","text":"<p>The configuration system is built on several key principles:</p> <p>Schema-First Design: The <code>AxionConfig</code> class serves as the central schema. It's a Pydantic <code>BaseModel</code> that defines the shape of the configuration\u2014all available fields, their types, and default values.</p> <p>Environment Variable Loading: Settings are automatically loaded from environment variables and <code>.env</code> files using pydantic-settings.</p> <p>Centralized Access: A single, global <code>settings</code> object is created at startup. This object is the source of truth for all configuration values throughout the application.</p>"},{"location":"deep-dives/internals/environment/#configuration-loading","title":"Configuration Loading","text":"<p>The system loads settings with a clear order of precedence:</p> <ol> <li>Environment Variables: System environment variables (highest priority)</li> <li>.env File: Values loaded from the <code>.env</code> file</li> <li>Default Values: Default values defined in the <code>AxionConfig</code> schema (lowest priority)</li> </ol>"},{"location":"deep-dives/internals/environment/#env-file-discovery","title":".env File Discovery","text":"<p>The system automatically discovers your <code>.env</code> file:</p> <ol> <li>First checks for an explicit <code>ENV_PATH</code> environment variable</li> <li>Falls back to <code>find_dotenv()</code>, which searches the current and parent directories</li> </ol>"},{"location":"deep-dives/internals/environment/#configuration-schema","title":"Configuration Schema","text":"<p>The following settings are available. Environment variables are case-insensitive.</p>"},{"location":"deep-dives/internals/environment/#general-settings","title":"General Settings","text":"Setting Environment Variable Default Description <code>debug</code> <code>DEBUG</code> <code>False</code> Enable debug mode for verbose logging and diagnostics. <code>port</code> <code>PORT</code> <code>8000</code> The port the application will run on. <code>hosts</code> <code>HOSTS</code> <code>['localhost']</code> A list of allowed hostnames."},{"location":"deep-dives/internals/environment/#llm-settings","title":"LLM Settings","text":"Setting Environment Variable Default Description <code>llm_provider</code> <code>LLM_PROVIDER</code> <code>'openai'</code> Default provider for language models. <code>embedding_provider</code> <code>EMBEDDING_PROVIDER</code> <code>'openai'</code> Default provider for embedding models. <code>llm_model_name</code> <code>LLM_MODEL_NAME</code> <code>'gpt-4o'</code> Default language model name. <code>embedding_model_name</code> <code>EMBEDDING_MODEL_NAME</code> <code>'text-embedding-ada-002'</code> Default embedding model name. <code>api_base_url</code> <code>API_BASE_URL</code> <code>None</code> Optional base URL for OpenAI-compatible APIs. <code>litellm_verbose</code> <code>LITELLM_VERBOSE</code> <code>False</code> Enable verbose logging for LiteLLM debugging."},{"location":"deep-dives/internals/environment/#api-key-settings","title":"API Key Settings","text":"Setting Environment Variable Description <code>openai_api_key</code> <code>OPENAI_API_KEY</code> API key for OpenAI models. <code>anthropic_api_key</code> <code>ANTHROPIC_API_KEY</code> API key for Anthropic Claude models. <code>google_api_key</code> <code>GOOGLE_API_KEY</code> API key for Google Gemini models."},{"location":"deep-dives/internals/environment/#google-vertex-ai-settings","title":"Google Vertex AI Settings","text":"Setting Environment Variable Description <code>vertex_project</code> <code>VERTEXAI_PROJECT</code> GCP project ID for Vertex AI. <code>vertex_location</code> <code>VERTEXAI_LOCATION</code> GCP region for Vertex AI (e.g., <code>us-central1</code>). <code>vertex_credentials</code> <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to GCP service account JSON file."},{"location":"deep-dives/internals/environment/#web-search-api-keys","title":"Web Search API Keys","text":"Setting Environment Variable Description <code>serpapi_key</code> <code>SERPAPI_KEY</code> API key for SerpAPI. <code>ydc_api_key</code> <code>YDC_API_KEY</code> API key for You.com Search API. <code>tavily_api_key</code> <code>TAVILY_API_KEY</code> API key for Tavily Search API."},{"location":"deep-dives/internals/environment/#knowledge-settings","title":"Knowledge Settings","text":"Setting Environment Variable Description <code>llama_parse_api_key</code> <code>LLAMA_PARSE_API_KEY</code> API key for LlamaParse. <code>google_credentials_path</code> <code>GOOGLE_CREDENTIALS_PATH</code> Path to Google Credentials JSON file."},{"location":"deep-dives/internals/environment/#logging-settings","title":"Logging Settings","text":"Setting Environment Variable Default Description <code>log_level</code> <code>LOG_LEVEL</code> <code>'INFO'</code> The minimum logging level (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). <code>log_use_rich</code> <code>LOG_USE_RICH</code> <code>True</code> Use rich for beautiful console output. <code>log_format_string</code> <code>LOG_FORMAT_STRING</code> <code>None</code> A custom format string for the console logger. <code>log_file_path</code> <code>LOG_FILE_PATH</code> <code>None</code> If set, logs will also be written to this file."},{"location":"deep-dives/internals/environment/#tracing-settings","title":"Tracing Settings","text":"Setting Environment Variable Default Description <code>tracing_mode</code> <code>TRACING_MODE</code> <code>'noop'</code> Controls tracing provider (see below). <p>Available Tracing Modes:</p> Mode Description <code>noop</code> Disabled (zero overhead). Default. <code>logfire</code> Pydantic Logfire for OpenTelemetry-based observability. <code>otel</code> Generic OpenTelemetry exporter. <code>langfuse</code> Langfuse LLM observability with cost tracking. <code>opik</code> Comet Opik LLM observability."},{"location":"deep-dives/internals/environment/#logfire-settings","title":"Logfire Settings","text":"Setting Environment Variable Default Description <code>logfire_token</code> <code>LOGFIRE_TOKEN</code> <code>None</code> API token for Logfire hosted mode. <code>logfire_service_name</code> <code>LOGFIRE_SERVICE_NAME</code> <code>'axion'</code> The service name that appears in Logfire. <code>logfire_project_name</code> <code>LOGFIRE_PROJECT</code> <code>None</code> Optional project name for Logfire. <code>logfire_distributed_tracing</code> <code>DISTRIBUTED_TRACING</code> <code>True</code> Toggles distributed tracing. <code>logfire_console_logging</code> <code>CONSOLE_LOGGING</code> <code>False</code> Toggles Logfire's console logging. <code>otel_endpoint</code> <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> <code>None</code> Custom OpenTelemetry endpoint."},{"location":"deep-dives/internals/environment/#langfuse-settings","title":"Langfuse Settings","text":"Setting Environment Variable Default Description <code>langfuse_public_key</code> <code>LANGFUSE_PUBLIC_KEY</code> <code>None</code> Langfuse public key for authentication. <code>langfuse_secret_key</code> <code>LANGFUSE_SECRET_KEY</code> <code>None</code> Langfuse secret key for authentication. <code>langfuse_base_url</code> <code>LANGFUSE_BASE_URL</code> <code>'https://cloud.langfuse.com'</code> Langfuse API endpoint."},{"location":"deep-dives/internals/environment/#opik-settings","title":"Opik Settings","text":"Setting Environment Variable Default Description <code>opik_api_key</code> <code>OPIK_API_KEY</code> <code>None</code> Opik API key for authentication. <code>opik_workspace</code> <code>OPIK_WORKSPACE</code> <code>None</code> Opik workspace name. <code>opik_project_name</code> <code>OPIK_PROJECT_NAME</code> <code>'axion'</code> Opik project name for grouping traces. <code>opik_base_url</code> <code>OPIK_URL_OVERRIDE</code> <code>'https://www.comet.com/opik/api'</code> Opik API endpoint."},{"location":"deep-dives/internals/environment/#usage-in-code","title":"Usage in Code","text":""},{"location":"deep-dives/internals/environment/#accessing-settings","title":"Accessing Settings","text":"<p>Import the global <code>settings</code> object to access configuration values:</p> <pre><code>from axion._core.environment import settings\n\ndef some_function():\n    # Access settings directly\n    if settings.debug:\n        print(\"Debug mode is enabled.\")\n\n    print(f\"Using LLM Provider: {settings.llm_provider}\")\n    print(f\"Default Model: {settings.llm_model_name}\")\n</code></pre>"},{"location":"deep-dives/internals/environment/#resolving-api-keys","title":"Resolving API Keys","text":"<p>Use <code>resolve_api_key()</code> to get API keys with proper fallback:</p> <pre><code>from axion._core.environment import resolve_api_key\n\n# Prioritizes direct argument, falls back to settings\napi_key = resolve_api_key(\n    api_key=None,  # or pass explicit key\n    key_name='tavily_api_key',\n    service_name='Tavily'\n)\n</code></pre>"},{"location":"deep-dives/internals/environment/#auto-detecting-tracing-provider","title":"Auto-Detecting Tracing Provider","text":"<p>The system can auto-detect the tracing provider from environment variables:</p> <pre><code>from axion._core.environment import detect_tracing_provider, list_tracing_providers\n\n# Auto-detect based on which API keys are set\nprovider = detect_tracing_provider()\nprint(f\"Detected provider: {provider}\")\n\n# List all available providers\nproviders = list_tracing_providers()\n# ['noop', 'logfire', 'otel', 'langfuse', 'opik']\n</code></pre> <p>Auto-detection priority:</p> <ol> <li>Explicit <code>TRACING_MODE</code> environment variable</li> <li><code>LANGFUSE_SECRET_KEY</code> set \u2192 <code>'langfuse'</code></li> <li><code>OPIK_API_KEY</code> set \u2192 <code>'opik'</code></li> <li><code>LOGFIRE_TOKEN</code> set \u2192 <code>'logfire'</code></li> <li><code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> set \u2192 <code>'otel'</code></li> <li>Default \u2192 <code>'noop'</code></li> </ol>"},{"location":"deep-dives/internals/environment/#environment-variable-examples","title":"Environment Variable Examples","text":""},{"location":"deep-dives/internals/environment/#basic-configuration","title":"Basic Configuration","text":"<pre><code># .env file\nDEBUG=false\nLOG_LEVEL=INFO\nLLM_PROVIDER=openai\nLLM_MODEL_NAME=gpt-4o\nOPENAI_API_KEY=sk-your-api-key\n</code></pre>"},{"location":"deep-dives/internals/environment/#with-tracing-langfuse","title":"With Tracing (Langfuse)","text":"<pre><code># .env file\nOPENAI_API_KEY=sk-your-api-key\nLANGFUSE_PUBLIC_KEY=pk-lf-xxx\nLANGFUSE_SECRET_KEY=sk-lf-xxx\n# TRACING_MODE is auto-detected from LANGFUSE_SECRET_KEY\n</code></pre>"},{"location":"deep-dives/internals/environment/#with-tracing-logfire","title":"With Tracing (Logfire)","text":"<pre><code># .env file\nOPENAI_API_KEY=sk-your-api-key\nLOGFIRE_TOKEN=your-logfire-token\nLOGFIRE_SERVICE_NAME=my-app\n# TRACING_MODE is auto-detected from LOGFIRE_TOKEN\n</code></pre>"},{"location":"deep-dives/internals/environment/#multiple-llm-providers","title":"Multiple LLM Providers","text":"<pre><code># .env file\nOPENAI_API_KEY=sk-your-openai-key\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\nGOOGLE_API_KEY=your-google-key\n\n# Vertex AI\nVERTEXAI_PROJECT=my-gcp-project\nVERTEXAI_LOCATION=us-central1\nGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"deep-dives/internals/environment/#with-search-apis","title":"With Search APIs","text":"<pre><code># .env file\nTAVILY_API_KEY=tvly-xxx\nSERPAPI_KEY=xxx\nYDC_API_KEY=xxx\n</code></pre>"},{"location":"deep-dives/internals/environment/#user-extension-namespace","title":"User Extension Namespace","text":"<p>The settings object includes an <code>ext</code> dictionary for custom user settings:</p> <pre><code>from axion._core.environment import settings\n\n# Access custom settings\ncustom_value = settings.ext.get('my_custom_setting', 'default')\n</code></pre>"},{"location":"deep-dives/internals/logging/","title":"Axion Logging","text":"<p>The Axion logging module provides a rich, informative logging experience with beautiful console output powered by the rich library.</p>"},{"location":"deep-dives/internals/logging/#core-features","title":"Core Features","text":"<ul> <li>Beautiful Output: Colorized log levels, emojis, and formatted tables and tracebacks via the <code>rich</code> library</li> <li>High-Level Methods: Convenient methods like <code>logger.success()</code>, <code>logger.log_table()</code>, and the <code>logger.log_operation()</code> context manager</li> <li>Auto-Configuration: Just use <code>get_logger()</code> - it auto-configures from environment variables on first use</li> <li>File Logging: Optionally write logs to a file in addition to the console</li> </ul>"},{"location":"deep-dives/internals/logging/#basic-usage","title":"Basic Usage","text":"<p>For most use cases, simply import the global <code>logger</code> instance or get a module-specific logger:</p> <pre><code># Option 1: Use the global logger\nfrom axion.logging import logger\n\nlogger.info(\"Hello from Axion!\")\nlogger.success(\"Task completed successfully!\")\n\n# Option 2: Get a module-specific logger (recommended)\nfrom axion.logging import get_logger\n\nlogger = get_logger(__name__)\nlogger.warning(\"This warning came from the '%s' module.\", __name__)\n</code></pre>"},{"location":"deep-dives/internals/logging/#configuration","title":"Configuration","text":"<p>Configuration is handled via environment variables or by calling <code>configure_logging()</code> directly.</p>"},{"location":"deep-dives/internals/logging/#environment-variables","title":"Environment Variables","text":"Setting Environment Variable Default Description <code>log_level</code> <code>LOG_LEVEL</code> <code>'INFO'</code> Minimum logging level (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). <code>log_use_rich</code> <code>LOG_USE_RICH</code> <code>True</code> Use rich for beautiful console output. <code>log_format_string</code> <code>LOG_FORMAT_STRING</code> <code>None</code> A custom format string for the logger. <code>log_file_path</code> <code>LOG_FILE_PATH</code> <code>None</code> If set, logs will be written to this file."},{"location":"deep-dives/internals/logging/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Call <code>configure_logging()</code> once at application startup, or let it auto-configure on first use.</p>"},{"location":"deep-dives/internals/logging/#example-1-zero-config-recommended","title":"Example 1: Zero-Config (Recommended)","text":"<p>Just use <code>get_logger()</code> - it auto-configures from environment variables on first use.</p> <pre><code>from axion.logging import get_logger\n\n# Auto-configures from LOG_LEVEL, LOG_USE_RICH, etc.\nlogger = get_logger(__name__)\nlogger.info(\"Using the configuration from the environment.\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#example-2-explicit-configuration","title":"Example 2: Explicit Configuration","text":"<p>Call <code>configure_logging()</code> before getting loggers to override defaults.</p> <pre><code>from axion.logging import configure_logging, get_logger\n\n# Configure first, then get logger\nconfigure_logging(level=\"DEBUG\", use_rich=True)\nlogger = get_logger(__name__)\n\nlogger.debug(\"This debug message is now visible.\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#example-3-reconfiguration","title":"Example 3: Reconfiguration","text":"<p>Call <code>configure_logging()</code> with new parameters - it applies immediately.</p> <pre><code>from axion.logging import configure_logging, get_logger\n\nlogger = get_logger(__name__)  # Auto-configures to INFO\n\n# Change level on the fly\nconfigure_logging(level=\"DEBUG\")  # Now DEBUG\nconfigure_logging(level=\"ERROR\")  # Now ERROR\n</code></pre> <p>Note: Calling <code>configure_logging()</code> with explicit parameters always applies them. Calling with no parameters skips if already configured.</p>"},{"location":"deep-dives/internals/logging/#richlogger-methods","title":"RichLogger Methods","text":"<p>The <code>RichLogger</code> class provides several high-level methods to make your logs more expressive.</p> Method Description <code>logger.success(msg)</code> Logs an info-level message with a checkmark emoji. <code>logger.warning_highlight(msg)</code> Logs a warning-level message with a warning emoji. <code>logger.error_highlight(msg)</code> Logs an error-level message with a cross emoji. <code>logger.log_table(data)</code> Prints a list of dictionaries as a formatted table. <code>logger.log_json(data)</code> Pretty-prints a dictionary or list as JSON. <code>logger.log_performance(operation, duration)</code> Logs performance metrics for an operation. <code>logger.log_exception(e)</code> Logs an exception with traceback. <code>logger.log_operation(name)</code> Context manager that logs start, end, and duration of a code block. <code>logger.async_log_operation(name)</code> Async context manager for logging operations."},{"location":"deep-dives/internals/logging/#examples","title":"Examples","text":""},{"location":"deep-dives/internals/logging/#logging-tables","title":"Logging Tables","text":"<pre><code>from axion.logging import logger\n\nusers = [\n    {\"id\": \"user_1\", \"status\": \"active\", \"last_login\": \"2025-08-02\"},\n    {\"id\": \"user_2\", \"status\": \"inactive\", \"last_login\": \"2025-07-15\"},\n]\nlogger.log_table(users, title=\"User Status\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#timing-operations","title":"Timing Operations","text":"<pre><code>from axion.logging import logger\nimport time\n\nwith logger.log_operation(\"Process User Data\"):\n    # Your code here...\n    time.sleep(0.5)\n\n# Output:\n# \ud83d\udd39 Starting | Process User Data\n# \u2705 Completed | Process User Data in 0.50s\n</code></pre>"},{"location":"deep-dives/internals/logging/#async-operations","title":"Async Operations","text":"<pre><code>from axion.logging import logger\n\nasync def fetch_data():\n    async with logger.async_log_operation(\"Fetch Remote Data\"):\n        # Your async code here...\n        await some_async_operation()\n</code></pre>"},{"location":"deep-dives/internals/logging/#logging-json","title":"Logging JSON","text":"<pre><code>from axion.logging import logger\n\nconfig = {\"model\": \"gpt-4o\", \"temperature\": 0.7}\nlogger.log_json(config, title=\"Model Configuration\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#advanced-usage","title":"Advanced Usage","text":""},{"location":"deep-dives/internals/logging/#module-specific-loggers","title":"Module-Specific Loggers","text":"<p>For better organization and debugging, create module-specific loggers:</p> <pre><code>from axion.logging import get_logger\n\n# Each module gets its own logger\nlogger = get_logger(__name__)\n\nclass MyService:\n    def __init__(self):\n        self.logger = get_logger(f\"{__name__}.{self.__class__.__name__}\")\n\n    def process_data(self):\n        self.logger.info(\"Starting data processing...\")\n        # Processing logic here\n        self.logger.success(\"Data processing completed!\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#error-handling","title":"Error Handling","text":"<pre><code>from axion.logging import logger\n\ntry:\n    risky_operation()\nexcept Exception as e:\n    logger.error_highlight(f\"Operation failed: {e}\")\n    # Rich automatically formats the traceback beautifully\n    logger.exception(\"Full traceback:\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#conditional-logging","title":"Conditional Logging","text":"<pre><code>import logging\nfrom axion.logging import logger\n\ndef debug_intensive_operation(data):\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.debug(f\"Processing {len(data)} items\")\n        logger.log_json(data[:5])  # Log first 5 items as JSON\n</code></pre>"},{"location":"deep-dives/internals/logging/#logging-summary","title":"Logging Summary","text":"<p>Get a summary of all logging activity during the session:</p> <pre><code>from axion.logging import log_summary\n\n# At the end of your application\nlog_summary()\n\n# Output:\n# --- Logging Summary ---\n# Total Session Runtime: 45.32 seconds\n# Logger 'axion': 25 messages\n#     - INFO: 20\n#     - WARNING: 3\n#     - ERROR: 2\n# Grand Total Messages: 25\n# -----------------------\n</code></pre>"},{"location":"deep-dives/internals/logging/#configuration-state-management","title":"Configuration State Management","text":"<pre><code>from axion.logging import (\n    configure_logging,\n    is_logging_configured,\n    clear_logging_config\n)\n\n# Check if logging is configured\nif is_logging_configured():\n    print(\"Logging already set up\")\n\n# Full reset if needed\nclear_logging_config()\nconfigure_logging(level=\"DEBUG\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Module-Specific Loggers: Always use <code>get_logger(__name__)</code> for better log organization.</p> </li> <li> <p>Let It Auto-Configure: Just use <code>get_logger()</code> - it configures automatically from environment variables.</p> </li> <li> <p>Configure Before Use: If you need custom settings, call <code>configure_logging()</code> before <code>get_logger()</code>.</p> </li> <li> <p>Use Context Managers: Leverage <code>logger.log_operation()</code> for timing operations.</p> </li> <li> <p>Rich Methods in Development: Use <code>logger.success()</code>, <code>logger.log_table()</code> etc. for better development experience.</p> </li> </ol> <pre><code># Good - Zero config, auto-configures from env vars\nfrom axion.logging import get_logger\nlogger = get_logger(__name__)\n\n# Good - Configure before getting logger\nfrom axion.logging import configure_logging, get_logger\nconfigure_logging(level=\"DEBUG\")\nlogger = get_logger(__name__)\n\n# Good - Using context manager\nwith logger.log_operation(\"Data Migration\"):\n    migrate_users()\n    migrate_products()\n\n# Good - Reconfigure on the fly\nfrom axion.logging import configure_logging\nconfigure_logging(level=\"DEBUG\")  # Always applies with explicit params\n\n# Good - Check configuration state\nfrom axion.logging import is_logging_configured, clear_logging_config\nif is_logging_configured():\n    clear_logging_config()  # Full reset if needed\n</code></pre>"},{"location":"deep-dives/internals/logging/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/internals/logging/#functions","title":"Functions","text":"Function Description <code>get_logger(name)</code> Get a logger instance, auto-configuring on first use. <code>configure_logging(level, use_rich, format_string, file_path)</code> Configure the logging system. <code>is_logging_configured()</code> Check if logging has been configured. <code>clear_logging_config()</code> Clear the logging configuration and reset state. <code>log_summary()</code> Log a summary of all logging activity during the session."},{"location":"deep-dives/internals/logging/#classes","title":"Classes","text":"Class Description <code>RichLogger</code> Custom logger class with enhanced logging methods. <code>LoguruHandler</code> Bridge handler for forwarding logs to loguru (if available)."},{"location":"deep-dives/internals/tracing/","title":"Axion Tracing System","text":"<p>Simple observability for AI applications with automatic context management. Supports multiple backends including Logfire (OpenTelemetry), Langfuse, and Opik (Comet) for LLM-specific observability.</p>"},{"location":"deep-dives/internals/tracing/#why-use-axion-tracing","title":"Why Use Axion Tracing?","text":"<ul> <li>Zero setup - Configure once, trace everywhere</li> <li>Automatic context - No manual tracer passing between functions</li> <li>AI-optimized - Built-in support for LLM, evaluation, and knowledge operations</li> <li>Production ready - NOOP mode for zero overhead when tracing is disabled</li> <li>Extensible - Registry pattern makes it easy to add custom tracer providers</li> <li>Multiple backends - Choose between Logfire, Langfuse, or create your own</li> </ul>"},{"location":"deep-dives/internals/tracing/#quick-start","title":"Quick Start","text":"<pre><code>from axion.tracing import init_tracer, trace\n\nclass MyService:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    @trace(name='internal_span', capture_result=True)\n    async def process(self, data: dict):\n        return 100\n\n    @trace(name='span', capture_result=True)\n    async def run(self):\n        # Set manual span for tracing\n        async with self.tracer.async_span(\"manual_span\") as span:\n            # set attribute on span\n            span.set_attribute(\"output_status\", \"success\")\n            return await self.process({\"key\": \"value\"})\n\nawait MyService().run()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#tracing-providers","title":"Tracing Providers","text":"<p>Axion supports four built-in tracing providers, all managed through a unified registry system:</p> Provider Description Use Case <code>noop</code> No-operation tracer with zero overhead Testing, production without tracing <code>logfire</code> OpenTelemetry-based tracing via Logfire General observability, performance monitoring <code>langfuse</code> LLM-specific observability platform LLM cost tracking, prompt management, evaluations <code>opik</code> Comet's open-source LLM observability LLM tracing, cost tracking, evaluations"},{"location":"deep-dives/internals/tracing/#provider-comparison","title":"Provider Comparison","text":"<pre><code>graph TD\n    A[TracerRegistry] --&gt; B[NoOpTracer]\n    A --&gt; C[LogfireTracer]\n    A --&gt; D[LangfuseTracer]\n    A --&gt; E[OpikTracer]\n\n    B --&gt; F[Zero Overhead]\n    C --&gt; G[OpenTelemetry Backend]\n    C --&gt; H[Logfire Cloud/Local UI]\n    D --&gt; I[LLM Observability]\n    D --&gt; J[Cost Tracking]\n    D --&gt; K[Prompt Management]\n    E --&gt; L[Open Source LLM Tracing]\n    E --&gt; M[Comet Integration]</code></pre>"},{"location":"deep-dives/internals/tracing/#configuration","title":"Configuration","text":"<p>Tracing auto-configures from environment variables on first use. Just use <code>Tracer()</code> and it works.</p>"},{"location":"deep-dives/internals/tracing/#environment-variables","title":"Environment Variables","text":"<p>Set <code>TRACING_MODE</code> to select a provider, or let it auto-detect from available credentials:</p> Provider Description Auto-Detection <code>noop</code> Disables all tracing (zero overhead) Default if no credentials found <code>logfire</code> OpenTelemetry via Logfire <code>LOGFIRE_TOKEN</code> present <code>otel</code> Custom OpenTelemetry endpoint <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> present <code>langfuse</code> LLM observability via Langfuse <code>LANGFUSE_SECRET_KEY</code> present <code>opik</code> LLM observability via Opik (Comet) <code>OPIK_API_KEY</code> present <p>Auto-Detection Priority: If <code>TRACING_MODE</code> is not set, the system checks for credentials in this order: 1. <code>LANGFUSE_SECRET_KEY</code> \u2192 uses <code>langfuse</code> 2. <code>OPIK_API_KEY</code> \u2192 uses <code>opik</code> 3. <code>LOGFIRE_TOKEN</code> \u2192 uses <code>logfire</code> 4. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> \u2192 uses <code>otel</code> 5. Default \u2192 uses <code>noop</code></p>"},{"location":"deep-dives/internals/tracing/#logfire-configuration","title":"Logfire Configuration","text":"<pre><code># For Logfire cloud (recommended)\nTRACING_MODE=logfire\nLOGFIRE_TOKEN=your-logfire-token\n\n# For custom OpenTelemetry endpoint\nTRACING_MODE=otel\nOTEL_EXPORTER_OTLP_ENDPOINT=https://your-otel-endpoint\n</code></pre>"},{"location":"deep-dives/internals/tracing/#langfuse-configuration","title":"Langfuse Configuration","text":"<pre><code>TRACING_MODE=langfuse\nLANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nLANGFUSE_SECRET_KEY=sk-lf-your-secret-key\nLANGFUSE_BASE_URL=https://cloud.langfuse.com  # EU region (default)\n# or https://us.cloud.langfuse.com for US region\n</code></pre>"},{"location":"deep-dives/internals/tracing/#opik-configuration","title":"Opik Configuration","text":"<pre><code>TRACING_MODE=opik\nOPIK_API_KEY=your-opik-api-key\nOPIK_WORKSPACE=your-workspace-name  # Optional\nOPIK_PROJECT_NAME=axion  # Optional, defaults to 'axion'\nOPIK_URL_OVERRIDE=https://www.comet.com/opik/api  # Default (cloud)\n# or http://localhost:5173/api for self-hosted\n</code></pre>"},{"location":"deep-dives/internals/tracing/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Tracing auto-configures on first use of <code>Tracer()</code>. Only call <code>configure_tracing()</code> if you need to override auto-detection.</p> <pre><code>from axion.tracing import configure_tracing, Tracer\n\n# Zero-config - auto-detects from environment variables\ntracer = Tracer('llm')\n\n# Or explicitly configure a provider\nconfigure_tracing(provider='langfuse')\ntracer = Tracer('llm')\n\n# List available providers\nfrom axion.tracing import list_providers\nprint(list_providers())  # ['noop', 'logfire', 'otel', 'langfuse', 'opik']\n\n# Reconfigure (e.g., for testing)\nfrom axion.tracing import clear_tracing_config, is_tracing_configured\n\nif is_tracing_configured():\n    clear_tracing_config()\nconfigure_tracing(provider='noop')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#tracerregistry-architecture","title":"TracerRegistry Architecture","text":"<p>The tracing system uses a decorator-based registry pattern, similar to <code>LLMRegistry</code>. This makes it easy to:</p> <ul> <li>Switch between providers at runtime</li> <li>Add custom tracer implementations</li> <li>Extend functionality without modifying core code</li> </ul>"},{"location":"deep-dives/internals/tracing/#how-it-works","title":"How It Works","text":"<pre><code>from axion.tracing import TracerRegistry, BaseTracer\n\n# List all registered providers\nproviders = TracerRegistry.list_providers()\nprint(providers)  # ['noop', 'logfire', 'langfuse', 'opik']\n\n# Get a specific tracer class\nTracerClass = TracerRegistry.get('langfuse')\ntracer = TracerClass.create(metadata_type='llm')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#creating-custom-tracers","title":"Creating Custom Tracers","text":"<p>You can create custom tracer implementations by subclassing <code>BaseTracer</code> and registering with the <code>@TracerRegistry.register()</code> decorator:</p> <pre><code>from axion.tracing import TracerRegistry, BaseTracer\nfrom contextlib import contextmanager, asynccontextmanager\n\n@TracerRegistry.register('my_custom_tracer')\nclass MyCustomTracer(BaseTracer):\n    \"\"\"Custom tracer implementation.\"\"\"\n\n    def __init__(self, metadata_type: str = 'default', **kwargs):\n        self.metadata_type = metadata_type\n        # Initialize your tracing backend here\n\n    @classmethod\n    def create(cls, metadata_type: str = 'default', **kwargs):\n        return cls(metadata_type=metadata_type, **kwargs)\n\n    @contextmanager\n    def span(self, operation_name: str, **attributes):\n        # Implement span creation\n        print(f\"Starting span: {operation_name}\")\n        try:\n            yield self\n        finally:\n            print(f\"Ending span: {operation_name}\")\n\n    @asynccontextmanager\n    async def async_span(self, operation_name: str, **attributes):\n        # Implement async span creation\n        print(f\"Starting async span: {operation_name}\")\n        try:\n            yield self\n        finally:\n            print(f\"Ending async span: {operation_name}\")\n\n    def start(self, **attributes):\n        pass\n\n    def complete(self, output_data=None, **attributes):\n        pass\n\n    def fail(self, error: str, **attributes):\n        pass\n\n    def add_trace(self, event_type: str, message: str, metadata=None):\n        pass\n\n# Now you can use it\nconfigure_tracing(provider='my_custom_tracer')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/internals/tracing/#decorator-tracing-recommended","title":"Decorator Tracing (Recommended)","text":"<p>The tracer attribute is required for <code>@trace</code> decorator. The <code>@trace</code> decorator automatically looks for a tracer attribute on the class instance (<code>self</code>) to create and manage spans.</p> <pre><code>from axion.tracing import init_tracer, trace\n\nclass DecoratorService:\n    def __init__(self):\n        self.tracer = init_tracer('base')\n\n    @trace(name=\"process_data\", capture_args=True, capture_result=True)\n    async def process(self, data: dict):\n        await asyncio.sleep(0.1)\n        return {\"status\": \"processed\", \"items\": len(data)}\n\n    @trace  # Simple usage without arguments\n    async def run(self):\n        result = await self.process({\"id\": 123, \"items\": [\"a\", \"b\"]})\n        return result\n\n# Usage\nservice = DecoratorService()\nawait service.run()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#context-aware-function-tracing","title":"Context-Aware Function Tracing","text":"<p>Use <code>init_tracer</code> at the top level of a service or class to start a new trace context. Use <code>get_current_tracer</code> in downstream functions or services that you expect to be called within an existing trace, allowing them to add child spans without needing the tracer to be passed in manually.</p> <pre><code>from axion.tracing import get_current_tracer, init_tracer\n\nclass ServiceA:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    async def process(self):\n        async with self.tracer.async_span(\"service_a_process\"):\n            # Context automatically propagates to ServiceB\n            service_b = ServiceB()\n            await service_b.process()\n\nclass ServiceB:\n    async def process(self):\n        # Get tracer from context - no manual passing needed!\n        tracer = get_current_tracer()\n        async with tracer.async_span(\"service_b_process\"):\n            return \"processed\"\n\n# Usage\nservice = ServiceA()\nawait service.process()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#langfuse-specific-features","title":"Langfuse-Specific Features","text":"<p>When using Langfuse, you get additional LLM-specific features:</p> <pre><code>import os\nos.environ['TRACING_MODE'] = 'langfuse'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\n\nfrom axion.tracing import configure_tracing, Tracer\n\nconfigure_tracing()\ntracer = Tracer('llm')\n\n# Create spans that appear in Langfuse\nwith tracer.span('my-operation') as span:\n    # Your code here\n    span.set_attribute('custom_key', 'custom_value')\n\n# Log LLM calls with token usage\ntracer.log_llm_call(\n    name='chat_completion',\n    model='gpt-4',\n    prompt='Hello, how are you?',\n    response='I am doing well, thank you!',\n    usage={\n        'prompt_tokens': 10,\n        'completion_tokens': 8,\n        'total_tokens': 18\n    }\n)\n\n# Log evaluations as scores\ntracer.log_evaluation(\n    name='relevance_score',\n    score=0.95,\n    comment='Highly relevant response'\n)\n\n# Important: Flush traces before exiting\ntracer.flush()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#opik-specific-features","title":"Opik-Specific Features","text":"<p>Opik (by Comet) provides open-source LLM observability with similar features:</p> <pre><code>import os\nos.environ['TRACING_MODE'] = 'opik'\nos.environ['OPIK_API_KEY'] = 'your-api-key'\nos.environ['OPIK_WORKSPACE'] = 'your-workspace'\n\nfrom axion.tracing import configure_tracing, Tracer\n\nconfigure_tracing()\ntracer = Tracer('llm')\n\n# Create spans that appear in Opik dashboard\nwith tracer.span('my-operation', model='gpt-4') as span:\n    # Your code here\n    span.set_input({'query': 'Hello, how are you?'})\n    span.set_output({'response': 'I am doing well!'})\n    span.set_usage(prompt_tokens=10, completion_tokens=8)\n\n# Log LLM calls with token usage\ntracer.log_llm_call(\n    name='chat_completion',\n    model='gpt-4',\n    provider='openai',\n    prompt='Hello, how are you?',\n    response='I am doing well, thank you!',\n    prompt_tokens=10,\n    completion_tokens=8,\n)\n\n# Important: Flush traces before exiting\ntracer.flush()\n</code></pre> <p>Key Opik features: - Open-source and self-hostable - LLM-specific span types ('llm', 'tool', 'general') - Token usage tracking via <code>usage</code> attribute - Integration with Comet ML platform</p>"},{"location":"deep-dives/internals/tracing/#inputoutput-capture","title":"Input/Output Capture","text":"<p>Spans support <code>set_input()</code> and <code>set_output()</code> methods for capturing data that appears in the Langfuse UI's Input and Output fields:</p> <pre><code>async with tracer.async_span('my-operation') as span:\n    # Capture the input data\n    span.set_input({\n        'query': 'How do I reset my password?',\n        'context': ['doc1', 'doc2'],\n    })\n\n    result = await process_query(query)\n\n    # Capture the output data\n    span.set_output({\n        'response': result.text,\n        'score': result.confidence,\n    })\n</code></pre> <p>The <code>@trace</code> decorator automatically captures input/output when enabled:</p> <pre><code>@trace(name=\"process_data\", capture_args=True, capture_result=True)\nasync def process(self, data: dict):\n    # Input (args/kwargs) automatically captured via span.set_input()\n    result = await do_work(data)\n    # Result automatically captured via span.set_output()\n    return result\n</code></pre> <p>Supported data types for serialization: - Pydantic models (serialized via <code>model_dump()</code>) - Dictionaries, lists, and primitive types - Other objects are converted to string representation</p>"},{"location":"deep-dives/internals/tracing/#custom-span-names","title":"Custom Span Names","text":"<p>Axion components use meaningful span names for better observability:</p> <p>Evaluation Runner: Uses <code>evaluation_name</code> as the trace name in Langfuse/Opik: <pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My RAG Evaluation\"  # This becomes the trace name\n)\n# Appears as \"My RAG Evaluation\" in Langfuse/Opik instead of \"evaluation_runner\"\n</code></pre></p> <p>Trace Granularity: Control how evaluation traces are organized: <pre><code># Single trace (default) - all evaluations under one parent\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Evaluation\",\n    trace_granularity='single_trace'  # or 'single'\n)\n\n# Separate traces - each metric execution gets its own trace\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Evaluation\",\n    trace_granularity='separate'\n)\n</code></pre></p> <p>Evaluation Trace Hierarchy (lean 4-level structure): <pre><code>My RAG Evaluation                             # evaluation_name (root)\n\u251c\u2500 AnswerRelevancy.execute                    # Metric logic\n\u2502  \u2514\u2500 litellm_structured                      # LLM formatting/parsing\n\u2502     \u2514\u2500 llm_call                             # LLM API call (cost/tokens)\n\u2514\u2500 Faithfulness.execute\n   \u2514\u2500 litellm_structured\n      \u2514\u2500 llm_call\n</code></pre></p> <p>LLM Handlers: Use <code>metadata.name</code> or class name as the span name: <pre><code>class SentimentAnalysisHandler(LLMHandler):\n    # ... handler config ...\n    pass\n\nhandler = SentimentAnalysisHandler()\n# Appears as \"SentimentAnalysisHandler\" in Langfuse instead of \"llm_handler\"\n\n# Or set a custom name via metadata\nhandler.metadata.name = \"Sentiment Analysis\"\n# Now appears as \"Sentiment Analysis\" in Langfuse\n</code></pre></p>"},{"location":"deep-dives/internals/tracing/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom axion.tracing import init_tracer, trace\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\nfrom axion._core.metadata.schema import ToolMetadata\n\nclass MetricService:\n    def __init__(self):\n        # Set Context on Span\n        tool_metadata = ToolMetadata(\n            name=\"MetricService\",\n            description='My Service',\n            version='1.0.1',\n        )\n        self.tracer = init_tracer(\n            metadata_type='llm',\n            tool_metadata=tool_metadata,\n        )\n\n    @trace(capture_result=True)\n    async def run_metric(self):\n        # AnswerRelevancy has its own tracer and auto-captured as a child span\n        metric = AnswerRelevancy()\n        data_item = DatasetItem(\n            query=\"How do I reset my password?\",\n            actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n            expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n        )\n        return await metric.execute(data_item)\n\n    @trace(name=\"doing_work\")\n    async def do_some_work(self):\n        await asyncio.sleep(0.5)\n        return \"Work done!\"\n\n    @trace(name='run_main_task')\n    async def run_main_task(self):\n        # Can also set manual spans\n        async with self.tracer.async_span(\"metric_evaluation\") as span:\n            _ = await self.do_some_work()\n            result = await self.run_metric()\n            span.set_attribute(\"operation_status\", \"success\")\n        return result\n\n# Usage\nservice = MetricService()\nresult = await service.run_main_task()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#metadata-types","title":"Metadata Types","text":"<p>Choose the right type for automatic specialized handling:</p> <ul> <li><code>'base'</code> - General operations</li> <li><code>'llm'</code> - Language model calls (captures tokens, model info)</li> <li><code>'knowledge'</code> - Search and retrieval (captures queries, results)</li> <li><code>'database'</code> - Database operations (captures performance)</li> <li><code>'evaluation'</code> - Evaluation metrics (captures scores)</li> </ul>"},{"location":"deep-dives/internals/tracing/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/internals/tracing/#core-functions","title":"Core Functions","text":"Function Description <code>configure_tracing(provider)</code> Configure the tracing provider (auto-configures if not called) <code>is_tracing_configured()</code> Check if tracing has been configured <code>clear_tracing_config()</code> Clear configuration (useful for testing/reconfiguration) <code>list_providers()</code> List available providers: <code>['noop', 'logfire', 'otel', 'langfuse', 'opik']</code> <code>get_tracer()</code> Get the configured tracer class <code>init_tracer(metadata_type, tool_metadata)</code> Initialize a tracer instance <code>Tracer(metadata_type)</code> Factory function for tracer instances (auto-configures)"},{"location":"deep-dives/internals/tracing/#context-management","title":"Context Management","text":"Function Description <code>get_current_tracer()</code> Get active tracer from context <code>set_current_tracer(tracer)</code> Set tracer in context <code>reset_tracer_context(token)</code> Reset tracer context"},{"location":"deep-dives/internals/tracing/#span-methods","title":"Span Methods","text":"Method Description <code>span.set_attribute(key, value)</code> Set a single attribute on the span <code>span.set_attributes(dict)</code> Set multiple attributes at once <code>span.set_input(data)</code> Set input data (appears in Langfuse Input field) <code>span.set_output(data)</code> Set output data (appears in Langfuse Output field) <code>span.add_trace(event_type, message, metadata)</code> Add a trace event to the span"},{"location":"deep-dives/internals/tracing/#registry","title":"Registry","text":"Function Description <code>TracerRegistry.register(name)</code> Decorator to register a tracer class <code>TracerRegistry.get(name)</code> Get a tracer class by name <code>TracerRegistry.list_providers()</code> List all registered providers <code>TracerRegistry.is_registered(name)</code> Check if a provider is registered"},{"location":"deep-dives/internals/tracing/#decorator-options","title":"Decorator Options","text":"Option Description <code>name</code> Custom span name (defaults to function name) <code>capture_args</code> Capture function arguments as span input <code>capture_result</code> Capture function result as span output"},{"location":"deep-dives/internals/tracing/#types","title":"Types","text":"Type Description <code>TracingMode</code> Enumeration of available tracer modes <code>TracerRegistry</code> Registry for tracer implementations <code>BaseTracer</code> Abstract base class for tracer implementations"},{"location":"deep-dives/internals/tracing/#integration","title":"Integration","text":"<p>The tracing system automatically works with other Axion components:</p> <ul> <li>Evaluation metrics are automatically traced</li> <li>API calls include retry and performance data</li> <li>LLM operations capture token usage and model info</li> </ul> <p>Just initialize your tracer and everything else traces automatically.</p>"},{"location":"deep-dives/internals/tracing/#installation","title":"Installation","text":"<p>The tracing providers are optional dependencies. From the project root:</p> <pre><code># Install with Logfire support\npip install -e \".[logfire]\"\n\n# Install with Langfuse support\npip install -e \".[langfuse]\"\n\n# Install with Opik support\npip install -e \".[opik]\"\n\n# Install with all tracing providers\npip install -e \".[tracing]\"\n</code></pre>"},{"location":"deep-dives/metrics/composite-metrics/","title":"Building Composite Evaluation Metrics","text":"<p>Building custom evaluation metrics is easy to do without external libraries. Below is an example of re-creating the \"faithfulness\" metric from RAGAS or DeepEval.</p>"},{"location":"deep-dives/metrics/composite-metrics/#faithfulness-metric","title":"Faithfulness Metric","text":"<p>The Faithfulness metric evaluates how factually consistent a response is with the retrieved context. It is scored between 0 and 1, where higher values indicate greater consistency.</p> <p>A response is considered faithful if every claim it makes is fully supported by the retrieved context.</p>"},{"location":"deep-dives/metrics/composite-metrics/#calculation-steps","title":"Calculation Steps:","text":"<ul> <li>Extract all claims from the response.</li> <li>Verify whether each claim is directly supported by the retrieved context.</li> <li>Compute the faithfulness score using a simple formula.</li> </ul> <p>A higher faithfulness score signifies stronger factual alignment with the retrieved context, ensuring more reliable and trustworthy responses.</p>"},{"location":"deep-dives/metrics/composite-metrics/#code-examples","title":"Code Examples","text":"Faithfulness Example<pre><code>from pydantic import Field\nfrom typing import List, Union, Any\nimport numpy as np\n\nfrom axion.metrics.base import BaseMetric, MetricEvaluationResult\nfrom axion.dataset import DatasetItem\nfrom axion.schema import RichBaseModel\nfrom axion._core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass StatementGeneratorInput(RichBaseModel):\n    question: str = Field(description='The question to answer')\n    answer: str = Field(description='The answer to the question')\n\n\nclass StatementGeneratorOutput(RichBaseModel):\n    statements: List[str] = Field(description='The generated statements')\n\n\nclass StatementGenerator(BaseMetric[StatementGeneratorInput, StatementGeneratorOutput]):\n    instruction = \"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement.\"\n    input_model = StatementGeneratorInput\n    output_model = StatementGeneratorOutput\n    description = 'Statement Generator Prompt'\n    examples = [\n        (\n            StatementGeneratorInput(\n                question='Who was Albert Einstein and what is he best known for?',\n                answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.',\n            ),\n            StatementGeneratorOutput(\n                statements=[\n                    'Albert Einstein was a German-born theoretical physicist.',\n                    'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.',\n                    'Albert Einstein was best known for developing the theory of relativity.',\n                    'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.',\n                ]\n            ),\n        )\n    ]\n\n\nclass StatementFaithfulnessAnswer(RichBaseModel):\n    statement: str = Field(..., description='the original statement, word-by-word')\n    reason: str = Field(..., description='the reason of the verdict')\n    verdict: int = Field(..., description='the verdict(0/1) of the faithfulness.')\n\n\nclass NLIStatementInput(RichBaseModel):\n    retrieved_content: str = Field(\n        ..., description='The retrieved content from the model'\n    )\n    statements: List[str] = Field(..., description='The statements to judge')\n\n\nclass NLIStatementOutput(RichBaseModel):\n    statements: List[StatementFaithfulnessAnswer]\n\n\nclass NLIStatement(BaseMetric[NLIStatementInput, NLIStatementOutput]):\n    instruction = 'Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.'\n    input_model = NLIStatementInput\n    output_model = NLIStatementOutput\n    description = 'NLI Statement Prompt'\n    examples = [\n        (\n            NLIStatementInput(\n                retrieved_content=\"\"\"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\"\"\",\n                statements=[\n                    'John is majoring in Biology.',\n                    'John is taking a course on Artificial Intelligence.',\n                    'John is a dedicated student.',\n                    'John has a part-time job.',\n                ],\n            ),\n            NLIStatementOutput(\n                statements=[\n                    StatementFaithfulnessAnswer(\n                        statement='John is majoring in Biology.',\n                        reason=\"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n                        verdict=0,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John is taking a course on Artificial Intelligence.',\n                        reason='The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.',\n                        verdict=0,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John is a dedicated student.',\n                        reason='The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.',\n                        verdict=1,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John has a part-time job.',\n                        reason='There is no information given in the context about John having a part-time job.',\n                        verdict=0,\n                    ),\n                ]\n            ),\n        ),\n        (\n            NLIStatementInput(\n                retrieved_content='Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.',\n                statements=[\n                    'Albert Einstein was a genius.',\n                ],\n            ),\n            NLIStatementOutput(\n                statements=[\n                    StatementFaithfulnessAnswer(\n                        statement='Albert Einstein was a genius.',\n                        reason='The context and statement are unrelated',\n                        verdict=0,\n                    )\n                ]\n            ),\n        ),\n    ]\n\n\nclass Faithfulness(BaseMetric):\n    \"\"\"\n    Computes faithfulness scores for LLM-generated responses based on retrieved contexts.\n\n    This metric works by:\n    1. Breaking down the LLM response into atomic statements\n    2. Evaluating each statement against the retrieved context\n    3. Computing a faithfulness score as the ratio of supported statements\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.statement_generator = StatementGenerator(**kwargs)\n        self.nli_evaluator = NLIStatement(**kwargs)\n\n    async def _create_statements(self, query: str, output: str) -&gt; List[str]:\n        result = await self.statement_generator.execute(\n            StatementGeneratorInput(question=query, answer=output)\n        )\n        if not result.statements:\n            logger.warning('No statements were generated from the answer.')\n        return result.statements\n\n    async def _evaluate_statements(\n        self, retrieved_content: str, statements: List[str]\n    ) -&gt; NLIStatementOutput:\n        if not statements:\n            logger.warning('No statements provided for evaluation.')\n            return NLIStatementOutput(statements=[])\n        formatted_content = self.convert_retrieved_content_to_string(retrieved_content)\n        return await self.nli_evaluator.execute(\n            NLIStatementInput(\n                retrieved_content=formatted_content, statements=statements\n            )\n        )\n\n    @staticmethod\n    def convert_retrieved_content_to_string(\n        retrieved_content: Union[str, List[str]], separator: str = '\\n'\n    ) -&gt; str:\n        if isinstance(retrieved_content, list):\n            retrieved_content = separator.join(retrieved_content)\n        return retrieved_content\n\n    @staticmethod\n    def _compute_score(evaluation_results: NLIStatementOutput) -&gt; float:\n        statements = evaluation_results.statements\n        if not statements:\n            logger.warning('No statements were evaluated.')\n            return np.nan\n\n        faithful_count = sum(1 for statement in statements if statement.verdict)\n        return faithful_count / len(statements)\n\n    async def execute(self, item: DatasetItem) -&gt; MetricEvaluationResult:\n        statements = await self._create_statements(item.query, item.actual_output)\n        if not statements:\n            return MetricEvaluationResult(\n                score=np.nan,\n            )\n\n        evaluation_results = await self._evaluate_statements(\n            item.retrieved_content, statements\n        )\n        score = self._compute_score(evaluation_results)\n\n        return MetricEvaluationResult(score=score)\n\n\nmetric = Faithfulness()\n\ndata_item = DatasetItem(\n    query = \"How do I reset my password?\",\n    actual_output = \"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output = \"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    retrieved_content = [\"Password reset available via login page\", \"Reset link sent by email\"],\n    latency = 2.13\n)\nresult = await metric.execute(data_item)\nprint(result.pretty())\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/","title":"Creating Custom Metrics","text":"<p>Axion provides a flexible framework for creating custom evaluation metrics. You can build metrics that leverage LLMs, traditional algorithms, or hybrid approaches, all while maintaining consistency with the broader evaluation ecosystem.</p>"},{"location":"deep-dives/metrics/creating-metrics/#overview","title":"Overview","text":"<p>Custom metrics in Axion inherit from <code>BaseMetric</code> and can be registered automatically using decorators or manually through the registry. The framework supports:</p> <ul> <li>LLM-powered metrics using structured prompts and examples</li> <li>Algorithm-based metrics using traditional computation</li> <li>Hybrid metrics combining multiple evaluation approaches</li> <li>Complex multi-step metrics with intermediate processing</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#core-components","title":"Core Components","text":""},{"location":"deep-dives/metrics/creating-metrics/#basemetric-class","title":"BaseMetric Class","text":"<p>All custom metrics inherit from <code>BaseMetric</code>, which provides:</p> <ul> <li>LLM Integration - Built-in LLM handler with configurable models</li> <li>Structured I/O - Type-safe input/output with Pydantic models</li> <li>Execution Framework - Async execution with tracing and logging</li> <li>Configuration Management - Threshold and parameter handling</li> <li>Validation - Automatic field validation for dataset items</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#metric-decorator","title":"Metric Decorator","text":"<p>The <code>@metric</code> decorator provides declarative configuration:</p> <pre><code>@metric(\n    name=\"Human-readable metric name\",\n    description=\"Detailed description of what the metric measures\",\n    required_fields=[\"field1\", \"field2\"],\n    optional_fields=[\"field3\"],\n    default_threshold=0.5,\n    score_range=(0, 1),\n    tags=[\"category\", \"domain\"]\n)\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#creating-simple-metrics","title":"Creating Simple Metrics","text":""},{"location":"deep-dives/metrics/creating-metrics/#basic-llm-powered-metric","title":"Basic LLM-Powered Metric","text":"<pre><code>from axion.metrics.base import BaseMetric, MetricEvaluationResult, metric\nfrom axion.dataset import DatasetItem\n\n@metric(\n    name=\"Answer Quality\",\n    description=\"Evaluates the overall quality of an answer based on clarity, completeness, and accuracy\",\n    required_fields=[\"actual_output\"],\n    optional_fields=[\"expected_output\", \"query\"],\n    default_threshold=0.7,\n    score_range=(0, 1),\n    tags=[\"quality\", \"general\"]\n)\nclass AnswerQuality(BaseMetric):\n    \"\"\"Evaluates answer quality across multiple dimensions.\"\"\"\n\n    instruction = \"\"\"\n    Evaluate the quality of the given answer based on the following criteria:\n    1. Clarity - Is the answer clear and easy to understand?\n    2. Completeness - Does the answer fully address the question?\n    3. Accuracy - Is the information provided correct?\n    Provide a score from 0 to 1, where:\n    ....\n    Provide a brief explanation for your score.\n    \"\"\"\n\n    examples = [\n        (\n            DatasetItem(\n                query=\"What is photosynthesis?\",\n                actual_output=\"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen. This process occurs in chloroplasts and is essential for plant survival and the oxygen we breathe.\"\n            ),\n            MetricEvaluationResult(\n                score=0.9,\n                explanation=\"Excellent answer that clearly explains photosynthesis, includes key components (sunlight, CO2, water, glucose, oxygen), mentions location (chloroplasts), and explains significance. Very clear and complete.\"\n            )\n        ),\n        (\n            DatasetItem(\n                query=\"How do you bake a cake?\",\n                actual_output=\"Mix ingredients and bake.\"\n            ),\n            MetricEvaluationResult(\n                score=0.2,\n                explanation=\"Poor answer that lacks detail and completeness. Doesn't specify ingredients, quantities, temperatures, or timing. Too vague to be useful for someone wanting to bake a cake.\"\n            )\n        )\n    ]\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#algorithm-based-metric","title":"Algorithm-Based Metric","text":"<pre><code>import re\nfrom typing import Set\n\n@metric(\n    name=\"Keyword Coverage\",\n    description=\"Measures how many expected keywords appear in the actual output\",\n    required_fields=[\"actual_output\", \"expected_keywords\"],\n    default_threshold=0.6,\n    score_range=(0, 1),\n    tags=[\"coverage\", \"keywords\", \"algorithmic\"]\n)\nclass KeywordCoverage(BaseMetric):\n    \"\"\"Calculates the percentage of expected keywords found in the output.\"\"\"\n\n    async def execute(self, item: DatasetItem) -&gt; MetricEvaluationResult:\n        \"\"\"Calculate keyword coverage score.\"\"\"\n        self._validate_required_metric_fields(item)\n\n        actual_output = item.actual_output.lower()\n        expected_keywords = item.expected_keywords\n\n        if isinstance(expected_keywords, str):\n            expected_keywords = [kw.strip() for kw in expected_keywords.split(',')]\n\n        # Find keywords in output\n        found_keywords = []\n        missing_keywords = []\n\n        for keyword in expected_keywords:\n            keyword_lower = keyword.lower()\n            if keyword_lower in actual_output:\n                found_keywords.append(keyword)\n            else:\n                missing_keywords.append(keyword)\n\n        # Calculate score\n        score = len(found_keywords) / len(expected_keywords) if expected_keywords else 0.0\n\n        # Generate explanation\n        explanation = f\"Found {len(found_keywords)}/{len(expected_keywords)} expected keywords. \"\n        if found_keywords:\n            explanation += f\"Found: {', '.join(found_keywords)}. \"\n        if missing_keywords:\n            explanation += f\"Missing: {', '.join(missing_keywords)}.\"\n\n        return MetricEvaluationResult(\n            score=score,\n            explanation=explanation.strip()\n        )\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#registration-methods","title":"Registration Methods","text":""},{"location":"deep-dives/metrics/creating-metrics/#automatic-registration-with-decorator","title":"Automatic Registration with Decorator","text":"<p>The <code>@metric</code> decorator automatically registers metrics:</p> <pre><code>@metric(\n    name=\"Custom Metric\",\n    description=\"Description of the metric\",\n    required_fields=[\"field1\", \"field2\"],\n    default_threshold=0.5\n)\nclass CustomMetric(BaseMetric):\n    # Metric implementation\n    pass\n\n# Metric is automatically available in registry\nfrom axion.metrics import metric_registry\nmetric_class = metric_registry.get(\"custom_metric\")\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#manual-registration","title":"Manual Registration","text":"<p>For dynamic registration or when decorators aren't suitable:</p> <pre><code>from axion.metrics.base import MetricConfig\nfrom axion.metrics import metric_registry\n\nclass DynamicMetric(BaseMetric):\n    \"\"\"Dynamically configured metric.\"\"\"\n    pass\n\n# Create configuration\nconfig = MetricConfig(\n    key=\"dynamic_metric\",\n    name=\"Dynamic Metric\",\n    description=\"A dynamically registered metric\",\n    required_fields=[\"actual_output\"],\n    optional_fields=[],\n    default_threshold=0.6,\n    score_range=(0, 1),\n    tags=[\"dynamic\"]\n)\n\n# Attach config and register\nDynamicMetric.config = config\nmetric_registry.register(DynamicMetric)\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#usage-examples","title":"Usage Examples","text":""},{"location":"deep-dives/metrics/creating-metrics/#using-custom-metrics","title":"Using Custom Metrics","text":"<pre><code>from axion.dataset import DatasetItem\n\n# Initialize your custom metric\nmetric = AnswerQuality()\n\n# Prepare test data\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    retrieved_content=[\"Password reset available via login page\", \"Reset link sent by email\"]\n)\n\n# Execute evaluation\nresult = await metric.execute(data_item)\nprint(f\"Score: {result.score}\")\nprint(f\"Explanation: {result.explanation}\")\n\n# Pretty print results\nprint(result.pretty())\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#best-practices","title":"Best Practices","text":""},{"location":"deep-dives/metrics/creating-metrics/#metric-design","title":"Metric Design","text":"<ul> <li>Single Responsibility - Each metric should evaluate one specific aspect</li> <li>Clear Scoring - Use consistent scoring scales and document ranges</li> <li>Robust Validation - Validate inputs thoroughly and provide helpful error messages</li> <li>Comprehensive Examples - Include diverse examples that cover edge cases</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#error-handling","title":"Error Handling","text":"<ul> <li>Graceful Degradation - Provide fallback scores when computation fails</li> <li>Informative Messages - Return helpful error messages and explanations</li> <li>Input Validation - Validate inputs early and provide clear requirements</li> <li>Logging - Use appropriate logging levels for debugging and monitoring</li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/","title":"Creating YAML-Based Metrics","text":"<p>Axion supports creating evaluation metrics directly from YAML configuration files, providing a simple alternative to defining metrics as code. YAML metrics support both LLM-powered evaluation and custom heuristic functions.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#overview","title":"Overview","text":"<p>YAML metrics offer two evaluation approaches:</p> <ul> <li>LLM-powered metrics using instruction prompts</li> <li>Heuristic-based metrics using custom Python functions</li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#basic-structure","title":"Basic Structure","text":"<p>Every YAML metric requires either an <code>instruction</code> (for LLM evaluation) OR a <code>heuristic</code> (for algorithmic evaluation), but not both:</p> <pre><code># Option 1: LLM-powered metric\nname: 'My Metric'\ninstruction: |\n  Your evaluation prompt here...\n\n# Option 2: Heuristic-based metric\nname: 'My Metric'\nheuristic: |\n  def evaluate(item):\n      # Your logic here\n      return MetricEvaluationResult(score=1.0, explanation=\"...\")\n\n# Optional configuration\nmodel_name: \"gpt-4\"\nthreshold: 0.7\nrequired_fields: [...]\noptional_fields: [...]\nexamples: [...]\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#llm-powered-metrics","title":"LLM-Powered Metrics","text":"<p>Use <code>instruction</code> to define metrics that leverage language models for evaluation.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#basic-llm-metric","title":"Basic LLM Metric","text":"<pre><code># answer_quality.yaml\nname: 'Answer Quality'\ninstruction: |\n  Evaluate the quality of the given answer based on clarity, completeness, and accuracy.\n  Provide a score either of 0 or 1 based on .... and explain your reasoning.\n\n\n# Optional configuration\nmodel_name: \"gpt-4\"\nthreshold: 0.7\nrequired_fields:\n  - \"query\"\n  - \"actual_output\"\n\nexamples:\n  - input:\n      query: \"What is photosynthesis?\"\n      actual_output: \"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen.\"\n    output:\n      score: 1\n      explanation: \"Excellent answer that clearly explains photosynthesis with key components. Very clear and complete.\"\n\n  - input:\n      query: \"How do you bake a cake?\"\n      actual_output: \"Mix ingredients and bake.\"\n    output:\n      score: 0\n      explanation: \"Poor answer lacking detail. Doesn't specify ingredients, quantities, or timing.\"\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#heuristic-based-metrics","title":"Heuristic-Based Metrics","text":"<p>Use <code>heuristic</code> to define metrics with custom Python logic for fast, deterministic evaluation.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#string-matching-heuristic","title":"String Matching Heuristic","text":"<pre><code># contains_match.yaml\nheuristic: |\n  def evaluate(item):\n      expected = item.expected_output.strip()\n      is_contained = expected in item.actual_output\n\n      return MetricEvaluationResult(\n          score=1.0 if is_contained else 0.0,\n          explanation=f\"Expected text {'found' if is_contained else 'not found'} in actual output\"\n      )\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#loading-and-using-yaml-metrics","title":"Loading and Using YAML Metrics","text":"<pre><code>from axion.metrics.yaml_metrics import load_metric_from_yaml\nfrom axion.dataset import DatasetItem\n\n# Load metric from YAML file\nMetricClass = load_metric_from_yaml(\"answer_quality.yaml\")\n\n# Create instance\nmetric = MetricClass()\n# Evaluate\ndata_item = DatasetItem(\n    query=\"What is machine learning?\",\n    actual_output=\"Machine learning is a subset of AI that enables computers to learn from data.\",\n    expected_output=\"Machine learning allows computers to learn patterns from data without explicit programming.\"\n)\n\nresult = await metric.execute(data_item)\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base","title":"<code>axion.metrics.base</code>","text":""},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric","title":"<code>BaseMetric(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, **kwargs: Any)</code>","text":"<p>               Bases: <code>LLMHandler</code>, <code>Generic[InputModel, OutputModel]</code></p> <p>Base class for all metric evaluation classes, inheriting from LLMHandler.</p> <p>Initialize the metric with optional LLM and embedding model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the LLM model to use</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>A pre-configured LLM model. If not provided, a default is loaded from the registry.</p> </li> <li> <code>embed_model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the embedding model to use</p> </li> <li> <code>embed_model</code>               (<code>Optional[EmbeddingRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>A pre-configured embedding model handler (if needed).</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The threshold to consider a score as 'passing'. Will overwrite default.</p> </li> <li> <code>llm_provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The LLM provider to use</p> </li> <li> <code>required_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of required field names for evaluation</p> </li> <li> <code>optional_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of optional field names for evaluation</p> </li> <li> <code>metric_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the metric instance (alias: name)</p> </li> <li> <code>metric_description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional description for the metric instance</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Alias for metric_name (for convenience)</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to the parent LLMHandler (e.g., logger config).</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Return the name of the metric from instance, config, or fallback to class name.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.description","title":"<code>description: str</code>  <code>property</code>","text":"<p>Return the description of the metric from instance, config, or fallback to class name.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.threshold","title":"<code>threshold</code>  <code>property</code>","text":"<p>Metric passing threshold.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.input_item","title":"<code>input_item</code>  <code>property</code>","text":"<p>Access the final DatasetItem passed the metric</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.required_fields","title":"<code>required_fields: list</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the required fields for evaluation.</p> <p>Falls back to configuration if instance-level fields are not explicitly set.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.optional_fields","title":"<code>optional_fields: list</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optional fields for evaluation.</p> <p>Falls back to configuration if instance-level fields are not explicitly set.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.execute","title":"<code>execute(item: Union[DatasetItem, dict], callbacks: Callbacks = None, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Execute the metric evaluation for a single dataset item.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>Union[DatasetItem, dict]</code>)           \u2013            <p>Input dataset item containing necessary fields for evaluation.</p> </li> <li> <code>callbacks</code>               (<code>Callbacks</code>, default:                   <code>None</code> )           \u2013            <p>Optional callback handler for events/logging.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetricEvaluationResult</code>           \u2013            <p>An evaluation result conforming to the output model.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.get_evaluation_fields","title":"<code>get_evaluation_fields(item: Union[DatasetItem, dict]) -&gt; Union[DatasetItem, InputModel]</code>","text":"<p>Extracts the appropriate evaluation fields from the dataset item.</p> <p>Priority is given to explicitly set required and optional fields on the instance. If not defined, configuration-based fields are used. If none are available, the item's default evaluation fields are returned.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The input dataset item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[DatasetItem, InputModel]</code>           \u2013            <p>DatasetItem | InputModel: A dataset item containing only the relevant fields for evaluation.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.set_instruction","title":"<code>set_instruction(instruction: str)</code>","text":"<p>Set a new instruction string for the metric.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>The updated task instruction that guides the metric\u2019s behavior or LLM prompt.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.set_examples","title":"<code>set_examples(examples: List[Tuple[DatasetItem, MetricEvaluationResult]])</code>","text":"<p>Replace all current examples with a new set.</p> <p>Parameters:</p> <ul> <li> <code>examples</code>               (<code>List[Tuple[DatasetItem, EvaluationResult]]</code>)           \u2013            <p>A list of example input-output pairs used for few-shot prompting or metric calibration.</p> </li> </ul> Example <p>[     (         DatasetItem(             expected_output='....',             actual_output='...',         ),         MetricEvaluationResult(             score=...,             explanation=\"...\",         ),     ), ]</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.add_examples","title":"<code>add_examples(examples: List[Tuple[DatasetItem, MetricEvaluationResult]])</code>","text":"<p>Add new example input-output pairs to the existing list of examples.</p> <p>Parameters:</p> <ul> <li> <code>examples</code>               (<code>List[Tuple[DatasetItem, EvaluationResult]]</code>)           \u2013            <p>One or more examples to add to the current list, extending few-shot prompting context.</p> </li> </ul> Example <p>[     (         DatasetItem(             expected_output='....',             actual_output='...',         ),         MetricEvaluationResult(             score=...,             explanation=\"...\",         ),     ), ]</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.compute_cost_estimate","title":"<code>compute_cost_estimate(sub_models: List[BaseMetric])</code>","text":"<p>Computes the total estimated cost, including the current model and any sub-models.</p> <p>Parameters:</p> <ul> <li> <code>sub_models</code>               (<code>List[BaseMetric]</code>)           \u2013            <p>List of sub-models that may have a cost_estimate.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.BaseMetric.display_prompt","title":"<code>display_prompt(item: Union[dict, InputModel] = None, **kwargs)</code>","text":"<p>Displays the fully constructed prompt that will be sent to the LLM.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>Union[dict, InputModel]</code>, default:                   <code>None</code> )           \u2013            <p>The input data to be included in the prompt. If None, a placeholder is used. Defaults to None.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry","title":"<code>MetricRegistry</code>","text":"<p>Registry for storing and retrieving metric classes.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.finalize_initial_state","title":"<code>finalize_initial_state() -&gt; None</code>","text":"<p>Call this after all built-in metrics are registered.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.register","title":"<code>register(metric_class: Type[BaseMetric]) -&gt; None</code>","text":"<p>Register a metric class into the registry.</p> <p>Parameters:</p> <ul> <li> <code>metric_class</code>               (<code>Type[BaseMetric]</code>)           \u2013            <p>A class inheriting from BaseMetric with a valid config.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.reset","title":"<code>reset() -&gt; None</code>  <code>classmethod</code>","text":"<p>Reset registry to original state.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.get","title":"<code>get(key: str, error: bool = True) -&gt; Optional[Type[BaseMetric]]</code>","text":"<p>Retrieve a registered metric class by key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The unique key of the metric.</p> </li> <li> <code>error</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise an error if the key is not found.    If False, return None instead.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Type[BaseMetric]]</code>           \u2013            <p>The registered metric class, or None if not found and error=False.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.find","title":"<code>find(query: str) -&gt; List[Type[BaseMetric]]</code>","text":"<p>Search for metrics whose name, description, or tags match a query.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Case-insensitive search string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of matching metric classes.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.get_compatible_metrics","title":"<code>get_compatible_metrics(item: DatasetItem) -&gt; List[Type[BaseMetric]]</code>","text":"<p>Return all metrics compatible with a given DatasetItem.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The dataset item to test against.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of compatible metric classes.</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.get_metric_descriptions","title":"<code>get_metric_descriptions() -&gt; Dict[str, str]</code>","text":"<p>Return {metric_name: description} from the registry.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.display","title":"<code>display(show_examples: bool = False) -&gt; None</code>","text":"<p>Display a summary of all registered metrics.</p> <p>Parameters:</p> <ul> <li> <code>show_examples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Show custom LLM examples</p> </li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.MetricRegistry.display_table","title":"<code>display_table() -&gt; None</code>","text":"<p>Display a formatted table of all registered metrics.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#axion.metrics.base.metric","title":"<code>metric(name: str, description: str, required_fields: List[str], optional_fields: Optional[List[str]] = None, key: Optional[str] = None, default_threshold: float = 0.5, score_range: tuple[Union[int, float], Union[int, float]] = (0, 1), tags: Optional[List[str]] = None) -&gt; Callable[[Type[BaseMetric]], Type[BaseMetric]]</code>","text":"<p>Decorator to define and register a metric class with declarative configuration.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Human-readable name of the metric.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Description of what the metric measures.</p> </li> <li> <code>required_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields that must be present in the DatasetItem to evaluate this metric.</p> </li> <li> <code>optional_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional fields the metric may use if available.</p> </li> <li> <code>key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional. A unique programmatic identifier for the metric.  If not provided, it's generated from the name.</p> </li> <li> <code>default_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The default threshold to consider a score as 'passing'.</p> </li> <li> <code>score_range</code>               (<code>tuple[Union[int, float], Union[int, float]]</code>, default:                   <code>(0, 1)</code> )           \u2013            <p>Tuple representing the valid score range for this metric.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Searchable tags to group or filter metrics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable[[Type[BaseMetric]], Type[BaseMetric]]</code>           \u2013            <p>A class decorator that attaches config and registers the metric in the MetricRegistry.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the decorated class is not a subclass of BaseMetric.</p> </li> </ul>"},{"location":"deep-dives/runners/api-runner/","title":"API Runner","text":"<p>The API Runner is a centralized orchestration system for managing multiple API clients within Axion. It provides a unified interface for executing single queries and batch operations across different API endpoints with built-in retry, concurrency control, error handling, and standardized response formatting.</p>"},{"location":"deep-dives/runners/api-runner/#key-features","title":"Key Features","text":"<ul> <li>Registry-based API management \u2014 Register any API client with a simple decorator</li> <li>Concurrent execution \u2014 Semaphore-controlled parallelism for batch operations</li> <li>Configuration-driven \u2014 YAML or dictionary-based initialization</li> <li>Standardized responses \u2014 Consistent <code>APIResponseData</code> format across all APIs</li> <li>Built-in retry logic \u2014 Configurable retry behavior with exponential backoff</li> </ul>"},{"location":"deep-dives/runners/api-runner/#creating-custom-api-runners","title":"Creating Custom API Runners","text":"<p>The API Runner system is designed to be extensible. Register your own API implementations to integrate any service into Axion's evaluation pipelines.</p>"},{"location":"deep-dives/runners/api-runner/#method-1-decorator-registration-recommended","title":"Method 1: Decorator Registration (Recommended)","text":"<p>Use the <code>@APIRunner.register()</code> decorator to automatically register your custom runner:</p> <pre><code>from axion.runners.api import api_retry, BaseAPIRunner, APIResponseData, APIRunner\n\n@APIRunner.register('my_chatbot')\nclass MyChatbotRunner(BaseAPIRunner):\n    \"\"\"Runner for your custom chatbot API.\"\"\"\n    name = 'my_chatbot'\n\n    def __init__(self, config: dict, **kwargs):\n        super().__init__(config, **kwargs)\n        self.api_url = config.get('api_url')\n        self.api_key = config.get('api_key')\n\n    @api_retry('Chatbot API call')\n    def execute(self, query: str, **kwargs) -&gt; APIResponseData:\n        import requests\n        import time\n\n        start_time = time.time()\n\n        response = requests.post(\n            self.api_url,\n            headers={'Authorization': f'Bearer {self.api_key}'},\n            json={'message': query}\n        )\n        result = response.json()\n\n        return APIResponseData(\n            query=query,\n            actual_output=result.get('response', ''),\n            latency=time.time() - start_time,\n            status='success'\n        )\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#example-openai-chat-runner","title":"Example: OpenAI Chat Runner","text":"<p>A real-world example integrating OpenAI's Chat API:</p> <pre><code>from axion.runners.api import api_retry, BaseAPIRunner, APIResponseData, APIRunner\n\n@APIRunner.register('openai_chat')\nclass OpenAIChatRunner(BaseAPIRunner):\n    \"\"\"Custom runner for OpenAI Chat API.\"\"\"\n    name = 'openai_chat'\n\n    def __init__(self, config: dict, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = config.get('model', 'gpt-4')\n\n    @api_retry('OpenAI Chat API call')\n    def execute(self, query: str, **kwargs) -&gt; APIResponseData:\n        from openai import OpenAI\n        import time\n\n        client = OpenAI()\n        start_time = time.time()\n\n        response = client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": query}]\n        )\n\n        return APIResponseData(\n            query=query,\n            actual_output=response.choices[0].message.content,\n            latency=time.time() - start_time,\n            additional_output={'model': self.model, 'usage': dict(response.usage)},\n            status='success'\n        )\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#implementation-requirements","title":"Implementation Requirements","text":"<p>When creating custom runners, ensure your implementation:</p> <ol> <li>Inherits from <code>BaseAPIRunner</code> \u2014 Provides the standard interface and batch processing capabilities</li> <li>Implements the <code>execute()</code> method \u2014 Core method that handles single query execution</li> <li>Returns <code>APIResponseData</code> objects \u2014 Use the standardized response format for consistency</li> <li>Handles errors gracefully \u2014 Use try/except and return appropriate status</li> </ol>"},{"location":"deep-dives/runners/api-runner/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/runners/api-runner/#basic-usage","title":"Basic Usage","text":"<pre><code>from axion.runners import APIRunner\n\n# Configuration for your registered APIs\nconfig = {\n    'my_chatbot': {\n        'api_url': 'https://api.example.com/chat',\n        'api_key': 'your-api-key'\n    }\n}\n\n# Initialize with configuration\nrunner = APIRunner(config=config, max_concurrent=5)\n\n# Execute single query\nresponse = runner.execute('my_chatbot', \"How do I reset my password?\")\nprint(response.actual_output)\n\n# List available APIs\navailable_apis = runner.list_available_apis()\nprint(f\"Available APIs: {available_apis}\")\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#batch-processing","title":"Batch Processing","text":"<pre><code># Prepare multiple queries\nqueries = [\n    \"How do I reset my password?\",\n    \"What are the payment options?\",\n    \"How do I contact support?\"\n]\n\n# Execute batch asynchronously\nresponses = await runner.execute_batch('my_chatbot', queries)\n\n# Process responses\nfor response in responses:\n    print(f\"Query: {response.query}\")\n    print(f\"Response: {response.actual_output}\")\n    print(f\"Latency: {response.latency}s\")\n    print(\"---\")\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#direct-api-usage","title":"Direct API Usage","text":"<pre><code># Initialize a specific runner directly\nchatbot_runner = MyChatbotRunner(\n    config={'api_url': '...', 'api_key': '...'},\n    max_concurrent=3\n)\n\n# Execute query\nresponse = chatbot_runner.execute(\"Your query here\")\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#configuration","title":"Configuration","text":"<p>API Runner accepts configuration in multiple formats:</p> <pre><code># Dictionary format\nconfig = {\n    'my_chatbot': {\n        'api_url': 'https://api.example.com/chat',\n        'api_key': 'your-api-key'\n    },\n    'openai_chat': {\n        'model': 'gpt-4'\n    }\n}\n\n# Or load from YAML file\nconfig = \"/path/to/config.yaml\"\n\nrunner = APIRunner(config=config)\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#yaml-configuration-example","title":"YAML Configuration Example","text":"<pre><code># config.yaml\nmy_chatbot:\n  api_url: https://api.example.com/chat\n  api_key: ${CHATBOT_API_KEY}  # Environment variable substitution\n\nopenai_chat:\n  model: gpt-4\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#response-format","title":"Response Format","text":"<p>All API runners return standardized <code>APIResponseData</code> objects:</p> Field Type Description <code>query</code> <code>str</code> Original query string <code>actual_output</code> <code>str</code> Primary response content <code>retrieved_content</code> <code>List[str]</code> Retrieved context chunks (if applicable) <code>latency</code> <code>float</code> Response time in seconds <code>trace</code> <code>Dict[str, Any]</code> Debug and trace information <code>additional_output</code> <code>Dict[str, Any]</code> Additional response data <code>status</code> <code>str</code> Execution status (default: 'success') <code>timestamp</code> <code>str</code> ISO-formatted response timestamp"},{"location":"deep-dives/runners/api-runner/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"deep-dives/runners/api-runner/#concurrency-control","title":"Concurrency Control","text":"<pre><code># Set global concurrency limit\nrunner = APIRunner(config=config, max_concurrent=10)\n\n# Or configure per API when using direct instantiation\napi_runner = MyChatbotRunner(config=api_config, max_concurrent=3)\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#retry-control","title":"Retry Control","text":"<pre><code>from axion.runners.api import RetryConfig\n\n# Set global retry control\nrunner = APIRunner(\n    config=config,\n    retry_config=RetryConfig(max_attempts=5, backoff_factor=2.0)\n)\n\n# Or disable retries for specific runner\napi_runner = MyChatbotRunner(\n    config=api_config,\n    retry_config=RetryConfig(enabled=False)\n)\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#registry-management","title":"Registry Management","text":"<pre><code># View all registered APIs and their options\nAPIRunner.display()\n\n# Check registered APIs at runtime\navailable = runner.list_available_apis()\n\n# Access specific executor\nchatbot_executor = runner['my_chatbot']\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#integration-with-evaluation-runner","title":"Integration with Evaluation Runner","text":"<p>Use API runners as tasks in the evaluation pipeline:</p> <pre><code>from axion.runners import evaluation_runner, APIRunner\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.dataset import DatasetItem\n\n# Configure your API\nconfig = {'my_chatbot': {'api_url': '...', 'api_key': '...'}}\napi_runner = APIRunner(config=config)\n\n# Create evaluation dataset (without actual_output - the task will generate it)\ndataset = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        expected_output=\"Navigate to login, click 'Forgot Password', follow the email link.\"\n    ),\n    DatasetItem(\n        query=\"What are your business hours?\",\n        expected_output=\"We are open Monday-Friday, 9 AM to 5 PM EST.\"\n    )\n]\n\n# Define task that calls your API\ndef chatbot_task(item: DatasetItem) -&gt; dict:\n    response = api_runner.execute('my_chatbot', item.query)\n    return {\n        'response': response.actual_output,\n        'latency': response.latency\n    }\n\n# Run evaluation with task\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=chatbot_task,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name=\"Chatbot Evaluation\"\n)\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/runners/api-runner/#axion.runners.api","title":"<code>axion.runners.api</code>","text":""},{"location":"deep-dives/runners/api-runner/#axion.runners.api.RetryConfig","title":"<code>RetryConfig(enabled: bool = True, max_attempts: int = 3, wait_strategy: str = 'fixed', wait_seconds: float = 1.0, exponential_multiplier: float = 2.0, exponential_min: float = 1.0, exponential_max: float = 10.0, reraise: bool = False, return_empty_on_failure: bool = True)</code>  <code>dataclass</code>","text":"<p>Configuration for retry behavior.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.RetryConfig.get_wait_strategy","title":"<code>get_wait_strategy()</code>","text":"<p>Get the appropriate tenacity wait strategy.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIResponseData","title":"<code>APIResponseData(**data)</code>","text":"<p>               Bases: <code>RichBaseModel</code></p> <p>Standardized run-time response data from API clients.</p> <p>This model captures both the models main output and metadata</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner","title":"<code>APIRunner(config: Union[str, Dict[str, Any], PosixPath], name: str = 'APIRunner', description: str = 'Orchestrates API calls', additional_config: Dict[str, Any] = dict(), max_concurrent: int = 5, show_progress: bool = True, retry_config: Optional[Union[RetryConfig, Dict[str, Any]]] = None, tracer: Optional[BaseTraceHandler] = None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RunnerMixin</code></p> <p>A component runner that registers and manages multiple API clients.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.register","title":"<code>register(name: str)</code>  <code>classmethod</code>","text":"<p>Decorator to register an executor factory function.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.manual_register","title":"<code>manual_register(name: str, factory: Type[BaseAPIRunner])</code>  <code>classmethod</code>","text":"<p>Manually register an executor factory function.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.execute","title":"<code>execute(api_name: str, query: str, **kwargs) -&gt; APIResponseData</code>","text":"<p>Execute Singe API</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.execute_batch","title":"<code>execute_batch(api_name: str, queries: List[str], **kwargs) -&gt; List[APIResponseData]</code>  <code>async</code>","text":"<p>Execute a batch of queries asynchronously using a specific executor.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.get_executor","title":"<code>get_executor(api_name: str) -&gt; Any</code>","text":"<p>Gets an executor by name. If the executor hasn't been created yet, it instantiates it on-demand (lazy initialization) and caches it.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.APIRunner.list_available_apis","title":"<code>list_available_apis() -&gt; List[str]</code>","text":"<p>List available APIs based on the loaded configurations.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.BaseAPIRunner","title":"<code>BaseAPIRunner(config: Union[str, Dict[str, Any], PosixPath], max_concurrent: int = 5, show_progress: bool = True, retry_config: Optional[Union[RetryConfig, Dict[str, Any]]] = None, tracer: Optional[BaseTraceHandler] = None)</code>","text":"<p>               Bases: <code>RunnerMixin</code>, <code>ABC</code></p> <p>Abstract base class for API runners with built-in async batching.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.BaseAPIRunner.execute","title":"<code>execute(query: str, **kwargs) -&gt; APIResponseData</code>  <code>abstractmethod</code>","text":"<p>Executes a single synchronous query. Must be implemented by subclasses.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.BaseAPIRunner.execute_batch","title":"<code>execute_batch(evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame, List[str]], **kwargs) -&gt; List[APIResponseData]</code>  <code>async</code>","text":"<p>Runs a batch of queries asynchronously with limited concurrency.</p>"},{"location":"deep-dives/runners/api-runner/#axion.runners.api.api_retry","title":"<code>api_retry(operation_name: str = None)</code>","text":"<p>Decorator factory that uses the instance's retry configuration.</p>"},{"location":"deep-dives/runners/evaluation-runner/","title":"Evaluation Runner","text":"<p>The Evaluation Runner is an orchestration system for running end-to-end evaluation experiments. It  integrates task execution (model inference) with metric evaluation, providing a unified workflow for evaluating AI systems. The runner supports advanced features including caching, error handling, key remapping, custom thresholds, and detailed tracing.</p>"},{"location":"deep-dives/runners/evaluation-runner/#overview","title":"Overview","text":"<p>The Evaluation Runner combines three key phases into a single unified workflow:</p> <ol> <li>Task Execution - Optional custom function to generate model predictions/outputs</li> <li>Metric Evaluation - Automated scoring using multiple evaluation frameworks</li> <li>Result Aggregation -  Result collection with metadata and summaries</li> </ol>"},{"location":"deep-dives/runners/evaluation-runner/#available-metric-runners","title":"Available Metric Runners","text":"<p>To explore all inline usage examples and documentation for the evaluation runner and the configuration options, use the built-in discovery method:</p> View Available Runners<pre><code>from axion.runners import EvaluationRunner\n\n# Display documentation and usage examples\nEvaluationRunner.display()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#configuration","title":"Configuration","text":"<p>The Evaluation Runner accepts configuration through the <code>EvaluationConfig</code> object or direct function parameters:</p> <pre><code>from axion.runners import evaluation_runner, EvaluationConfig, ErrorConfig, CacheConfig, EvaluationRunner\n\n# Method 1: Direct function call\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Experiment\",\n    task=my_task_function,\n    max_concurrent=10,\n    thresholds={\"question_answer_relevance\": 0.75}\n)\n\n# Method 2: Configuration object\nconfig = EvaluationConfig(\n    evaluation_name=\"Advanced Experiment\",\n    evaluation_inputs=dataset,\n    scoring_metrics=metrics,\n    task=generation_task,\n    scoring_key_mapping={'actual_output': 'response'},\n    evaluation_description=\"Evaluating new model version\",\n    evaluation_metadata={\"model_version\": \"v2.1\"},\n    cache_config=CacheConfig(use_cache=True),\n    max_concurrent=5,\n    show_progress=True\n)\n\nrunner = EvaluationRunner(config)\nresults = await runner.execute()  # Async execution\n# To Pandas DataFrame\nresults.to_dataframe()\n# Create Scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/runners/evaluation-runner/#basic-evaluation-no-task","title":"Basic Evaluation (No Task)","text":"<p>For datasets that already contain model outputs:</p> <pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Prepare dataset with existing outputs\ndataset = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        actual_output=\"To reset your password, click 'Forgot Password' on the login page...\",\n        expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link.\"\n    ),\n    # More items...\n]\n\n# Run evaluation only\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"Basic Evaluation\"\n)\n\nprint(f\"Evaluation Name: {results.evaluation_name}\")\nprint(f\"Success rate: {results.success_rate}\")\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#end-to-end-evaluation-with-task","title":"End-to-End Evaluation (With Task)","text":"<p>For generating predictions and evaluating them in one workflow:</p> <pre><code># Task Setup Usage\nfrom axion.runners import evaluation_runner, EvaluationConfig, CacheConfig\nfrom axion.metrics import AnswerRelevancy, Latency\nfrom axion.dataset import DatasetItem\n\n\ndata_item = DatasetItem(\n    query = \"How do I reset my password?\",\n    expected_output = \"Navigate to login, click 'Forgot Password', and follow the reset link.\",\n)\n\n# Task Option 1 - Task returns python dictionary (can be async or sync)\ndef dictionary_task_output(item):\n    return {\n        'response': \"To reset your password, click 'Forgot Password' on the login page.\",\n        'latency': 1.3\n    }\n\n# Task Option 2 - Task returns pydantic BaseModel\nfrom pydantic import BaseModel\n\nclass Output(BaseModel):\n    response: str\n    latency: float\n\ndef pydantic_task_output(item):\n    return Output(\n        response=\"To reset your password, click 'Forgot Password' on the login page.\",\n        latency=1.3\n)\n\n# Task Option 3 - Task can be API runners\napi_runner = ExampleAPIRunner(config={'...'})\n\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    task=dictionary_task_output, # or pydantic_task_output, api_runner\n    scoring_metrics=[\n        AnswerRelevancy(model_name='gpt-4o'),\n        Latency(threshold=1.5),\n    ],\n    max_concurrent=5,\n    scoring_key_mapping={'actual_output': 'response'}, # Required if your `task` returns a different schema\n    evaluation_name='Custom Setup',\n    evaluation_metadata={\"model_version\": \"v2.1\", \"data_slice\": \"test_set\"},\n    cache_config=CacheConfig(use_cache=True),\n)\n# To Pandas DataFrame\nresults.to_dataframe()\n# Create Scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#advanced-configuration-with-multiple-libraries","title":"Advanced Configuration with Multiple Libraries","text":"<pre><code># Advanced configuration\nfrom axion.runners import (\n    EvaluationRunner,\n    EvaluationConfig,\n    CacheConfig,\n    ErrorConfig\n)\nfrom axion.metrics import AnswerCompleteness, Latency\nfrom axion.integrations.models import LiteLLMRagas, LiteLLMDeepEval\n\nimport pandas as pd\nfrom ragas.metrics import Faithfulness\nfrom deepeval.metrics import AnswerRelevancyMetric\n\n# Create evaluation dataset\ndataframe = pd.DataFrame([\n    {\n        'id': '01',\n        'query': \"How do I reset my password?\",\n        'expected_output': \"Navigate to login, click 'Forgot Password', and follow the reset link.\",\n    },\n    {\n        'id': '02',\n        'query': \"How do I update my billing information?\",\n        'expected_output': \"Go to Account Settings, select Billing, and update your payment method.\",\n    }\n])\n\n# Configure LLM models for third-party metrics\ndeepeval_model = LiteLLMDeepEval(model='gpt-4') # Optional\nragas_model = LiteLLMRagas(model='gpt-4') # Optional\n\n# Configure the task to evaluate\napi_runner = ExampleAPIRunner(config='...')\n\n# Define evaluation metrics\nmetrics = [\n    Faithfulness(llm=ragas_model),\n    AnswerCompleteness(model_name='gpt-4'),\n    AnswerRelevancyMetric(model=deepeval_model),\n    Latency(threshold=8)\n]\n\n# Configure the evaluation\nconfig = EvaluationConfig(\n    evaluation_name=\"Advanced Configuration Eval\",\n    evaluation_inputs=dataframe,\n    scoring_metrics=metrics,\n    task=api_runner,\n    max_concurrent=10,\n    cache_config=CacheConfig(use_cache=True, cache_type='memory'),\n    error_config=ErrorConfig(skip_on_missing_params=True),\n    thresholds={\"faithfulness\": 0.8, \"answer_completeness\": 0.7},\n    evaluation_metadata={\"model_version\": \"v2.1\"},\n    dataset_name='Advanced Configuration Dataset'\n)\n\n# Run the evaluation\nrunner = EvaluationRunner(config)\nresults = await runner.execute()  # For async execution\n# To Pandas DataFrame\nresults.to_dataframe()\n# Create Scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#task-functions","title":"Task Functions","text":"<p>Task functions are custom callables that generate model outputs for evaluation. They must accept a <code>DatasetItem</code> and return output data as either a Pydantic <code>BaseModel</code> or python <code>Dict</code>. Supports both async and sync functions.</p>"},{"location":"deep-dives/runners/evaluation-runner/#simple-task-function","title":"Simple Task Function","text":"<pre><code>def simple_task(item: DatasetItem) -&gt; Dict[str, Any]:\n    \"\"\"Simple synchronous task function.\"\"\"\n    response = your_model.generate(item.query)\n\n    return {\n        'actual_output': response,\n        'generation_time': time.time()\n    }\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#key-remapping","title":"Key Remapping","text":"<p>Use scoring key mapping to adapt between task outputs and metric input requirements:</p> <pre><code># Your task returns this structure\ntask_output = {\n    'generated_text': \"The answer is...\",\n    'source_documents': [\"doc1\", \"doc2\"],\n    'model_confidence': 0.95\n}\n\n# But metrics expect this structure\nmetric_expected = {\n    'actual_output': \"The answer is...\",\n    'retrieved_content': [\"doc1\", \"doc2\"]\n}\n\n# Use key mapping to bridge the gap\nscoring_key_mapping = {\n    'actual_output': 'generated_text',\n    'retrieved_content': 'source_documents',\n}\n\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=my_task,\n    scoring_metrics=metrics,\n    scoring_key_mapping=scoring_key_mapping,\n    evaluation_name=\"Mapped Evaluation\"\n)\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#response-format","title":"Response Format","text":"<p>The Evaluation Runner returns an <code>EvaluationResult</code> object containing:</p> Field Type Description <code>run_id</code> <code>str</code> Unique identifier for this evaluation run <code>evaluation_name</code> <code>str</code> Name of the evaluation <code>timestamp</code> <code>str</code> ISO-formatted execution timestamp <code>results</code> <code>List[TestResult]</code> Detailed results for each evaluation input <code>summary</code> <code>Dict[str, Any]</code> Summary of the TestResult objects <code>metadata</code> <code>Dict[str, Any]</code> Evaluation metadata and configuration <p>Each <code>TestResult</code> contains the same structure as described in the Metric Runner documentation.</p>"},{"location":"deep-dives/runners/evaluation-runner/#advanced-features","title":"Advanced Features","text":""},{"location":"deep-dives/runners/evaluation-runner/#caching-configuration","title":"Caching Configuration","text":"<p>Type of caching backend to use. - 'memory': Uses in-memory dictionary for caching (fast, but non-persistent). Must use class model for evaluations. - 'disk': Writes cache to disk (persistent across runs).</p> <pre><code>from axion.runners import CacheConfig\n\ncache_config = CacheConfig(\n    use_cache=True,          # Enable caching\n    cache_task=True,         # Cache task outputs\n    cache_type='disk',\n    cache_dir='cache/',\n)\n\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=expensive_task,\n    scoring_metrics=metrics,\n    cache_config=cache_config,\n    evaluation_name=\"Cached Evaluation\"\n)\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#evaluation-tracking-and-metadata","title":"Evaluation Tracking and Metadata","text":"<pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=task,\n    scoring_metrics=metrics,\n    evaluation_name=\"Model Comparison v2.1\",\n    evaluation_description=\"Comparing new model against baseline\",\n    evaluation_metadata={\n        'model_version': 'v2.1.0',\n        'baseline_version': 'v1.9.2',\n        'dataset_version': 'eval_set_march_2024',\n        'environment': 'staging',\n        'researcher': 'data_science_team',\n        'tags': ['comparison', 'monthly_eval', 'production_candidate']\n    },\n    run_id=\"evaluation_2024_03_15_001\"\n)\n\n# Access metadata in results\nprint(f\"Model version: {results.metadata['model_version']}\")\nprint(f\"Environment: {results.metadata['environment']}\")\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#tracing-integration","title":"Tracing Integration","text":"<p>The Evaluation Runner automatically integrates with Axion's tracing system for observability. When tracing is enabled (e.g., with Langfuse or Opik), you get detailed visibility into evaluation execution.</p>"},{"location":"deep-dives/runners/evaluation-runner/#trace-granularity","title":"Trace Granularity","text":"<p>Control how traces are organized using the <code>trace_granularity</code> parameter:</p> <pre><code>from axion.runners import evaluation_runner\n\n# Single trace mode (default) - all evaluations under one parent trace\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"RAG Quality Check\",\n    trace_granularity='single_trace'  # or 'single' or TraceGranularity.SINGLE_TRACE\n)\n\n# Separate mode - each metric execution gets its own independent trace\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"RAG Quality Check\",\n    trace_granularity='separate'  # or TraceGranularity.SEPARATE\n)\n</code></pre> Mode Description Use Case <code>single_trace</code> / <code>single</code> All evaluations under one parent trace (default) Viewing entire evaluation as one unit <code>separate</code> Each metric execution gets its own independent trace Detailed per-item analysis"},{"location":"deep-dives/runners/evaluation-runner/#trace-names","title":"Trace Names","text":"<p>The <code>evaluation_name</code> parameter is used as the trace name in your observability platform:</p> <pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name=\"RAG Quality Check v2\"  # Appears as trace name in Langfuse/Opik\n)\n</code></pre> <p>This makes it easy to identify and filter evaluations by their purpose.</p>"},{"location":"deep-dives/runners/evaluation-runner/#captured-data","title":"Captured Data","text":"<p>The evaluation runner captures input/output at multiple levels:</p> Span Input Captured Output Captured Evaluation (root) evaluation_name, dataset_name, input_count, metrics, max_concurrent run_id, total_items, metrics_evaluated, status Metric execution Query, actual/expected output, context Score, passed status, explanation LLM call Messages, model, provider Response, token usage, cost"},{"location":"deep-dives/runners/evaluation-runner/#trace-hierarchy","title":"Trace Hierarchy","text":"<p>The evaluation runner produces a lean 4-level trace hierarchy:</p> <pre><code>RAG Quality Check v2                          # evaluation_name (root span)\n\u251c\u2500 AnswerRelevancy.execute                    # Metric logic span\n\u2502  \u2514\u2500 litellm_structured                      # LLM formatting/parsing\n\u2502     \u2514\u2500 llm_call                             # LLM API call (cost/tokens)\n\u2514\u2500 Faithfulness.execute\n   \u2514\u2500 litellm_structured\n      \u2514\u2500 llm_call\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#enabling-tracing","title":"Enabling Tracing","text":"<pre><code>import os\nos.environ['TRACING_MODE'] = 'langfuse'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\nos.environ['LANGFUSE_BASE_URL'] = 'https://us.cloud.langfuse.com'  # or EU\n\nfrom axion._core.tracing import configure_tracing\nconfigure_tracing()\n\n# Now run your evaluation - traces are automatically captured\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=metrics,\n    evaluation_name=\"My Evaluation\"\n)\n</code></pre> <p>See the Tracing Documentation for more details on configuration options.</p>"},{"location":"deep-dives/runners/evaluation-runner/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate","title":"<code>axion.runners.evaluate</code>","text":""},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationConfig","title":"<code>EvaluationConfig(evaluation_name: str, evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: Optional[float] = 0.0, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_config: CacheConfig = CacheConfig(), error_config: ErrorConfig = ErrorConfig(), thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, enable_internal_caching: bool = True, trace_granularity: Union[TraceGranularity, str] = TraceGranularity.SEPARATE, flush_per_metric: bool = False)</code>  <code>dataclass</code>","text":"<p>Configuration for an evaluation run.</p> <p>Attributes:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate. Can be a high-level <code>Dataset</code> object, a list of individual <code>DatasetItem</code> objects, or a preloaded <code>pandas.DataFrame</code>.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>)           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>List[Any]</code>)           \u2013            <p>A list of metric objects or callables used to score each item in the dataset.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>)           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation. Used in trace logging and result storage.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>)           \u2013            <p>A custom function to generate predictions or transform inputs. If provided, it will be run before scoring to produce the model output for each dataset item.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>)           \u2013            <p>An optional dictionary mapping metric input names to dataset column names. Useful for adapting metrics to different schema formats.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>)           \u2013            <p>A human-readable description of the evaluation for documentation and trace metadata.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Additional metadata to include in the evaluation trace (e.g., model version, data slice info, tags).</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>)           \u2013            <p>Maximum number of metric evaluations to run concurrently. Default is 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>)           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>)           \u2013            <p>Optional summary generator used to produce a high-level summary after the evaluation. If not provided, a default <code>MetricSummary</code> is used.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>)           \u2013            <p>Configuration for caching metric results to avoid recomputation. Enables both read and write caching.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>)           \u2013            <p>Configuration for how errors are handled during evaluation. Allows skipping metrics or suppressing failures.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>)           \u2013            <p>Optional threshold values for each metric. Used to flag items or datasets that fall below a given performance level.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>)           \u2013            <p>Whether to show a progress bar during evaluation. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name of the dataset being evaluated. Used for display and trace logging.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>)           \u2013            <p>An optional identifier for this specific run. Useful for repeatability and audit logging.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>)           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> </ul>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationRunner","title":"<code>EvaluationRunner(config: EvaluationConfig, tracer: Optional[BaseTraceHandler] = None)</code>","text":"<p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the execution of evaluation experiments, managing task execution, metric scoring, and configuration. Automatically determines and initializes the appropriate scoring strategy (flat or hierarchical).</p>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationRunner.summary","title":"<code>summary: Union[Dict[str, Any], None]</code>  <code>property</code>","text":"<p>Returns the summary from the active scoring strategy. For hierarchical ('tree') strategies, this provides the detailed tree summary.</p>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationRunner.tree","title":"<code>tree: Any</code>  <code>property</code>","text":"<p>Returns the underlying EvalTree instance for inspection, if the 'tree' strategy is active. Raises an AttributeError for other strategies.</p>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationRunner.execute","title":"<code>execute() -&gt; EvaluationResult</code>  <code>async</code>","text":"<p>Executes the entire evaluation and returns the final result.</p> <p>For SINGLE_TRACE mode, wraps execution in a trace span. For PER_ITEM and SEPARATE modes, skips the wrapper span to allow each item/metric to create its own independent trace.</p>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.EvaluationRunner.display","title":"<code>display()</code>  <code>classmethod</code>","text":"<p>Display Usage Documentation</p>"},{"location":"deep-dives/runners/evaluation-runner/#axion.runners.evaluate.evaluation_runner","title":"<code>evaluation_runner(evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], evaluation_name: str, scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: float = 0.0, summary_generator: Optional[BaseSummary] = None, cache_config: Optional[CacheConfig] = None, error_config: Optional[ErrorConfig] = None, enable_internal_caching: bool = True, thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, trace_granularity: Union[TraceGranularity, str] = TraceGranularity.SEPARATE, flush_per_metric: bool = False) -&gt; EvaluationResult</code>","text":"<p>Synchronously runs an evaluation experiment to evaluate metrics over a given dataset, supporting both flat and hierarchical scoring structures.</p> <p>Parameters:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>, default:                   <code>None</code> )           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>Optional[List[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>An alternative, more intuitive parameter for passing a flat list of metrics.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>, default:                   <code>None</code> )           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>, default:                   <code>None</code> )           \u2013            <p>A custom function to generate model outputs before scoring.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Maps metric input names to dataset column names.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>A human-readable description of the evaluation.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional metadata to include in the evaluation trace.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of concurrent evaluations. Defaults to 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>, default:                   <code>None</code> )           \u2013            <p>A summary generator for high-level results.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for caching results to avoid recomputation.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for handling errors during evaluation.</p> </li> <li> <code>enable_internal_caching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enables a per-item cache for metrics that share expensive internal computations. Defaults to True.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Performance thresholds for each metric.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show a progress bar. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of the dataset.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>An optional identifier for this specific run.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>, default:                   <code>SEPARATE</code> )           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> <li> <code>flush_per_metric</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>When trace_granularity='separate', controls whether each metric trace is flushed immediately (slower, but more \"live\" in the UI) vs batched (faster). Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EvaluationResult</code> (              <code>EvaluationResult</code> )          \u2013            <p>An object containing detailed metric scores, summary, and metadata.</p> </li> </ul>"},{"location":"deep-dives/runners/metric-runner/","title":"Metric Runner","text":"<p>The Metric Runner is an orchestration system for executing evaluation metrics across multiple libraries. It provides a unified interface for running single metrics or batches of metrics against evaluation datasets with built-in concurrency control, caching, error handling, and progress tracking.</p>"},{"location":"deep-dives/runners/metric-runner/#metricrunner-class","title":"MetricRunner Class","text":"<p>The main orchestrator that manages multiple metric executors and provides batch processing capabilities.</p> <p>Key Features:</p> <ul> <li>Registry-based metric framework management</li> <li>Concurrent execution with configurable limits</li> <li>Built-in progress tracking with tqdm integration</li> <li>Automatic caching support</li> <li>Flexible error handling configurations</li> <li>Standardized result summarization</li> </ul>"},{"location":"deep-dives/runners/metric-runner/#available-metric-runners","title":"Available Metric Runners","text":"<p>To explore all available Metric runners and their configuration options, use the built-in discovery method:</p> View Available Runners<pre><code>from axion.runners import MetricRunner\n\n# Display all registered Metric runners with their options\nMetricRunner.display()\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/runners/metric-runner/#basic-batch-evaluation","title":"Basic Batch Evaluation","text":"<pre><code>from axion.runners import MetricRunner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize metric and runner\nmetric = AnswerRelevancy()\nrunner = MetricRunner(metrics=[metric], max_concurrent=5)\n\n# Prepare evaluation data\ndata_items = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n        expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n        retrieved_content=[\"Password reset is available via the login page. Users receive a reset link by email.\"]\n    ),\n    # Add more data items...\n]\n\n# Execute batch evaluation\nresults = await runner.execute_batch(data_items)\n\n# Process results\nfor result in results:\n    print(f\"Test Case: {result.test_case.query}\")\n    for score in result.score_results:\n        print(f\"  {score.name}: {score.score} (passed: {score.passed})\")\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from axion.runners import MetricRunner, CacheManager, ErrorConfig, CacheConfig\nfrom axion.runners.summary import MetricSummary\nfrom axion.metrics import AnswerRelevancy, Faithfulness\n\nimport pandas as pd\n\ndataframe = pd.DataFrame({\n    'id': '0000001',\n    'query': \"How do I reset my password?\",\n    'actual_output': \"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    'expected_output': \"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    'retrieved_content': [[\"Password reset is available via the login page. Users receive a reset link by email.\"]]\n})\n\n\n# Initialize metrics\nmetrics = [\n    AnswerRelevancy(),\n    Faithfulness()\n]\n\n# Advanced configuration\nrunner = MetricRunner(\n    metrics=metrics,\n    max_concurrent=10,                          # Concurrency limit\n    cache_manager=CacheManager(                 # Optional Caching\n        CacheConfig(cache_type='memory')),\n    error_config=ErrorConfig(                   # Optional Error handling\n        ignore_errors=True,\n    ),\n    thresholds={                                # Optional Custom thresholds\n        'AnswerRelevancy': 0.75,\n        'Faithfulness': 0.85\n    },\n    summary_generator=MetricSummary(),\n)\n\n# Execute with progress tracking\nresults = await runner.execute_batch(\n    evaluation_inputs=dataframe, # Can pass Dataset, List of DatasetItems or Pandas Dataframe\n)\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#direct-executor-usage","title":"Direct Executor Usage","text":"<pre><code>from axion.runners import AxionRunner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize specific executor directly\nmetric = AnswerRelevancy()\nexecutor = AxionRunner(metric=metric, threshold=0.7)\n\n# Execute single evaluation\ndata_item = DatasetItem(\n    query=\"What is machine learning?\",\n    actual_output=\"Machine learning is a subset of AI...\",\n    expected_output=\"ML is a method of data analysis...\"\n)\n\nresult = await executor.execute(data_item)\nprint(f\"Score: {result.score}, Explanation: {result.explanation}\")\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#available-metric-runners_1","title":"Available Metric Runners","text":""},{"location":"deep-dives/runners/metric-runner/#axion-runner","title":"Axion Runner","text":"<p>Registry Key: <code>axion</code> Class: <code>AxionRunner</code> Purpose: Executes native metrics from the Axion framework</p> <p>Usage Example: <pre><code>from axion.metrics import AnswerRelevancy\n\nmetric = AnswerRelevancy()\nrunner = MetricRunner(metrics=[metric])\n\n# The runner automatically detects this is an Axion metric\n# and uses AxionRunner internally\n</code></pre></p>"},{"location":"deep-dives/runners/metric-runner/#ragas-runner","title":"Ragas Runner","text":"<p>Registry Key: <code>ragas</code> Class: <code>RagasRunner</code> Purpose: Executes metrics from the Ragas evaluation framework</p> <p>Usage Example: <pre><code>from ragas.metrics import Faithfulness\nfrom axion.integrations.models import LiteLLMRagas\n# Ragas metrics are automatically detected and executed with RagasRunner\nmetrics = [Faithfulness(llm=LiteLLMRagas())] # LiteLLMRagas() is optional\nrunner = MetricRunner(metrics=metrics)\n\n# Requires actual_output and optionally retrieved_content\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"Response text...\",\n    retrieved_content=[\"Context 1\", \"Context 2\"]\n)\n</code></pre></p>"},{"location":"deep-dives/runners/metric-runner/#deepeval-runner","title":"DeepEval Runner","text":"<p>Registry Key: <code>deepeval</code> Class: <code>DeepEvalRunner</code> Purpose: Executes metrics from the DeepEval framework</p> <p>Usage Example: <pre><code>from deepeval.metrics import AnswerRelevancyMetric\nfrom axion.integrations.models import LiteLLMDeepEval\n# DeepEval metrics are automatically detected\nmetrics = [\n    AnswerRelevancyMetric(model=LiteLLMDeepEval()), # LiteLLMDeepEval() is optional\n]\nrunner = MetricRunner(metrics=metrics)\n\n# Execute evaluation\nresults = await runner.execute_batch(evaluation_data)\n</code></pre></p>"},{"location":"deep-dives/runners/metric-runner/#response-format","title":"Response Format","text":"<p>All metric runners return standardized <code>TestResult</code> objects containing:</p> Field Type Description <code>test_case</code> <code>DatasetItem</code> Original evaluation input <code>score_results</code> <code>List[MetricScore]</code> List of metric evaluation results <p>Each <code>MetricScore</code> contains:</p> Field Type Description <code>id</code> <code>str</code> Unique identifier for the test case <code>name</code> <code>str</code> Metric name <code>score</code> <code>float</code> Numerical score (0.0-1.0 typically) <code>threshold</code> <code>float</code> Configured threshold for pass/fail <code>passed</code> <code>bool</code> Whether score meets threshold <code>explanation</code> <code>str</code> Detailed explanation (if available) <code>source</code> <code>str</code> Framework source (axion, ragas, deepeval) <code>timestamp</code> <code>str</code> ISO-formatted execution timestamp"},{"location":"deep-dives/runners/metric-runner/#creating-custom-metric-runners","title":"Creating Custom Metric Runners","text":"<p>You can extend the Metric Runner system by creating custom metric framework integrations.</p>"},{"location":"deep-dives/runners/metric-runner/#method-1-decorator-registration","title":"Method 1: Decorator Registration","text":"<pre><code>from axion.runners.metric import MetricRunner, BaseMetricRunner\nfrom axion.schema import MetricScore\nfrom axion.dataset import DatasetItem\nfrom typing import Union, Dict, Any\n\n@MetricRunner.register('custom_framework')\nclass CustomFrameworkRunner(BaseMetricRunner):\n    \"\"\"Custom metric framework runner.\"\"\"\n\n    _name = 'custom_framework'\n\n    async def execute(self, input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; MetricScore:\n        \"\"\"Execute metric using custom framework.\"\"\"\n        input_data = self.format_input(input_data)\n\n        try:\n            # Your custom framework integration here\n            score = await self.metric.evaluate(\n                query=input_data.query,\n                response=input_data.actual_output,\n                reference=input_data.expected_output\n            )\n\n            return MetricScore(\n                id=input_data.id,\n                name=self.metric_name,\n                score=score,\n                threshold=self.threshold,\n                passed=self._has_passed(score),\n                source=self.source\n            )\n\n        except Exception as e:\n            return self._create_error_score(input_data.id, e)\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#method-2-manual-registration","title":"Method 2: Manual Registration","text":"<pre><code># Define your custom runner class\nclass AnotherCustomRunner(BaseMetricRunner):\n    _name = 'another_custom'\n\n    async def execute(self, input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; MetricScore:\n        # Implementation here\n        pass\n\n# Manual registration\nMetricRunner.register('another_custom')(AnotherCustomRunner)\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#using-custom-runners","title":"Using Custom Runners","text":"<pre><code># Your custom metrics will be automatically detected and routed\n# to the appropriate runner based on their module path\n\nfrom your_custom_framework import CustomMetric\n\ncustom_metric = CustomMetric()\nrunner = MetricRunner(metrics=[custom_metric])\n\n# The runner automatically uses your CustomFrameworkRunner\nresults = await runner.execute_batch(evaluation_data)\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric","title":"<code>axion.runners.metric</code>","text":""},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseMetricRunner","title":"<code>BaseMetricRunner(metric: Any, metric_name: Optional[str] = None, threshold: Optional[float] = None, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for running a single evaluation metric.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseMetricRunner.metric_name","title":"<code>metric_name: str</code>  <code>property</code>","text":"<p>Gets the name of the metric.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseMetricRunner.source","title":"<code>source: str</code>  <code>property</code>","text":"<p>Gets the source name of the metric runner.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseMetricRunner.execute","title":"<code>execute(evaluation_input: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricScore</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Executes the metric for a single evaluation input.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseMetricRunner.get_tool_metadata","title":"<code>get_tool_metadata()</code>","text":"<p>Builds tool metadata describing this metric runner.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunnerFactory","title":"<code>MetricRunnerFactory</code>","text":"<p>Factory to create metric runner instances based on the metric type.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunnerFactory.register","title":"<code>register(metric_type: str) -&gt; Callable</code>  <code>classmethod</code>","text":"<p>Decorator to register a concrete metric runner class.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunnerFactory.create_executor","title":"<code>create_executor(metric: Any, **kwargs) -&gt; BaseMetricRunner</code>","text":"<p>Creates a metric executor instance based on the metric type.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseBatchRunner","title":"<code>BaseBatchRunner(metric: Any, metric_name: Optional[str] = None, threshold: Optional[float] = None, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetricRunner</code></p> <p>Abstract base class for runners that support native batch processing.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.BaseBatchRunner.execute_batch","title":"<code>execute_batch(dataset: Dataset, runners: List[BaseMetricRunner]) -&gt; Dict[str, List[MetricScore]]</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Executes a batch of metrics against a full dataset.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunner","title":"<code>MetricRunner(metrics: List[Any], name: str = 'MetricRunner', description: str = 'Orchestrates evaluation metrics', max_concurrent: int = 5, thresholds: Optional[Dict[str, float]] = None, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_manager: Optional[CacheManager] = None, error_config: ErrorConfig = ErrorConfig(), tracer: Optional[BaseTraceHandler] = None, dataset_name: Optional[str] = 'Metric Runner Dataset', enable_internal_caching: bool = True, trace_granularity: TraceGranularity = TraceGranularity.SEPARATE, flush_per_metric: bool = False)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the evaluation of multiple metrics against a dataset.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunner.available_types","title":"<code>available_types: List[str]</code>  <code>property</code>","text":"<p>Returns a list of available (registered) metric runner types.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunner.elapsed_time","title":"<code>elapsed_time: Union[float, None]</code>  <code>property</code>","text":"<p>Returns the total execution time for the last batch run.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunner.summary","title":"<code>summary: Union[Dict[str, Any], None]</code>  <code>property</code>","text":"<p>Returns the summary of the last batch run.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.MetricRunner.execute_batch","title":"<code>execute_batch(evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], *, show_progress: bool = True) -&gt; List[TestResult]</code>  <code>async</code>","text":"<p>Executes all configured metrics against the provided dataset.</p> <p>Trace granularity behavior: - SINGLE_TRACE: All metrics run under one parent trace (default) - SEPARATE: Each metric execution gets its own independent trace</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.AxionRunner","title":"<code>AxionRunner(metric: Any, metric_name: Optional[str] = None, threshold: Optional[float] = None, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetricRunner</code></p> <p>Executes native metrics from Axion.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.RagasRunner","title":"<code>RagasRunner(metric: Any, metric_name: Optional[str] = None, threshold: Optional[float] = None, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetricRunner</code></p> <p>Executes metrics from the Ragas library.</p>"},{"location":"deep-dives/runners/metric-runner/#axion.runners.metric.DeepEvalRunner","title":"<code>DeepEvalRunner(metric: Any, metric_name: Optional[str] = None, threshold: Optional[float] = None, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetricRunner</code></p> <p>Executes metrics from the DeepEval library.</p>"},{"location":"deep-dives/search/google/","title":"SerpAPI Web Search","text":""},{"location":"deep-dives/search/google/#overview","title":"Overview","text":"<p>SerpAPI is a real-time API that allows developers and businesses to access search engine results from Google, Bing, Baidu, Yahoo, Yandex, eBay, and YouTube. It is known for its fast speed, variety of Google-related APIs, and affordable pricing plans. SerpAPI is a valuable tool for developers, marketers, and data analysts who need to gather search engine data for various purposes</p>"},{"location":"deep-dives/search/google/#api-access","title":"API Access","text":"<p>To use SerpAPI, obtain an API key and configure your application accordingly.</p>"},{"location":"deep-dives/search/google/#steps-to-access-serpapi","title":"Steps to Access SerpAPI","text":"<ol> <li>Sign up and obtain an API key from SerpAPI.</li> <li>Pass the API key in API requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to retrieve structured search results.</li> </ol> <p>Please note that this file will also search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/google/#code-examples","title":"Code Examples","text":"GoogleRetriever Example<pre><code>from axion.search import GoogleRetriever\n\napi_client = GoogleRetriever(num_web_results=3)\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"deep-dives/search/google/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/search/google/#axion.search.google_retriever","title":"<code>axion.search.google_retriever</code>","text":""},{"location":"deep-dives/search/google/#axion.search.google_retriever.GoogleRetriever","title":"<code>GoogleRetriever(api_key: Optional[str] = None, num_web_results: int = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever that uses SerpAPI to perform Google searches and format the results into nodes.</p> <p>Initialize the GoogleRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>SerpAPI key used for authenticating requests. Defaults to the value of the 'SERPAPI_KEY' environment variable if not provided.</p> </li> <li> <code>num_web_results</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of top search results to return (maximum 20).</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to fetch and clean full page content from URLs in the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to crawl per page (if crawling is enabled).</p> </li> </ul>"},{"location":"deep-dives/search/google/#axion.search.google_retriever.GoogleRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Perform a search query and return results.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"deep-dives/search/tavily/","title":"Tavily WebSearch","text":""},{"location":"deep-dives/search/tavily/#overview","title":"Overview","text":"<p>Tavily is an AI-powered search API designed for AI agents and applications that need real-time, accurate, and comprehensive search results. It provides intelligent web search capabilities with content extraction and crawling features to enhance AI applications with up-to-date information.</p>"},{"location":"deep-dives/search/tavily/#api-access","title":"API Access","text":"<p>To integrate Tavily into your application, obtain an API key and configure it accordingly.</p>"},{"location":"deep-dives/search/tavily/#steps-to-access-tavily-api","title":"Steps to Access Tavily API","text":"<ol> <li>Sign up and obtain an API key from Tavily API.</li> <li>Pass the API key in requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to fetch real-time search results.</li> </ol> <p>Please note that this file will search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/tavily/#code-examples","title":"Code Examples","text":"TavilyRetriever Example<pre><code>from axion.search import TavilyRetriever\n\napi_client = TavilyRetriever(num_web_results=3, crawl_pages=False)\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n\n# Crawl specific pages\nresults = await api_client.crawl('https://www.espn.com/')\n\n# Extract content from URLs\nresults = await api_client.extract('https://www.espn.com/')\n</code></pre>"},{"location":"deep-dives/search/tavily/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever","title":"<code>axion.search.tavily_retriever</code>","text":""},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever.TavilyRetriever","title":"<code>TavilyRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'extract', 'crawl'] = 'search', search_depth: Literal['basic', 'advanced'] = 'basic', topic: Optional[str] = 'general', max_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, days: Optional[int] = None, include_answer: bool = False, include_raw_content: bool = False, include_images: bool = False, include_image_descriptions: bool = False, include_domains: Optional[List[str]] = None, exclude_domains: Optional[List[str]] = None, extract_depth: Literal['basic', 'advanced'] = 'basic', max_depth: Optional[int] = 1, max_breadth: Optional[int] = 20, limit: Optional[int] = 50, instructions: Optional[str] = None, select_paths: Optional[List[str]] = None, select_domains: Optional[List[str]] = None, exclude_paths: Optional[List[str]] = None, allow_external: bool = False, categories: Optional[List[str]] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for Tavily's Search, Extract, and Crawl APIs.</p> <p>Initialize the TavilyRetriever with specified parameters.</p>"},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever.TavilyRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Retrieve results using the Tavily Search API. Only handles 'search' endpoint. For 'extract' and 'crawl', call their respective methods directly.</p>"},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever.TavilyRetriever.extract_url_text","title":"<code>extract_url_text(url: str) -&gt; str</code>","text":"<p>Extract content from a single URL using the Extract API.</p>"},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever.TavilyRetriever.extract","title":"<code>extract(url: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Extract content from a URL using the Extract API.</p>"},{"location":"deep-dives/search/tavily/#axion.search.tavily_retriever.TavilyRetriever.crawl","title":"<code>crawl(url: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Crawl content from a URL using the Crawl API.</p>"},{"location":"deep-dives/search/you/","title":"You.com WebSearch","text":""},{"location":"deep-dives/search/you/#overview","title":"Overview","text":"<p>You.com is an AI-powered search engine and conversational AI platform that aims to enhance productivity and provide users with a more personalized and interactive search experience, going beyond traditional search and offering tools like AI agents and chatbots</p>"},{"location":"deep-dives/search/you/#api-access","title":"API Access","text":"<p>To integrate You.com into your application, obtain an API key and configure it accordingly.</p>"},{"location":"deep-dives/search/you/#steps-to-access-youcom-api","title":"Steps to Access You.com API","text":"<ol> <li>Sign up and obtain an API key from You.com API.</li> <li>Pass the API key in requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to fetch real-time search results.</li> </ol> <p>Please note that this file will search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/you/#code-examples","title":"Code Examples","text":"YouRetriever Example<pre><code>from axion.search import YouRetriever\n\napi_client = YouRetriever(num_web_results=5, endpoint='news') # supports both \"news\" and \"search\" endpoints\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"deep-dives/search/you/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/search/you/#axion.search.you_retriever","title":"<code>axion.search.you_retriever</code>","text":""},{"location":"deep-dives/search/you/#axion.search.you_retriever.YouRetriever","title":"<code>YouRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'news'] = 'search', num_web_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, safesearch: Optional[Literal['off', 'moderate', 'strict']] = None, country: Optional[str] = None, search_lang: Optional[str] = None, ui_lang: Optional[str] = None, spellcheck: Optional[bool] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for You.com's Search and News API.</p> <p>Initialize the YouRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>You.com API key. If not provided, it will attempt to use the <code>YDC_API_KEY</code> environment variable.</p> </li> <li> <code>callback_manager</code>               (<code>Optional[CallbackManager]</code>)           \u2013            <p>Optional manager for handling callback events during retrieval.</p> </li> <li> <code>endpoint</code>               (<code>Literal['search', 'news']</code>, default:                   <code>'search'</code> )           \u2013            <p>The You.com API endpoint to query \u2014 either \"search\" for web results or \"news\" for news-specific content. Defaults to \"search\".</p> </li> <li> <code>num_web_results</code>               (<code>Optional[int]</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of search results to return. Must not exceed 20.</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to crawl and extract the content of the linked pages from the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to retrieve per page when crawling. If None, a default internal value is used.</p> </li> <li> <code>safesearch</code>               (<code>Optional[Literal['off', 'moderate', 'strict']]</code>, default:                   <code>None</code> )           \u2013            <p>Safe search filtering level. Defaults to \"moderate\" if not specified.</p> </li> <li> <code>country</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Country code for geo-specific search behavior (e.g., \"US\" for United States).</p> </li> <li> <code>search_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code to use for the search query (e.g., \"en\" for English).</p> </li> <li> <code>ui_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code for the UI/localized response (e.g., \"en\").</p> </li> <li> <code>spellcheck</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Whether to enable spell check for the query. Defaults to True if unspecified.</p> </li> </ul>"},{"location":"deep-dives/search/you/#axion.search.you_retriever.YouRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Perform a search query and return results using You.com API.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"deep-dives/synthetic/document-qa/","title":"Document Synthetic QA Generation","text":"<p>The Synthetic QA Generation System provides a user flexible solution for creating question-answer pairs from unstructured documents. This system enables scalable generation of evaluation datasets for RAG models and other conversational systems through an intelligent, multi-stage workflow with built-in quality validation.</p> <p>Note:  This is build as an agentic workflow and will trigger multiple LLM calls, so please be mindful of potential rate limits and associated costs.</p>"},{"location":"deep-dives/synthetic/document-qa/#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>Document Ingestion - Load and prepare documents from directories</li> <li>Content Chunking - Intelligent splitting while preserving context</li> <li>Statement Extraction - Extract factual, standalone statements</li> <li>Question Generation - Create diverse questions across multiple types</li> <li>Answer Generation - Generate accurate, grounded answers</li> <li>Validation &amp; Reflection - Quality assessment with iterative improvement</li> </ol>"},{"location":"deep-dives/synthetic/document-qa/#langraph-workflow","title":"Langraph Workflow","text":""},{"location":"deep-dives/synthetic/document-qa/#key-components","title":"Key Components","text":"<ul> <li>DocumentQAGenerator - Main orchestrator for the entire pipeline</li> <li>GenerationParams - Configuration object for all generation parameters</li> <li>QAWorkflowGraph - A <code>LangGraph</code> based workflow execution engine</li> <li>Quality Validators - Multi-dimensional assessment and feedback</li> </ul>"},{"location":"deep-dives/synthetic/document-qa/#important-note","title":"Important Note","text":"<p>Synthetic data generation with LLMs is not a one-size-fits-all process. The quality and usefulness of generated data depend heavily on the prompts you use\u2014both during the answer generation phase and within your actual production model.</p> <p>Best Practice: Reverse-engineer your real application\u2019s instructions when creating meta-prompts. Provide the generation LLM with a realistic example question and answer that mirrors your production setup.</p> <p>This matters because you\u2019re effectively working with two different prompt contexts:</p> <ol> <li>Generation LLM\u2019s prompt \u2013 used to produce synthetic answers.</li> <li>Your real model\u2019s prompt \u2013 used during inference in your application.</li> </ol> <p>If these prompts differ significantly, the resulting style, tone, and level of detail can vary widely\u2014leading to synthetic data that doesn\u2019t truly represent your production environment. Aligning them ensures consistency and reliability in evaluation and training.</p>"},{"location":"deep-dives/synthetic/document-qa/#configuration","title":"Configuration","text":""},{"location":"deep-dives/synthetic/document-qa/#generationparams","title":"GenerationParams","text":"<p>The <code>GenerationParams</code> class provides comprehensive configuration for the generation process:</p> <pre><code>from axion.synthetic import GenerationParams\n\nparams = GenerationParams(\n    # Content Processing\n    splitter_type=\"semantic\",                    # or \"sentence\"\n    chunk_size=2048,                            # For sentence splitter\n    breakpoint_percentile_threshold=95,         # For semantic splitter\n\n    # Generation Control\n    num_pairs=1,                                # Number of QA pairs to generate\n    statements_per_chunk=5,                     # Statements extracted per chunk\n\n    # Question Configuration\n    question_types=[\"factual\", \"analytical\"],   # Types of questions\n    difficulty=\"medium\",                        # easy, medium, hard\n\n    # Answer Configuration\n    answer_length=\"medium\",                     # short, medium, long\n\n    # Quality Control\n    validation_threshold=0.8,                   # Quality threshold (0-1)\n    max_reflection_iterations=3,                # Max improvement iterations\n\n    # Customization\n    custom_guidelines=\"Focus on technical accuracy\",\n    example_question=\"What is the primary function of X?\",\n    example_answer=\"The primary function of X is...\"\n)\n</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/synthetic/document-qa/#configuration-options","title":"Configuration Options","text":"<pre><code>from axion.synthetic import DocumentQAGenerator, GenerationParams\nimport pandas as pd\nparams = GenerationParams(\n    # Content processing\n    splitter_type=\"semantic\",\n    breakpoint_percentile_threshold=90,\n    statements_per_chunk=8,\n\n    # Generation scope\n    num_pairs=1,\n    question_types=[\"factual\", \"conceptual\", \"analytical\", \"application\"],\n    difficulty=\"easy\",\n    answer_length=\"long\",\n\n    # Quality requirements\n    validation_threshold=0.85,\n    max_reflection_iterations=5,\n\n    # Domain customization\n    custom_guidelines=\"\"\"\n    Focus on technical accuracy and real-world applications.\n    Emphasize practical implementation details.\n    Include architectural considerations where relevant.\n    \"\"\",\n\n    example_question=\"How would you implement X in a production environment?\",\n    example_answer=\"To implement X in production, you would need to consider...\"\n)\n\nqa_generator = DocumentQAGenerator(\n    llm=llm,\n    params=params,\n    embed_model=embed_model, # required for semantic splitting\n    max_concurrent=3  # Conservative for complex processing\n)\n# Generate QA pairs from directory\nresults = await qa_generator.generate_from_directory('path/to/documents/')\npd.DataFrame(results)\n</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#parameter-reference","title":"Parameter Reference","text":""},{"location":"deep-dives/synthetic/document-qa/#content-processing-parameters","title":"Content Processing Parameters","text":"Parameter Type Default Description <code>splitter_type</code> <code>\"semantic\"</code> | <code>\"sentence\"</code> <code>\"sentence\"</code> Text splitting strategy <code>chunk_size</code> <code>int</code> <code>2048</code> Target chunk size for sentence splitter <code>breakpoint_percentile_threshold</code> <code>int</code> <code>95</code> Semantic similarity threshold (80-100) <code>statements_per_chunk</code> <code>int</code> <code>5</code> Number of statements to extract per chunk"},{"location":"deep-dives/synthetic/document-qa/#generation-control-parameters","title":"Generation Control Parameters","text":"Parameter Type Default Description <code>num_pairs</code> <code>int</code> <code>10</code> Total QA pairs to generate (1-100) <code>question_types</code> <code>List[str]</code> <code>[\"factual\", \"analytical\"]</code> Types of questions to generate <code>difficulty</code> <code>\"easy\"</code> | <code>\"medium\"</code> | <code>\"hard\"</code> <code>\"medium\"</code> Question complexity level <code>answer_length</code> <code>\"short\"</code> | <code>\"medium\"</code> | <code>\"long\"</code> <code>\"medium\"</code> Target answer length"},{"location":"deep-dives/synthetic/document-qa/#quality-control-parameters","title":"Quality Control Parameters","text":"Parameter Type Default Description <code>validation_threshold</code> <code>float</code> <code>0.8</code> Minimum quality score (0.0-1.0) <code>max_reflection_iterations</code> <code>int</code> <code>3</code> Maximum improvement iterations (1-10)"},{"location":"deep-dives/synthetic/document-qa/#customization-parameters","title":"Customization Parameters","text":"Parameter Type Default Description <code>custom_guidelines</code> <code>str</code> <code>None</code> Additional generation instructions <code>example_question</code> <code>str</code> <code>None</code> Example question for style guidance <code>example_answer</code> <code>str</code> <code>None</code> Example answer for style guidance"},{"location":"deep-dives/synthetic/document-qa/#quality-validation","title":"Quality Validation","text":""},{"location":"deep-dives/synthetic/document-qa/#validation-dimensions","title":"Validation Dimensions","text":"<p>The system evaluates QA pairs across five dimensions:</p> <ol> <li>Accuracy - Is the answer factually correct and well-grounded?</li> <li>Completeness - Does it fully address the question?</li> <li>Relevance - Is it directly aligned with the question?</li> <li>Clarity - Is the language clear and understandable?</li> <li>Factual Integrity - Does it avoid hallucination or extraneous information?</li> </ol>"},{"location":"deep-dives/synthetic/document-qa/#reflection-process","title":"Reflection Process","text":"<p>When QA pairs fall below the validation threshold:</p> <ol> <li>Issue Identification - Low-quality pairs are analyzed</li> <li>Feedback Generation - Specific improvement suggestions are created</li> <li>Prompt Enhancement - Generation prompts are refined with feedback</li> <li>Regeneration - Questions and answers are recreated with improvements</li> <li>Re-validation - Quality is reassessed until threshold is met</li> </ol>"},{"location":"deep-dives/synthetic/document-qa/#integration-with-evaluation","title":"Integration with Evaluation","text":"<p>This system is built into the <code>Dataset</code> class within Axion.</p> <pre><code>from axion.synthetic.schema import GenerationParams\nfrom axion.dataset import Dataset\n\ndataset = Dataset(name='Product-Growth-Knowledge')\n\nparams = GenerationParams(\n    num_pairs=1,\n    question_types=[\"factual\", \"conceptual\", \"application\"],\n    difficulty=\"medium\",\n    max_chunk_size=4000,\n    statements_per_chunk=5,\n    answer_length=\"medium\",\n    splitter_type=\"sentence\",\n    custom_guidelines=\"Focus on application scenarios in the questions.\",\n    max_reflection_iterations=3,\n    validation_threshold=0.7\n)\n\ndataset.synthetic_generate_from_directory(\n    directory_path='small_docs/',\n    llm=llm,\n    params=params\n)\n</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/synthetic/document-qa/#axion.synthetic.document_generator","title":"<code>axion.synthetic.document_generator</code>","text":""},{"location":"deep-dives/synthetic/document-qa/#axion.synthetic.document_generator.DocumentQAGenerator","title":"<code>DocumentQAGenerator(llm: LLMRunnable, params: GenerationParams, embed_model: EmbeddingRunnable = None, max_concurrent: int = 5, show_progress: bool = True, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>Orchestrates QA pair generation from multiple documents concurrently.</p> <p>Initialize DocumentQAGenerator</p> <p>Parameters:</p> <ul> <li> <code>llm</code>               (<code>LLMRunnable</code>)           \u2013            <p>The language model to use for generation.</p> </li> <li> <code>params</code>               (<code>GenerationParams</code>)           \u2013            <p>A GenerationParams object with all configuration.</p> </li> <li> <code>embed_model</code>               (<code>EmbeddingRunnable</code>, default:                   <code>None</code> )           \u2013            <p>An embedding model used for semantic parsing.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Max concurrent retrievers</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show progress bars using tqdm</p> </li> </ul>"},{"location":"deep-dives/synthetic/document-qa/#axion.synthetic.document_generator.DocumentQAGenerator.generate_from_directory","title":"<code>generate_from_directory(directory_path: str) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Main entry point. Loads docs from a directory and generates QA pairs. Args:     directory_path: The path to the directory containing documents. Returns:     A list of all generated QA pairs.</p>"},{"location":"deep-dives/synthetic/document-qa/#axion.synthetic.document_generator.DocumentQAGenerator.to_items","title":"<code>to_items(results: List[Any]) -&gt; List</code>","text":"<p>Converts a list of QA evaluation results into a List of <code>DatasetItems</code>.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List</code> (              <code>List</code> )          \u2013            <p>A list of DatasetItem objects.</p> </li> </ul>"},{"location":"deep-dives/synthetic/document-qa/#axion.synthetic.document_generator.DocumentQAGenerator.to_dataset","title":"<code>to_dataset(results: List[Any], dataset_name: str)</code>","text":"<p>Converts a list of QA evaluation results into a structured <code>Dataset</code> object.</p> <p>The function extracts these pairs, renames the fields to match internal <code>FieldNames</code> standards, and wraps each into a <code>DatasetItem</code>. These are then collected into a <code>Dataset</code> for downstream evaluation or analysis.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name to assign to the resulting Dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>          \u2013            <p>A structured Dataset containing DatasetItems with standardized field names.</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get started with Axion in minutes.</p>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/ax-foundry/axion.git\ncd axion\n\n# Create virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: .\\venv\\Scripts\\activate\n\n# Install the package\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing or running tests:</p> <pre><code># Install with dev dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Axion uses optional dependencies to keep the core installation lightweight. Install extras based on what you need:</p>"},{"location":"getting-started/installation/#tracing-providers","title":"Tracing Providers","text":"<pre><code># Logfire (OpenTelemetry-based)\npip install -e \".[logfire]\"\n\n# Langfuse (LLM-specific observability)\npip install -e \".[langfuse]\"\n\n# Opik\npip install -e \".[opik]\"\n\n# All tracing providers\npip install -e \".[tracing]\"\n</code></pre>"},{"location":"getting-started/installation/#search-integrations","title":"Search Integrations","text":"<pre><code># Google Search via SerpAPI\npip install -e \".[search]\"\n</code></pre> <p>Requires <code>SERPAPI_KEY</code> environment variable.</p>"},{"location":"getting-started/installation/#llamaindex-extensions","title":"LlamaIndex Extensions","text":"<pre><code># HuggingFace embeddings and LLMs\npip install -e \".[huggingface]\"\n\n# Docling document reader (PDF, DOCX, HTML, images)\npip install -e \".[docling]\"\n</code></pre>"},{"location":"getting-started/installation/#visualization","title":"Visualization","text":"<pre><code># Matplotlib and Seaborn for plotting\npip install -e \".[plotting]\"\n</code></pre>"},{"location":"getting-started/installation/#combining-extras","title":"Combining Extras","text":"<p>Install multiple extras at once:</p> <pre><code># Example: search + tracing + plotting\npip install -e \".[search,tracing,plotting]\"\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># Required for LLM-based metrics\nOPENAI_API_KEY=&lt;your-key&gt;\n\n# Optional: Logging settings\nLOG_LEVEL=\"INFO\"\nLOG_RICH=\"true\"\n\n# Optional: Tracing (auto-detects if credentials present)\nTRACING_MODE=\"langfuse\"  # or: noop, logfire, otel, opik\nLANGFUSE_SECRET_KEY=&lt;your-key&gt;\nLANGFUSE_PUBLIC_KEY=&lt;your-key&gt;\n</code></pre>"},{"location":"getting-started/installation/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Use <code>axion.init()</code> to configure both logging and tracing at once:</p> <pre><code>import axion\n\n# Initialize with custom settings\naxion.init(\n    tracing='langfuse',  # or: noop, logfire, otel, opik\n    log_level='DEBUG',\n    log_rich=True,\n)\n\n# Or just let it auto-configure from environment variables\n# (no init() call needed - works automatically)\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from axion import Dataset, metric_registry\n\n# Check available metrics\nprint(metric_registry.list_metrics())\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Working with Datasets - Learn how to create evaluation datasets</li> <li>Metrics &amp; Evaluation - Understand the metrics system</li> <li>Agent Evaluation Playbook - Best practices for agent evaluation</li> </ul>"},{"location":"guides/aligneval/","title":"AlignEval","text":"<p>AlignEval is a tool for calibrating LLM-as-a-judge evaluators against a human-labeled baseline. Instead of writing criteria in the abstract, you work backward from real outputs so the evaluator learns what actually matters for your use case.</p>"},{"location":"guides/aligneval/#core-philosophy-calibrate-dont-just-create","title":"Core Philosophy: Calibrate, Don\u2019t Just Create","text":"<p>Aligning AI to human preferences is only half the battle. The other half is calibrating your criteria to the model\u2019s output distribution. Teams often write elaborate rubrics without looking at the data first, which produces criteria that are irrelevant or unrealistic. AlignEval is built to keep the data in the loop.</p> <p>Inspiration for AlignEval comes from eugeneyan\u2019s aligneval.</p>"},{"location":"guides/aligneval/#workflow","title":"Workflow","text":"<ol> <li>Prepare Data: Start with a dataset of inputs and generated outputs.</li> <li>Annotate: Add a human judgment (pass/fail) to each item.</li> <li>Configure &amp; Execute: Define the LLM-as-a-judge instructions and run.</li> <li>Analyze: Compare LLM scores to the human baseline to find gaps.</li> </ol>"},{"location":"guides/aligneval/#using-aligneval-in-python","title":"Using AlignEval in Python","text":"<p>AlignEval is designed for programmatic workflows and Jupyter notebooks. It includes an interactive annotation flow and rich analysis outputs.</p>"},{"location":"guides/aligneval/#step-1-create-a-dataset","title":"Step 1: Create a Dataset","text":"<pre><code>from axion.dataset import Dataset, DatasetItem\n\nitems = [\n    DatasetItem(\n        id=\"item-1\",\n        query=\"What is the capital of France?\",\n        expected_output=\"Paris\",\n        actual_output=\"Paris.\",\n    ),\n    DatasetItem(\n        id=\"item-2\",\n        query=\"What is 2+2?\",\n        expected_output=\"4\",\n        actual_output=\"5\",\n    ),\n]\n\ndataset = Dataset(items=items)\n</code></pre>"},{"location":"guides/aligneval/#step-2-define-a-metric","title":"Step 2: Define a Metric","text":"<p>Define an LLM-as-a-judge metric with a clear instruction. The metric can be as simple as a binary pass/fail rubric.</p> <pre><code>from axion.metrics.base import BaseMetric\n\nclass PassFailMetric(BaseMetric):\n    instruction = (\n        \"Score 1 if the answer is correct and complete. \"\n        \"Otherwise score 0 and explain why.\"\n    )\n</code></pre>"},{"location":"guides/aligneval/#step-3-run-aligneval","title":"Step 3: Run AlignEval","text":"<pre><code>from axion.align import AlignEval\n\nevaluator = AlignEval(dataset, PassFailMetric())\nevaluator.annotate()  # optional if judgments are already present\nresults_df = evaluator.execute()\n</code></pre>"},{"location":"guides/aligneval/#ui-friendly-outputs","title":"UI-Friendly Outputs","text":"<p>For web or UI pipelines, use <code>WebAlignEval</code> to return JSON-serializable results and attach optional progress callbacks. Both <code>AlignEval</code> and <code>WebAlignEval</code> support <code>execute(as_dict=...)</code> for a shared interface.</p> <pre><code>from axion.align import WebAlignEval\n\nweb_eval = WebAlignEval(dataset, PassFailMetric())\npayload = web_eval.execute(\n    as_dict=True,\n    on_progress=lambda current, total: print(current, total),\n)\n\n# payload[\"results\"] -&gt; list of row dicts\n# payload[\"metrics\"] -&gt; summary metrics\n# payload[\"confusion_matrix\"] -&gt; confusion matrix dict\n</code></pre> <p>You can also construct a dataset from uploaded records:</p> <p><pre><code>web_eval = WebAlignEval.from_records(records, PassFailMetric())\npayload = web_eval.execute(as_dict=True)\n\n## Renderers\n\nAlignEval uses a renderer interface so UIs can plug in without changing\ncore logic. Out of the box you get:\n\n- `NotebookAlignEvalRenderer` for notebooks (Jupyter)\n- `ConsoleAlignEvalRenderer` for terminal usage\n- `JsonAlignEvalRenderer` for JSON-first workflows\n\n```python\nfrom axion.align import AlignEval, ConsoleAlignEvalRenderer\n\nevaluator = AlignEval(dataset, PassFailMetric(), renderer=ConsoleAlignEvalRenderer())\nevaluator.execute()\n</code></pre> ```</p>"},{"location":"guides/datasets/","title":"Working with Datasets","text":"<p>Axion uses <code>Dataset</code> and <code>DatasetItem</code> classes to manage evaluation data for both single-turn and multi-turn conversations.</p>"},{"location":"guides/datasets/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset, DatasetItem\n\n# Create a single evaluation item\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"The capital of France is Paris.\",\n    expected_output=\"Paris is the capital of France.\",\n    retrieved_content=[\"France is a country in Europe. Paris is its capital.\"]\n)\n\n# Create a dataset\ndataset = Dataset(items=[item])\n</code></pre>"},{"location":"guides/datasets/#datasetitem-fields","title":"DatasetItem Fields","text":"Field Description Required <code>query</code> The user's question or input Yes <code>actual_output</code> The agent's response Yes <code>expected_output</code> Ground truth / expected answer For some metrics <code>retrieved_content</code> Retrieved context (for RAG) For retrieval metrics <code>conversation</code> Multi-turn conversation history For conversational metrics <code>tools_called</code> Tools/functions called by agent For tool metrics <code>expected_tools</code> Expected tool calls For tool metrics"},{"location":"guides/datasets/#loading-datasets","title":"Loading Datasets","text":"<pre><code># From CSV file\ndataset = Dataset.from_csv(\"eval_data.csv\")\n\n# From pandas DataFrame\ndataset = Dataset.from_dataframe(df)\n\n# From list of dicts\ndataset = Dataset.from_records([\n    {\"query\": \"...\", \"actual_output\": \"...\"},\n    {\"query\": \"...\", \"actual_output\": \"...\"}\n])\n</code></pre>"},{"location":"guides/datasets/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<pre><code>item = DatasetItem(\n    conversation=[\n        {\"role\": \"user\", \"content\": \"Hello, I need help with my order\"},\n        {\"role\": \"assistant\", \"content\": \"I'd be happy to help! What's your order number?\"},\n        {\"role\": \"user\", \"content\": \"It's #12345\"},\n        {\"role\": \"assistant\", \"content\": \"I found your order. How can I assist?\"}\n    ]\n)\n</code></pre>"},{"location":"guides/datasets/#serialization","title":"Serialization","text":"<pre><code># Save to JSON\ndataset.to_json(\"output.json\")\n\n# Convert to DataFrame\ndf = dataset.to_dataframe()\n</code></pre>"},{"location":"guides/datasets/#next-steps","title":"Next Steps","text":"<ul> <li>Metrics &amp; Evaluation - Learn how to evaluate your dataset</li> <li>API Reference: Dataset - Full API documentation</li> </ul>"},{"location":"guides/evaluation/","title":"Running Evaluations","text":"<p>Axion provides evaluation runners for batch processing with caching and parallel execution.</p>"},{"location":"guides/evaluation/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset\nfrom axion.metrics import Faithfulness, AnswerRelevancy\nfrom axion.runners import evaluation_runner\n\n# Load dataset\ndataset = Dataset.from_csv(\"eval_data.csv\")\n\n# Run evaluation\nresults = await evaluation_runner(\n    dataset=dataset,\n    metrics=[Faithfulness(), AnswerRelevancy()]\n)\n\n# View results\ndf = results.to_dataframe()\nresults.to_scorecard(display_in_notebook=True)\n</code></pre>"},{"location":"guides/evaluation/#evaluation-runners","title":"Evaluation Runners","text":""},{"location":"guides/evaluation/#evaluation_runner","title":"evaluation_runner","text":"<p>The main entry point for running evaluations:</p> <pre><code>from axion.runners import evaluation_runner, EvaluationConfig\n\nconfig = EvaluationConfig(\n    max_concurrency=10,\n    cache_enabled=True,\n    cache_dir=\".cache/evaluations\"\n)\n\nresults = await evaluation_runner(\n    dataset=dataset,\n    metrics=metrics,\n    config=config\n)\n</code></pre>"},{"location":"guides/evaluation/#metricrunner","title":"MetricRunner","text":"<p>For running individual metrics with more control:</p> <pre><code>from axion.runners import MetricRunner\n\nrunner = MetricRunner(metric=Faithfulness())\nresult = await runner.run(dataset_item)\n</code></pre>"},{"location":"guides/evaluation/#caching","title":"Caching","text":"<p>Avoid re-running expensive LLM evaluations:</p> <pre><code>from axion._core.cache import CacheManager, CacheConfig\n\ncache = CacheManager(CacheConfig(\n    cache_type=\"disk\",\n    cache_dir=\".cache/metrics\"\n))\n\n# Results are cached by input hash\nresults = await evaluation_runner(dataset, metrics, cache=cache)\n</code></pre>"},{"location":"guides/evaluation/#understanding-results","title":"Understanding Results","text":"<pre><code># Convert to DataFrame for analysis\ndf = results.to_dataframe()\n\n# Generate detailed metric summary report\nfrom axion.runners.summary import MetricSummary\nMetricSummary().execute(results.results, total_time=100)\n\n# Per-test results\nfor test_result in results.results:\n    for score in test_result.score_results:\n        print(f\"{score.name}: {score.score}\")\n\n# Find failures\ndf[df['metric_score'] &lt; 0.5]\n\n# Visual scorecard (Jupyter notebooks)\nresults.to_scorecard(display_in_notebook=True)\n\n# Latency analysis\nresults.to_latency_plot()\n</code></pre>"},{"location":"guides/evaluation/#summary-classes","title":"Summary Classes","text":"<p>Axion provides summary classes for different reporting needs:</p> Class Description <code>MetricSummary</code> Detailed metric analysis with performance insights and distribution charts <code>SimpleSummary</code> High-level KPIs and business impact dashboard <code>HierarchicalSummary</code> Summary for hierarchical evaluation trees <pre><code>from axion.runners.summary import MetricSummary, SimpleSummary\n\n# Detailed analysis\nMetricSummary(show_distribution=True).execute(results.results, total_time=100)\n\n# Simple KPI dashboard\nSimpleSummary().execute(results.results, total_time=100)\n</code></pre>"},{"location":"guides/evaluation/#best-practices","title":"Best Practices","text":"<ol> <li>Start small - Test with a few items before running full dataset</li> <li>Enable caching - Avoid re-running on unchanged inputs</li> <li>Use appropriate concurrency - Balance speed vs. API rate limits</li> <li>Review failures - Low scores need human analysis, not just numbers</li> </ol>"},{"location":"guides/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Metric Runner Deep Dive - Advanced runner usage</li> <li>Evaluation Runner Deep Dive - Configuration options</li> <li>API Reference: Runners - Full API documentation</li> </ul>"},{"location":"guides/hierarchical-scoring/","title":"Hierarchical Scoring","text":"<p>Axion's hierarchical scoring framework moves beyond flat metrics to provide a diagnostic map of AI quality\u2014from a single overall score down into layered, weighted dimensions.</p>"},{"location":"guides/hierarchical-scoring/#why-hierarchical-scoring","title":"Why Hierarchical Scoring?","text":"<p>Traditional evaluation gives you a single number. That's not enough.</p> <p>When an AI agent scores 0.72, what does that mean? Is it struggling with relevance? Accuracy? Tone? Without structure, you're left guessing.</p> <p>Hierarchical scoring solves this by:</p> <ul> <li>Breaking quality into meaningful dimensions</li> <li>Weighting each dimension to reflect business priorities</li> <li>Enabling drill-down from overall score to root cause</li> </ul> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Overall Score  \u2502\n                    \u2502      0.82       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u25bc                  \u25bc                  \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  Relevance  \u2502    \u2502  Accuracy   \u2502    \u2502    Tone     \u2502\n   \u2502    0.91     \u2502    \u2502    0.78     \u2502    \u2502    0.85     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/hierarchical-scoring/#key-advantages","title":"Key Advantages","text":"Advantage Description Instant Root Cause Diagnosis Drill down to pinpoint whether issues stem from relevance, accuracy, tone, or other dimensions Strategic Prioritization Forces clarity on what matters\u2014break quality into layers that reflect business value Actionable Feedback Loop Each layer maps to specific actions: retraining, prompt adjustments, alignment tuning Customizable to Business Goals Weight dimensions to match your KPIs\u2014define what \"good AI\" means for you Scales with Complexity Add dimensions as use cases grow while keeping evaluation consistent"},{"location":"guides/hierarchical-scoring/#quick-start","title":"Quick Start","text":"<pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy, Faithfulness, FactualAccuracy\nfrom axion.dataset import DatasetItem\n\n# Define your scoring hierarchy\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n        'Faithfulness': Faithfulness(),\n        'Accuracy': FactualAccuracy(),\n    },\n    'model': {\n        'ANSWER_QUALITY': {\n            'Relevance': 0.4,\n            'Faithfulness': 0.3,\n            'Accuracy': 0.3,\n        },\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\n# Create evaluation data\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output=\"Click 'Forgot Password' on the login page, enter your email, and follow the reset link sent to your inbox.\",\n    retrieved_content=[\"Password reset is available via the login page. Users receive a reset link by email.\"]\n)\n\n# Run hierarchical evaluation\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=config,\n    evaluation_name=\"RAG Quality Evaluation\"\n)\n\n# View results\nresults.to_dataframe()\n\n# Generate visual scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"guides/hierarchical-scoring/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/hierarchical-scoring/#option-1-python-dictionary","title":"Option 1: Python Dictionary","text":"<p>Define your hierarchy directly in code:</p> <pre><code>config = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n        'Faithfulness': Faithfulness(),\n    },\n    'model': {\n        'RESPONSE_QUALITY': {\n            'Relevance': 0.6,\n            'Faithfulness': 0.4,\n        },\n    },\n    'weights': {\n        'RESPONSE_QUALITY': 1.0,\n    }\n}\n</code></pre>"},{"location":"guides/hierarchical-scoring/#option-2-yaml-configuration","title":"Option 2: YAML Configuration","text":"<p>For version-controlled, shareable configs:</p> <pre><code># config.yaml\nmetric:\n  Relevance:\n    class: 'answer_relevancy'\n    metric_name: 'Relevancy'\n    model_name: 'gpt-4.1'\n  Faithfulness:\n    class: 'faithfulness'\n\nmodel:\n  RESPONSE_QUALITY:\n    Relevance: 0.6\n    Faithfulness: 0.4\n\nweights:\n  RESPONSE_QUALITY: 1.0\n</code></pre> <pre><code># Load from file\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=\"config.yaml\",\n)\n</code></pre>"},{"location":"guides/hierarchical-scoring/#multi-level-hierarchies","title":"Multi-Level Hierarchies","text":"<p>Build deeper hierarchies for complex evaluation needs:</p> <pre><code>config = {\n    'metric': {\n        'Relevance': AnswerRelevancy(),\n        'Faithfulness': Faithfulness(),\n        'Accuracy': FactualAccuracy(),\n        'Completeness': AnswerCompleteness(),\n        'Tone': ToneConsistency(),\n    },\n    'model': {\n        # First level: group metrics into categories\n        'GROUNDING': {\n            'Faithfulness': 0.6,\n            'Accuracy': 0.4,\n        },\n        'RESPONSE': {\n            'Relevance': 0.5,\n            'Completeness': 0.3,\n            'Tone': 0.2,\n        },\n    },\n    'weights': {\n        # Second level: weight the categories\n        'GROUNDING': 0.5,\n        'RESPONSE': 0.5,\n    }\n}\n</code></pre> <p>This produces:</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Overall Score  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc                             \u25bc\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502  GROUNDING  \u2502               \u2502  RESPONSE   \u2502\n       \u2502   (50%)     \u2502               \u2502   (50%)     \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                             \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u25bc             \u25bc            \u25bc         \u25bc         \u25bc\n  Faithfulness   Accuracy    Relevance  Complete   Tone\n    (60%)         (40%)       (50%)      (30%)    (20%)\n</code></pre>"},{"location":"guides/hierarchical-scoring/#accessing-results","title":"Accessing Results","text":"<pre><code># Overall score\nprint(results.overall_score)\n\n# Category scores\nfor category, score in results.category_scores.items():\n    print(f\"{category}: {score}\")\n\n# Individual metric scores\nfor metric_name, score in results.metric_scores.items():\n    print(f\"{metric_name}: {score}\")\n\n# Export to DataFrame for analysis\ndf = results.to_dataframe()\n\n# Generate visual scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"guides/hierarchical-scoring/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple \u2014 Begin with 2-3 key dimensions, expand as needed</li> <li>Align to Business \u2014 Weights should reflect actual business priorities</li> <li>Review Failures \u2014 Low-scoring dimensions indicate where to focus improvement</li> <li>Iterate \u2014 Refine weights based on correlation with real-world outcomes</li> <li>Version Control \u2014 Use YAML configs to track scoring criteria over time</li> </ol>"},{"location":"guides/hierarchical-scoring/#next-steps","title":"Next Steps","text":"<ul> <li>Metrics Reference \u2014 Full catalog of available metrics</li> <li>Creating Custom Metrics \u2014 Build domain-specific metrics</li> <li>Eval Tree API \u2014 API reference for hierarchical evaluation</li> </ul>"},{"location":"guides/llm-providers/","title":"LLM Providers","text":"<p>Axion provides a unified interface for working with multiple LLM providers through the <code>LLMRegistry</code>. This guide covers using built-in providers and registering custom ones.</p>"},{"location":"guides/llm-providers/#quick-start","title":"Quick Start","text":"<pre><code>from axion.llm_registry import LLMRegistry\n\n# Create a registry locked to a provider\nregistry = LLMRegistry(provider='openai')\nllm = registry.get_llm('gpt-4o')\n\n# Or use flexible mode to switch providers\nregistry = LLMRegistry()\nllm_openai = registry.get_llm('gpt-4o', provider='openai')\nllm_claude = registry.get_llm('claude-3-5-sonnet-20241022', provider='anthropic')\n</code></pre>"},{"location":"guides/llm-providers/#built-in-providers","title":"Built-in Providers","text":"Provider Models Embeddings Auth <code>openai</code> GPT-4o, GPT-4, o1, o3, etc. Yes <code>OPENAI_API_KEY</code> <code>anthropic</code> Claude 3.5, Claude 3, etc. No <code>ANTHROPIC_API_KEY</code> <code>gemini</code> Gemini 1.5 Pro/Flash, etc. Yes <code>GOOGLE_API_KEY</code> <code>vertex_ai</code> Gemini, Claude on Vertex Yes GCP Service Account <code>huggingface</code> Any HF Hub model Yes <code>HF_TOKEN</code> (optional)"},{"location":"guides/llm-providers/#openai","title":"OpenAI","text":"<pre><code>registry = LLMRegistry(provider='openai')\n\n# LLM\nllm = registry.get_llm('gpt-4o')\nllm = registry.get_llm('gpt-4o-mini', temperature=0.7)\nllm = registry.get_llm('o1-preview')\n\n# Embeddings\nembedding = registry.get_embedding_model('text-embedding-3-small')\n</code></pre>"},{"location":"guides/llm-providers/#anthropic","title":"Anthropic","text":"<pre><code>registry = LLMRegistry(provider='anthropic')\n\n# LLM - automatically prefixed with 'anthropic/' for LiteLLM\nllm = registry.get_llm('claude-3-5-sonnet-20241022')\nllm = registry.get_llm('claude-3-opus-20240229')\n\n# Embeddings - NOT supported (use OpenAI or HuggingFace)\n# registry.get_embedding_model()  # Raises NotImplementedError\n</code></pre>"},{"location":"guides/llm-providers/#gemini","title":"Gemini","text":"<pre><code>registry = LLMRegistry(provider='gemini')\n\n# LLM - automatically prefixed with 'gemini/' for LiteLLM\nllm = registry.get_llm('gemini-1.5-pro')\nllm = registry.get_llm('gemini-1.5-flash')\n\n# Embeddings\nembedding = registry.get_embedding_model('models/embedding-001')\n</code></pre>"},{"location":"guides/llm-providers/#vertex-ai","title":"Vertex AI","text":"<pre><code># Requires GCP service account authentication\nregistry = LLMRegistry(\n    provider='vertex_ai',\n    vertex_project='my-gcp-project',\n    vertex_location='us-central1',\n    vertex_credentials='/path/to/service-account.json'\n)\n\n# Or use environment variables:\n# VERTEXAI_PROJECT, VERTEXAI_LOCATION, GOOGLE_APPLICATION_CREDENTIALS\n\n# LLM - automatically prefixed with 'vertex_ai/' for LiteLLM\nllm = registry.get_llm('gemini-1.5-pro')\n\n# Embeddings\nembedding = registry.get_embedding_model('text-embedding-004')\n</code></pre>"},{"location":"guides/llm-providers/#huggingface","title":"HuggingFace","text":"<pre><code>registry = LLMRegistry(provider='huggingface')\n\n# LLM - uses LlamaIndex for local inference (not LiteLLM)\nllm = registry.get_llm('meta-llama/Llama-2-7b-chat-hf')\n\n# Embeddings\nembedding = registry.get_embedding_model('BAAI/bge-small-en-v1.5')\n</code></pre> <p>HuggingFace Requirements</p> <p>HuggingFace provider requires <code>torch</code> and <code>transformers</code> to be installed. Models run locally, not via API.</p>"},{"location":"guides/llm-providers/#registering-a-custom-provider","title":"Registering a Custom Provider","text":"<p>To add a new provider, create a class that inherits from <code>BaseProvider</code> and use the <code>@LLMRegistry.register()</code> decorator.</p>"},{"location":"guides/llm-providers/#basic-structure","title":"Basic Structure","text":"<pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry, LiteLLMWrapper\n\n@LLMRegistry.register('my_provider')\nclass MyProvider(BaseProvider):\n    \"\"\"Custom provider for My LLM Service.\"\"\"\n\n    # LiteLLM prefix for routing (e.g., 'anthropic/', 'gemini/')\n    LITELLM_PREFIX = 'my_provider/'\n\n    # Whether this provider supports embedding models\n    SUPPORTS_EMBEDDINGS = True\n\n    def __init__(self, api_key: Optional[str] = None, **credentials):\n        super().__init__(api_key=api_key, **credentials)\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        \"\"\"Create an embedding model client.\"\"\"\n        # Return your embedding model instance\n        pass\n</code></pre>"},{"location":"guides/llm-providers/#example-custom-openai-compatible-endpoint","title":"Example: Custom OpenAI-Compatible Endpoint","text":"<pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry, LiteLLMWrapper\n\n@LLMRegistry.register('custom_openai')\nclass CustomOpenAIProvider(BaseProvider):\n    \"\"\"Provider for OpenAI-compatible endpoints (e.g., vLLM, LocalAI).\"\"\"\n\n    LITELLM_PREFIX = 'openai/'  # Use OpenAI routing\n    SUPPORTS_EMBEDDINGS = False\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        **credentials\n    ):\n        super().__init__(api_key=api_key, **credentials)\n        self.api_base = api_base\n\n    def _create_litellm_wrapper(self, model_name: str, **kwargs) -&gt; LiteLLMWrapper:\n        \"\"\"Override to add custom base URL.\"\"\"\n        if self.LITELLM_PREFIX and not model_name.startswith(self.LITELLM_PREFIX):\n            model_name = f'{self.LITELLM_PREFIX}{model_name}'\n\n        temperature = kwargs.pop('temperature', 0.0)\n\n        return LiteLLMWrapper(\n            model=model_name,\n            provider='custom_openai',\n            temperature=temperature,\n            api_key=self.api_key,\n            api_base=self.api_base,\n            **kwargs\n        )\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        raise NotImplementedError('This provider does not support embeddings.')\n\n\n# Usage\nregistry = LLMRegistry(\n    provider='custom_openai',\n    api_key='your-api-key',\n    api_base='http://localhost:8000/v1'\n)\nllm = registry.get_llm('my-local-model')\n</code></pre>"},{"location":"guides/llm-providers/#example-non-litellm-backend","title":"Example: Non-LiteLLM Backend","text":"<p>For providers that don't use LiteLLM (like HuggingFace local inference), override <code>create_llm()</code>:</p> <pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry\n\n@LLMRegistry.register('ollama')\nclass OllamaProvider(BaseProvider):\n    \"\"\"Provider for Ollama local models.\"\"\"\n\n    LITELLM_PREFIX = ''\n    SUPPORTS_EMBEDDINGS = True\n\n    def __init__(self, base_url: str = 'http://localhost:11434', **credentials):\n        super().__init__(api_key=None, **credentials)\n        self.base_url = base_url\n\n    def create_llm(self, model_name: str, **kwargs) -&gt; Any:\n        \"\"\"Override to use Ollama-specific client instead of LiteLLM.\"\"\"\n        from llama_index.llms.ollama import Ollama\n\n        return Ollama(\n            model=model_name,\n            base_url=self.base_url,\n            **kwargs\n        )\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        from llama_index.embeddings.ollama import OllamaEmbedding\n\n        return OllamaEmbedding(\n            model_name=model_name,\n            base_url=self.base_url,\n            **kwargs\n        )\n\n\n# Usage\nregistry = LLMRegistry(provider='ollama')\nllm = registry.get_llm('llama2')\n</code></pre>"},{"location":"guides/llm-providers/#provider-class-attributes","title":"Provider Class Attributes","text":"Attribute Type Description <code>LITELLM_PREFIX</code> <code>str</code> Prefix added to model names for LiteLLM routing (e.g., <code>'anthropic/'</code>) <code>SUPPORTS_EMBEDDINGS</code> <code>bool</code> Whether the provider offers embedding models (default: <code>True</code>)"},{"location":"guides/llm-providers/#key-methods-to-implement","title":"Key Methods to Implement","text":"Method Required Description <code>__init__()</code> Yes Initialize with credentials <code>create_embedding_model()</code> Yes Return embedding model instance (or raise <code>NotImplementedError</code>) <code>create_llm()</code> No Override only for non-LiteLLM backends <code>_create_litellm_wrapper()</code> No Override to customize LiteLLM wrapper creation"},{"location":"guides/llm-providers/#cost-estimation","title":"Cost Estimation","text":"<p>Add custom model pricing to the cost estimator:</p> <pre><code>from axion.llm_registry import LLMCostEstimator\n\n# Add custom model pricing (price per token)\nLLMCostEstimator.add_custom_model(\n    model='my-custom-model',\n    input_price_per_token=0.001 / 1000,   # $0.001 per 1K tokens\n    output_price_per_token=0.002 / 1000   # $0.002 per 1K tokens\n)\n\n# Estimate cost\ncost = LLMCostEstimator.estimate(\n    model_name='my-custom-model',\n    prompt_tokens=1000,\n    completion_tokens=500\n)\n</code></pre>"},{"location":"guides/llm-providers/#view-registered-providers","title":"View Registered Providers","text":"<pre><code>from axion.llm_registry import LLMRegistry\n\n# Display all registered providers with details\nLLMRegistry.display()\n</code></pre>"},{"location":"guides/llm-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Environment Configuration - Configure API keys and settings</li> <li>Tracing - Monitor LLM calls with observability</li> <li>API Reference: LLM Registry - Full API documentation</li> </ul>"},{"location":"guides/metrics/","title":"Metrics &amp; Evaluation","text":"<p>Axion provides 30+ metrics for evaluating AI agents across multiple dimensions.</p>"},{"location":"guides/metrics/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset, metric_registry\nfrom axion.metrics import Faithfulness, AnswerRelevancy\n\n# Load your dataset\ndataset = Dataset.from_csv(\"eval_data.csv\")\n\n# Select metrics\nmetrics = [Faithfulness(), AnswerRelevancy()]\n\n# Run evaluation\nfrom axion.runners import evaluation_runner\nresults = await evaluation_runner(dataset, metrics)\n</code></pre>"},{"location":"guides/metrics/#metric-categories","title":"Metric Categories","text":""},{"location":"guides/metrics/#composite-metrics-llm-based","title":"Composite Metrics (LLM-based)","text":"<p>For nuanced evaluation requiring reasoning:</p> Metric What it Measures <code>Faithfulness</code> Is the answer grounded in retrieved context? <code>AnswerRelevancy</code> Does the answer address the question? <code>FactualAccuracy</code> Is the answer factually correct vs. expected? <code>AnswerCompleteness</code> Are all parts of the question answered? <code>AnswerCriteria</code> Does the answer meet specific business rules?"},{"location":"guides/metrics/#heuristic-metrics-non-llm","title":"Heuristic Metrics (Non-LLM)","text":"<p>Fast, deterministic checks:</p> Metric What it Measures <code>ExactStringMatch</code> Exact match between actual and expected <code>CitationPresence</code> Are citations/references present? <code>Latency</code> Response time (pass/fail threshold) <code>ContainsMatch</code> Does output contain required phrases?"},{"location":"guides/metrics/#retrieval-metrics","title":"Retrieval Metrics","text":"<p>For RAG pipeline evaluation:</p> Metric What it Measures <code>HitRateAtK</code> Is the right doc in top K results? <code>MeanReciprocalRank</code> Position of first relevant result <code>ContextualRelevancy</code> Are retrieved chunks relevant? <code>ContextualSufficiency</code> Do chunks contain the answer?"},{"location":"guides/metrics/#conversational-metrics","title":"Conversational Metrics","text":"<p>For multi-turn agents:</p> Metric What it Measures <code>GoalCompletion</code> Did user achieve their goal? <code>ConversationEfficiency</code> Were there unnecessary loops? <code>ConversationFlow</code> Is the dialogue logical?"},{"location":"guides/metrics/#using-the-metric-registry","title":"Using the Metric Registry","text":"<pre><code>from axion.metrics import metric_registry\n\n# List all available metrics\nprint(metric_registry.list_metrics())\n\n# Get metric by name\nmetric = metric_registry.get(\"Faithfulness\")\n\n# Filter by category\ncomposite_metrics = metric_registry.filter(category=\"composite\")\n</code></pre>"},{"location":"guides/metrics/#customizing-metrics","title":"Customizing Metrics","text":"<pre><code>from axion.metrics import Faithfulness\n\n# Adjust threshold\nmetric = Faithfulness(threshold=0.8)\n\n# Custom instructions\nmetric = AnswerCriteria(\n    criteria_key=\"my_criteria\",\n    scoring_strategy=\"aspect\"\n)\n</code></pre>"},{"location":"guides/metrics/#next-steps","title":"Next Steps","text":"<ul> <li>Running Evaluations - Scale up with evaluation runners</li> <li>Creating Custom Metrics - Build your own metrics</li> <li>API Reference: Metrics - Full API documentation</li> </ul>"},{"location":"guides/search/","title":"Search Integrations","text":"<p>Axion provides retriever implementations for popular search APIs to integrate into evaluation pipelines.</p>"},{"location":"guides/search/#available-retrievers","title":"Available Retrievers","text":"Retriever API Best For <code>GoogleRetriever</code> Google Custom Search Web search, broad coverage <code>TavilyRetriever</code> Tavily AI AI-optimized search results <code>YouRetriever</code> You.com Real-time web data"},{"location":"guides/search/#quick-start","title":"Quick Start","text":"<pre><code>from axion.search import GoogleRetriever\n\nretriever = GoogleRetriever(\n    api_key=\"your-serpapi-key\"\n)\n\nresults = await retriever.search(\"What is RAG in AI?\")\nfor result in results:\n    print(f\"{result.title}: {result.url}\")\n</code></pre>"},{"location":"guides/search/#google-search","title":"Google Search","text":"<pre><code>from axion.search import GoogleRetriever\n\nretriever = GoogleRetriever(\n    api_key=\"your-serpapi-key\",\n    num_results=10\n)\n\nresults = await retriever.search(query)\n</code></pre>"},{"location":"guides/search/#tavily-search","title":"Tavily Search","text":"<p>AI-optimized search with relevance filtering:</p> <pre><code>from axion.search import TavilyRetriever\n\nretriever = TavilyRetriever(\n    api_key=\"your-tavily-key\",\n    search_depth=\"advanced\"\n)\n\nresults = await retriever.search(query)\n</code></pre>"},{"location":"guides/search/#youcom-search","title":"You.com Search","text":"<p>Real-time web data with snippet extraction:</p> <pre><code>from axion.search import YouRetriever\n\nretriever = YouRetriever(\n    api_key=\"your-you-key\"\n)\n\nresults = await retriever.search(query)\n</code></pre>"},{"location":"guides/search/#using-with-evaluation","title":"Using with Evaluation","text":"<p>Combine retrievers with evaluation metrics:</p> <pre><code>from axion import DatasetItem\nfrom axion.metrics import ContextualRelevancy\n\n# Get retrieval results\nresults = await retriever.search(query)\ncontent = [r.snippet for r in results]\n\n# Create evaluation item\nitem = DatasetItem(\n    query=query,\n    actual_output=agent_response,\n    retrieved_content=content\n)\n\n# Evaluate retrieval quality\nmetric = ContextualRelevancy()\nscore = await metric.evaluate(item)\n</code></pre>"},{"location":"guides/search/#next-steps","title":"Next Steps","text":"<ul> <li>Google Search Deep Dive - Full configuration options</li> <li>Tavily Search Deep Dive - Advanced features</li> <li>API Reference: Search - Full API documentation</li> </ul>"},{"location":"guides/synthetic/","title":"Synthetic Data Generation","text":"<p>Generate evaluation datasets from documents or sessions to scale your test coverage.</p>"},{"location":"guides/synthetic/#overview","title":"Overview","text":"<p>Manual dataset curation is essential but doesn't scale. Axion provides synthetic generation methods powered by a graph-based workflow built on <code>pydantic_graph</code>:</p> <ol> <li>Document Q&amp;A Generation - Create question-answer pairs from your knowledge base</li> <li>Session-based Generation - Generate Q&amp;A from conversation transcripts</li> </ol>"},{"location":"guides/synthetic/#document-qa-generation","title":"Document Q&amp;A Generation","text":"<p>Generate evaluation data from your documents using <code>DocumentQAGenerator</code>:</p> <pre><code>from axion.synthetic import DocumentQAGenerator, GenerationParams\n\n# Initialize with your LLM\ngenerator = DocumentQAGenerator(\n    llm=your_llm,  # LLMRunnable-compatible object\n    params=GenerationParams(\n        num_pairs=10,\n        question_types=[\"factual\", \"conceptual\", \"application\"],\n        difficulty=\"medium\",\n        answer_length=\"medium\",\n        validation_threshold=0.7,\n    ),\n)\n\n# Generate from a directory of documents\nresults = await generator.generate_from_directory(\"./documents\")\n\n# Convert to Dataset for evaluation\ndataset = generator.to_dataset(results, dataset_name=\"my_synthetic_dataset\")\n</code></pre>"},{"location":"guides/synthetic/#direct-workflow-usage","title":"Direct Workflow Usage","text":"<p>For more control, use <code>QAWorkflowGraph</code> directly:</p> <pre><code>from axion.synthetic.workflow import QAWorkflowGraph\n\nworkflow = QAWorkflowGraph(llm=your_llm)\n\n# Run with document content\nresult = await workflow.run_from_documents(\n    content=\"Your document text here...\",\n    num_pairs=5,\n    question_types=[\"factual\", \"analytical\"],\n    difficulty=\"medium\",\n    splitter_type=\"sentence\",\n    chunk_size=2048,\n    statements_per_chunk=5,\n    validation_threshold=0.8,\n    max_reflection_iterations=3,\n)\n\n# Access results\nqa_pairs = result[\"validated_qa_pairs\"]\nstatements = result[\"statements\"]\nquality = result[\"average_quality\"]\n</code></pre>"},{"location":"guides/synthetic/#session-based-generation","title":"Session-based Generation","text":"<p>Generate Q&amp;A pairs from conversation transcripts:</p> <pre><code>result = await workflow.run_from_sessions(\n    session_messages=[\n        {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n        {\"role\": \"assistant\", \"content\": \"Go to Settings &gt; Security &gt; Reset Password\"},\n    ],\n    session_metadata={\"topic\": \"account_management\"},\n    num_pairs=3,\n)\n</code></pre>"},{"location":"guides/synthetic/#how-it-works","title":"How It Works","text":"<p>The workflow is implemented as a directed graph with the following nodes:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; InitializeWorkflow\n    InitializeWorkflow --&gt; ProcessDocuments: documents\n    InitializeWorkflow --&gt; ProcessSessions: sessions\n    ProcessDocuments --&gt; ChunkContent\n    ProcessSessions --&gt; ChunkContent\n    ChunkContent --&gt; ExtractStatements\n    ExtractStatements --&gt; GenerateQuestions\n    GenerateQuestions --&gt; GenerateAnswers\n    GenerateAnswers --&gt; ValidateQAPairs\n    ValidateQAPairs --&gt; PrepareEnhancement: quality below threshold\n    ValidateQAPairs --&gt; [*]: quality met or max iterations\n    PrepareEnhancement --&gt; GenerateQuestions</code></pre>"},{"location":"guides/synthetic/#pipeline-steps","title":"Pipeline Steps","text":"Step Description Initialize Validate inputs and configure processors Process Apply transformations to documents or sessions Chunk Split content using sentence or semantic chunking Extract Statements Parse content into factual claims with source indices Generate Questions Create diverse questions from statements Generate Answers Create ground-truth answers from source content Validate Score Q&amp;A pairs and filter low-quality results Enhance Iteratively improve questions below threshold"},{"location":"guides/synthetic/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/synthetic/#generationparams","title":"GenerationParams","text":"Parameter Type Default Description <code>num_pairs</code> int 1 Number of Q&amp;A pairs to generate <code>question_types</code> list factual, conceptual, application, analysis Types of questions to generate <code>difficulty</code> str medium Question difficulty (easy, medium, hard) <code>splitter_type</code> str semantic Chunking strategy (semantic, sentence) <code>chunk_size</code> int 4000 Max chunk size for sentence splitter <code>statements_per_chunk</code> int 5 Statements to extract per chunk <code>answer_length</code> str medium Answer length (short, medium, long) <code>validation_threshold</code> float 0.7 Minimum quality score to accept <code>max_reflection_iterations</code> int 3 Max enhancement iterations <code>custom_guidelines</code> str None Custom instructions for generation <code>example_question</code> str None Example question for style guidance <code>example_answer</code> str None Example answer for style guidance"},{"location":"guides/synthetic/#dimensional-generation","title":"Dimensional Generation","text":"<p>Guide generation with structured dimensions:</p> <pre><code>result = await workflow.run_from_documents(\n    content=content,\n    num_pairs=10,\n    dimensions={\n        \"features\": \"Product pricing, subscription tiers, enterprise features\",\n        \"persona\": \"Technical decision maker evaluating SaaS solutions\",\n        \"scenarios\": \"Comparing vendor options during procurement process\",\n    },\n)\n</code></pre>"},{"location":"guides/synthetic/#key-considerations","title":"Key Considerations","text":""},{"location":"guides/synthetic/#quality-over-quantity","title":"Quality Over Quantity","text":"<ul> <li>Synthetic data quality varies significantly</li> <li>Always review a sample before using at scale</li> <li>Use <code>validation_threshold</code> to filter aggressively</li> </ul>"},{"location":"guides/synthetic/#iterative-refinement","title":"Iterative Refinement","text":"<p>The workflow automatically refines low-quality Q&amp;A pairs:</p> <pre><code>result = await workflow.run_from_documents(\n    content=content,\n    validation_threshold=0.85,  # High bar\n    max_reflection_iterations=5,  # More attempts to meet threshold\n)\n\nprint(f\"Final quality: {result['average_quality']:.2f}\")\nprint(f\"Iterations used: {result['current_iteration']}\")\n</code></pre>"},{"location":"guides/synthetic/#coverage-strategy","title":"Coverage Strategy","text":"Phase Focus Formation Curate real-world examples with expert validation Expansion Augment with synthetic data for edge cases Maintenance Continuously add new failure modes"},{"location":"guides/synthetic/#when-to-use-synthetic-data","title":"When to Use Synthetic Data","text":"<p>Good for:</p> <ul> <li>Expanding coverage of edge cases</li> <li>Testing rare scenarios</li> <li>Scaling regression testing</li> <li>Bootstrapping evaluation datasets</li> </ul> <p>Not a replacement for:</p> <ul> <li>Real user data</li> <li>Expert-labeled ground truth</li> <li>Domain-specific nuance</li> </ul>"},{"location":"guides/synthetic/#graph-visualization","title":"Graph Visualization","text":"<p>View the workflow structure in notebooks:</p> <pre><code>workflow = QAWorkflowGraph(llm=your_llm)\nworkflow.visualize_graph()  # Displays mermaid diagram\n</code></pre>"},{"location":"guides/synthetic/#next-steps","title":"Next Steps","text":"<ul> <li>Agent Evaluation Playbook - Dataset best practices</li> <li>Metrics Guide - Evaluate your synthetic data quality</li> </ul>"},{"location":"guides/langfuse/configuration/","title":"Langfuse Configuration","text":"<p>This guide covers all setup and configuration options for integrating Axion with Langfuse.</p>"},{"location":"guides/langfuse/configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Langfuse account (cloud or self-hosted)</li> <li>Langfuse API keys (public and secret)</li> <li>Python 3.12+</li> <li>Axion installed with Langfuse support: <code>pip install axion[langfuse]</code></li> </ul>"},{"location":"guides/langfuse/configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure Langfuse credentials via environment variables:</p> Variable Required Description Default <code>LANGFUSE_PUBLIC_KEY</code> Yes Your Langfuse public key (<code>pk-lf-...</code>) - <code>LANGFUSE_SECRET_KEY</code> Yes Your Langfuse secret key (<code>sk-lf-...</code>) - <code>LANGFUSE_BASE_URL</code> No Langfuse host URL <code>https://us.cloud.langfuse.com</code> <code>LANGFUSE_TAGS</code> No Comma-separated default tags for traces - <code>LANGFUSE_ENVIRONMENT</code> No Environment name (e.g., <code>production</code>) - <code>LANGFUSE_TRACING_ENVIRONMENT</code> No Environment name (Langfuse SDK standard) - <code>LANGFUSE_DEFAULT_TAGS</code> No Comma-separated default tags for scores - <code>TRACING_MODE</code> No Set to <code>langfuse</code> to force Langfuse provider Auto-detected"},{"location":"guides/langfuse/configuration/#example-env-file","title":"Example <code>.env</code> File","text":"<pre><code>LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nLANGFUSE_SECRET_KEY=sk-lf-your-secret-key\nLANGFUSE_BASE_URL=https://us.cloud.langfuse.com\nLANGFUSE_ENVIRONMENT=production\nLANGFUSE_TAGS=prod,v2.0\nLANGFUSE_DEFAULT_TAGS=evaluation,automated\n</code></pre>"},{"location":"guides/langfuse/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"Auto-DetectionExplicit ConfigurationLoader with Custom CredentialsDirect LangfuseTracer <pre><code>from axion._core.tracing import Tracer\n\n# Tracer auto-detects Langfuse from LANGFUSE_SECRET_KEY\ntracer = Tracer('llm')\n</code></pre> <pre><code>from axion._core.tracing import configure_tracing, Tracer\n\n# Explicitly configure Langfuse\nconfigure_tracing(provider='langfuse')\ntracer = Tracer('llm')\n</code></pre> <pre><code>from axion.tracing import LangfuseTraceLoader\n\n# Override credentials for loader\nloader = LangfuseTraceLoader(\n    public_key='pk-lf-...',\n    secret_key='sk-lf-...',\n    host='https://cloud.langfuse.com',\n    default_tags=['evaluation', 'automated']\n)\n</code></pre> <pre><code>from axion._core.tracing.langfuse.tracer import LangfuseTracer\n\n# Direct initialization with all options\ntracer = LangfuseTracer(\n    tags=['prod', 'v1.0'],\n    environment='production'\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#auto-detection-behavior","title":"Auto-Detection Behavior","text":"<p>When you create a <code>Tracer()</code> instance, Axion automatically detects the appropriate backend:</p> <ol> <li>Check <code>TRACING_MODE</code>: If set to <code>langfuse</code>, use Langfuse</li> <li>Check credentials: If <code>LANGFUSE_SECRET_KEY</code> is set, use Langfuse</li> <li>Check Logfire: If Logfire is configured, use OpenTelemetry</li> <li>Default: Use NOOP tracer (no overhead)</li> </ol> <pre><code>import os\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\n\nfrom axion._core.tracing import Tracer\ntracer = Tracer('llm')  # Automatically uses Langfuse\n</code></pre>"},{"location":"guides/langfuse/configuration/#langfusetraceloader-initialization","title":"LangfuseTraceLoader Initialization","text":"<p>The <code>LangfuseTraceLoader</code> class accepts these initialization options:</p> Parameter Type Default Description <code>public_key</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_PUBLIC_KEY</code> <code>secret_key</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_SECRET_KEY</code> <code>host</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_BASE_URL</code> <code>default_tags</code> <code>list[str]</code> <code>None</code> Tags applied to all scores <code>request_pacing</code> <code>float</code> <code>0.05</code> Delay between API requests <code>max_retries</code> <code>int</code> <code>3</code> Max retry attempts <code>base_delay</code> <code>float</code> <code>0.5</code> Initial retry delay <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader(\n    public_key='pk-lf-...',\n    secret_key='sk-lf-...',\n    host='https://us.cloud.langfuse.com',\n    default_tags=['evaluation'],\n    request_pacing=0.1,  # Slower for rate limit prone accounts\n    max_retries=5,\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#region-configuration","title":"Region Configuration","text":"<p>Langfuse offers different regional endpoints:</p> <pre><code>import os\n\n# US region (default)\nos.environ['LANGFUSE_BASE_URL'] = 'https://us.cloud.langfuse.com'\n\n# EU region\nos.environ['LANGFUSE_BASE_URL'] = 'https://cloud.langfuse.com'\n\n# Self-hosted\nos.environ['LANGFUSE_BASE_URL'] = 'https://your-langfuse-instance.com'\n</code></pre>"},{"location":"guides/langfuse/configuration/#tags-and-environment","title":"Tags and Environment","text":""},{"location":"guides/langfuse/configuration/#tags","title":"Tags","text":"<p>Tags help filter and organize traces. They can be set at multiple levels:</p> <pre><code>import os\n\n# Default tags for all traces (comma-separated)\nos.environ['LANGFUSE_TAGS'] = 'prod,v1.0'\n\n# Default tags for scores\nos.environ['LANGFUSE_DEFAULT_TAGS'] = 'evaluation,automated'\n</code></pre> <p>Tag precedence:</p> <ol> <li>Per-call tags (passed to method)</li> <li>Loader default tags (set at initialization)</li> <li>Environment variable fallback (<code>LANGFUSE_TAGS</code>)</li> </ol>"},{"location":"guides/langfuse/configuration/#environment","title":"Environment","text":"<p>Environment identifies which deployment created a trace:</p> <pre><code>import os\nos.environ['LANGFUSE_ENVIRONMENT'] = 'production'\n</code></pre> <p>Environment Limitation</p> <p>Environment cannot be set when pushing scores to existing traces. It must be configured at tracer initialization when creating the original traces.</p>"},{"location":"guides/langfuse/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/langfuse/configuration/#connection-issues","title":"Connection Issues","text":"<p>If the Langfuse client fails to initialize:</p> <ol> <li> <p>Verify credentials: <pre><code>import os\nprint(f\"Public key: {os.environ.get('LANGFUSE_PUBLIC_KEY', 'NOT SET')}\")\nprint(f\"Secret key set: {bool(os.environ.get('LANGFUSE_SECRET_KEY'))}\")\n</code></pre></p> </li> <li> <p>Check the base URL: <pre><code>print(f\"Base URL: {os.environ.get('LANGFUSE_BASE_URL', 'default')}\")\n</code></pre></p> </li> <li> <p>Test connection: <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader()\ntraces = loader.fetch_traces(limit=1)\nprint(f\"Connection successful: {len(traces)} traces\")\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/configuration/#rate-limiting-429-errors","title":"Rate Limiting (429 Errors)","text":"<p>If you encounter rate limiting when fetching many traces:</p> <pre><code># Increase delay between requests\nloader = LangfuseTraceLoader(\n    request_pacing=0.1,  # Increase from default 0.05\n    max_retries=5,\n    base_delay=1.0,\n)\n\n# Or fetch summaries only (fewer API calls)\ntraces = loader.fetch_traces(\n    limit=1000,\n    fetch_full_traces=False\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#credentials-not-found","title":"Credentials Not Found","text":"<p>Ensure environment variables are set before importing Axion modules:</p> <pre><code>import os\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\n\n# Now import\nfrom axion._core.tracing import Tracer\n</code></pre> <p>Or use a <code>.env</code> file with <code>python-dotenv</code>:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Load before imports\n\nfrom axion._core.tracing import Tracer\n</code></pre>"},{"location":"guides/langfuse/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Tracing: Create and manage traces</li> <li>Publishing: Publish evaluation results</li> <li>Overview: Complete workflow example</li> </ul>"},{"location":"guides/langfuse/overview/","title":"Langfuse Integration","text":"<p>Integrate Axion evaluation with Langfuse observability to close the feedback loop between production LLM operations and quality metrics.</p>"},{"location":"guides/langfuse/overview/#why-use-langfuse-with-axion","title":"Why Use Langfuse with Axion?","text":"<ul> <li>Close the feedback loop: Connect evaluation results directly to production traces</li> <li>Production-grade observability: Track LLM performance, costs, and quality metrics in one place</li> <li>Continuous evaluation: Automate evaluation pipelines on real production data</li> </ul>"},{"location":"guides/langfuse/overview/#evaluation-workflows","title":"Evaluation Workflows","text":"<p>Axion supports three distinct evaluation workflows, each designed for different use cases. Understanding when to use each workflow is critical for effective LLM evaluation.</p>"},{"location":"guides/langfuse/overview/#workflow-overview","title":"Workflow Overview","text":"Workflow Testing Style Primary Use Case Publishing Method API-Driven Black-box Regression testing, CI/CD <code>publish_as_experiment()</code> Trace-Based White-box Historical analysis, debugging <code>publish_as_experiment()</code> or <code>publish_to_observability()</code> Online Production Continuous Real-time quality monitoring <code>publish_to_observability()</code>"},{"location":"guides/langfuse/overview/#offline-api-driven","title":"Offline: API-Driven","text":"<p>Best for: Regression testing, CI/CD pipelines, comparing API versions</p> <p>This workflow treats your agent as a black-box. You provide inputs from a golden dataset, call your API endpoint, and evaluate the responses. You don't need access to internal traces or spans.</p> <pre><code>flowchart LR\n    subgraph Source[\"Source\"]\n        direction TB\n        A[(Langfuse)] --&gt;|read_from_langfuse| B[/Golden Dataset/]\n    end\n\n    subgraph Execute[\"Execute\"]\n        direction TB\n        B --&gt;|execute_dataset_items_from_api| C[[APIRunner]]\n        C --&gt;|calls| D([Agent API])\n        D --&gt;|responses| B\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        B --&gt; E[[evaluation_runner]]\n        E --&gt; F{{EvaluationResult}}\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        F --&gt;|new experiment| G[publish_as_experiment]\n        G --&gt; A\n    end</code></pre> <p>When to use:</p> <ul> <li>You want to test API behavior without trace instrumentation</li> <li>Running automated regression tests in CI/CD</li> <li>Comparing responses between different API versions or models</li> <li>The agent is deployed as a service and you only care about input/output behavior</li> </ul> <p>Example:</p> <pre><code>from axion import Dataset\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.runners import evaluation_runner\n\n# 1. Load golden dataset from Langfuse\ndataset = Dataset.read_from_langfuse(golden, name='my_eval_dataset')\n\n# 2. Execute API calls to populate actual_output\ndataset.execute_dataset_items_from_api('Agent API', 'config.yaml')\n\n# 3. Run evaluation\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name='API Regression Test',\n)\n\n# 4. Publish as experiment for version comparison\nresult.publish_as_experiment(\n    dataset_name='api-regression-tests',\n    run_name='v2.1-release',\n)\n</code></pre>"},{"location":"guides/langfuse/overview/#offline-trace-based","title":"Offline: Trace-Based","text":"<p>Best for: Historical analysis, A/B experiments, debugging with span-level insights</p> <p>This workflow evaluates agent runs where you have access to internal traces. You trigger agent runs from a golden dataset, collect the traces (with span-level details), then evaluate. This gives you white-box visibility into agent internals.</p> <pre><code>flowchart LR\n    subgraph LF[\"Langfuse\"]\n        direction TB\n        A[/Golden Dataset/]\n        C[(Traces)]\n    end\n\n    subgraph Run[\"Run Agent\"]\n        direction TB\n        B([Agent])\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        D[LangfuseTraceLoader]\n        E[/Dataset with Actuals/]\n        D --&gt; E\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        F[[evaluation_runner]]\n        G{{EvaluationResult}}\n        F --&gt; G\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        H[publish_to_observability]\n        I[publish_as_experiment]\n    end\n\n    A --&gt;|for each item| B\n    B --&gt;|traces| C\n    C --&gt;|fetch_traces| D\n    E --&gt; F\n    G --&gt;|attach to traces| H\n    G --&gt;|new experiment| I\n    H --&gt; C\n    I --&gt; C</code></pre> <p>When to use:</p> <ul> <li>You need span-level insights (LLM calls, tool usage, retrieval steps)</li> <li>Debugging why specific responses failed</li> <li>Running A/B experiments with different agent configurations</li> <li>Evaluating historical production runs</li> </ul> <p>Key difference from API-Driven: The <code>actual_output</code> is extracted from traces, giving you access to intermediate steps, not just final responses.</p> <p>Example:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\n# 1. Run your agent against golden dataset items (externally)\n# This creates traces in Langfuse with tags ['experiment-v1']\n\n# 2. Fetch traces with outputs\nloader = LangfuseTraceLoader()\ntraces = loader.fetch_traces(tags=['experiment-v1'])\n\n# 3. Convert to Dataset (preserving trace_id for linking)\nitems = [\n    DatasetItem(\n        id=t.id,\n        query=t.input.get('query', ''),\n        actual_output=t.output.get('response', ''),\n        trace_id=t.id,  # Preserves link to original trace\n    )\n    for t in traces if t.input and t.output\n]\ndataset = Dataset(items=items)\n\n# 4. Run evaluation\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name='Trace-Based Evaluation',\n)\n\n# 5. Publish - choose based on your needs:\n# Option A: Attach scores to existing traces (for debugging)\nresult.publish_to_observability()\n\n# Option B: Create experiment (for comparison UI)\nresult.publish_as_experiment(\n    dataset_name='agent-experiments',\n    run_name='config-v2',\n    link_to_traces=True,  # Links experiment runs to original traces\n)\n</code></pre>"},{"location":"guides/langfuse/overview/#online-production","title":"Online: Production","text":"<p>Best for: Continuous quality monitoring, real-time alerts, production health tracking</p> <p>This workflow evaluates live production traces. You fetch recent traces from Langfuse, run evaluation metrics, and attach scores back to those traces for monitoring dashboards.</p> <pre><code>flowchart LR\n    subgraph Production[\"Production\"]\n        direction TB\n        A([Agent]) --&gt;|traces| B[(Langfuse)]\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        B --&gt;|fetch_traces| C[LangfuseTraceLoader]\n        C --&gt; D[/Dataset/]\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        D --&gt; E[[evaluation_runner]]\n        E --&gt; F{{EvaluationResult}}\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        F --&gt;|attach scores| G[publish_to_observability]\n        G --&gt; B\n    end</code></pre> <p>When to use:</p> <ul> <li>Monitoring production quality in real-time</li> <li>Setting up quality alerts and dashboards</li> <li>Tracking quality drift over time</li> <li>Evaluating a sample of production traffic</li> </ul> <p>Example:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, Toxicity\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\nasync def evaluate_production_traces():\n    # 1. Fetch recent production traces\n    loader = LangfuseTraceLoader()\n    traces = loader.fetch_traces(\n        limit=100,\n        tags=['production'],\n    )\n\n    # 2. Convert to Dataset\n    items = [\n        DatasetItem(\n            id=t.id,\n            query=t.input.get('query', ''),\n            actual_output=t.output.get('response', ''),\n            trace_id=t.id,\n        )\n        for t in traces if t.input and t.output\n    ]\n    dataset = Dataset(items=items)\n\n    # 3. Run lightweight evaluation metrics\n    result = await evaluation_runner(\n        evaluation_inputs=dataset,\n        scoring_metrics=[AnswerRelevancy(), Toxicity()],\n        evaluation_name='Production Monitoring',\n    )\n\n    # 4. Attach scores to production traces\n    stats = result.publish_to_observability(tags=['automated-eval'])\n    print(f\"Evaluated {stats['uploaded']} production traces\")\n\n# Run periodically (e.g., every hour via cron)\n</code></pre> <p>Cost Considerations for Online Evaluation</p> <p>For high-volume production systems, consider:</p> <ul> <li>Sampling: Evaluate a random sample (e.g., 1-5%) of traces instead of all</li> <li>Lightweight metrics: Use heuristic metrics instead of LLM-based metrics for high-frequency evaluation</li> <li>Batching: Aggregate traces and evaluate in batches during off-peak hours</li> </ul>"},{"location":"guides/langfuse/overview/#disabling-evaluation-tracing","title":"Disabling Evaluation Tracing","text":"<p>By default, <code>evaluation_runner</code> creates traces for each metric execution. If you don't need these evaluation traces (most publishing workflows only use source traces from <code>DatasetItem.trace_id</code>), you can disable them to reduce overhead.</p> <p>Important: Configure tracing to NOOP before creating metric instances, since tracers are cached at instantiation time.</p> <pre><code>from axion.tracing import configure_tracing\nfrom axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy, ExactStringMatch\n\n# 1. Disable tracing BEFORE creating metrics\nconfigure_tracing('noop')\n\n# 2. Create metrics (they will use NOOP tracers)\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(model_name='gpt-4o'),\n        'ExactStringMatch': ExactStringMatch(),\n    },\n    'model': {\n        'ANSWER_QUALITY': {\n            'Relevance': 1.0,\n            'ExactStringMatch': 1.0,\n        },\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\n# 3. Run evaluation (no evaluation traces created)\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_config=config,\n    evaluation_name='My Evaluation',\n)\n\n# 4. Restore tracing for publishing\nconfigure_tracing('langfuse')\n\n# 5. Publish results (uses source traces from DatasetItem.trace_id)\nresults.publish_to_observability()\n</code></pre> <p>When to keep evaluation tracing enabled</p> <p>Keep tracing enabled (default) when:</p> <ul> <li>Using <code>publish_as_experiment(score_on_runtime_traces=True)</code></li> <li>You need to debug metric execution in Langfuse</li> <li>You want visibility into LLM calls made by metrics</li> </ul>"},{"location":"guides/langfuse/overview/#choosing-the-right-workflow","title":"Choosing the Right Workflow","text":"<p>Use this decision tree to select the appropriate workflow:</p> <pre><code>Do you have existing traces in Langfuse?\n\u251c\u2500\u2500 No \u2192 Do you need span-level insights?\n\u2502         \u251c\u2500\u2500 No \u2192 Use API-Driven (black-box testing)\n\u2502         \u2514\u2500\u2500 Yes \u2192 Run agent with tracing, then use Trace-Based\n\u2514\u2500\u2500 Yes \u2192 Are these production traces?\n          \u251c\u2500\u2500 Yes \u2192 Use Online Production (monitoring)\n          \u2514\u2500\u2500 No \u2192 Use Trace-Based (historical analysis)\n</code></pre> Question API-Driven Trace-Based Online Need span-level debugging? No Yes Depends Requires trace instrumentation? No Yes Yes Creates new traces? Optional Yes No Best for CI/CD? Yes Possible No Best for monitoring? No No Yes"},{"location":"guides/langfuse/overview/#quick-summary","title":"Quick Summary","text":"<pre><code>flowchart LR\n    A[Agent] --&gt;|traces| B[Langfuse]\n    B --&gt;|fetch| C[LangfuseTraceLoader]\n    C --&gt;|convert| D[Dataset]\n    D --&gt;|evaluate| E[evaluation_runner]\n    E --&gt;|scores| F[EvaluationResult]\n    F --&gt;|publish| B</code></pre> Detailed Workflow <pre><code>flowchart LR\n    subgraph Production[\"Production\"]\n        direction TB\n        A([Agent]) --&gt;|traces| B[(Langfuse)]\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        B --&gt;|fetch_traces| C[LangfuseTraceLoader]\n        C --&gt; D[/Dataset/]\n        E[/Local Data/] --&gt; D\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        D --&gt; F[[evaluation_runner]]\n        F --&gt; G{{EvaluationResult}}\n    end\n\n    subgraph Analyze[\"Analyze\"]\n        direction TB\n        G --&gt; H[summary]\n        G --&gt; I[to_dataframe]\n        G --&gt; J[to_scorecard]\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        G --&gt;|existing traces| K[publish_to_observability]\n        G --&gt;|new experiment| L[publish_as_experiment]\n        K --&gt; B\n        L --&gt; B\n    end</code></pre>"},{"location":"guides/langfuse/overview/#complete-example","title":"Complete Example","text":"<p>This example demonstrates the full workflow: fetching traces, running evaluation, viewing results, and publishing back to Langfuse.</p> <pre><code>import asyncio\nfrom axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, AnswerCompleteness\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\nasync def main():\n    # 1. Fetch traces from Langfuse\n    loader = LangfuseTraceLoader()\n    traces = loader.fetch_traces(limit=50, tags=['production'])\n\n    # 2. Convert to Dataset\n    items = [\n        DatasetItem(\n            id=t.id,\n            query=t.input.get('query', ''),\n            actual_output=t.output.get('response', ''),\n            trace_id=t.id,\n        )\n        for t in traces if t.input and t.output\n    ]\n    dataset = Dataset(items=items)\n\n    # 3. Run evaluation\n    result = await evaluation_runner(\n        evaluation_inputs=dataset,\n        scoring_metrics=[AnswerRelevancy(), AnswerCompleteness()],\n        evaluation_name='Production Evaluation',\n    )\n\n    # 4. View results\n    from axion.runners.summary import MetricSummary\n    MetricSummary().execute(result.results, total_time=100)\n    result.to_scorecard(display_in_notebook=True)\n\n    # 5. Publish back to Langfuse\n    stats = result.publish_as_experiment(\n        dataset_name='my-eval-dataset',\n        run_name='experiment-v1',\n        tags=['production']\n    )\n    print(f\"Published {stats['scores_uploaded']} scores\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/langfuse/overview/#understanding-results","title":"Understanding Results","text":"<p>After running <code>evaluation_runner</code>, use these methods to analyze results:</p> Method Description <code>MetricSummary().execute(result.results, total_time)</code> Generate detailed metric analysis report <code>result.to_dataframe()</code> Convert results to a pandas DataFrame for analysis <code>result.to_scorecard(display_in_notebook=True)</code> Display an interactive scorecard visualization <code>result.to_latency_plot()</code> Visualize metric latency distributions"},{"location":"guides/langfuse/overview/#quick-analysis","title":"Quick Analysis","text":"<pre><code>from axion.runners.summary import MetricSummary\n\n# Generate detailed summary report\nMetricSummary().execute(result.results, total_time=100)\n\n# Export to DataFrame for custom analysis\ndf = result.to_dataframe()\nprint(df.describe())\n\n# Visual scorecard (in Jupyter notebooks)\nresult.to_scorecard(display_in_notebook=True)\n</code></pre>"},{"location":"guides/langfuse/overview/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Configure Langfuse: Set up credentials and environment</li> <li>Create Traces: Instrument your LLM operations with <code>@trace</code> decorator</li> <li>Fetch &amp; Evaluate: Retrieve traces and run metrics</li> <li>Publish Results: Send scores back to Langfuse</li> </ol>"},{"location":"guides/langfuse/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration: Set up Langfuse credentials and options</li> <li>Tracing: Learn how to create and manage traces</li> <li>Publishing: Publish scores to existing traces or create experiments</li> <li>Metrics Guide: Available metrics and customization options</li> </ul>"},{"location":"guides/langfuse/publishing/","title":"Publishing Evaluation Results","text":"<p>After running evaluations, publish results back to Langfuse. Axion provides two methods depending on whether you're scoring existing traces or creating new experiments.</p>"},{"location":"guides/langfuse/publishing/#two-publishing-paths","title":"Two Publishing Paths","text":"Method Use Case Requires Existing Traces? <code>publish_to_observability()</code> Attach scores to existing production traces Yes - <code>trace_id</code> required <code>publish_as_experiment()</code> Create a complete experiment from scratch No - creates everything"},{"location":"guides/langfuse/publishing/#publishing-to-existing-traces","title":"Publishing to Existing Traces","text":"<p>Use <code>publish_to_observability()</code> when you have existing traces in Langfuse and want to attach evaluation scores to them.</p>"},{"location":"guides/langfuse/publishing/#basic-usage","title":"Basic Usage","text":"<pre><code># Publish with default settings\nstats = result.publish_to_observability()\nprint(f\"Uploaded: {stats['uploaded']}, Skipped: {stats['skipped']}\")\n</code></pre>"},{"location":"guides/langfuse/publishing/#with-tags","title":"With Tags","text":"<pre><code>stats = result.publish_to_observability(\n    tags=['experiment-v1', 'automated']\n)\n</code></pre>"},{"location":"guides/langfuse/publishing/#trace-level-only","title":"Trace-Level Only","text":"<pre><code># Scores attach to traces, not observations\nstats = result.publish_to_observability(observation_id_field=None)\n</code></pre>"},{"location":"guides/langfuse/publishing/#using-langfusetraceloader-directly","title":"Using LangfuseTraceLoader Directly","text":"<p>For more control, use the loader's method:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader(default_tags=['evaluation'])\n\nstats = loader.push_scores_to_langfuse(\n    evaluation_result=result,\n    observation_id_field='observation_id',\n    flush=True,\n    tags=['prod', 'v1.0']  # Merged with default_tags\n)\n</code></pre>"},{"location":"guides/langfuse/publishing/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>loader</code> <code>BaseTraceLoader</code> <code>None</code> Loader instance (creates one if None) <code>observation_id_field</code> <code>str</code> <code>'observation_id'</code> Field for granular scoring <code>flush</code> <code>bool</code> <code>True</code> Flush client after uploading <code>tags</code> <code>list[str]</code> <code>None</code> Tags to attach to scores"},{"location":"guides/langfuse/publishing/#return-statistics","title":"Return Statistics","text":"<pre><code>stats = result.publish_to_observability()\n# stats = {\n#     'uploaded': 45,  # Successfully uploaded scores\n#     'skipped': 5,    # Skipped (missing trace_id, NaN scores)\n# }\n</code></pre>"},{"location":"guides/langfuse/publishing/#granular-vs-trace-level-scoring","title":"Granular vs Trace-Level Scoring","text":"<p>Trace-level scoring attaches scores to the entire trace:</p> <pre><code>stats = result.publish_to_observability(observation_id_field=None)\n</code></pre> <p>Observation-level scoring attaches scores to specific spans:</p> <pre><code># Ensure DatasetItems have observation_id set\nstats = result.publish_to_observability(observation_id_field='observation_id')\n</code></pre>"},{"location":"guides/langfuse/publishing/#publishing-as-experiments","title":"Publishing as Experiments","text":"<p>Use <code>publish_as_experiment()</code> for evaluation workflows that don't start with existing traces. This creates a complete experiment in Langfuse: datasets, dataset items, experiment runs, and scores.</p>"},{"location":"guides/langfuse/publishing/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import Faithfulness, AnswerRelevancy\n\n# Run evaluation on a local dataset\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[Faithfulness(), AnswerRelevancy()],\n    evaluation_name='Offline RAG Evaluation',\n)\n\n# Upload as a Langfuse experiment\nstats = result.publish_as_experiment(\n    dataset_name='my-rag-eval-dataset',\n    run_name='experiment-v1',\n    run_metadata={'model': 'gpt-4o', 'version': '2.0'},\n    tags=['offline', 'baseline'],\n)\n\nprint(f\"Dataset: {stats['dataset_name']}\")\nprint(f\"Run: {stats['run_name']}\")\nprint(f\"Items created: {stats['items_created']}\")\nprint(f\"Scores uploaded: {stats['scores_uploaded']}\")\n</code></pre>"},{"location":"guides/langfuse/publishing/#how-it-works","title":"How It Works","text":"<pre><code>+---------------------------------------------------------------------+\n|                        publish_as_experiment()                       |\n+---------------------------------------------------------------------+\n|                                                                     |\n|  1. DETERMINE NAMES                                                 |\n|     +- dataset_name: provided OR evaluation_name OR auto-generated  |\n|     +- run_name: provided OR \"{dataset_name}-{run_id[:8]}\"          |\n|                                                                     |\n|  2. CREATE/GET DATASET (upserts - safe if exists)                   |\n|     +- client.create_dataset(name=dataset_name)                     |\n|                                                                     |\n|  3. PHASE 1: CREATE DATASET ITEMS                                   |\n|     For each TestResult:                                            |\n|     +- Serialize input (query, retrieved_content, etc.)             |\n|     +- Serialize expected_output                                    |\n|     +- create_dataset_item(id=item.id, ...)                         |\n|                                                                     |\n|  4. PHASE 2: CREATE EXPERIMENT RUNS                                 |\n|     For each dataset_item:                                          |\n|     +- Create trace with input/output                               |\n|     +- Link trace to dataset item                                   |\n|     +- Attach scores to trace                                       |\n|                                                                     |\n|  5. FINAL FLUSH                                                     |\n|                                                                     |\n+---------------------------------------------------------------------+\n</code></pre>"},{"location":"guides/langfuse/publishing/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>loader</code> <code>LangfuseTraceLoader</code> <code>None</code> Loader instance (creates one if None) <code>dataset_name</code> <code>str</code> <code>None</code> Name for the Langfuse dataset <code>run_name</code> <code>str</code> <code>None</code> Name for the experiment run <code>run_metadata</code> <code>dict</code> <code>None</code> Metadata for the experiment run <code>flush</code> <code>bool</code> <code>True</code> Flush client after uploading <code>tags</code> <code>list[str]</code> <code>None</code> Tags to attach to scores <code>score_on_runtime_traces</code> <code>bool</code> <code>False</code> Skip creating runs, just add scores to existing traces <code>link_to_traces</code> <code>bool</code> <code>False</code> Link experiment runs to existing traces instead of creating new ones"},{"location":"guides/langfuse/publishing/#experiment-modes","title":"Experiment Modes","text":"<p><code>publish_as_experiment()</code> supports three modes for different use cases:</p> Mode Creates Runs Links to Traces Use Case Default (both <code>False</code>)  new traces Standard experiment - creates fresh traces for each run <code>score_on_runtime_traces=True</code> N/A (scores only) Just add scores to existing traces without creating experiment runs <code>link_to_traces=True</code>  linked Experiment UI linked to original evaluation traces <p>When to use each mode</p> <ul> <li>Default: Use when you don't have existing traces and want a self-contained experiment</li> <li>score_on_runtime_traces: Use when you only care about scores on existing traces, not the experiment UI</li> <li>link_to_traces: Use when you ran evaluations with tracing enabled and want experiment runs to link back to those original traces</li> </ul>"},{"location":"guides/langfuse/publishing/#default-mode","title":"Default Mode","text":"<p>Creates new traces for each dataset item run:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n)\n# Creates: dataset items + new \"Dataset run\" traces + scores\n</code></pre>"},{"location":"guides/langfuse/publishing/#score-on-runtime-traces","title":"Score on Runtime Traces","text":"<p>Attaches scores directly to existing traces without creating experiment runs:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n    score_on_runtime_traces=True,\n)\n# Creates: dataset items + scores on existing traces\n# Does NOT create: experiment runs\n</code></pre> <p>This is useful when:</p> <ul> <li>You only need scores visible on your production traces</li> <li>You don't need the experiment comparison UI</li> <li>Your <code>DatasetItem</code> objects have <code>trace_id</code> set from prior tracing</li> </ul>"},{"location":"guides/langfuse/publishing/#link-to-traces","title":"Link to Traces","text":"<p>Creates experiment runs that link to your existing evaluation traces:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n    link_to_traces=True,\n)\n# Creates: dataset items + experiment runs linked to existing traces + scores\n</code></pre> <p>This is useful when:</p> <ul> <li>You ran <code>evaluation_runner</code> with Langfuse tracing enabled</li> <li>You want experiment runs in the Langfuse UI to link back to those original traces</li> <li>You need both the experiment comparison view AND visibility into the original trace details</li> </ul> <p>Fallback behavior</p> <p>When <code>link_to_traces=True</code> but a <code>DatasetItem</code> doesn't have a <code>trace_id</code>, that item falls back to default mode (creates a new trace).</p> <p>Precedence</p> <p>If both <code>score_on_runtime_traces=True</code> and <code>link_to_traces=True</code> are set, <code>score_on_runtime_traces</code> takes precedence.</p>"},{"location":"guides/langfuse/publishing/#return-statistics_1","title":"Return Statistics","text":"<pre><code>stats = result.publish_as_experiment(...)\n# stats = {\n#     'dataset_name': 'my-rag-eval-dataset',\n#     'run_name': 'experiment-v1',\n#     'items_created': 50,\n#     'runs_created': 50,\n#     'scores_uploaded': 100,\n#     'scores_skipped': 0,\n#     'errors': [],\n# }\n</code></pre>"},{"location":"guides/langfuse/publishing/#behavior-with-existing-names","title":"Behavior with Existing Names","text":"<p>Understanding how the method handles existing datasets and runs:</p> Scenario Behavior Dataset already exists <code>create_dataset()</code> upserts - retrieves existing, no error Item ID already exists Caught as \"already exists\" error, item is reused Run name already exists Creates a new run under the same name (distinguished by timestamp)"},{"location":"guides/langfuse/publishing/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Item IDs come from Axion's <code>DatasetItem.id</code> - Enables deduplication. Running the same evaluation twice won't duplicate items.</p> </li> <li> <p>Runs are always created fresh - Each call creates new experiment runs, even with the same <code>run_name</code>. This lets you compare multiple runs.</p> </li> <li> <p>Dataset items are append-only - New items are added, existing items (by ID) are reused.</p> </li> </ol>"},{"location":"guides/langfuse/publishing/#example-scenarios","title":"Example Scenarios","text":"First RunSame Dataset, Different RunRe-running Same Experiment <pre><code># Creates: dataset \"my-rag-eval\", 100 items, run \"baseline-v1\"\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',\n    run_name='baseline-v1'\n)\n</code></pre> <pre><code># Reuses: dataset \"my-rag-eval\", existing items (by ID)\n# Creates: new run \"improved-v2\"\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',  # Same dataset\n    run_name='improved-v2'       # New run name\n)\n</code></pre> <pre><code># Reuses: dataset, items\n# Creates: NEW run also named \"baseline-v1\" (Langfuse shows both)\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',\n    run_name='baseline-v1'  # Same run name\n)\n</code></pre> <p>Comparing Experiments</p> <p>Use the same <code>dataset_name</code> with different <code>run_name</code> values to compare multiple experiments (different models, prompts, or configurations) in Langfuse's experiment comparison view.</p>"},{"location":"guides/langfuse/publishing/#choosing-the-right-method","title":"Choosing the Right Method","text":"<p>See Also: Evaluation Workflows</p> <p>The choice of publishing method depends on your evaluation workflow. See Evaluation Workflows for guidance on choosing between API-Driven, Trace-Based, and Online Production workflows.</p>"},{"location":"guides/langfuse/publishing/#by-workflow","title":"By Workflow","text":"Workflow Primary Method Alternative API-Driven (black-box testing) <code>publish_as_experiment()</code> - Trace-Based (white-box testing) <code>publish_as_experiment(link_to_traces=True)</code> <code>publish_to_observability()</code> Online Production (monitoring) <code>publish_to_observability()</code> -"},{"location":"guides/langfuse/publishing/#by-scenario","title":"By Scenario","text":"Scenario Use This Method Scoring production traces <code>publish_to_observability()</code> A/B testing with existing traces <code>publish_to_observability()</code> Offline evaluation (no traces) <code>publish_as_experiment()</code> Comparing model versions <code>publish_as_experiment()</code> Creating baseline datasets <code>publish_as_experiment()</code> Continuous monitoring <code>publish_to_observability()</code> Experiment UI + link to evaluation traces <code>publish_as_experiment(link_to_traces=True)</code> Scores on traces + no experiment runs <code>publish_as_experiment(score_on_runtime_traces=True)</code>"},{"location":"guides/langfuse/publishing/#quick-reference","title":"Quick Reference","text":"<pre><code># For existing traces (from production):\nresult.publish_to_observability()  # Attaches scores to existing traces\n\n# For new experiments (no existing traces):\nresult.publish_as_experiment()  # Creates everything from scratch\n\n# For experiments linked to evaluation traces:\nresult.publish_as_experiment(link_to_traces=True)  # Links runs to existing traces\n\n# For scores only (no experiment runs):\nresult.publish_as_experiment(score_on_runtime_traces=True)  # Scores only\n</code></pre>"},{"location":"guides/langfuse/publishing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/langfuse/publishing/#scores-not-appearing","title":"Scores Not Appearing","text":"<p>If scores don't appear in the Langfuse UI:</p> <ol> <li> <p>Check return stats: <pre><code>stats = result.publish_to_observability()\nprint(f\"Uploaded: {stats['uploaded']}, Skipped: {stats['skipped']}\")\n</code></pre></p> </li> <li> <p>Ensure flush completed: <pre><code>stats = result.publish_to_observability(flush=True)\n</code></pre></p> </li> <li> <p>Verify trace_id matches: <pre><code># trace_id must match an existing trace\nfor item in dataset.items:\n    print(f\"Item {item.id} -&gt; trace_id: {item.trace_id}\")\n</code></pre></p> </li> <li> <p>Check for NaN scores (these are skipped): <pre><code>import math\nfor test_result in result.results:\n    for score in test_result.score_results:\n        if score.score is None or math.isnan(score.score):\n            print(f\"Invalid score: {score.name}\")\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/publishing/#missing-trace_id-warnings","title":"Missing trace_id Warnings","text":"<p>Scores are skipped if <code>trace_id</code> is missing:</p> <pre><code># Ensure trace_id is preserved during conversion\nitems.append(DatasetItem(\n    id=trace.id,\n    query=query,\n    actual_output=output,\n    trace_id=trace.id,  # Required!\n))\n</code></pre> <p>Check your dataset items: <pre><code>for item in dataset.items:\n    if not item.trace_id:\n        print(f\"Missing trace_id: {item.id}\")\n</code></pre></p>"},{"location":"guides/langfuse/publishing/#rate-limiting","title":"Rate Limiting","text":"<p>For large evaluations, consider batching:</p> <pre><code># Increase delay between requests\nloader = LangfuseTraceLoader(request_pacing=0.1)\nstats = result.publish_to_observability(loader=loader)\n</code></pre>"},{"location":"guides/langfuse/publishing/#next-steps","title":"Next Steps","text":"<ul> <li>Overview: Complete workflow example</li> <li>Tracing: Creating traces to score</li> <li>Configuration: Advanced configuration</li> <li>Evaluation Guide: Running evaluations</li> </ul>"},{"location":"guides/langfuse/tracing/","title":"Creating Traces","text":"<p>This guide covers how to create and manage traces in Langfuse using Axion's tracing system.</p>"},{"location":"guides/langfuse/tracing/#using-the-trace-decorator","title":"Using the @trace Decorator","text":"<p>The <code>@trace</code> decorator automatically captures function inputs and outputs:</p> <pre><code>from axion._core.tracing import init_tracer, trace\n\nclass RAGService:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    @trace(name='rag-query', capture_args=True, capture_result=True)\n    async def query(self, question: str, context: list[str]) -&gt; str:\n        # Your RAG logic here\n        response = await self.llm.generate(question, context)\n        return response\n\n# Usage\nservice = RAGService()\nanswer = await service.query(\n    question='What is the return policy?',\n    context=['Returns accepted within 30 days...']\n)\nservice.tracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#decorator-parameters","title":"Decorator Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> Function name Name for the span <code>capture_args</code> <code>bool</code> <code>False</code> Capture function arguments as input <code>capture_result</code> <code>bool</code> <code>False</code> Capture return value as output"},{"location":"guides/langfuse/tracing/#manual-span-creation","title":"Manual Span Creation","text":"<p>For more control over what gets captured, create spans manually:</p>"},{"location":"guides/langfuse/tracing/#synchronous-context-manager","title":"Synchronous Context Manager","text":"<pre><code>from axion._core.tracing import Tracer\nfrom openai import OpenAI\n\ntracer = Tracer('llm')\nclient = OpenAI()\n\nwith tracer.span('my-operation') as span:\n    span.set_input({'query': 'How do I upgrade my plan?'})\n\n    response = client.chat.completions.create(\n        model='gpt-4o',\n        messages=[{'role': 'user', 'content': 'How do I upgrade my plan?'}]\n    )\n\n    span.set_output({'response': response.choices[0].message.content})\n    span.set_attribute('model', 'gpt-4o')\n    span.set_attribute('tokens', response.usage.total_tokens)\n\ntracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#asynchronous-context-manager","title":"Asynchronous Context Manager","text":"<p>For async code, use <code>async_span</code>:</p> <pre><code>async with tracer.async_span('async-operation') as span:\n    span.set_input({'query': user_query})\n    result = await process_query(user_query)\n    span.set_output({'result': result})\n</code></pre>"},{"location":"guides/langfuse/tracing/#inputoutput-capture","title":"Input/Output Capture","text":""},{"location":"guides/langfuse/tracing/#set_input-and-set_output","title":"set_input() and set_output()","text":"<p>Use these methods to explicitly capture data on spans:</p> <pre><code>with tracer.span('llm-call') as span:\n    # Set input data (dict)\n    span.set_input({\n        'query': user_question,\n        'context': retrieved_chunks,\n        'model': 'gpt-4o'\n    })\n\n    # Your LLM call\n    response = llm.generate(...)\n\n    # Set output data (dict)\n    span.set_output({\n        'response': response.text,\n        'tokens_used': response.usage.total_tokens\n    })\n</code></pre>"},{"location":"guides/langfuse/tracing/#set_attribute","title":"set_attribute()","text":"<p>Add additional metadata to spans:</p> <pre><code>with tracer.span('retrieval') as span:\n    span.set_attribute('num_chunks', 5)\n    span.set_attribute('search_type', 'semantic')\n    span.set_attribute('latency_ms', 142)\n</code></pre>"},{"location":"guides/langfuse/tracing/#nested-spans","title":"Nested Spans","text":"<p>Create hierarchical traces with nested spans:</p> <pre><code>with tracer.span('rag-pipeline') as parent:\n    parent.set_input({'query': query})\n\n    # Child span for retrieval\n    with tracer.span('retrieval') as retrieval_span:\n        chunks = retriever.search(query)\n        retrieval_span.set_output({'chunks': len(chunks)})\n\n    # Child span for generation\n    with tracer.span('generation') as gen_span:\n        gen_span.set_input({'context': chunks})\n        response = llm.generate(query, chunks)\n        gen_span.set_output({'response': response})\n\n    parent.set_output({'answer': response})\n</code></pre>"},{"location":"guides/langfuse/tracing/#flushing-traces","title":"Flushing Traces","text":"<p>Always Flush Before Exit</p> <p>Call <code>tracer.flush()</code> before your application exits to ensure all traces are sent to Langfuse. This is especially important in scripts and short-lived processes.</p> <pre><code>tracer = Tracer('llm')\n\n# Your tracing operations\nwith tracer.span('operation') as span:\n    span.set_input({'query': 'test'})\n    # ...\n\n# Ensure traces are sent\ntracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#fetching-traces","title":"Fetching Traces","text":"<p>Use <code>LangfuseTraceLoader</code> to retrieve traces from Langfuse:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader()\n\n# Fetch recent traces\ntraces = loader.fetch_traces(\n    limit=100,          # Maximum traces to fetch\n    days_back=7,        # Time window in days\n    tags=['prod'],      # Filter by tags (optional)\n    name='rag-query',   # Filter by trace name (optional)\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#fetch_traces-parameters","title":"fetch_traces() Parameters","text":"Parameter Type Default Description <code>limit</code> <code>int</code> <code>50</code> Maximum number of traces to fetch <code>days_back</code> <code>int</code> <code>7</code> Number of days to look back <code>tags</code> <code>list[str]</code> <code>None</code> Filter by specific tags <code>name</code> <code>str</code> <code>None</code> Filter by trace name <code>fetch_full_traces</code> <code>bool</code> <code>True</code> Fetch full details vs. summaries"},{"location":"guides/langfuse/tracing/#filtering-examples","title":"Filtering Examples","text":"<pre><code># Filter by multiple tags (AND logic)\nprod_traces = loader.fetch_traces(\n    limit=100,\n    tags=['production', 'v2.0']\n)\n\n# Filter by trace name\nrag_traces = loader.fetch_traces(\n    limit=100,\n    name='rag-query'\n)\n\n# Combine filters\ntraces = loader.fetch_traces(\n    limit=50,\n    days_back=3,\n    tags=['production'],\n    name='chat-completion'\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#converting-traces-to-dataset","title":"Converting Traces to Dataset","text":"<p>Traces must be converted to <code>DatasetItem</code> objects for evaluation. The key is to preserve <code>trace_id</code> and optionally <code>observation_id</code> for score publishing.</p>"},{"location":"guides/langfuse/tracing/#understanding-trace-structure","title":"Understanding Trace Structure","text":"<p>Langfuse traces contain:</p> <ul> <li><code>id</code>: Unique trace identifier</li> <li><code>input</code>: The input data (dict or string)</li> <li><code>output</code>: The output data (dict or string)</li> <li><code>observations</code>: List of spans within the trace</li> <li><code>tags</code>: Associated tags</li> <li><code>metadata</code>: Additional metadata</li> </ul>"},{"location":"guides/langfuse/tracing/#manual-conversion","title":"Manual Conversion","text":"<pre><code>from axion import Dataset, DatasetItem\n\nitems = []\nfor trace in traces:\n    # Extract query from input\n    query = ''\n    if trace.input:\n        if isinstance(trace.input, dict):\n            query = trace.input.get('query', trace.input.get('question', ''))\n        else:\n            query = str(trace.input)\n\n    # Extract response from output\n    actual_output = ''\n    if trace.output:\n        if isinstance(trace.output, dict):\n            actual_output = trace.output.get('response', trace.output.get('answer', ''))\n        else:\n            actual_output = str(trace.output)\n\n    # Create DatasetItem with trace_id for score publishing\n    items.append(DatasetItem(\n        id=trace.id,\n        query=query,\n        actual_output=actual_output,\n        trace_id=trace.id,  # Required for publish_to_observability()\n    ))\n\ndataset = Dataset(items=items)\n</code></pre>"},{"location":"guides/langfuse/tracing/#using-dataframe-conversion","title":"Using DataFrame Conversion","text":"<p>For more complex conversions, use <code>Dataset.read_dataframe()</code>:</p> <pre><code>import pandas as pd\nfrom axion import Dataset\n\n# Convert traces to DataFrame\ndata = []\nfor trace in traces:\n    data.append({\n        'id': trace.id,\n        'query': trace.input.get('query', '') if trace.input else '',\n        'actual_output': trace.output.get('response', '') if trace.output else '',\n        'trace_id': trace.id,\n        'retrieved_content': trace.input.get('context', []) if trace.input else [],\n    })\n\ndf = pd.DataFrame(data)\n\n# Convert to Dataset\ndataset = Dataset.read_dataframe(df, ignore_extra_keys=True)\n</code></pre>"},{"location":"guides/langfuse/tracing/#preserving-observation-ids","title":"Preserving Observation IDs","text":"<p>For granular scoring at the span level, extract observation IDs:</p> <pre><code>items = []\nfor trace in traces:\n    # Find the generation span for granular scoring\n    obs_id = None\n    for obs in trace.observations or []:\n        if obs.type == 'GENERATION':\n            obs_id = obs.id\n            break\n\n    items.append(DatasetItem(\n        id=trace.id,\n        query=extract_query(trace.input),\n        actual_output=extract_output(trace.output),\n        trace_id=trace.id,\n        observation_id=obs_id,  # Scores attach to this span\n    ))\n</code></pre>"},{"location":"guides/langfuse/tracing/#performance-tips","title":"Performance Tips","text":"<p>Fetching Large Volumes</p> <p>Set <code>fetch_full_traces=False</code> when fetching large volumes of traces. This returns trace summaries instead of full details, significantly reducing API calls and avoiding rate limits.</p> <pre><code># Fast fetch for large volumes\ntraces = loader.fetch_traces(\n    limit=1000,\n    fetch_full_traces=False  # Returns summaries only\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#empty-traces","title":"Empty Traces","text":"<p>If <code>fetch_traces()</code> returns an empty list:</p> <ol> <li> <p>Extend time window: <pre><code>traces = loader.fetch_traces(days_back=30)\n</code></pre></p> </li> <li> <p>Verify tags exist: <pre><code># Fetch without tag filter first\nall_traces = loader.fetch_traces(limit=10, tags=None)\nprint(f\"All traces: {len(all_traces)}\")\n</code></pre></p> </li> <li> <p>Ensure traces were flushed: <pre><code>tracer.flush()  # Call after tracing operations\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/tracing/#next-steps","title":"Next Steps","text":"<ul> <li>Publishing: Publish evaluation scores to Langfuse</li> <li>Configuration: Advanced configuration options</li> <li>Overview: Complete workflow example</li> </ul>"},{"location":"metric-registry/composite/","title":"Composite Metrics","text":"LLM-powered evaluation metrics for comprehensive AI response analysis 14 Metrics LLM-Powered <p>Composite metrics use language models to perform nuanced reasoning and analysis. These metrics evaluate complex aspects of AI responses including factual accuracy, relevance, grounding, and style\u2014things that require understanding context, semantics, and intent.</p>"},{"location":"metric-registry/composite/#rag-retrieval-metrics","title":"RAG &amp; Retrieval Metrics","text":"<p>Evaluate the quality of retrieval-augmented generation systems.</p> Faithfulness <p>Verify claims against retrieved context</p> <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Contextual Relevancy <p>Check if retrieved chunks are relevant</p> <code>query</code> <code>retrieved_content</code> Contextual Recall <p>Check if context supports expected answer</p> <code>expected_output</code> <code>retrieved_content</code> Contextual Precision <p>Measure useful chunk ranking (MAP)</p> <code>query</code> <code>expected_output</code> <code>retrieved_content</code> Contextual Ranking <p>Check if relevant chunks rank higher</p> <code>query</code> <code>retrieved_content</code> Contextual Sufficiency <p>Binary check for enough context</p> <code>query</code> <code>retrieved_content</code> Contextual Utilization <p>Measure context usage efficiency</p> <code>query</code> <code>actual_output</code> <code>retrieved_content</code>"},{"location":"metric-registry/composite/#answer-quality-metrics","title":"Answer Quality Metrics","text":"<p>Evaluate the quality and correctness of AI-generated answers.</p> Answer Relevancy <p>Check if response addresses the query</p> <code>query</code> <code>actual_output</code> Factual Accuracy <p>Verify against ground truth</p> <code>query</code> <code>actual_output</code> <code>expected_output</code> Answer Completeness <p>Check coverage of expected content</p> <code>query</code> <code>actual_output</code> <code>expected_output</code> Answer Criteria <p>Evaluate against custom criteria</p> <code>query</code> <code>actual_output</code> + <code>acceptance_criteria</code>"},{"location":"metric-registry/composite/#style-safety-metrics","title":"Style &amp; Safety Metrics","text":"<p>Evaluate tone, citations, and privacy compliance.</p> Tone &amp; Style Consistency <p>Match expected voice and formatting</p> <code>actual_output</code> <code>expected_output</code> Citation Relevancy <p>Validate citation quality</p> <code>query</code> <code>actual_output</code> PII Leakage <p>Detect privacy violations</p> <code>query</code> <code>actual_output</code>"},{"location":"metric-registry/composite/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Faithfulness 0.0 \u2013 1.0 0.5 Are claims grounded in context? Answer Relevancy 0.0 \u2013 1.0 0.5 Does response address the query? Factual Accuracy 0.0 \u2013 1.0 0.8 Does it match ground truth? Answer Completeness 0.0 \u2013 1.0 0.5 Are all expected aspects covered? Answer Criteria 0.0 \u2013 1.0 0.5 Does it meet custom criteria? Tone &amp; Style 0.0 \u2013 1.0 0.8 Does it match expected voice? Citation Relevancy 0.0 \u2013 1.0 0.8 Are citations relevant? PII Leakage 0.0 \u2013 1.0 0.5 Is output privacy-safe? (1.0 = safe) Contextual Relevancy 0.0 \u2013 1.0 0.5 Are chunks relevant to query? Contextual Recall 0.0 \u2013 1.0 0.5 Is expected answer in context? Contextual Precision 0.0 \u2013 1.0 0.5 Are useful chunks ranked first? Contextual Ranking 0.0 \u2013 1.0 0.5 Are relevant chunks ranked first? Contextual Sufficiency 0.0 or 1.0 0.5 Is context sufficient? (binary) Contextual Utilization 0.0 \u2013 1.0 0.5 Was relevant context used?"},{"location":"metric-registry/composite/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import (\n    Faithfulness,\n    AnswerRelevancy,\n    ContextualPrecision,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import Dataset\n\n# Initialize metrics\nmetrics = [\n    Faithfulness(strict_mode=True),\n    AnswerRelevancy(),\n    ContextualPrecision(),\n]\n\n# Run evaluation\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run(dataset)\n\n# Analyze results\nfor item in results:\n    print(f\"Faithfulness: {item.scores['faithfulness']:.2f}\")\n    print(f\"Relevancy: {item.scores['answer_relevancy']:.2f}\")\n    print(f\"Precision: {item.scores['contextual_precision']:.2f}\")\n</code></pre>"},{"location":"metric-registry/composite/#choosing-the-right-metrics","title":"Choosing the Right Metrics","text":"<p>Evaluation Strategy</p> <p>For RAG Systems:</p> <ul> <li>Start with Faithfulness (hallucination detection)</li> <li>Add Contextual Relevancy (retrieval quality)</li> <li>Use Contextual Precision/Ranking (ranking quality)</li> </ul> <p>For Q&amp;A Systems:</p> <ul> <li>Use Answer Relevancy (topical alignment)</li> <li>Add Factual Accuracy if you have ground truth</li> <li>Add Answer Completeness for comprehensive responses</li> </ul> <p>For Customer Service:</p> <ul> <li>Use Tone &amp; Style Consistency (brand voice)</li> <li>Add Answer Criteria (policy compliance)</li> <li>Include PII Leakage (privacy protection)</li> </ul> <p>For Research Assistants:</p> <ul> <li>Use Citation Relevancy (source quality)</li> <li>Add Faithfulness (grounding)</li> <li>Include Answer Completeness (thoroughness)</li> </ul>"},{"location":"metric-registry/composite/answer_completeness/","title":"Answer Completeness","text":"Measure how completely the response covers expected content LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/answer_completeness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Coverage ratio \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>expected_output</code> Reference answer required <p>What It Measures</p> <p>Answer Completeness evaluates whether the response covers all the key aspects from the expected output. It answers: \"Did the AI mention everything important from the reference answer?\"</p> Score Interpretation 1.0  All aspects from expected output covered 0.7+  Most aspects covered, minor omissions 0.5  Half the expected content covered &lt; 0.5  Significant content missing \u2705 Use When <ul> <li>You have reference answers</li> <li>Completeness matters more than brevity</li> <li>Testing comprehensive responses</li> <li>Evaluating educational content</li> </ul> \u274c Don't Use When <ul> <li>Brevity is preferred</li> <li>Multiple valid answer formats</li> <li>No expected_output available</li> <li>Creative/generative tasks</li> </ul> <p>See Also: Answer Criteria</p> <p>Answer Completeness checks coverage of expected output aspects. Answer Criteria checks coverage of custom acceptance criteria.</p> <p>Use Completeness when you have a reference answer; use Criteria for custom requirements.</p> How It Works  Computation Scoring System <p>The metric extracts key aspects from the expected output and checks if each is covered in the actual response.</p> <p> \u2705 COVERED 1 Aspect from expected output is present in the response. </p> <p> \u274c NOT COVERED 0 Aspect from expected output is missing from the response. </p> <p>Score Formula</p> <pre><code>score = covered_aspects / total_aspects\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Expected Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Aspect Extraction\"]\n        D[Extract Aspects from Expected]\n        E[\"Key Aspects List\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Coverage Check\"]\n        F[Check Each Aspect in Response]\n        G[\"Covered / Not Covered\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Covered Aspects\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>use_expected_output</code> <code>bool</code> <code>True</code> Use expected_output for aspect extraction <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Alternative Mode</p> <p>When <code>use_expected_output=False</code>, the metric uses sub-question decomposition instead of aspect extraction.</p>"},{"location":"metric-registry/composite/answer_completeness/#code-examples","title":"Code Examples","text":"Basic Usage With Runner <pre><code>from axion.metrics import AnswerCompleteness\nfrom axion.dataset import DatasetItem\n\nmetric = AnswerCompleteness()\n\nitem = DatasetItem(\n    query=\"What are the benefits of exercise?\",\n    actual_output=\"Exercise improves cardiovascular health and boosts mood.\",\n    expected_output=\"Exercise improves cardiovascular health, strengthens muscles, boosts mood, and helps with weight management.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (2 of 4 aspects covered)\n</code></pre> <pre><code>from axion.metrics import AnswerCompleteness\nfrom axion.runners import MetricRunner\n\nmetric = AnswerCompleteness()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Covered: {item_result.signals.covered_aspects_count}/{item_result.signals.total_aspects_count}\")\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerCompletenessResult Structure <pre><code>AnswerCompletenessResult(\n{\n    \"score\": 0.5,\n    \"covered_aspects_count\": 2,\n    \"total_aspects_count\": 4,\n    \"concept_coverage_score\": 0.5,\n    \"aspect_breakdown\": [\n        {\n            \"aspect\": \"cardiovascular health improvement\",\n            \"covered\": true,\n            \"concepts_covered\": [\"cardiovascular health\"],\n            \"reason\": \"Mentioned in response\"\n        },\n        {\n            \"aspect\": \"muscle strengthening\",\n            \"covered\": false,\n            \"concepts_missing\": [\"muscles\", \"strength\"],\n            \"reason\": \"Not mentioned in response\"\n        },\n        {\n            \"aspect\": \"mood improvement\",\n            \"covered\": true,\n            \"concepts_covered\": [\"mood\", \"boosts\"],\n            \"reason\": \"Mentioned in response\"\n        },\n        {\n            \"aspect\": \"weight management\",\n            \"covered\": false,\n            \"concepts_missing\": [\"weight\"],\n            \"reason\": \"Not mentioned in response\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall completeness score <code>covered_aspects_count</code> <code>int</code> Aspects found in response <code>total_aspects_count</code> <code>int</code> Total aspects from expected output <code>aspect_breakdown</code> <code>List</code> Per-aspect coverage details"},{"location":"metric-registry/composite/answer_completeness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Complete Coverage (Score: 1.0) <p>All Aspects Covered</p> <p>Expected Output:</p> <p>\"Python is a high-level programming language known for readability, extensive libraries, and cross-platform support.\"</p> <p>AI Response:</p> <p>\"Python is a high-level language with clean, readable syntax. It has a vast ecosystem of libraries and runs on Windows, Mac, and Linux.\"</p> <p>Analysis:</p> Aspect Covered High-level language \u2705 Readability \u2705 Extensive libraries \u2705 Cross-platform \u2705 <p>Final Score: <code>4 / 4 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Coverage (Score: 0.6) <p>Some Aspects Missing</p> <p>Expected Output:</p> <p>\"Our product offers: free shipping, 30-day returns, 24/7 support, price matching, and warranty.\"</p> <p>AI Response:</p> <p>\"We provide free shipping on all orders and a 30-day return policy. Our support team is available around the clock.\"</p> <p>Analysis:</p> Aspect Covered Free shipping \u2705 30-day returns \u2705 24/7 support \u2705 Price matching \u274c Warranty \u274c <p>Final Score: <code>3 / 5 = 0.6</code> </p> \u274c Scenario 3: Poor Coverage (Score: 0.25) <p>Most Aspects Missing</p> <p>Expected Output:</p> <p>\"The recipe requires flour, sugar, eggs, and butter. Preheat oven to 350\u00b0F. Mix ingredients, pour into pan, bake 25 minutes.\"</p> <p>AI Response:</p> <p>\"You'll need flour and sugar.\"</p> <p>Analysis:</p> Aspect Covered Flour \u2705 Sugar \u2705 Eggs \u274c Butter \u274c Oven temperature \u274c Mixing instructions \u274c Baking time \u274c <p>Final Score: <code>2 / 7 = 0.29</code> </p>"},{"location":"metric-registry/composite/answer_completeness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcdd Content Coverage <p>Ensures AI responses include all important information, not just some of it.</p> \ud83c\udf93 Educational Quality <p>Critical for tutoring systems where incomplete answers leave knowledge gaps.</p> \ud83d\udccb Requirements Coverage <p>Verify that responses address all parts of complex queries.</p>"},{"location":"metric-registry/composite/answer_completeness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Completeness = Does the response cover all aspects from the expected answer?</p> <ul> <li>Use it when: You have reference answers and need comprehensive coverage</li> <li>Score interpretation: Higher = more aspects from expected output covered</li> <li>Key difference: Measures coverage, not accuracy</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerCompleteness</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Criteria \u00b7 Factual Accuracy \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/answer_criteria/","title":"Answer Criteria","text":"Evaluate responses against user-defined acceptance criteria LLM-Powered Knowledge Single Turn Multi-Turn"},{"location":"metric-registry/composite/answer_criteria/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Criteria coverage ratio \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>acceptance_criteria</code> <p>What It Measures</p> <p>Answer Criteria evaluates whether a response meets user-defined acceptance criteria. It decomposes criteria into aspects and concepts, then checks coverage. This is ideal for custom evaluation requirements that don't fit standard metrics.</p> Score Interpretation 1.0  All criteria aspects fully covered 0.7+  Most criteria met, minor gaps 0.5  Half the criteria covered &lt; 0.5  Significant criteria not met \u2705 Use When <ul> <li>Custom acceptance criteria exist</li> <li>Domain-specific requirements</li> <li>Multi-aspect evaluation needed</li> <li>Testing conversational agents</li> </ul> \u274c Don't Use When <ul> <li>Standard metrics suffice</li> <li>No clear acceptance criteria</li> <li>Purely factual evaluation</li> <li>Simple pass/fail needed</li> </ul> <p>See Also: Answer Completeness</p> <p>Answer Criteria evaluates against custom acceptance criteria. Answer Completeness evaluates against expected output aspects.</p> <p>Use Criteria for custom requirements; use Completeness when you have a reference answer.</p> How It Works  Computation Scoring Strategies <p>The metric decomposes acceptance criteria into aspects, identifies key concepts per aspect, then checks if the response covers them.</p> <p>Choose how to calculate the final score based on aspect and concept coverage.</p> <p> \ud83d\udcca CONCEPT Score = total_concepts_covered / total_conceptsDefault. Granular concept-level coverage. </p> <p> \ud83d\udccb ASPECT Score = covered_aspects / total_aspectsBinary per-aspect (all-or-nothing). </p> <p> \u2696\ufe0f WEIGHTED Score = 0.7 \u00d7 concept_score + 0.3 \u00d7 aspect_scoreBlend of both approaches. </p>"},{"location":"metric-registry/composite/answer_criteria/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Acceptance Criteria]\n    end\n\n    subgraph DECOMPOSE[\"\ud83d\udd0d Step 1: Criteria Decomposition\"]\n        D[Extract Aspects]\n        E[\"Aspects with Key Concepts\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Coverage Check\"]\n        F[Check Each Aspect]\n        G[\"Covered / Missing Concepts\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Apply Scoring Strategy\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style DECOMPOSE stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#configuration","title":"Configuration","text":"Parameters Scoring Strategies Parameter Type Default Description <code>criteria_key</code> <code>str</code> <code>'Complete'</code> Key to look up criteria <code>scoring_strategy</code> <code>'concept'</code> | <code>'aspect'</code> | <code>'weighted'</code> <code>'concept'</code> How to calculate score <code>check_for_contradictions</code> <code>bool</code> <code>False</code> Check if response contradicts criteria <code>weighted_concept_score_weight</code> <code>float</code> <code>0.7</code> Weight for concept score in weighted strategy <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>multi_turn_aggregation</code> <code>'cumulative'</code> | <code>'average'</code> <code>'cumulative'</code> How to aggregate multi-turn scores <pre><code>from axion.metrics import AnswerCriteria\n\n# Concept-level (default, most granular)\nmetric = AnswerCriteria(scoring_strategy='concept')\n\n# Aspect-level (binary per aspect)\nmetric = AnswerCriteria(scoring_strategy='aspect')\n\n# Weighted blend\nmetric = AnswerCriteria(\n    scoring_strategy='weighted',\n    weighted_concept_score_weight=0.7  # 70% concept, 30% aspect\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#code-examples","title":"Code Examples","text":"Basic Usage Custom Criteria Multi-Turn <pre><code>from axion.metrics import AnswerCriteria\nfrom axion.dataset import DatasetItem\n\nmetric = AnswerCriteria()\n\nitem = DatasetItem(\n    query=\"Explain how to make a good cup of coffee\",\n    actual_output=\"Use fresh beans, grind just before brewing, use water at 200\u00b0F, and brew for 4 minutes.\",\n    acceptance_criteria=\"Must mention: bean freshness, grind timing, water temperature, brew time\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import AnswerCriteria\n\n# Strict aspect-level scoring\nmetric = AnswerCriteria(\n    scoring_strategy='aspect',\n    check_for_contradictions=True\n)\n\nitem = DatasetItem(\n    query=\"What's your return policy?\",\n    actual_output=\"You can return items within 30 days with receipt.\",\n    acceptance_criteria=\"\"\"\n    Must cover:\n    1. Return window (30 days)\n    2. Receipt requirement\n    3. Condition of items\n    4. Refund method\n    \"\"\",\n)\n</code></pre> <pre><code>from axion.metrics import AnswerCriteria\n\nmetric = AnswerCriteria(\n    multi_turn_strategy='all_turns',\n    multi_turn_aggregation='cumulative'  # Criteria can be met across turns\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerCriteriaResult Structure <pre><code>AnswerCriteriaResult(\n{\n    \"scoring_strategy\": \"concept\",\n    \"covered_aspects_count\": 3,\n    \"total_aspects_count\": 4,\n    \"total_concepts_covered\": 5,\n    \"total_concepts\": 7,\n    \"concept_coverage_score\": 0.71,\n    \"aspect_breakdown\": [\n        {\n            \"aspect\": \"Bean freshness\",\n            \"covered\": true,\n            \"concepts_covered\": [\"fresh beans\", \"quality\"],\n            \"concepts_missing\": [],\n            \"reason\": \"Response mentions using fresh beans\"\n        },\n        {\n            \"aspect\": \"Water temperature\",\n            \"covered\": true,\n            \"concepts_covered\": [\"200\u00b0F\"],\n            \"concepts_missing\": [\"optimal range\"],\n            \"reason\": \"Specific temperature provided\"\n        }\n    ],\n    \"evaluated_turns_count\": 1\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>scoring_strategy</code> <code>str</code> Strategy used (concept/aspect/weighted) <code>covered_aspects_count</code> <code>int</code> Aspects fully covered <code>total_aspects_count</code> <code>int</code> Total aspects in criteria <code>total_concepts_covered</code> <code>int</code> Concepts found in response <code>total_concepts</code> <code>int</code> Total concepts across all aspects <code>concept_coverage_score</code> <code>float</code> Concept-level coverage ratio <code>aspect_breakdown</code> <code>List</code> Per-aspect coverage details"},{"location":"metric-registry/composite/answer_criteria/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Full Coverage (Score: 1.0) <p>All Criteria Met</p> <p>Criteria:</p> <p>\"Must mention: greeting, issue acknowledgment, solution, follow-up offer\"</p> <p>AI Response:</p> <p>\"Hello! I understand you're having trouble with your order. I've issued a full refund which will appear in 3-5 days. Is there anything else I can help with?\"</p> <p>Analysis:</p> Aspect Covered Concepts Greeting \u2705 \"Hello\" Issue acknowledgment \u2705 \"trouble with your order\" Solution \u2705 \"full refund\", \"3-5 days\" Follow-up offer \u2705 \"anything else I can help\" <p>Final Score: <code>4 / 4 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Coverage (Score: 0.5) <p>Some Criteria Missing</p> <p>Criteria:</p> <p>\"Must include: product name, price, availability, shipping info\"</p> <p>AI Response:</p> <p>\"The Widget Pro costs $49.99 and is currently in stock.\"</p> <p>Analysis:</p> Aspect Covered Concepts Product name \u2705 \"Widget Pro\" Price \u2705 \"$49.99\" Availability \u2705 \"in stock\" Shipping info \u274c missing <p>Final Score (aspect): <code>3 / 4 = 0.75</code></p> <p>No shipping information provided.</p> \u274c Scenario 3: Poor Coverage (Score: 0.25) <p>Most Criteria Not Met</p> <p>Criteria:</p> <p>\"Must cover: apology, explanation, compensation, prevention steps\"</p> <p>AI Response:</p> <p>\"We apologize for the inconvenience.\"</p> <p>Analysis:</p> Aspect Covered Concepts Apology \u2705 \"apologize\" Explanation \u274c missing Compensation \u274c missing Prevention steps \u274c missing <p>Final Score: <code>1 / 4 = 0.25</code> </p>"},{"location":"metric-registry/composite/answer_criteria/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Custom Requirements <p>Define exactly what a good response looks like for your specific use case.</p> \ud83d\udccb Policy Compliance <p>Ensure AI responses follow company guidelines, scripts, or regulatory requirements.</p> \ud83d\udcac Agent Quality <p>Evaluate customer service agents against expected response patterns.</p>"},{"location":"metric-registry/composite/answer_criteria/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Criteria = Does the response meet your custom acceptance criteria?</p> <ul> <li>Use it when: You have specific requirements beyond standard metrics</li> <li>Score interpretation: Higher = more criteria aspects covered</li> <li>Key config: Choose <code>scoring_strategy</code> based on granularity needs</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerCriteria</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Completeness \u00b7 Answer Relevancy \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/answer_relevancy/","title":"Answer Relevancy","text":"Evaluate how well an AI response addresses the input query LLM-Powered Knowledge Single Turn Multi-Turn"},{"location":"metric-registry/composite/answer_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant statements \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>conversation</code> <p>What It Measures</p> <p>Answer Relevancy evaluates whether each statement in the AI's response directly addresses the user's query. Unlike Faithfulness (which checks factual grounding), this metric measures topical alignment\u2014did the AI stay on topic or go off on tangents?</p> Score Interpretation 1.0  Every statement directly addresses the query 0.7+  Mostly relevant with minor tangents 0.5  Threshold\u2014mix of relevant and off-topic content &lt; 0.5  Significant off-topic or irrelevant content \u2705 Use When <ul> <li>Q&amp;A systems &amp; chatbots</li> <li>Customer support agents</li> <li>Search result evaluation</li> <li>Any query-response system</li> </ul> \u274c Don't Use When <ul> <li>Open-ended conversations</li> <li>Exploratory discussions</li> <li>No clear query/question</li> <li>Tasks where tangents are valuable</li> </ul> <p>See Also: Faithfulness</p> <p>Answer Relevancy checks if statements address the user's query (topical alignment). Faithfulness checks if claims are grounded in the source context (factual accuracy).</p> <p>Use both together for comprehensive RAG evaluation.</p> How It Works  Computation Verdict System <p>The metric uses an Evaluator LLM to decompose the response into atomic statements, then judge each statement's relevance to the query.</p> <p>Each extracted statement receives a verdict indicating its relevance to the query.</p> <p> \u2705 YES 1.0 Statement directly addresses the query. Clearly relevant. </p> <p> \u2753 IDK 1.0 Ambiguous relevance. Configurable\u2014can be 0.0 with <code>penalize_ambiguity=True</code> <p> \u274c NO 0.0 Statement is off-topic or doesn't address the query at all. </p> <p>Score Formula</p> <pre><code>score = (yes_count + idk_count*) / total_statements\n\n* idk_count included only if penalize_ambiguity=False (default)\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        C[StatementExtractor LLM]\n        D[\"Atomic Statements&lt;br/&gt;&lt;small&gt;Self-contained facts&lt;/small&gt;\"]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Step 2: Relevancy Judgment\"]\n        E[RelevancyJudge LLM]\n        F[\"Verdict per Statement&lt;br/&gt;&lt;small&gt;yes / no / idk&lt;/small&gt;\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Relevant\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    A --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#configuration","title":"Configuration","text":"Parameters Strict Configuration Multi-Turn Parameter Type Default Description <code>relevancy_mode</code> <code>'strict'</code> | <code>'task'</code> <code>'task'</code> strict: Only direct answers count. task: Helpful related info also counts <code>penalize_ambiguity</code> <code>bool</code> <code>False</code> When <code>True</code>, ambiguous (<code>idk</code>) verdicts score 0.0 instead of 1.0 <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Relevancy Modes</p> <ul> <li><code>task</code> mode (default): More lenient\u2014counts closely related, helpful information as relevant</li> <li><code>strict</code> mode: Only statements that directly answer the question count as relevant</li> </ul> <p>For high-precision evaluation where tangential information should be penalized:</p> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Strict evaluation: only direct answers, penalize ambiguity\nmetric = AnswerRelevancy(\n    relevancy_mode='strict',\n    penalize_ambiguity=True\n)\n</code></pre> <p>For conversational AI evaluation:</p> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Evaluate all turns in a conversation\nmetric = AnswerRelevancy(\n    multi_turn_strategy='all_turns'  # or 'last_turn' (default)\n)\n</code></pre> <ul> <li><code>last_turn</code>: Only evaluates the final Human\u2192AI exchange</li> <li><code>all_turns</code>: Evaluates every turn and aggregates via micro-averaging</li> </ul>"},{"location":"metric-registry/composite/answer_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Strict Mode Multi-Turn Conversation <pre><code>from axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize with defaults (task mode, lenient)\nmetric = AnswerRelevancy()\n\nitem = DatasetItem(\n    query=\"What features does this laptop have?\",\n    actual_output=(\n        \"The laptop has a 15-inch Retina display and 16GB of RAM. \"\n        \"It also comes with a 1-year warranty. \"\n        \"Our company was founded in 2010.\"\n    ),\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score ~0.67: warranty is borderline, founding year is irrelevant\n</code></pre> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Strict: only direct answers count\nmetric = AnswerRelevancy(\n    relevancy_mode='strict',\n    penalize_ambiguity=True\n)\n\n# Now only \"15-inch display\" and \"16GB RAM\" statements count\n# Warranty = ambiguous (0.0), founding year = no (0.0)\n</code></pre> <pre><code>from axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem, MultiTurnConversation\nfrom axion.schema import HumanMessage, AIMessage\n\nconversation = MultiTurnConversation(messages=[\n    HumanMessage(content=\"What is Python?\"),\n    AIMessage(content=\"Python is a programming language known for readability.\"),\n    HumanMessage(content=\"What are its main uses?\"),\n    AIMessage(content=\"Python is used for web dev, data science, and automation.\"),\n])\n\nmetric = AnswerRelevancy(multi_turn_strategy='all_turns')\nitem = DatasetItem(conversation=conversation)\n\nresult = await metric.execute(item)\nprint(f\"Evaluated {result.signals.evaluated_turns_count} turns\")\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerRelevancyResult Structure <pre><code>AnswerRelevancyResult(\n{\n    \"overall_score\": 1.0,\n    \"explanation\": \"The score is 1.0 because the response fully and accurately explains...\",\n    \"relevant_statements_count\": 2,\n    \"irrelevant_statements_count\": 0,\n    \"ambiguous_statements_count\": 0,\n    \"total_statements_count\": 2,\n    \"statement_breakdown\": [\n        {\n            \"statement\": \"The infield fly rule prevents the defense from dropping a fly ball.\",\n            \"verdict\": \"yes\",\n            \"is_relevant\": true,\n            \"turn_index\": 0\n        },\n        {\n            \"statement\": \"The rule prevents an easy double play when runners are on base.\",\n            \"verdict\": \"yes\",\n            \"is_relevant\": true,\n            \"turn_index\": 0\n        }\n    ],\n    \"evaluated_turns_count\": 1\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> The 0-1 relevancy score <code>explanation</code> <code>str</code> Human-readable summary of why the score was given <code>relevant_statements_count</code> <code>int</code> Count of <code>yes</code> verdicts <code>irrelevant_statements_count</code> <code>int</code> Count of <code>no</code> verdicts <code>ambiguous_statements_count</code> <code>int</code> Count of <code>idk</code> verdicts <code>total_statements_count</code> <code>int</code> Total statements extracted <code>statement_breakdown</code> <code>List</code> Per-statement verdict details <code>evaluated_turns_count</code> <code>int</code> Number of conversation turns evaluated"},{"location":"metric-registry/composite/answer_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Relevancy (Score: 1.0) <p>All Statements Relevant</p> <p>Query:</p> <p>\"What are the health benefits of green tea?\"</p> <p>AI Response:</p> <p>\"Green tea contains antioxidants that may reduce inflammation. It also has caffeine which can improve alertness.\"</p> <p>Analysis:</p> Statement Verdict Score Green tea contains antioxidants that may reduce inflammation yes 1.0 Green tea has caffeine which can improve alertness yes 1.0 <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Relevancy (Score: 0.67) <p>Mixed Verdicts</p> <p>Query:</p> <p>\"What features does this laptop have?\"</p> <p>AI Response:</p> <p>\"The laptop has a 15-inch display. It has 16GB RAM. Our company has excellent customer service.\"</p> <p>Analysis:</p> Statement Verdict Score The laptop has a 15-inch display yes 1.0 The laptop has 16GB RAM yes 1.0 Our company has excellent customer service no 0.0 <p>Final Score: <code>2 / 3 = 0.67</code> </p> <p>The customer service statement doesn't address laptop features.</p> \u274c Scenario 3: Poor Relevancy (Score: 0.25) <p>Mostly Off-Topic</p> <p>Query:</p> <p>\"How do I reset my password?\"</p> <p>AI Response:</p> <p>\"Our platform uses industry-standard encryption. We were founded in 2015. Password resets can be done via email. We have offices in 3 countries.\"</p> <p>Analysis:</p> Statement Verdict Score Our platform uses industry-standard encryption no 0.0 We were founded in 2015 no 0.0 Password resets can be done via email yes 1.0 We have offices in 3 countries no 0.0 <p>Final Score: <code>1 / 4 = 0.25</code> </p> <p>Only one statement actually answers the question.</p>"},{"location":"metric-registry/composite/answer_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf User Experience <p>Users expect direct answers. Off-topic responses frustrate users and reduce trust in your AI system.</p> \ud83d\udcac Conversation Quality <p>For chatbots and assistants, staying on topic is crucial. Tangential responses break conversational flow.</p> \ud83d\udd0d Debug Generation <p>Identifies when your model goes off-topic\u2014separate from retrieval issues (Faithfulness) or factual errors.</p>"},{"location":"metric-registry/composite/answer_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Relevancy = Does the AI's response actually address what the user asked?</p> <ul> <li>Use it when: You need to ensure responses stay on topic</li> <li>Score interpretation: Higher = more statements directly address the query</li> <li>Key config: Use <code>relevancy_mode='strict'</code> for precision, <code>'task'</code> for lenient evaluation</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Completeness \u00b7 Context Precision</p> </li> </ul>"},{"location":"metric-registry/composite/citation_relevancy/","title":"Citation Relevancy","text":"Measure the quality and relevance of citations in AI responses LLM-Powered Knowledge Multi-Turn Citation"},{"location":"metric-registry/composite/citation_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant citations \u26a1 Default Threshold <code>0.8</code> High bar for citation quality \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>conversation</code> <p>What It Measures</p> <p>Citation Relevancy evaluates whether the citations included in an AI response are actually relevant to the user's query. It extracts citations using pattern matching, then judges each citation's relevance using an LLM. Essential for research assistants and fact-checking systems.</p> Score Interpretation 1.0  All citations directly relevant to query 0.8+  Most citations relevant, minor tangents 0.5  Mixed relevance\u2014some helpful, some off-topic &lt; 0.5  Mostly irrelevant or unrelated citations \u2705 Use When <ul> <li>Building research assistants</li> <li>Fact-checking systems</li> <li>Academic writing tools</li> <li>Any system that generates citations</li> </ul> \u274c Don't Use When <ul> <li>Responses don't include citations</li> <li>Citation format is non-standard</li> <li>Internal linking (not external sources)</li> <li>Pure conversational AI</li> </ul> <p>See Also: Faithfulness</p> <p>Citation Relevancy checks if cited sources are relevant to the query. Faithfulness checks if claims are grounded in retrieved context.</p> <p>Use Citation Relevancy for output validation; use Faithfulness for RAG grounding.</p> How It Works  Computation Supported Citation Formats Verdict System <p>The metric uses regex-based extraction followed by LLM-based relevance judgment.</p> <p>The metric extracts citations using multiple regex patterns:</p> <p> \ud83d\udcdd Markdown Links <code>Title</code> </p> <p> \ud83d\udd17 HTTP/HTTPS URLs <code>https://example.com/article</code> </p> <p> \ud83c\udf10 WWW URLs <code>www.example.com/page</code> </p> <p> \ud83d\udcda DOI Patterns <code>doi:10.1234/example</code> </p> <p> \ud83c\udf93 Academic Format <code>(Smith et al., 2023)</code> or <code>(Smith, 2023)</code> </p> <p>Each citation receives a binary relevance verdict.</p> <p> \u2705 RELEVANT 1 Citation directly supports answering the user's query. </p> <p> \u274c IRRELEVANT 0 Citation is off-topic or doesn't help answer the question. </p> <p>Score Formula</p> <pre><code>score = relevant_citations / total_citations\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response with Citations]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Citation Extraction\"]\n        C[Regex Pattern Matching]\n        D[\"Extracted Citations\"]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Step 2: Relevance Judgment\"]\n        E[CitationRelevanceJudge LLM]\n        F[\"Verdict per Citation\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Relevant\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    A --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Multi-Turn Support</p> <p>In multi-turn conversations, citations are associated with their corresponding query context:</p> <ul> <li><code>last_turn</code>: Only evaluates citations in the final response</li> <li><code>all_turns</code>: Evaluates citations across all turns, matching each to its original query</li> </ul>"},{"location":"metric-registry/composite/citation_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Multi-Turn With Runner <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.dataset import DatasetItem\n\nmetric = CitationRelevancy()\n\nitem = DatasetItem(\n    query=\"What are the health benefits of green tea?\",\n    actual_output=\"\"\"\n    Green tea has numerous health benefits:\n\n    1. Rich in antioxidants [Source](https://healthline.com/green-tea-benefits)\n    2. May improve brain function (Smith et al., 2020)\n    3. Great for parties! [Party Guide](https://party-planning.com)\n    \"\"\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.67 (2 of 3 citations relevant)\n</code></pre> <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.dataset import DatasetItem, MultiTurnConversation\nfrom axion.schema import HumanMessage, AIMessage\n\nconversation = MultiTurnConversation(messages=[\n    HumanMessage(content=\"What causes climate change?\"),\n    AIMessage(content=\"Climate change is primarily caused by greenhouse gases. [IPCC Report](https://ipcc.ch/report)\"),\n    HumanMessage(content=\"How can I reduce my carbon footprint?\"),\n    AIMessage(content=\"You can reduce emissions by using public transport. [EPA Guide](https://epa.gov/guide)\"),\n])\n\nmetric = CitationRelevancy(multi_turn_strategy='all_turns')\nitem = DatasetItem(conversation=conversation)\n\nresult = await metric.execute(item)\nprint(f\"Evaluated {result.signals.total_citations} citations across turns\")\n</code></pre> <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = CitationRelevancy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_citations_count}/{item_result.signals.total_citations}\")\n    for citation in item_result.signals.citation_breakdown:\n        status = \"\u2705\" if citation.relevance_verdict else \"\u274c\"\n        print(f\"  {status} {citation.citation_text[:50]}...\")\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationRelevancyResult Structure <pre><code>CitationRelevancyResult(\n{\n    \"relevance_score\": 0.67,\n    \"total_citations\": 3,\n    \"relevant_citations_count\": 2,\n    \"irrelevant_citations_count\": 1,\n    \"citation_breakdown\": [\n        {\n            \"citation_text\": \"[Source](https://healthline.com/green-tea-benefits)\",\n            \"relevance_verdict\": true,\n            \"relevance_reason\": \"Directly addresses health benefits of green tea\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        },\n        {\n            \"citation_text\": \"(Smith et al., 2020)\",\n            \"relevance_verdict\": true,\n            \"relevance_reason\": \"Academic source on tea and brain function\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        },\n        {\n            \"citation_text\": \"[Party Guide](https://party-planning.com)\",\n            \"relevance_verdict\": false,\n            \"relevance_reason\": \"Party planning is unrelated to health benefits\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>relevance_score</code> <code>float</code> Ratio of relevant citations (0.0-1.0) <code>total_citations</code> <code>int</code> Total citations extracted <code>relevant_citations_count</code> <code>int</code> Count of relevant citations <code>irrelevant_citations_count</code> <code>int</code> Count of irrelevant citations <code>citation_breakdown</code> <code>List</code> Per-citation verdict details"},{"location":"metric-registry/composite/citation_relevancy/#citation-breakdown-fields","title":"Citation Breakdown Fields","text":"Field Type Description <code>citation_text</code> <code>str</code> The extracted citation <code>relevance_verdict</code> <code>bool</code> Whether citation is relevant <code>relevance_reason</code> <code>str</code> Explanation for verdict <code>turn_index</code> <code>int</code> Conversation turn (for multi-turn) <code>original_query</code> <code>str</code> Query that prompted this citation"},{"location":"metric-registry/composite/citation_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: All Citations Relevant (Score: 1.0) <p>High-Quality Citations</p> <p>Query:</p> <p>\"What are effective treatments for type 2 diabetes?\"</p> <p>AI Response:</p> <p>\"Type 2 diabetes can be managed through:</p> <ol> <li>Lifestyle changes including diet and exercise ADA Guidelines</li> <li>Metformin as first-line medication (Johnson et al., 2021)</li> <li>Regular blood glucose monitoring CDC Recommendations\"</li> </ol> <p>Analysis:</p> Citation Verdict Reason ADA Guidelines \u2705 Authoritative diabetes organization Johnson et al., 2021 \u2705 Academic source on medication CDC Recommendations \u2705 Government health authority <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Relevance (Score: 0.5) <p>Some Citations Off-Topic</p> <p>Query:</p> <p>\"How does machine learning work?\"</p> <p>AI Response:</p> <p>\"Machine learning uses algorithms to learn from data:</p> <ul> <li>Neural networks mimic brain structure ML Textbook</li> <li>Training requires large datasets Data Science Blog</li> <li>My favorite coffee shop uses ML! Best Coffee</li> <li>Check out this unrelated video Cat Video\"</li> </ul> <p>Analysis:</p> Citation Verdict Reason ML Textbook \u2705 Directly about machine learning Data Science Blog \u2705 Relevant to ML data requirements Best Coffee \u274c Coffee reviews unrelated to ML Cat Video \u274c Entertainment, not educational <p>Final Score: <code>2 / 4 = 0.5</code> </p> \u274c Scenario 3: Mostly Irrelevant (Score: 0.25) <p>Citation Spam</p> <p>Query:</p> <p>\"What is the capital of France?\"</p> <p>AI Response:</p> <p>\"Paris is the capital of France. Here are some links:</p> <ul> <li>My Portfolio</li> <li>Buy Cheap Flights</li> <li>Wikipedia - France</li> <li>Dating Site\"</li> </ul> <p>Analysis:</p> Citation Verdict Reason My Portfolio \u274c Self-promotion, irrelevant Buy Cheap Flights \u274c Commercial, off-topic Wikipedia - France \u2705 Relevant geographic source Dating Site \u274c Completely unrelated <p>Final Score: <code>1 / 4 = 0.25</code> </p>"},{"location":"metric-registry/composite/citation_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Source Quality <p>Ensures AI-generated citations actually support the response, not random links or self-promotion.</p> \ud83c\udf93 Research Integrity <p>Critical for academic and research tools where citations must be relevant and authoritative.</p> \u2705 User Trust <p>Users expect citations to be helpful. Irrelevant citations damage credibility and waste time.</p>"},{"location":"metric-registry/composite/citation_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Relevancy = Are the citations actually relevant to the user's question?</p> <ul> <li>Use it when: AI responses include citations that need quality validation</li> <li>Score interpretation: Higher = more citations are relevant</li> <li>Key feature: Supports multiple citation formats (URLs, DOIs, academic)</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.CitationRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Relevancy \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_precision/","title":"Contextual Precision","text":"Evaluate if useful context chunks are ranked higher LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_precision/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Mean Average Precision \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>expected_output</code> <code>retrieved_content</code> Ground truth + context <p>What It Measures</p> <p>Contextual Precision evaluates whether useful chunks are ranked higher in the retrieval results. Using Mean Average Precision (MAP), it rewards retrieval systems that place the most helpful documents at the top. A useful chunk is one that contributes to generating the expected answer.</p> Score Interpretation 1.0  All useful chunks at the top 0.7+  Good ranking, useful chunks near top 0.5  Mixed ranking quality &lt; 0.5  Useful chunks buried low in results \u2705 Use When <ul> <li>Evaluating retrieval ranking quality</li> <li>Tuning re-ranking algorithms</li> <li>Testing with limited context windows</li> <li>Optimizing for top-k retrieval</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Chunk order doesn't matter</li> <li>Using all retrieved chunks equally</li> <li>Single-chunk retrieval</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Precision asks: \"Are the most useful chunks ranked first?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant to the query?</li> <li>Contextual Recall: Do chunks cover the expected answer?</li> <li>Contextual Ranking: Are relevant chunks ranked higher?</li> </ul> How It Works  Computation MAP Calculation <p>The metric evaluates chunk usefulness for generating the expected answer, then calculates MAP.</p> <p>Mean Average Precision rewards useful chunks appearing early in the ranking.</p> <p>Example with 5 chunks (U = useful, X = not useful):</p> <pre><code>Position:  1    2    3    4    5\nChunks:   [U]  [X]  [U]  [X]  [U]\n\nPrecision@1 = 1/1 = 1.0   (first useful at position 1)\nPrecision@3 = 2/3 = 0.67  (second useful at position 3)\nPrecision@5 = 3/5 = 0.6   (third useful at position 5)\n\nMAP = (1.0 + 0.67 + 0.6) / 3 = 0.76\n</code></pre> <p> \u2705 USEFUL +P@k Chunk helps generate the expected answer. Contributes to MAP score. </p> <p> \u274c NOT USEFUL 0 Chunk doesn't contribute to the answer. Dilutes precision. </p> <p>Score Formula</p> <pre><code>MAP = sum(Precision@k for each useful chunk) / total_useful_chunks\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Expected Output]\n        C[Retrieved Chunks in Order]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Usefulness Check\"]\n        D[RAGAnalyzer Engine]\n        E1[\"Chunk 1: U/\u2717\"]\n        E2[\"Chunk 2: U/\u2717\"]\n        E3[\"Chunk 3: U/\u2717\"]\n        EN[\"...\"]\n    end\n\n    subgraph MAP[\"\ud83d\udcca Step 2: Calculate MAP\"]\n        F[\"For each useful chunk at position k\"]\n        G[\"Precision@k = useful_seen / k\"]\n        H[\"Average all Precision@k values\"]\n        I[\"Final MAP Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E1 &amp; E2 &amp; E3 &amp; EN\n    E1 &amp; E2 &amp; E3 &amp; EN --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style MAP stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Usefulness vs Relevancy</p> <ul> <li>Relevancy (Contextual Relevancy): \"Is this chunk about the topic?\"</li> <li>Usefulness (Contextual Precision): \"Does this chunk help generate the correct answer?\"</li> </ul> <p>A chunk can be relevant but not useful (e.g., background info that isn't needed).</p>"},{"location":"metric-registry/composite/contextual_precision/#code-examples","title":"Code Examples","text":"Basic Usage Perfect vs Poor Ranking With Runner <pre><code>from axion.metrics import ContextualPrecision\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualPrecision()\n\nitem = DatasetItem(\n    query=\"Who invented the telephone?\",\n    expected_output=\"Alexander Graham Bell invented the telephone in 1876.\",\n    retrieved_content=[\n        \"Alexander Graham Bell invented the telephone.\",  # Useful\n        \"The telephone revolutionized communication.\",    # Not useful\n        \"Bell patented it in 1876.\",                      # Useful\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# MAP: (1/1 + 2/3) / 2 = 0.83\n</code></pre> <pre><code>from axion.metrics import ContextualPrecision\n\nmetric = ContextualPrecision()\n\n# Perfect ranking: useful chunks first\nperfect_order = DatasetItem(\n    query=\"What is Python?\",\n    expected_output=\"Python is a programming language created by Guido van Rossum.\",\n    retrieved_content=[\n        \"Python is a high-level programming language.\",   # Useful (pos 1)\n        \"Guido van Rossum created Python.\",               # Useful (pos 2)\n        \"Programming is fun.\",                            # Not useful\n    ],\n)\n# MAP = (1/1 + 2/2) / 2 = 1.0\n\n# Poor ranking: useful chunks last\npoor_order = DatasetItem(\n    query=\"What is Python?\",\n    expected_output=\"Python is a programming language created by Guido van Rossum.\",\n    retrieved_content=[\n        \"Programming is fun.\",                            # Not useful\n        \"Python is a high-level programming language.\",   # Useful (pos 2)\n        \"Guido van Rossum created Python.\",               # Useful (pos 3)\n    ],\n)\n# MAP = (1/2 + 2/3) / 2 = 0.58\n</code></pre> <pre><code>from axion.metrics import ContextualPrecision\nfrom axion.runners import MetricRunner\n\nmetric = ContextualPrecision()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"MAP Score: {item_result.score}\")\n    print(f\"Useful chunks: {item_result.signals.useful_chunks}/{item_result.signals.total_chunks}\")\n    print(f\"First useful at position: {item_result.signals.first_useful_position}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_useful else \"\u274c\"\n        print(f\"  {i+1}. {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualPrecisionResult Structure <pre><code>ContextualPrecisionResult(\n{\n    \"map_score\": 0.83,\n    \"total_chunks\": 3,\n    \"useful_chunks\": 2,\n    \"first_useful_position\": 1,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Alexander Graham Bell invented the telephone.\",\n            \"is_useful\": true,\n            \"position\": 1\n        },\n        {\n            \"chunk_text\": \"The telephone revolutionized communication.\",\n            \"is_useful\": false,\n            \"position\": 2\n        },\n        {\n            \"chunk_text\": \"Bell patented it in 1876.\",\n            \"is_useful\": true,\n            \"position\": 3\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>map_score</code> <code>float</code> Mean Average Precision (0.0-1.0) <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>useful_chunks</code> <code>int</code> Chunks useful for generating answer <code>first_useful_position</code> <code>int</code> Rank of first useful chunk <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_precision/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_useful</code> <code>bool</code> Whether chunk helps generate expected answer <code>position</code> <code>int</code> Rank position in retrieval results"},{"location":"metric-registry/composite/contextual_precision/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Precision (Score: 1.0) <p>Useful Chunks Ranked First</p> <p>Query:</p> <p>\"What are the three states of matter?\"</p> <p>Expected Output:</p> <p>\"The three states of matter are solid, liquid, and gas.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Matter exists in three states: solid, liquid, and gas.\" \u2705</li> <li>\"Solids have fixed shape, liquids take container shape.\" \u2705</li> <li>\"Gases expand to fill available space.\" \u2705</li> <li>\"Matter is anything that has mass.\" \u274c</li> <li>\"Chemistry is the study of matter.\" \u274c</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 1, 2, 3\nP@1 = 1/1 = 1.0\nP@2 = 2/2 = 1.0\nP@3 = 3/3 = 1.0\nMAP = (1.0 + 1.0 + 1.0) / 3 = 1.0\n</code></pre> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Precision (Score: 0.58) <p>Useful Chunks Buried</p> <p>Query:</p> <p>\"Who wrote Romeo and Juliet?\"</p> <p>Expected Output:</p> <p>\"William Shakespeare wrote Romeo and Juliet.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Shakespeare was born in Stratford-upon-Avon.\" \u274c</li> <li>\"Romeo and Juliet is a famous tragedy.\" \u274c</li> <li>\"William Shakespeare wrote Romeo and Juliet.\" \u2705</li> <li>\"The play was written in the 1590s.\" \u2705</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 3, 4\nP@3 = 1/3 = 0.33\nP@4 = 2/4 = 0.50\nMAP = (0.33 + 0.50) / 2 = 0.42\n</code></pre> <p>Final Score: <code>0.42</code> </p> <p>Key information buried at positions 3-4 instead of 1-2.</p> \u274c Scenario 3: Poor Precision (Score: 0.2) <p>Useful Chunk at Bottom</p> <p>Query:</p> <p>\"What is the speed of light?\"</p> <p>Expected Output:</p> <p>\"The speed of light is approximately 299,792 km/s.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Light is a form of electromagnetic radiation.\" \u274c</li> <li>\"Light travels in waves.\" \u274c</li> <li>\"Light can be refracted through prisms.\" \u274c</li> <li>\"Light behaves as both particles and waves.\" \u274c</li> <li>\"The speed of light is about 300,000 km/s in vacuum.\" \u2705</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 5\nP@5 = 1/5 = 0.2\nMAP = 0.2 / 1 = 0.2\n</code></pre> <p>Final Score: <code>0.2</code> </p> <p>The only useful chunk is at the very bottom.</p>"},{"location":"metric-registry/composite/contextual_precision/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcca Ranking Quality <p>Measures not just what you retrieve, but how well you rank it. Critical for top-k systems.</p> \u26a1 Efficiency <p>When context windows are limited, having useful chunks first means better answers faster.</p> \ud83d\udd27 Re-ranking Tuning <p>Directly measures re-ranking model performance. Low MAP = improve your re-ranker.</p>"},{"location":"metric-registry/composite/contextual_precision/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Precision = Are the most useful chunks ranked at the top?</p> <ul> <li>Use it when: Evaluating retrieval ranking, especially with limited context windows</li> <li>Score interpretation: Higher MAP = useful chunks appear earlier</li> <li>Key formula: Mean Average Precision over useful chunk positions</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualPrecision</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Ranking \u00b7 Contextual Recall \u00b7 Contextual Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_ranking/","title":"Contextual Ranking","text":"Evaluate if relevant context chunks are ranked higher LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_ranking/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Precision-weighted ranking score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No expected_output needed <p>What It Measures</p> <p>Contextual Ranking evaluates whether relevant chunks are positioned higher in retrieval results. Unlike Contextual Precision (which checks usefulness for generating an answer), Ranking simply checks query relevance\u2014making it usable without ground truth.</p> Score Interpretation \u2265 0.9  Excellent ranking\u2014relevant chunks at top \u2265 0.7  Good ranking quality 0.5  Mediocre\u2014relevant chunks scattered &lt; 0.5  Poor ranking\u2014relevant chunks buried \u2705 Use When <ul> <li>No expected_output available</li> <li>Evaluating retrieval ranking</li> <li>Comparing re-ranking algorithms</li> <li>Testing search relevance</li> </ul> \u274c Don't Use When <ul> <li>You have expected_output (use Precision)</li> <li>Chunk order doesn't matter</li> <li>Single-chunk retrieval</li> <li>Evaluating answer quality</li> </ul> <p>Ranking vs Precision</p> <p>Contextual Ranking checks: \"Are relevant chunks ranked higher?\" (based on query) Contextual Precision checks: \"Are useful chunks ranked higher?\" (based on expected answer)</p> <p>Use Ranking when you don't have ground truth; use Precision when you do.</p> How It Works  Computation Ranking Calculation <p>The metric evaluates chunk relevance to the query, then calculates a precision-weighted ranking score.</p> <p>The score heavily penalizes relevant chunks ranked low.</p> <p>Example with 5 chunks (R = relevant, X = not relevant):</p> <pre><code>Position:  1    2    3    4    5\nChunks:   [R]  [X]  [R]  [X]  [R]\n\nPrecision@1 = 1/1 = 1.0   (first relevant at position 1)\nPrecision@3 = 2/3 = 0.67  (second relevant at position 3)\nPrecision@5 = 3/5 = 0.6   (third relevant at position 5)\n\nScore = (1.0 + 0.67 + 0.6) / 3 = 0.76\n</code></pre> <p> \u2705 RELEVANT +P@k Chunk is relevant to the query. Contributes to ranking score. </p> <p> \u274c NOT RELEVANT 0 Chunk is off-topic. Dilutes precision at each position. </p> <p>Score Formula</p> <pre><code>score = sum(Precision@k for each relevant chunk) / total_relevant_chunks\nscore = clamp(score, 0.0, 1.0)\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks in Order]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Relevancy Check\"]\n        C[RAGAnalyzer Engine]\n        D1[\"Chunk 1: R/\u2717\"]\n        D2[\"Chunk 2: R/\u2717\"]\n        D3[\"Chunk 3: R/\u2717\"]\n        DN[\"...\"]\n    end\n\n    subgraph RANK[\"\ud83d\udcca Step 2: Calculate Ranking Score\"]\n        E[\"For each relevant chunk at position k\"]\n        F[\"Precision@k = relevant_seen / k\"]\n        G[\"Sum all Precision@k values\"]\n        H[\"Divide by total relevant chunks\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style RANK stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Interpretation Guide</p> Score Range Quality Recommendation \u2265 0.9 Excellent Ranking is optimal \u2265 0.7 Good Acceptable for most use cases &lt; 0.7 Poor Consider improving re-ranking 0.0 None No relevant chunks found"},{"location":"metric-registry/composite/contextual_ranking/#code-examples","title":"Code Examples","text":"Basic Usage Compare Rankings With Runner <pre><code>from axion.metrics import ContextualRanking\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRanking()\n\nitem = DatasetItem(\n    query=\"What is machine learning?\",\n    retrieved_content=[\n        \"Machine learning is a subset of AI.\",       # Relevant\n        \"Python is a programming language.\",         # Not relevant\n        \"ML models learn from data.\",                # Relevant\n        \"The weather is nice today.\",                # Not relevant\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: (1/1 + 2/3) / 2 = 0.83\n</code></pre> <pre><code>from axion.metrics import ContextualRanking\n\nmetric = ContextualRanking()\n\n# Good ranking: relevant first\ngood_order = DatasetItem(\n    query=\"Benefits of exercise\",\n    retrieved_content=[\n        \"Exercise improves cardiovascular health.\",   # Relevant\n        \"Regular workouts boost energy levels.\",      # Relevant\n        \"Cooking is a useful skill.\",                 # Not relevant\n    ],\n)\n# Score: (1/1 + 2/2) / 2 = 1.0\n\n# Bad ranking: relevant last\nbad_order = DatasetItem(\n    query=\"Benefits of exercise\",\n    retrieved_content=[\n        \"Cooking is a useful skill.\",                 # Not relevant\n        \"Exercise improves cardiovascular health.\",   # Relevant\n        \"Regular workouts boost energy levels.\",      # Relevant\n    ],\n)\n# Score: (1/2 + 2/3) / 2 = 0.58\n</code></pre> <pre><code>from axion.metrics import ContextualRanking\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRanking()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Ranking Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_chunks}/{item_result.signals.total_chunks}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_relevant else \"\u274c\"\n        print(f\"  {i+1}. {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRankingResult Structure <pre><code>ContextualRankingResult(\n{\n    \"final_score\": 0.83,\n    \"relevant_chunks\": 2,\n    \"total_chunks\": 4,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Machine learning is a subset of AI.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"Python is a programming language.\",\n            \"is_relevant\": false\n        },\n        {\n            \"chunk_text\": \"ML models learn from data.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"The weather is nice today.\",\n            \"is_relevant\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Ranking score (0.0-1.0) <code>relevant_chunks</code> <code>int</code> Number of relevant chunks <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_ranking/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_relevant</code> <code>bool</code> Whether chunk is relevant to query"},{"location":"metric-registry/composite/contextual_ranking/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Excellent Ranking (Score: 1.0) <p>All Relevant Chunks First</p> <p>Query:</p> <p>\"How does photosynthesis work?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Photosynthesis converts light energy into chemical energy.\" \u2705</li> <li>\"Plants use chlorophyll to absorb sunlight.\" \u2705</li> <li>\"Photosynthesis produces glucose and oxygen.\" \u2705</li> <li>\"The ocean covers 71% of Earth.\" \u274c</li> <li>\"Volcanic eruptions release gases.\" \u274c</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 1, 2, 3\nP@1 = 1/1 = 1.0\nP@2 = 2/2 = 1.0\nP@3 = 3/3 = 1.0\nScore = (1.0 + 1.0 + 1.0) / 3 = 1.0\n</code></pre> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Mediocre Ranking (Score: 0.5) <p>Relevant Chunks Scattered</p> <p>Query:</p> <p>\"What are the benefits of meditation?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Yoga is an ancient practice.\" \u274c</li> <li>\"Meditation reduces stress and anxiety.\" \u2705</li> <li>\"Cooking can be therapeutic.\" \u274c</li> <li>\"Mindfulness improves focus.\" \u2705</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 2, 4\nP@2 = 1/2 = 0.5\nP@4 = 2/4 = 0.5\nScore = (0.5 + 0.5) / 2 = 0.5\n</code></pre> <p>Final Score: <code>0.5</code> </p> <p>Relevant content not prioritized at top positions.</p> \u274c Scenario 3: Poor Ranking (Score: 0.33) <p>Relevant Chunks at Bottom</p> <p>Query:</p> <p>\"What is the capital of Japan?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Japan has a population of 125 million.\" \u274c</li> <li>\"Japanese cuisine includes sushi.\" \u274c</li> <li>\"Tokyo is the capital of Japan.\" \u2705</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 3\nP@3 = 1/3 = 0.33\nScore = 0.33 / 1 = 0.33\n</code></pre> <p>Final Score: <code>0.33</code> </p> <p>The only relevant chunk is last\u2014poor ranking quality.</p>"},{"location":"metric-registry/composite/contextual_ranking/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf No Ground Truth Needed <p>Evaluate ranking quality without expected answers\u2014ideal for production monitoring.</p> \ud83d\udcca Re-ranker Evaluation <p>Directly measures whether your re-ranking model improves result ordering.</p> \u26a1 Context Window Efficiency <p>When using top-k results, good ranking ensures the best content is included.</p>"},{"location":"metric-registry/composite/contextual_ranking/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Ranking = Are relevant chunks positioned at the top of results?</p> <ul> <li>Use it when: Evaluating retrieval ranking without ground truth</li> <li>Score interpretation: Higher = relevant chunks appear earlier</li> <li>Key difference: Uses query relevance, not answer usefulness</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRanking</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Relevancy \u00b7 Faithfulness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_recall/","title":"Contextual Recall","text":"Measure if retrieved context supports the expected answer LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_recall/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of supported statements \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>expected_output</code> <code>retrieved_content</code> Ground truth required <p>What It Measures</p> <p>Contextual Recall evaluates whether the retrieved context contains sufficient information to support the expected answer. It extracts statements from the ground truth and checks if each is supported by the retrieved chunks. High recall means the retrieval didn't miss important information.</p> Score Interpretation 1.0  All expected facts are in retrieved context 0.7+  Most expected facts supported, minor gaps 0.5  Half the expected facts missing from context &lt; 0.5  Significant information not retrieved \u2705 Use When <ul> <li>You have ground truth answers</li> <li>Evaluating retrieval completeness</li> <li>Testing if critical info is retrieved</li> <li>Debugging \"information not found\" errors</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Multiple valid answers exist</li> <li>Testing retrieval ranking (use Precision)</li> <li>Evaluating generation quality</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Recall asks: \"Does the context contain everything needed to answer correctly?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant to the query?</li> <li>Contextual Precision: Are useful chunks ranked higher?</li> <li>Contextual Sufficiency: Is there enough info overall?</li> </ul> How It Works  Computation Verdict System <p>The metric extracts factual statements from the expected answer and checks context support.</p> <p>Each ground truth statement receives a support verdict.</p> <p> \u2705 SUPPORTED 1 Statement from expected answer is found in retrieved context. </p> <p> \u274c NOT SUPPORTED 0 Statement from expected answer is missing from retrieved context. </p> <p>Score Formula</p> <pre><code>score = supported_statements / total_statements\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Expected Output]\n        B[Retrieved Context]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        C[Extract Factual Statements]\n        D[\"Ground Truth Statements\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Support Check\"]\n        E[Check Against Context]\n        F1[\"Stmt 1: \u2713/\u2717\"]\n        F2[\"Stmt 2: \u2713/\u2717\"]\n        F3[\"Stmt 3: \u2713/\u2717\"]\n        FN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Supported\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F1 &amp; F2 &amp; F3 &amp; FN\n    F1 &amp; F2 &amp; F3 &amp; FN --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Ground Truth Focus</p> <p>Unlike Contextual Relevancy (which asks \"is this chunk relevant?\"), Recall asks \"is this expected fact present?\" It measures retrieval from the answer's perspective.</p>"},{"location":"metric-registry/composite/contextual_recall/#code-examples","title":"Code Examples","text":"Basic Usage Complete Recall With Runner <pre><code>from axion.metrics import ContextualRecall\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRecall()\n\nitem = DatasetItem(\n    expected_output=\"Paris is the capital of France. It has a population of about 2 million.\",\n    retrieved_content=[\n        \"Paris is the capital and largest city of France.\",\n        \"The Eiffel Tower is located in Paris.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (capital fact supported, population fact missing)\n</code></pre> <pre><code>from axion.metrics import ContextualRecall\n\nmetric = ContextualRecall()\n\nitem = DatasetItem(\n    expected_output=\"Python was created by Guido van Rossum in 1991.\",\n    retrieved_content=[\n        \"Python is a programming language created by Guido van Rossum.\",\n        \"Python was first released in 1991.\",\n        \"Python emphasizes code readability.\",\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (both creator and year facts are in context)\n</code></pre> <pre><code>from axion.metrics import ContextualRecall\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRecall()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Supported: {item_result.signals.supported_gt_statements}/{item_result.signals.total_gt_statements}\")\n    for stmt in item_result.signals.statement_breakdown:\n        status = \"\u2705\" if stmt.is_supported else \"\u274c\"\n        print(f\"  {status} {stmt.statement_text}\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRecallResult Structure <pre><code>ContextualRecallResult(\n{\n    \"recall_score\": 0.5,\n    \"total_gt_statements\": 2,\n    \"supported_gt_statements\": 1,\n    \"statement_breakdown\": [\n        {\n            \"statement_text\": \"Paris is the capital of France\",\n            \"is_supported\": true\n        },\n        {\n            \"statement_text\": \"Paris has a population of about 2 million\",\n            \"is_supported\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>recall_score</code> <code>float</code> Ratio of supported statements (0.0-1.0) <code>total_gt_statements</code> <code>int</code> Factual statements from expected output <code>supported_gt_statements</code> <code>int</code> Statements found in context <code>statement_breakdown</code> <code>List</code> Per-statement verdict details"},{"location":"metric-registry/composite/contextual_recall/#statement-breakdown-fields","title":"Statement Breakdown Fields","text":"Field Type Description <code>statement_text</code> <code>str</code> The ground truth statement <code>is_supported</code> <code>bool</code> Whether statement is in context"},{"location":"metric-registry/composite/contextual_recall/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Recall (Score: 1.0) <p>All Facts Retrieved</p> <p>Expected Output:</p> <p>\"The Great Wall of China is over 13,000 miles long. It was built over many centuries, starting in the 7th century BC.\"</p> <p>Retrieved Context:</p> <ol> <li>\"The Great Wall of China stretches over 13,000 miles.\"</li> <li>\"Construction began in the 7th century BC.\"</li> <li>\"Multiple dynasties contributed to building the wall over centuries.\"</li> </ol> <p>Analysis:</p> Statement Verdict Over 13,000 miles long \u2705 Supported Built over many centuries \u2705 Supported Starting in 7th century BC \u2705 Supported <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Recall (Score: 0.5) <p>Missing Information</p> <p>Expected Output:</p> <p>\"Water boils at 100\u00b0C at sea level. At higher altitudes, it boils at lower temperatures due to reduced pressure.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Water boils at 100 degrees Celsius under standard conditions.\"</li> <li>\"Water is essential for life on Earth.\"</li> </ol> <p>Analysis:</p> Statement Verdict Boils at 100\u00b0C at sea level \u2705 Supported Higher altitudes = lower boiling point \u274c Not found Due to reduced pressure \u274c Not found <p>Final Score: <code>1 / 3 = 0.33</code> </p> <p>The altitude/pressure relationship wasn't retrieved.</p> \u274c Scenario 3: Poor Recall (Score: 0.0) <p>Critical Information Missing</p> <p>Expected Output:</p> <p>\"Einstein developed the theory of relativity and won the Nobel Prize for the photoelectric effect.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Albert Einstein was a famous physicist.\"</li> <li>\"Einstein was born in Germany in 1879.\"</li> </ol> <p>Analysis:</p> Statement Verdict Developed theory of relativity \u274c Not found Won Nobel Prize \u274c Not found For photoelectric effect \u274c Not found <p>Final Score: <code>0 / 3 = 0.0</code> </p> <p>None of the key facts from the expected answer were retrieved.</p>"},{"location":"metric-registry/composite/contextual_recall/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Completeness Check <p>Ensures retrieval captures all necessary information, not just some of it.</p> \ud83c\udfaf Answer-Focused <p>Evaluates retrieval from the answer's perspective\u2014did we get what's needed to answer correctly?</p> \ud83d\udc1b Debug Missing Info <p>Identifies exactly which expected facts weren't retrieved, guiding retrieval improvements.</p>"},{"location":"metric-registry/composite/contextual_recall/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Recall = Does the retrieved context contain all facts from the expected answer?</p> <ul> <li>Use it when: You have ground truth and want to measure retrieval completeness</li> <li>Score interpretation: Higher = more expected facts found in context</li> <li>Key insight: Low recall means the retriever missed important information</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRecall</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Relevancy \u00b7 Answer Completeness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_relevancy/","title":"Contextual Relevancy","text":"Evaluate if retrieved context is relevant to the user's query LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant chunks \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No answer required <p>What It Measures</p> <p>Contextual Relevancy evaluates whether the retrieved context chunks are relevant to the user's query. It measures retrieval quality independent of generation\u2014answering: \"Did we retrieve the right documents?\"</p> Score Interpretation 1.0  All retrieved chunks are relevant 0.7+  Most chunks relevant, some noise 0.5  Mixed relevance\u2014half helpful &lt; 0.5  Mostly irrelevant retrieval \u2705 Use When <ul> <li>Evaluating RAG retrieval quality</li> <li>Tuning vector search parameters</li> <li>Debugging poor answer quality</li> <li>Comparing retrieval strategies</li> </ul> \u274c Don't Use When <ul> <li>No retrieval component exists</li> <li>Evaluating answer quality (use Faithfulness)</li> <li>All chunks are from same document</li> <li>Retrieval is keyword-based only</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Relevancy asks: \"Are the retrieved chunks relevant to the query?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Precision: Are relevant chunks ranked higher?</li> <li>Contextual Recall: Do chunks cover the expected answer?</li> <li>Contextual Sufficiency: Is there enough info to answer?</li> </ul> How It Works  Computation Verdict System <p>The metric evaluates each retrieved chunk's relevance to the query.</p> <p>Each chunk receives a binary relevance verdict.</p> <p> \u2705 RELEVANT 1 Chunk contains information useful for answering the query. </p> <p> \u274c IRRELEVANT 0 Chunk is off-topic or doesn't help answer the query. </p> <p>Score Formula</p> <pre><code>score = relevant_chunks / total_chunks\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Relevancy Check\"]\n        C[RAGAnalyzer Engine]\n        D1[\"Chunk 1: \u2713/\u2717\"]\n        D2[\"Chunk 2: \u2713/\u2717\"]\n        D3[\"Chunk 3: \u2713/\u2717\"]\n        DN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 2: Scoring\"]\n        E[\"Count Relevant Chunks\"]\n        F[\"Calculate Ratio\"]\n        G[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style G fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Shared Cache</p> <p>Contextual Relevancy shares an internal cache with other contextual metrics. Running multiple retrieval metrics together is efficient.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Comparing Retrieval With Runner <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRelevancy()\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    retrieved_content=[\n        \"Paris is the capital and largest city of France.\",\n        \"France is known for its wine and cuisine.\",\n        \"The Eiffel Tower was built in 1889.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.67 (2 of 3 chunks relevant)\n</code></pre> <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRelevancy()\nrunner = MetricRunner(metrics=[metric])\n\n# Compare two retrieval strategies\nresults_v1 = await runner.run(dataset_with_bm25)\nresults_v2 = await runner.run(dataset_with_embeddings)\n\navg_v1 = sum(r.score for r in results_v1) / len(results_v1)\navg_v2 = sum(r.score for r in results_v2) / len(results_v2)\n\nprint(f\"BM25 Relevancy: {avg_v1:.2f}\")\nprint(f\"Embedding Relevancy: {avg_v2:.2f}\")\n</code></pre> <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRelevancy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_chunks}/{item_result.signals.total_chunks}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_relevant else \"\u274c\"\n        print(f\"  {status} Chunk {i+1}: {chunk.chunk_text[:50]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRelevancyResult Structure <pre><code>ContextualRelevancyResult(\n{\n    \"relevancy_score\": 0.67,\n    \"total_chunks\": 3,\n    \"relevant_chunks\": 2,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Paris is the capital and largest city of France.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"France is known for its wine and cuisine.\",\n            \"is_relevant\": false\n        },\n        {\n            \"chunk_text\": \"The Eiffel Tower was built in 1889.\",\n            \"is_relevant\": true\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>relevancy_score</code> <code>float</code> Ratio of relevant chunks (0.0-1.0) <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>relevant_chunks</code> <code>int</code> Number of relevant chunks <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_relevancy/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_relevant</code> <code>bool</code> Whether chunk is relevant to query"},{"location":"metric-registry/composite/contextual_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High Relevancy (Score: 1.0) <p>All Chunks Relevant</p> <p>Query:</p> <p>\"How does photosynthesis work?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Photosynthesis converts light energy into chemical energy.\"</li> <li>\"Plants use chlorophyll to absorb sunlight.\"</li> <li>\"The process produces glucose and oxygen from CO2 and water.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Light energy conversion \u2705 Core concept Chlorophyll absorption \u2705 Key mechanism Glucose/oxygen production \u2705 Process outputs <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Relevancy (Score: 0.5) <p>Retrieval Noise</p> <p>Query:</p> <p>\"What are the symptoms of diabetes?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Diabetes symptoms include increased thirst and frequent urination.\"</li> <li>\"Exercise is important for overall health.\"</li> <li>\"Blurred vision and fatigue are common in diabetic patients.\"</li> <li>\"Healthy eating includes fruits and vegetables.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Thirst and urination \u2705 Direct symptoms Exercise importance \u274c General health, not symptoms Blurred vision, fatigue \u2705 Diabetes symptoms Fruits and vegetables \u274c Diet info, not symptoms <p>Final Score: <code>2 / 4 = 0.5</code> </p> \u274c Scenario 3: Poor Relevancy (Score: 0.0) <p>Retrieval Failure</p> <p>Query:</p> <p>\"What is quantum computing?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Classical computers use binary bits.\"</li> <li>\"The internet was invented in the 1960s.\"</li> <li>\"Programming languages include Python and Java.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Binary bits \u274c Classical computing, not quantum Internet history \u274c Completely off-topic Programming languages \u274c Unrelated to quantum concepts <p>Final Score: <code>0 / 3 = 0.0</code> </p> <p>Retrieval completely failed to find quantum computing content.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Retrieval Quality <p>Identifies when your retrieval system returns irrelevant documents, causing poor answer quality.</p> \ud83c\udfaf Debug Isolation <p>Separates retrieval problems from generation problems. Low relevancy = fix retrieval, not the LLM.</p> \u26a1 Efficiency <p>Irrelevant chunks waste context window space and can confuse the generator.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Relevancy = Are the retrieved chunks relevant to the query?</p> <ul> <li>Use it when: Evaluating or tuning RAG retrieval</li> <li>Score interpretation: Higher = more relevant retrieval</li> <li>Key insight: Measures retrieval, not generation</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Recall \u00b7 Faithfulness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_sufficiency/","title":"Contextual Sufficiency","text":"Evaluate if retrieved context contains enough information to answer the query LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_sufficiency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary sufficiency verdict \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No answer required <p>What It Measures</p> <p>Contextual Sufficiency evaluates whether the retrieved context contains enough information to fully answer the user's query. Unlike other metrics that measure partial coverage, this is a binary judgment: either the context is sufficient or it isn't.</p> Score Interpretation 1.0  Context is sufficient to answer the query 0.0  Context is insufficient\u2014information missing \u2705 Use When <ul> <li>Diagnosing retrieval quality</li> <li>Testing retrieval before generation</li> <li>Identifying information gaps</li> <li>Deciding when to expand search</li> </ul> \u274c Don't Use When <ul> <li>Need granular coverage scores</li> <li>Evaluating answer quality</li> <li>Comparing retrieval strategies</li> <li>Need partial credit</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Sufficiency asks: \"Is there enough context to answer this question?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant?</li> <li>Contextual Recall: Are expected facts present?</li> <li>Contextual Utilization: Was the context actually used?</li> </ul> How It Works  Computation Verdict System <p>The metric uses an LLM to make a binary judgment about context sufficiency.</p> <p>A single binary verdict for the entire context.</p> <p> \u2705 SUFFICIENT 1.0 Context contains all necessary information to answer the query completely. </p> <p> \u274c INSUFFICIENT 0.0 Context is missing critical information needed to answer the query. </p> <p>Diagnostic Purpose</p> <p>This metric helps diagnose retrieval issues independent of generation. If sufficiency is low but faithfulness is high, your retriever needs improvement.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Context]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Sufficiency Judgment\"]\n        C[RAGAnalyzer Engine]\n        D[\"Can this context answer the query?\"]\n        E[\"Binary Verdict\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"1.0 = Sufficient\"]\n        G[\"0.0 = Insufficient\"]\n        H[\"Reasoning Provided\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F &amp; G\n    F &amp; G --&gt; H\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px\n    style E fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Binary by Design</p> <p>Unlike other metrics that provide granular scores, Sufficiency is intentionally binary. For partial coverage scores, use Contextual Recall or Contextual Relevancy.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#code-examples","title":"Code Examples","text":"Basic Usage Insufficient Example With Runner <pre><code>from axion.metrics import ContextualSufficiency\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualSufficiency()\n\nitem = DatasetItem(\n    query=\"What is the boiling point of water?\",\n    retrieved_content=[\n        \"Water boils at 100 degrees Celsius at sea level.\",\n        \"This is equivalent to 212 degrees Fahrenheit.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (context is sufficient)\n</code></pre> <pre><code>from axion.metrics import ContextualSufficiency\n\nmetric = ContextualSufficiency()\n\nitem = DatasetItem(\n    query=\"What is the boiling point of water at high altitude?\",\n    retrieved_content=[\n        \"Water boils at 100 degrees Celsius at sea level.\",\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 0.0 (missing altitude information)\nprint(result.signals.reasoning)\n# \"Context only mentions sea level; no information about altitude effects.\"\n</code></pre> <pre><code>from axion.metrics import ContextualSufficiency\nfrom axion.runners import MetricRunner\n\nmetric = ContextualSufficiency()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nsufficient_count = sum(1 for r in results if r.score == 1.0)\nprint(f\"Sufficient: {sufficient_count}/{len(results)}\")\n\nfor item_result in results:\n    if item_result.score == 0.0:\n        print(f\"\u26a0\ufe0f Insufficient for: {item_result.signals.query[:50]}...\")\n        print(f\"   Reason: {item_result.signals.reasoning}\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualSufficiencyResult Structure <pre><code>ContextualSufficiencyResult(\n{\n    \"sufficiency_score\": 1.0,\n    \"is_sufficient\": true,\n    \"reasoning\": \"The context fully addresses the query by providing the boiling point of water (100\u00b0C) and its Fahrenheit equivalent (212\u00b0F).\",\n    \"query\": \"What is the boiling point of water?\",\n    \"context\": \"Water boils at 100 degrees Celsius at sea level. This is equivalent to 212 degrees Fahrenheit.\"\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>sufficiency_score</code> <code>float</code> Binary score (1.0 or 0.0) <code>is_sufficient</code> <code>bool</code> Whether context is sufficient <code>reasoning</code> <code>str</code> Explanation for the verdict <code>query</code> <code>str</code> The user query (preview) <code>context</code> <code>str</code> The retrieved context (preview)"},{"location":"metric-registry/composite/contextual_sufficiency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Sufficient Context (Score: 1.0) <p>Complete Information</p> <p>Query:</p> <p>\"Who invented the telephone and when?\"</p> <p>Retrieved Context:</p> <p>\"Alexander Graham Bell invented the telephone in 1876. He was granted the patent on March 7th of that year.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Inventor identified: Alexander Graham Bell</li> <li>\u2705 Year provided: 1876</li> <li>\u2705 Additional detail: Patent date</li> </ul> <p>Verdict: Sufficient</p> <p>Reasoning: \"The context directly answers both parts of the query\u2014who (Alexander Graham Bell) and when (1876).\"</p> <p>Final Score: <code>1.0</code> </p> \u274c Scenario 2: Insufficient - Missing Key Info (Score: 0.0) <p>Critical Information Missing</p> <p>Query:</p> <p>\"What are the side effects of aspirin?\"</p> <p>Retrieved Context:</p> <p>\"Aspirin is a common pain reliever. It belongs to a class of drugs called NSAIDs. It can be purchased over the counter.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Drug identification: Correct</li> <li>\u2705 Drug class: NSAIDs</li> <li>\u274c Side effects: Not mentioned</li> </ul> <p>Verdict: Insufficient</p> <p>Reasoning: \"The context describes what aspirin is but does not mention any side effects, which is the core of the query.\"</p> <p>Final Score: <code>0.0</code> </p> \u274c Scenario 3: Insufficient - Partial Answer (Score: 0.0) <p>Incomplete Coverage</p> <p>Query:</p> <p>\"Compare the populations of Tokyo and New York City.\"</p> <p>Retrieved Context:</p> <p>\"Tokyo is the capital of Japan with a metropolitan population of over 37 million people, making it the world's most populous metropolitan area.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Tokyo population: Provided</li> <li>\u274c NYC population: Missing</li> <li>\u274c Comparison: Cannot be made</li> </ul> <p>Verdict: Insufficient</p> <p>Reasoning: \"Context only provides Tokyo's population. NYC population is missing, making a comparison impossible.\"</p> <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/composite/contextual_sufficiency/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Retrieval Diagnosis <p>Quickly identify if poor answers stem from insufficient retrieval, not generation quality.</p> \ud83d\udd04 Adaptive Search <p>Use as a signal to expand search or trigger alternative retrieval strategies.</p> \u26a1 Pre-Generation Check <p>Evaluate context before generating\u2014don't waste tokens on insufficient information.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Sufficiency = Is there enough context to fully answer the query?</p> <ul> <li>Use it when: Diagnosing retrieval gaps or deciding to expand search</li> <li>Score interpretation: 1.0 = sufficient, 0.0 = insufficient (binary)</li> <li>Key insight: Identifies \"missing information\" problems in retrieval</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualSufficiency</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Recall \u00b7 Contextual Relevancy \u00b7 Contextual Utilization</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_utilization/","title":"Contextual Utilization","text":"Measure the efficiency of context usage in generation LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_utilization/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of utilized chunks \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Answer + context required <p>What It Measures</p> <p>Contextual Utilization measures the efficiency of context usage\u2014what proportion of relevant retrieved chunks were actually used in the generated answer. Low utilization means relevant information was retrieved but ignored.</p> Score Interpretation 1.0  All relevant chunks were utilized 0.7+  Good utilization, minor waste 0.5  Half of relevant chunks unused &lt; 0.5  Significant waste\u2014relevant info ignored \u2705 Use When <ul> <li>Optimizing context window usage</li> <li>Debugging incomplete answers</li> <li>Identifying generation issues</li> <li>Measuring retrieval efficiency</li> </ul> \u274c Don't Use When <ul> <li>No actual_output available</li> <li>Evaluating retrieval only</li> <li>Testing factual correctness</li> <li>All chunks are equally important</li> </ul> <p>Utilization vs Faithfulness</p> <p>Contextual Utilization asks: \"Was the relevant context actually used?\" Faithfulness asks: \"Is the answer grounded in context?\"</p> <p>High Faithfulness + Low Utilization = Answer is correct but incomplete (missed relevant info).</p> How It Works  Computation Verdict System <p>The metric evaluates which relevant chunks were actually utilized in the answer.</p> <p>Each relevant chunk is checked for utilization.</p> <p> \u2705 UTILIZED 1 Information from this chunk appears in the generated answer. </p> <p> \u274c NOT UTILIZED 0 Relevant chunk was ignored\u2014information not used in answer. </p> <p>Score Formula</p> <p><pre><code>score = utilized_chunks / total_relevant_chunks\n</code></pre> Only relevant chunks are counted\u2014irrelevant chunks don't affect the score.</p>"},{"location":"metric-registry/composite/contextual_utilization/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks]\n        C[Generated Answer]\n    end\n\n    subgraph FILTER[\"\ud83d\udd0d Step 1: Identify Relevant\"]\n        D[Check Chunk Relevancy]\n        E[\"Relevant Chunks Only\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Check Utilization\"]\n        F[Compare to Answer]\n        G1[\"Chunk 1: \u2713/\u2717\"]\n        G2[\"Chunk 2: \u2713/\u2717\"]\n        G3[\"Chunk 3: \u2713/\u2717\"]\n        GN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Utilized\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; D\n    D --&gt; E\n    E --&gt; F\n    C --&gt; F\n    F --&gt; G1 &amp; G2 &amp; G3 &amp; GN\n    G1 &amp; G2 &amp; G3 &amp; GN --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style FILTER stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Relevance Filtering</p> <p>The metric first filters to only relevant chunks (using the same logic as Contextual Relevancy), then checks which of those were utilized. This means irrelevant chunks don't penalize or inflate the score.</p>"},{"location":"metric-registry/composite/contextual_utilization/#code-examples","title":"Code Examples","text":"Basic Usage Full Utilization With Runner <pre><code>from axion.metrics import ContextualUtilization\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualUtilization()\n\nitem = DatasetItem(\n    query=\"What are the health benefits of green tea?\",\n    actual_output=\"Green tea contains antioxidants that reduce inflammation.\",\n    retrieved_content=[\n        \"Green tea is rich in antioxidants.\",                    # Relevant, utilized\n        \"Antioxidants help reduce inflammation.\",                # Relevant, utilized\n        \"Green tea can boost metabolism.\",                       # Relevant, NOT utilized\n        \"Tea originated in China thousands of years ago.\",       # Not relevant\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 2/3 = 0.67 (2 of 3 relevant chunks utilized)\n</code></pre> <pre><code>from axion.metrics import ContextualUtilization\n\nmetric = ContextualUtilization()\n\nitem = DatasetItem(\n    query=\"What is Python?\",\n    actual_output=\"Python is a high-level programming language created by Guido van Rossum, known for its readability.\",\n    retrieved_content=[\n        \"Python is a high-level programming language.\",          # Utilized\n        \"Guido van Rossum created Python.\",                      # Utilized\n        \"Python emphasizes code readability.\",                   # Utilized\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (all relevant chunks utilized)\n</code></pre> <pre><code>from axion.metrics import ContextualUtilization\nfrom axion.runners import MetricRunner\n\nmetric = ContextualUtilization()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Utilization: {item_result.score:.0%}\")\n    print(f\"Used: {item_result.signals.utilized_chunks}/{item_result.signals.total_relevant_chunks}\")\n    for chunk in item_result.signals.chunk_breakdown:\n        status = \"\u2705\" if chunk.is_utilized else \"\u274c\"\n        print(f\"  {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualUtilizationResult Structure <pre><code>ContextualUtilizationResult(\n{\n    \"utilization_score\": 0.67,\n    \"total_relevant_chunks\": 3,\n    \"utilized_chunks\": 2,\n    \"utilization_rate\": \"66.7%\",\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Green tea is rich in antioxidants.\",\n            \"is_utilized\": true\n        },\n        {\n            \"chunk_text\": \"Antioxidants help reduce inflammation.\",\n            \"is_utilized\": true\n        },\n        {\n            \"chunk_text\": \"Green tea can boost metabolism.\",\n            \"is_utilized\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>utilization_score</code> <code>float</code> Ratio of utilized chunks (0.0-1.0) <code>total_relevant_chunks</code> <code>int</code> Relevant chunks in context <code>utilized_chunks</code> <code>int</code> Chunks actually used in answer <code>utilization_rate</code> <code>str</code> Human-readable percentage <code>chunk_breakdown</code> <code>List</code> Per-chunk (relevant only) details"},{"location":"metric-registry/composite/contextual_utilization/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The relevant chunk content <code>is_utilized</code> <code>bool</code> Whether chunk was used in answer"},{"location":"metric-registry/composite/contextual_utilization/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Full Utilization (Score: 1.0) <p>All Relevant Info Used</p> <p>Query:</p> <p>\"What is the boiling point of water?\"</p> <p>Retrieved Context:</p> <ol> <li>\"Water boils at 100\u00b0C at sea level.\" \u2705 Relevant</li> <li>\"This is equivalent to 212\u00b0F.\" \u2705 Relevant</li> <li>\"Ice cream is a popular dessert.\" \u274c Not relevant</li> </ol> <p>Generated Answer:</p> <p>\"Water boils at 100\u00b0C (212\u00b0F) at sea level.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Boils at 100\u00b0C \u2705 Used Equivalent to 212\u00b0F \u2705 Used <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Utilization (Score: 0.5) <p>Relevant Info Ignored</p> <p>Query:</p> <p>\"What are the benefits of exercise?\"</p> <p>Retrieved Context:</p> <ol> <li>\"Exercise improves cardiovascular health.\" \u2705 Relevant</li> <li>\"Regular exercise boosts mood and energy.\" \u2705 Relevant</li> <li>\"Exercise helps with weight management.\" \u2705 Relevant</li> <li>\"Gyms offer various equipment.\" \u274c Not relevant</li> </ol> <p>Generated Answer:</p> <p>\"Exercise improves cardiovascular health and boosts mood.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Cardiovascular health \u2705 Used Mood and energy \u2705 Used (partial) Weight management \u274c Not used <p>Final Score: <code>2 / 3 = 0.67</code> </p> <p>Weight management benefit was retrieved but not mentioned.</p> \u274c Scenario 3: Poor Utilization (Score: 0.25) <p>Most Relevant Info Wasted</p> <p>Query:</p> <p>\"Explain the causes of World War I.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Assassination of Archduke Franz Ferdinand triggered WWI.\" \u2705 Relevant</li> <li>\"Alliance systems escalated regional conflicts.\" \u2705 Relevant</li> <li>\"Nationalism and imperialism created tensions.\" \u2705 Relevant</li> <li>\"The war lasted from 1914 to 1918.\" \u2705 Relevant</li> </ol> <p>Generated Answer:</p> <p>\"World War I began after the assassination of Archduke Franz Ferdinand.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Assassination \u2705 Used Alliance systems \u274c Not used Nationalism/imperialism \u274c Not used Duration \u274c Not used <p>Final Score: <code>1 / 4 = 0.25</code> </p> <p>Only one cause mentioned despite retrieving multiple.</p>"},{"location":"metric-registry/composite/contextual_utilization/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Answer Completeness <p>Low utilization often indicates incomplete answers that miss relevant information.</p> \ud83d\udcb0 Efficiency <p>Retrieved content costs tokens. Low utilization = wasted context window space.</p> \ud83d\udd27 Debug Generation <p>If retrieval is good but utilization is low, the problem is in generation, not retrieval.</p>"},{"location":"metric-registry/composite/contextual_utilization/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Utilization = Was the relevant retrieved context actually used in the answer?</p> <ul> <li>Use it when: Debugging incomplete answers or optimizing context usage</li> <li>Score interpretation: Higher = more efficient use of retrieved information</li> <li>Key insight: Measures generation efficiency, not retrieval quality</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualUtilization</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Contextual Relevancy \u00b7 Answer Completeness</p> </li> </ul>"},{"location":"metric-registry/composite/factual_accuracy/","title":"Factual Accuracy","text":"Verify AI responses against ground truth statements LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/factual_accuracy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of supported statements \u26a1 Default Threshold <code>0.8</code> Higher bar for accuracy \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>expected_output</code> Ground truth required <p>What It Measures</p> <p>Factual Accuracy calculates the percentage of statements in the AI's response that are factually supported by the ground truth (expected_output). Unlike Faithfulness (which checks against retrieved context), this metric verifies against a known-correct answer.</p> Score Interpretation 1.0  Every statement matches ground truth 0.8+  Most statements accurate, minor gaps 0.5  Half the statements are unsupported &lt; 0.5  Significant factual errors \u2705 Use When <ul> <li>You have ground truth answers</li> <li>Testing against known-correct responses</li> <li>Evaluating factual Q&amp;A systems</li> <li>Regression testing AI outputs</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Multiple valid answers exist</li> <li>Testing creative/generative tasks</li> <li>Ground truth may be incomplete</li> </ul> <p>See Also: Faithfulness</p> <p>Factual Accuracy verifies against ground truth (expected_output). Faithfulness verifies against retrieved context.</p> <p>Use Factual Accuracy when you have known-correct answers; use Faithfulness for RAG systems.</p> How It Works  Computation Verdict System <p>The metric extracts statements from the AI response and checks each against the ground truth.</p> <p>Each statement receives a binary verdict\u2014either supported or not supported by the ground truth.</p> <p> \u2705 SUPPORTED 1 Statement is factually consistent with the ground truth. </p> <p> \u274c NOT SUPPORTED 0 Statement is not found or contradicts the ground truth. </p> <p>Score Formula</p> <pre><code>score = supported_statements / total_statements\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Expected Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        D[Extract Statements from Response]\n        E[\"Atomic Statements\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 2: Ground Truth Check\"]\n        F[Compare to Expected Output]\n        G[\"Supported / Not Supported\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Supported\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    C --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VERIFY stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Simple Configuration</p> <p>Factual Accuracy has minimal configuration\u2014it focuses on binary correctness against ground truth.</p>"},{"location":"metric-registry/composite/factual_accuracy/#code-examples","title":"Code Examples","text":"Basic Usage With Runner <pre><code>from axion.metrics import FactualAccuracy\nfrom axion.dataset import DatasetItem\n\nmetric = FactualAccuracy()\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"Paris is the capital of France. It has a population of about 2 million.\",\n    expected_output=\"Paris is the capital of France. The city has approximately 2.1 million inhabitants.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import FactualAccuracy\nfrom axion.runners import MetricRunner\n\nmetric = FactualAccuracy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    for verdict in item_result.signals.verdicts:\n        status = \"\u2705\" if verdict.is_supported else \"\u274c\"\n        print(f\"  {status} {verdict.statement}\")\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca FactualityReport Structure <pre><code>FactualityReport(\n{\n    \"verdicts\": [\n        {\n            \"statement\": \"Paris is the capital of France.\",\n            \"is_supported\": 1,\n            \"reason\": \"The ground truth confirms Paris is the capital of France.\"\n        },\n        {\n            \"statement\": \"It has a population of about 2 million.\",\n            \"is_supported\": 1,\n            \"reason\": \"The ground truth states approximately 2.1 million, which aligns with 'about 2 million'.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>verdicts</code> <code>List[StatementVerdict]</code> Per-statement verdicts"},{"location":"metric-registry/composite/factual_accuracy/#statement-verdict-fields","title":"Statement Verdict Fields","text":"Field Type Description <code>statement</code> <code>str</code> The extracted statement <code>is_supported</code> <code>int</code> 1 = supported, 0 = not supported <code>reason</code> <code>str</code> Explanation for the verdict"},{"location":"metric-registry/composite/factual_accuracy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Accuracy (Score: 1.0) <p>All Statements Supported</p> <p>Query:</p> <p>\"What year did World War II end?\"</p> <p>Expected Output:</p> <p>\"World War II ended in 1945. Germany surrendered in May, and Japan in September.\"</p> <p>AI Response:</p> <p>\"World War II ended in 1945. Germany surrendered in May, Japan in September.\"</p> <p>Analysis:</p> Statement Verdict Score World War II ended in 1945 SUPPORTED 1 Germany surrendered in May SUPPORTED 1 Japan surrendered in September SUPPORTED 1 <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Accuracy (Score: 0.67) <p>Mixed Verdicts</p> <p>Query:</p> <p>\"What is the speed of light?\"</p> <p>Expected Output:</p> <p>\"The speed of light is approximately 299,792 km/s in a vacuum.\"</p> <p>AI Response:</p> <p>\"The speed of light is about 300,000 km/s. It travels slower through water. Light is the fastest thing in the universe.\"</p> <p>Analysis:</p> Statement Verdict Score Speed of light is about 300,000 km/s SUPPORTED 1 It travels slower through water NOT SUPPORTED 0 Light is the fastest thing in the universe NOT SUPPORTED 0 <p>Final Score: <code>1 / 3 = 0.33</code> </p> <p>The ground truth only mentions vacuum speed\u2014other claims are unsupported.</p> \u274c Scenario 3: Poor Accuracy (Score: 0.0) <p>No Statements Supported</p> <p>Query:</p> <p>\"Who wrote Romeo and Juliet?\"</p> <p>Expected Output:</p> <p>\"Romeo and Juliet was written by William Shakespeare in the 1590s.\"</p> <p>AI Response:</p> <p>\"Romeo and Juliet was written by Christopher Marlowe in 1610.\"</p> <p>Analysis:</p> Statement Verdict Score Written by Christopher Marlowe NOT SUPPORTED 0 Written in 1610 NOT SUPPORTED 0 <p>Final Score: <code>0 / 2 = 0.0</code> </p> <p>Both claims contradict the ground truth.</p>"},{"location":"metric-registry/composite/factual_accuracy/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Ground Truth Validation <p>When you have known-correct answers, this metric tells you exactly how well your AI matches reality.</p> \ud83e\uddea Regression Testing <p>Track factual accuracy over time as you update models or prompts. Catch regressions before deployment.</p> \ud83d\udcca Benchmark Evaluation <p>Compare different models or configurations using the same ground truth dataset.</p>"},{"location":"metric-registry/composite/factual_accuracy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Factual Accuracy = Does the AI's response match the known-correct answer?</p> <ul> <li>Use it when: You have ground truth (expected_output) to compare against</li> <li>Score interpretation: Higher = more statements verified against ground truth</li> <li>Key difference: Compares to expected_output, not retrieved_content</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.FactualAccuracy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Completeness \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/faithfulness/","title":"Faithfulness","text":"Measure factual consistency between AI responses and source documents LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/faithfulness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Clamped from weighted average \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Three fields needed <p>What It Measures</p> <p>Faithfulness evaluates whether every claim in the AI's response can be directly inferred from the provided source material. It acts as your primary defense against hallucinations\u2014ensuring the AI summarizes existing knowledge rather than inventing facts.</p> Score Interpretation 1.0  Every claim is fully supported by context 0.7+  Most claims supported, minor gaps 0.5  Threshold\u2014mixture of supported and unsupported &lt; 0.5  Significant hallucinations or contradictions \u2705 Use When <ul> <li>RAG systems &amp; document Q&amp;A</li> <li>Knowledge base assistants</li> <li>Summarization tasks</li> <li>Any system with retrieved context</li> </ul> \u274c Don't Use When <ul> <li>Creative writing / brainstorming</li> <li>Opinion or preference questions</li> <li>No retrieved context available</li> <li>Open-ended generation tasks</li> </ul> <p>See Also: Answer Relevancy</p> <p>Faithfulness checks if claims are grounded in the source context (factual accuracy). Answer Relevancy checks if statements address the user's query (topical alignment).</p> <p>Use both together for comprehensive RAG evaluation.</p> How It Works  Computation Verdict System <p>The metric uses an Evaluator LLM to decompose the response into atomic claims, then verify each against the retrieved context.</p> <p>Each extracted claim receives a verdict with a corresponding weight. The final score is the weighted average, clamped to <code>[0, 1]</code>.</p> <p> \u2705 FULLY_SUPPORTED +1.0 The claim is explicitly stated in the context. Direct evidence exists. </p> <p> \u26a0\ufe0f PARTIALLY_SUPPORTED +0.5 Core subject is correct but claim exaggerates certainty or has minor inaccuracies. </p> <p> \u2753 NO_EVIDENCE 0.0 Context doesn't contain information to verify the claim. Hallucination. </p> <p> \u274c CONTRADICTORY -1.0 Evidence directly contradicts the claim. Critical factual error. </p> <p>Score Formula</p> <pre><code>score = max(0.0, min(1.0, sum(verdict_weights) / total_claims))\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Context]\n        C[AI Response]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Claim Extraction\"]\n        D[StatementExtractor LLM]\n        E[\"Atomic Claims&lt;br/&gt;&lt;small&gt;Self-contained, verifiable&lt;/small&gt;\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 2: Verification\"]\n        F[FaithfulnessJudge LLM]\n        G[\"Verdict per Claim\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Sum Weighted Verdicts\"]\n        I[\"Clamp to [0, 1]\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VERIFY stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/faithfulness/#configuration","title":"Configuration","text":"Parameters Custom Weights Parameter Type Default Description <code>strict_mode</code> <code>bool</code> <code>False</code> When <code>True</code>, <code>NO_EVIDENCE</code> verdicts receive -1.0 (same as contradictions), heavily penalizing hallucinations <code>verdict_scores</code> <code>Dict[str, float]</code> <code>None</code> Custom override for verdict weights. Takes precedence over <code>strict_mode</code> <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level (<code>GRANULAR</code> or <code>HOLISTIC</code>) <p>Strict Mode</p> <p>Enable <code>strict_mode=True</code> for high-stakes domains (legal, medical, financial) where any uncited claim is unacceptable\u2014even if not directly contradicted.</p> <p>Override default verdict weights for domain-specific calibration:</p> <pre><code>from axion.metrics import Faithfulness\n\n# Extra penalty for contradictions, higher partial credit\nmetric = Faithfulness(\n    verdict_scores={\n        'FULLY_SUPPORTED': 1.0,\n        'PARTIALLY_SUPPORTED': 0.75,  # More generous\n        'NO_EVIDENCE': -0.5,          # Moderate penalty\n        'CONTRADICTORY': -2.0,        # Severe penalty\n    }\n)\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#code-examples","title":"Code Examples","text":"Basic Usage Strict Mode With Runner <pre><code>from axion.metrics import Faithfulness\nfrom axion.dataset import DatasetItem\n\n# Initialize with defaults\nmetric = Faithfulness()\n\nitem = DatasetItem(\n    query=\"What is the infield fly rule in baseball?\",\n    actual_output=\"The infield fly rule prevents the defense from intentionally dropping a fly ball to turn a double play.\",\n    retrieved_content=[\n        \"The infield fly rule prevents unfair advantage.\",\n        \"Applies with runners on first and second.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import Faithfulness\n\n# Zero tolerance for hallucinations\nmetric = Faithfulness(strict_mode=True)\n\n# Any NO_EVIDENCE claim now scores -1.0 instead of 0.0\n# This dramatically lowers scores for responses with uncited claims\n</code></pre> <pre><code>from axion.metrics import Faithfulness\nfrom axion.runners import MetricRunner\n\n# Initialize with strict mode\nfaithfulness = Faithfulness(strict_mode=True)\n\nrunner = MetricRunner(metrics=[faithfulness])\nresults = await runner.run(dataset)\n\n# Access detailed breakdown\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Claims analyzed: {item_result.data.total_claims}\")\n    for claim in item_result.data.judged_claims:\n        print(f\"  - {claim.verdict}: {claim.text}\")\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca FaithfulnessResult Structure <pre><code>FaithfulnessResult(\n{\n    \"overall_score\": 0.5,\n    \"total_claims\": 2,\n    \"verdict_counts\": {\n        \"fully_supported\": 1,\n        \"partially_supported\": 0,\n        \"no_evidence\": 1,\n        \"contradictory\": 0\n    },\n    \"judged_claims\": [\n        {\n            \"claim_text\": \"The infield fly rule prevents the defense from intentionally dropping a fly ball.\",\n            \"faithfulness_verdict\": \"Fully Supported\",\n            \"reason\": \"The evidence states that the infield fly rule prevents the defense from intentionally dropping a catchable fly ball.\"\n        },\n        {\n            \"claim_text\": \"The infield fly rule is designed to prevent an easy double play when runners are on base.\",\n            \"faithfulness_verdict\": \"No Evidence\",\n            \"reason\": \"The evidence does not mention anything about preventing an easy double play when runners are on base.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> The 0-1 faithfulness score <code>total_claims</code> <code>int</code> Total claims extracted from the response <code>verdict_counts</code> <code>Dict</code> Breakdown by verdict type (<code>fully_supported</code>, <code>partially_supported</code>, <code>no_evidence</code>, <code>contradictory</code>) <code>judged_claims</code> <code>List</code> Per-claim verdict details"},{"location":"metric-registry/composite/faithfulness/#judged-claim-fields","title":"Judged Claim Fields","text":"Field Type Description <code>claim_text</code> <code>str</code> The extracted claim text <code>faithfulness_verdict</code> <code>str</code> Verdict: <code>Fully Supported</code>, <code>Partially Supported</code>, <code>No Evidence</code>, or <code>Contradictory</code> <code>reason</code> <code>str</code> Human-readable explanation for the verdict"},{"location":"metric-registry/composite/faithfulness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Faithfulness (Score: 1.0) <p>FULLY_SUPPORTED</p> <p>Context:</p> <p>\"The Apollo 11 mission launched on July 16, 1969. Neil Armstrong was the mission commander. The lunar module was named Eagle.\"</p> <p>AI Response:</p> <p>\"Apollo 11 launched in July 1969 with Neil Armstrong as commander. The lunar module was called Eagle.\"</p> <p>Analysis:</p> Claim Verdict Weight Apollo 11 launched in July 1969 FULLY_SUPPORTED +1.0 Neil Armstrong was commander FULLY_SUPPORTED +1.0 Lunar module was called Eagle FULLY_SUPPORTED +1.0 <p>Final Score: <code>3.0 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Support (Score: 0.5) <p>Mixed Verdicts</p> <p>Context:</p> <p>\"Our refund policy allows returns within 30 days. Items must be unused and in original packaging.\"</p> <p>AI Response:</p> <p>\"You can return items within 30 days if unused. Refunds are processed within 24 hours.\"</p> <p>Analysis:</p> Claim Verdict Weight Returns within 30 days if unused FULLY_SUPPORTED +1.0 Refunds processed within 24 hours NO_EVIDENCE 0.0 <p>Final Score: <code>1.0 / 2 = 0.5</code> </p> <p>In strict mode: <code>(1.0 + -1.0) / 2 = 0.0</code></p> \u274c Scenario 3: Contradiction (Score: 0.0) <p>CONTRADICTORY</p> <p>Context:</p> <p>\"The maximum dosage is 500mg per day. Do not exceed this limit.\"</p> <p>AI Response:</p> <p>\"You can safely take up to 1000mg daily.\"</p> <p>Analysis:</p> Claim Verdict Weight Safe to take up to 1000mg daily CONTRADICTORY -1.0 <p>Final Score: <code>max(0, -1.0 / 1) = 0.0</code> </p> <p>Critical: This response could cause patient harm.</p>"},{"location":"metric-registry/composite/faithfulness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udee1\ufe0f Risk Mitigation <p>Primary guardrail against hallucinations. Protects your brand from legal and reputational liability caused by invented facts.</p> \u2705 User Trust <p>Essential for high-stakes domains (legal, financial, medical) where users must trust the AI is summarizing, not creating.</p> \ud83d\udd0d Debug Isolation <p>Distinguishes retrieval errors (wrong docs found) from generation errors (right docs ignored).</p>"},{"location":"metric-registry/composite/faithfulness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Faithfulness = Does the AI's response stick to the facts in the retrieved documents?</p> <ul> <li>Use it when: You need to ensure AI responses don't contain hallucinations</li> <li>Score interpretation: Higher = more grounded in source material</li> <li>Key config: Enable <code>strict_mode</code> for zero-tolerance on uncited claims</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.Faithfulness</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Relevancy \u00b7 Context Precision \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/pii_leakage/","title":"PII Leakage","text":"Detect personally identifiable information and privacy violations in AI outputs LLM-Powered Security Single Turn"},{"location":"metric-registry/composite/pii_leakage/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> 1.0 = no PII found (safe) \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>expected_output</code> <p>What It Measures</p> <p>PII Leakage evaluates whether AI outputs contain personally identifiable information or privacy violations. It detects names, addresses, contact info, financial data, medical records, government IDs, and confidential business information. Higher scores mean safer outputs.</p> Score Interpretation 1.0  No PII detected\u2014completely safe 0.7+  Minor potential PII, low risk 0.5  Some PII detected\u2014review needed &lt; 0.5  Significant PII leakage\u2014unsafe \u2705 Use When <ul> <li>Processing user data</li> <li>Healthcare or financial systems</li> <li>Customer service applications</li> <li>Any system with privacy requirements</li> </ul> \u274c Don't Use When <ul> <li>PII is expected/required in output</li> <li>Internal tools with no privacy concerns</li> <li>Synthetic data generation</li> <li>Testing environments with fake data</li> </ul> <p>Privacy &amp; Compliance</p> <p>This metric helps identify potential GDPR, HIPAA, CCPA, and other regulatory violations. Use it as part of a comprehensive privacy strategy\u2014not as a sole compliance mechanism.</p> How It Works  Computation PII Categories Verdict System <p>The metric uses a 3-step LLM-based process to identify and evaluate potential PII.</p> <p>The metric detects multiple categories of personally identifiable information:</p> <p> \ud83d\udc64 Personal Identity Full names, dates of birth, age, gender </p> <p> \ud83d\udccd Location Data Home addresses, work addresses, GPS coordinates </p> <p> \ud83d\udcde Contact Info Phone numbers, email addresses, social handles </p> <p> \ud83d\udcb3 Financial Data Credit cards, bank accounts, income details </p> <p> \ud83c\udfe5 Medical Info Health conditions, medications, medical records </p> <p> \ud83c\udd94 Government IDs SSN, passport numbers, driver's license </p> <p>Each statement receives a binary privacy verdict.</p> <p> \u2705 CLEAN 1 Statement does not contain personally identifiable information. </p> <p> \u26a0\ufe0f PII DETECTED 0 Statement contains personally identifiable information. </p> <p>Score Formula</p> <p><pre><code>score = clean_statements / total_statements\n</code></pre> Higher scores = safer outputs (1.0 = no PII found)</p>"},{"location":"metric-registry/composite/pii_leakage/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: PII Extraction\"]\n        C[Extract Potential PII Statements]\n        D[\"Candidate Statements\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Privacy Evaluation\"]\n        E[Evaluate Each Statement]\n        F[\"PII / Clean Verdicts\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Clean Statements\"]\n        H[\"Calculate Safety Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Simple Configuration</p> <p>PII Leakage has minimal configuration\u2014it focuses on comprehensive PII detection across all categories.</p>"},{"location":"metric-registry/composite/pii_leakage/#code-examples","title":"Code Examples","text":"Basic Usage Safe Output With Runner <pre><code>from axion.metrics import PIILeakage\nfrom axion.dataset import DatasetItem\n\nmetric = PIILeakage()\n\nitem = DatasetItem(\n    query=\"Tell me about the customer's order\",\n    actual_output=\"The order for John Smith at 123 Main St was shipped yesterday. His phone is 555-1234.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.0 (all statements contain PII)\n</code></pre> <pre><code>from axion.metrics import PIILeakage\n\nmetric = PIILeakage()\n\nitem = DatasetItem(\n    query=\"What's the status of order #12345?\",\n    actual_output=\"Order #12345 was shipped on January 15th and is expected to arrive within 3-5 business days.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (no PII detected)\n</code></pre> <pre><code>from axion.metrics import PIILeakage\nfrom axion.runners import MetricRunner\n\nmetric = PIILeakage()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    if item_result.score &lt; 1.0:\n        print(f\"\u26a0\ufe0f PII detected! Score: {item_result.score}\")\n        for stmt in item_result.signals.statement_breakdown:\n            if stmt.pii_verdict == \"yes\":\n                print(f\"  - {stmt.statement_text}\")\n                print(f\"    Reason: {stmt.reasoning}\")\n</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca PIILeakageResult Structure <pre><code>PIILeakageResult(\n{\n    \"final_score\": 0.33,\n    \"total_statements\": 3,\n    \"violation_count\": 2,\n    \"clean_statements\": 1,\n    \"score_calculation\": \"clean_statements / total_statements = 1 / 3\",\n    \"statement_breakdown\": [\n        {\n            \"statement_text\": \"The order for John Smith was shipped\",\n            \"pii_verdict\": \"yes\",\n            \"reasoning\": \"Contains a person's full name (John Smith)\"\n        },\n        {\n            \"statement_text\": \"shipped to 123 Main St\",\n            \"pii_verdict\": \"yes\",\n            \"reasoning\": \"Contains a physical address\"\n        },\n        {\n            \"statement_text\": \"shipped yesterday\",\n            \"pii_verdict\": \"no\",\n            \"reasoning\": \"General shipping information without PII\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Safety score (1.0 = no violations) <code>total_statements</code> <code>int</code> Potentially sensitive statements extracted <code>violation_count</code> <code>int</code> Statements identified as containing PII <code>clean_statements</code> <code>int</code> Statements without PII <code>score_calculation</code> <code>str</code> Formula explanation <code>statement_breakdown</code> <code>List</code> Per-statement verdict details"},{"location":"metric-registry/composite/pii_leakage/#statement-breakdown-fields","title":"Statement Breakdown Fields","text":"Field Type Description <code>statement_text</code> <code>str</code> The extracted statement <code>pii_verdict</code> <code>str</code> <code>\"yes\"</code> (PII) or <code>\"no\"</code> (clean) <code>reasoning</code> <code>str</code> Explanation for the verdict"},{"location":"metric-registry/composite/pii_leakage/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: No PII (Score: 1.0) <p>Privacy-Safe Response</p> <p>Query:</p> <p>\"What are your business hours?\"</p> <p>AI Response:</p> <p>\"Our store is open Monday through Friday from 9 AM to 6 PM, and Saturday from 10 AM to 4 PM. We're closed on Sundays.\"</p> <p>Analysis:</p> Statement Verdict Reason Open Monday-Friday 9 AM to 6 PM Clean General business info Saturday 10 AM to 4 PM Clean General business info Closed on Sundays Clean General business info <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial PII (Score: 0.5) <p>Some PII Detected</p> <p>Query:</p> <p>\"Can you help me with my account?\"</p> <p>AI Response:</p> <p>\"I can see your account was created in 2020. The email on file is john.doe@email.com. Your subscription is active.\"</p> <p>Analysis:</p> Statement Verdict Reason Account created in 2020 Clean General account info Email is john.doe@email.com PII Contains email address Subscription is active Clean General status info <p>Final Score: <code>2 / 3 = 0.67</code> </p> \u274c Scenario 3: Severe PII Leakage (Score: 0.0) <p>Critical Privacy Violation</p> <p>Query:</p> <p>\"Show me customer details\"</p> <p>AI Response:</p> <p>\"Customer: Sarah Johnson, SSN: 123-45-6789. Address: 456 Oak Ave, Boston, MA. Credit card ending in 4242.\"</p> <p>Analysis:</p> Statement Verdict Reason Sarah Johnson PII Full name SSN: 123-45-6789 PII Social Security Number 456 Oak Ave, Boston, MA PII Physical address Credit card ending 4242 PII Financial information <p>Final Score: <code>0 / 4 = 0.0</code> </p> <p>Critical: Multiple categories of sensitive PII exposed.</p>"},{"location":"metric-registry/composite/pii_leakage/#why-it-matters","title":"Why It Matters","text":"\ud83d\udee1\ufe0f Privacy Protection <p>Prevents accidental exposure of sensitive personal information in AI responses.</p> \u2696\ufe0f Regulatory Compliance <p>Helps maintain compliance with GDPR, HIPAA, CCPA, and other privacy regulations.</p> \ud83d\udd12 Trust &amp; Security <p>Protects user trust by ensuring AI systems don't inadvertently leak personal data.</p>"},{"location":"metric-registry/composite/pii_leakage/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>PII Leakage = Does the AI output contain personally identifiable information?</p> <ul> <li>Use it when: Processing user data or building privacy-sensitive applications</li> <li>Score interpretation: Higher = safer (1.0 = no PII found)</li> <li>Key difference: Detects PII in outputs, not inputs</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.PIILeakage</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Relevancy \u00b7 Tone &amp; Style Consistency</p> </li> </ul>"},{"location":"metric-registry/composite/tone_style_consistency/","title":"Tone &amp; Style Consistency","text":"Evaluate if responses match the expected tone, persona, and formatting style LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/tone_style_consistency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Style alignment score \u26a1 Default Threshold <code>0.8</code> Higher bar for consistency \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Optional: <code>persona_description</code> <p>What It Measures</p> <p>Tone &amp; Style Consistency evaluates whether a response matches the emotional tone and writing style of an expected answer. For customer service agents, \"Voice\" is as important as \"Fact\"\u2014this metric ensures your AI maintains the right persona.</p> Score Interpretation 1.0  Perfect match\u2014exact emotion, enthusiasm, formatting 0.8  Minor drift\u2014generally correct but slightly off 0.5  Significant mismatch\u2014wrong tone or style 0.0  Complete failure\u2014robotic, rude, or ignores persona \u2705 Use When <ul> <li>Building customer service agents</li> <li>Persona consistency matters</li> <li>Brand voice guidelines exist</li> <li>Comparing against reference responses</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Tone flexibility is acceptable</li> <li>Only factual accuracy matters</li> <li>Creative writing tasks</li> </ul> <p>See Also: Answer Completeness</p> <p>Tone &amp; Style Consistency evaluates how something is said (voice, formatting). Answer Completeness evaluates what is said (content coverage).</p> <p>Use both together for comprehensive response quality evaluation.</p> How It Works  Computation Scoring Rubric <p>The metric uses an LLM-based judge to evaluate both emotional tone and writing style.</p> <p>The metric evaluates responses on a detailed rubric with clear benchmarks.</p> <p> \u2705 PERFECT MATCH 1.0 Exact emotion, enthusiasm level, and formatting style. </p> <p> \ud83d\udcca MINOR DRIFT 0.8 Generally correct but slightly less enthusiastic or formal. </p> <p> \u26a0\ufe0f SIGNIFICANT MISMATCH 0.5 Neutral when should be excited, or style completely different. </p> <p> \u274c COMPLETE FAILURE 0.0 Robotic, rude, or completely ignores persona. </p> <p>Two Dimensions</p> <ul> <li>Tone Match: Emotional alignment (enthusiasm, empathy, formality)</li> <li>Style Match: Formatting, length, vocabulary, structure</li> </ul>"},{"location":"metric-registry/composite/tone_style_consistency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n        C[Persona Description]\n    end\n\n    subgraph ANALYZE[\"\ud83d\udd0d Step 1: Style Analysis\"]\n        D[ToneJudge LLM]\n        E[\"Tone &amp; Style Comparison\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Dimension Scoring\"]\n        F[Evaluate Tone Match]\n        G[Evaluate Style Match]\n        H[\"Identify Differences\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Final Score\"]\n        I[\"Combine Dimensions\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F &amp; G\n    F &amp; G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#8B9F4F,stroke-width:2px\n    style ANALYZE stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#8B9F4F,stroke:#6B7A3A,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>persona_description</code> <code>str</code> <code>None</code> Optional persona to enforce (e.g., \"Helpful, excited, professional\") <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Persona Description</p> <p>When provided, the persona description guides the judge on expected tone characteristics, making evaluation more precise for specific brand voices.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#code-examples","title":"Code Examples","text":"Basic Usage With Persona With Runner <pre><code>from axion.metrics import ToneStyleConsistency\nfrom axion.dataset import DatasetItem\n\nmetric = ToneStyleConsistency()\n\nitem = DatasetItem(\n    actual_output=\"Your order has been shipped. It will arrive in 3-5 business days.\",\n    expected_output=\"Great news! \ud83c\udf89 Your order is on its way! You can expect it within 3-5 business days. We're so excited for you!\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (tone mismatch - neutral vs enthusiastic)\n</code></pre> <pre><code>from axion.metrics import ToneStyleConsistency\n\n# Define expected persona\nmetric = ToneStyleConsistency()\n\nitem = DatasetItem(\n    actual_output=\"I apologize for the inconvenience. Let me help resolve this.\",\n    expected_output=\"I'm truly sorry this happened. I completely understand your frustration, and I'm here to make things right!\",\n    persona_description=\"Empathetic, warm, solution-oriented customer service agent\",\n)\n\nresult = await metric.execute(item)\n</code></pre> <pre><code>from axion.metrics import ToneStyleConsistency\nfrom axion.runners import MetricRunner\n\nmetric = ToneStyleConsistency()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Tone Match: {item_result.signals.tone_match}\")\n    print(f\"Style Match: {item_result.signals.style_match}\")\n    for diff in item_result.signals.differences:\n        print(f\"  - {diff.dimension}: {diff.description}\")\n</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ToneStyleResult Structure <pre><code>ToneStyleResult(\n{\n    \"final_score\": 0.5,\n    \"tone_match\": false,\n    \"style_match\": true,\n    \"differences\": [\n        {\n            \"dimension\": \"Enthusiasm Level\",\n            \"expected\": \"Excited, celebratory with emoji\",\n            \"actual\": \"Neutral, matter-of-fact\",\n            \"impact\": \"Major - missed opportunity to delight customer\"\n        },\n        {\n            \"dimension\": \"Emotional Warmth\",\n            \"expected\": \"Personal, caring language\",\n            \"actual\": \"Formal, impersonal\",\n            \"impact\": \"Moderate - feels robotic\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Overall tone &amp; style alignment score <code>tone_match</code> <code>bool</code> Whether emotional tone matches expected <code>style_match</code> <code>bool</code> Whether formatting/writing style matches <code>differences</code> <code>List</code> Specific differences identified"},{"location":"metric-registry/composite/tone_style_consistency/#difference-fields","title":"Difference Fields","text":"Field Type Description <code>dimension</code> <code>str</code> Aspect that differs (e.g., \"Enthusiasm Level\") <code>expected</code> <code>str</code> What was expected <code>actual</code> <code>str</code> What was observed <code>impact</code> <code>str</code> Severity of the mismatch"},{"location":"metric-registry/composite/tone_style_consistency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>Tone &amp; Style Aligned</p> <p>Expected Output:</p> <p>\"Hi there! \ud83d\udc4b Thanks for reaching out! I'd be happy to help you with your question about returns. Our policy allows full refunds within 30 days!\"</p> <p>AI Response:</p> <p>\"Hello! \ud83d\ude0a Thanks so much for contacting us! I'm thrilled to assist with your returns question. You can get a full refund within 30 days\u2014no problem at all!\"</p> <p>Analysis:</p> Dimension Match Enthusiasm \u2705 Both excited and welcoming Emoji usage \u2705 Appropriate friendly emoji Formality \u2705 Casual, approachable Helpfulness \u2705 Eager to assist <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Style Drift (Score: 0.5) <p>Tone Mismatch</p> <p>Expected Output:</p> <p>\"Great news! \ud83c\udf89 Your order is on its way! You can expect delivery within 3-5 business days. We're so excited for you!\"</p> <p>AI Response:</p> <p>\"Your order has been shipped. Estimated delivery: 3-5 business days.\"</p> <p>Analysis:</p> Dimension Match Information \u2705 Same facts conveyed Enthusiasm \u274c Neutral vs celebratory Emoji usage \u274c None vs appropriate celebration Warmth \u274c Impersonal vs personal <p>Final Score: <code>0.5</code> </p> <p>Content is correct but voice is completely wrong.</p> \u274c Scenario 3: Complete Mismatch (Score: 0.0) <p>Persona Ignored</p> <p>Expected Output:</p> <p>\"I'm so sorry to hear about this issue! \ud83d\ude14 That's definitely not the experience we want for you. Let me personally look into this right away and make it right!\"</p> <p>AI Response:</p> <p>\"Your complaint has been logged. Reference number: #12345. Allow 5-7 business days for review.\"</p> <p>Analysis:</p> Dimension Match Empathy \u274c None vs deeply apologetic Tone \u274c Cold/bureaucratic vs warm Personal touch \u274c Ticket number vs personal commitment Resolution focus \u274c Process vs solution <p>Final Score: <code>0.0</code> </p> <p>Response is robotic when empathy was expected.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfad Brand Voice <p>Ensures AI maintains your brand's personality across all interactions. Inconsistent tone damages brand perception.</p> \ud83d\udcac Customer Experience <p>Customers expect warmth and empathy, not robotic responses. Tone directly impacts satisfaction and loyalty.</p> \ud83d\udd04 Consistency <p>Maintain uniform voice across all AI-generated responses, regardless of the underlying model or prompt.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Tone &amp; Style Consistency = Does the AI response sound like it should?</p> <ul> <li>Use it when: Brand voice and persona consistency matter</li> <li>Score interpretation: Higher = better alignment with expected tone</li> <li>Key difference: Measures how something is said, not what is said</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ToneStyleConsistency</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Completeness \u00b7 Answer Relevancy \u00b7 Answer Criteria</p> </li> </ul>"},{"location":"metric-registry/conversational/","title":"Conversational Metrics","text":"<p>Conversational metrics evaluate multi-turn interactions and dialogue quality. These metrics assess how well AI agents maintain context, flow, and achieve user goals across conversation turns.</p>"},{"location":"metric-registry/conversational/#available-metrics","title":"Available Metrics","text":"<p>Coming soon...</p>"},{"location":"metric-registry/heuristic/","title":"Heuristic Metrics","text":"Fast, deterministic evaluation metrics using rule-based and statistical methods 12 Metrics No LLM Required <p>Heuristic metrics are rule-based evaluation metrics that don't require LLM calls. They're ideal for production environments where speed, cost efficiency, and deterministic results are critical. These metrics use pattern matching, statistical analysis, and algorithmic comparisons.</p>"},{"location":"metric-registry/heuristic/#string-matching-metrics","title":"String Matching Metrics","text":"<p>Compare actual outputs against expected outputs using various matching strategies.</p> Exact String Match <p>Check for identical strings</p> <code>actual_output</code> <code>expected_output</code> Contains Match <p>Check if output contains expected text</p> <code>actual_output</code> <code>expected_output</code> Levenshtein Ratio <p>Character-level string similarity</p> <code>actual_output</code> <code>expected_output</code> Sentence BLEU <p>N-gram precision similarity</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/heuristic/#safety-compliance-metrics","title":"Safety &amp; Compliance Metrics","text":"<p>Evaluate outputs for privacy, citations, and policy compliance.</p> PII Leakage (Heuristic) <p>Detect PII using regex patterns</p> <code>query</code> <code>actual_output</code> Citation Presence <p>Verify responses include citations</p> <code>actual_output</code>"},{"location":"metric-registry/heuristic/#performance-metrics","title":"Performance Metrics","text":"<p>Monitor execution time and operational performance.</p> Latency <p>Measure and evaluate execution time</p> <code>latency</code>"},{"location":"metric-registry/heuristic/#retrieval-metrics-ir","title":"Retrieval Metrics (IR)","text":"<p>Standard information retrieval metrics for evaluating search and ranking quality.</p> Hit Rate @ K <p>Any relevant result in top K?</p> <code>actual_ranking</code> <code>expected_reference</code> MRR <p>Rank of first relevant result</p> <code>actual_ranking</code> <code>expected_reference</code> NDCG @ K <p>Graded relevance with discounting</p> <code>actual_ranking</code> <code>expected_reference</code> Precision @ K <p>Fraction of top K that's relevant</p> <code>actual_ranking</code> <code>expected_reference</code> Recall @ K <p>Coverage of relevant documents</p> <code>actual_ranking</code> <code>expected_reference</code>"},{"location":"metric-registry/heuristic/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Exact String Match 0.0 or 1.0 0.5 Are strings identical? Contains Match 0.0 or 1.0 0.5 Is expected text in output? Levenshtein Ratio 0.0 \u2013 1.0 0.2 How similar are the strings? Sentence BLEU 0.0 \u2013 1.0 0.5 How much n-gram overlap? PII Leakage (Heuristic) 0.0 \u2013 1.0 0.8 Is output privacy-safe? (1.0 = safe) Citation Presence 0.0 or 1.0 0.5 Are citations included? Latency 0.0 \u2013 \u221e 5.0s How fast was the response? Hit Rate @ K 0.0 or 1.0 - Any relevant in top K? MRR 0.0 \u2013 1.0 - How early is first relevant? NDCG @ K 0.0 \u2013 1.0 - Is ranking optimal? Precision @ K 0.0 \u2013 1.0 - Are results mostly relevant? Recall @ K 0.0 \u2013 1.0 - Did we find all relevant?"},{"location":"metric-registry/heuristic/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import (\n    ExactStringMatch,\n    LevenshteinRatio,\n    PIILeakageHeuristic,\n    HitRateAtK,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import Dataset\n\n# Initialize metrics\nmetrics = [\n    ExactStringMatch(),\n    LevenshteinRatio(case_sensitive=False),\n    PIILeakageHeuristic(confidence_threshold=0.7),\n    HitRateAtK(k=10),\n]\n\n# Run evaluation\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run(dataset)\n\n# Analyze results\nfor item in results:\n    print(f\"Exact Match: {item.scores.get('exact_string_match', 'N/A')}\")\n    print(f\"Similarity: {item.scores.get('levenshtein_ratio', 'N/A'):.2f}\")\n    print(f\"Privacy Safe: {item.scores.get('pii_leakage_heuristic', 'N/A'):.2f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/#choosing-the-right-metrics","title":"Choosing the Right Metrics","text":"<p>Evaluation Strategy</p> <p>For Exact Outputs (Code, JSON, IDs):</p> <ul> <li>Use Exact String Match for strict equality</li> <li>Add Contains Match for partial verification</li> </ul> <p>For Natural Language:</p> <ul> <li>Use Levenshtein Ratio for typo/variation tolerance</li> <li>Use Sentence BLEU for paraphrase comparison</li> </ul> <p>For Privacy &amp; Compliance:</p> <ul> <li>Use PII Leakage (Heuristic) for fast screening</li> <li>Add Citation Presence for source attribution</li> </ul> <p>For Search/Retrieval:</p> <ul> <li>Use Hit Rate for quick sanity checks</li> <li>Use NDCG for comprehensive ranking evaluation</li> <li>Use Precision/Recall for classic IR metrics</li> </ul>"},{"location":"metric-registry/heuristic/#why-heuristic-metrics","title":"Why Heuristic Metrics?","text":"\u26a1 Instant Results <p>No LLM calls needed\u2014microsecond latency.</p> \ud83d\udcb0 Zero Cost <p>No API costs or token usage.</p> \ud83d\udd04 Deterministic <p>Same input always produces same output.</p> \ud83d\udcc8 Scalable <p>Evaluate millions of items without limits.</p>"},{"location":"metric-registry/heuristic/citation_presence/","title":"Citation Presence","text":"Verify responses include properly formatted citations Heuristic Knowledge Multi-Turn"},{"location":"metric-registry/heuristic/citation_presence/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> Optional: conversation <p>What It Measures</p> <p>Citation Presence evaluates whether AI responses include properly formatted citations\u2014URLs, DOIs, or academic references. It supports both single-turn responses and multi-turn conversations.</p> Score Interpretation 1.0  Citations present (in at least one message) 0.0  No citations found \u2705 Use When <ul> <li>Requiring sourced responses</li> <li>Building research assistants</li> <li>Enforcing citation policies</li> <li>Validating knowledge retrieval</li> </ul> \u274c Don't Use When <ul> <li>Citations aren't required</li> <li>Checking citation accuracy (use Faithfulness)</li> <li>Creative/generative tasks</li> <li>Simple Q&amp;A without sources</li> </ul> <p>Citation Presence vs Faithfulness</p> <p>Citation Presence checks: \"Are citations included?\" Faithfulness checks: \"Is the content accurate to the source?\"</p> <p>Use Citation Presence for format compliance; use Faithfulness for content verification.</p> How It Works  Computation Detected Citation Formats Evaluation Modes <p>The metric extracts citations using regex patterns and evaluates based on the configured mode.</p> Format Pattern Example HTTP/HTTPS URLs <code>https?://...</code> <code>https://docs.python.org/3/</code> WWW URLs <code>www.domain.com</code> <code>www.wikipedia.org</code> DOI References <code>doi:10.xxxx/...</code> <code>doi:10.1000/xyz123</code> Academic <code>(Author, Year)</code> <code>(Smith et al., 2023)</code> <p> any_citation (default) Pass if any citation appears anywhere in the response. </p> <p> resource_section Pass only if citations appear in a dedicated Resources/References section. </p>"},{"location":"metric-registry/heuristic/citation_presence/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Response Text]\n        B[Mode Setting]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Extract Citations\"]\n        C[Run citation patterns]\n        D1[\"HTTP/HTTPS URLs\"]\n        D2[\"DOI references\"]\n        D3[\"Academic citations\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Mode-Based Evaluation\"]\n        E{Mode?}\n        F[\"any_citation: Any URL/DOI found?\"]\n        G[\"resource_section: Section with citations?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        H[\"1.0 = Pass\"]\n        I[\"0.0 = Fail\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3\n    D1 &amp; D2 &amp; D3 --&gt; E\n    E --&gt;|any_citation| F\n    E --&gt;|resource_section| G\n    F &amp; G --&gt;|Yes| H\n    F &amp; G --&gt;|No| I\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#8b5cf6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>str</code> <code>any_citation</code> Evaluation mode: <code>any_citation</code> or <code>resource_section</code> <code>strict</code> <code>bool</code> <code>False</code> If True, validates URLs are live <code>use_semantic_search</code> <code>bool</code> <code>False</code> Use embeddings for fallback detection <code>embed_model</code> <code>EmbeddingRunnable</code> <code>None</code> Embedding model (required if semantic search enabled) <code>resource_similarity_threshold</code> <code>float</code> <code>0.8</code> Threshold for semantic matching <code>custom_resource_phrases</code> <code>List[str]</code> <code>None</code> Custom phrases to identify resource sections <p>Strict Mode</p> <p>When <code>strict=True</code>, the metric validates that URLs are live by making HEAD requests. This ensures citations point to actual resources but adds latency.</p>"},{"location":"metric-registry/heuristic/citation_presence/#code-examples","title":"Code Examples","text":"Basic Usage No Citations Resource Section Mode Multi-Turn Conversation <pre><code>from axion.metrics import CitationPresence\nfrom axion.dataset import DatasetItem\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"Python is a programming language. Learn more at https://python.org\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - URL citation found\n</code></pre> <pre><code>from axion.metrics import CitationPresence\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"Python is a great programming language for beginners.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - no citations\nprint(result.explanation)\n# \"Mode: any_citation. FAILURE: No assistant message satisfied the citation requirement.\"\n</code></pre> <pre><code>from axion.metrics import CitationPresence\n\n# Require citations in a dedicated section\nmetric = CitationPresence(mode='resource_section')\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Python is versatile and beginner-friendly.\n\n    For More Information:\n    - https://docs.python.org/3/\n    - https://realpython.com/\n    \"\"\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - resource section with citations\n</code></pre> <pre><code>from axion.metrics import CitationPresence\nfrom axion._core.schema import Conversation, HumanMessage, AIMessage\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"\",  # Will check conversation instead\n    conversation=Conversation(messages=[\n        HumanMessage(content=\"What is Python?\"),\n        AIMessage(content=\"Python is a programming language.\"),\n        HumanMessage(content=\"Where can I learn more?\"),\n        AIMessage(content=\"Check out https://python.org and https://realpython.com\"),\n    ]),\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - citation in second AI message\nprint(result.signals.messages_with_citations)  # [3] (index of 2nd AI message)\n</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationPresenceResult Structure <pre><code>CitationPresenceResult(\n{\n    \"passes_presence_check\": True,\n    \"total_assistant_messages\": 2,\n    \"messages_with_citations\": [3]  # 0-indexed message positions\n}\n)\n</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>passes_presence_check</code> <code>bool</code> Whether citation requirement was met <code>total_assistant_messages</code> <code>int</code> Number of AI messages evaluated <code>messages_with_citations</code> <code>List[int]</code> Indices of messages with valid citations"},{"location":"metric-registry/heuristic/citation_presence/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: URL Citation (Score: 1.0) <p>HTTP URL Found</p> <p>Output:</p> <p>\"Machine learning is a subset of AI. See https://scikit-learn.org for tutorials.\"</p> <p>Citations Detected: <code>https://scikit-learn.org</code></p> <p>Final Score: <code>1.0</code> </p> \u2705 Scenario 2: Academic Citation (Score: 1.0) <p>Author-Year Format</p> <p>Output:</p> <p>\"Attention mechanisms transformed NLP (Vaswani et al., 2017).\"</p> <p>Citations Detected: <code>(Vaswani et al., 2017)</code></p> <p>Final Score: <code>1.0</code> </p> \u274c Scenario 3: No Citations (Score: 0.0) <p>Missing Citations</p> <p>Output:</p> <p>\"Deep learning uses neural networks with multiple layers to process data.\"</p> <p>Citations Detected: None</p> <p>Final Score: <code>0.0</code> </p> \u26a0\ufe0f Scenario 4: Resource Section Required <p>Wrong Mode</p> <p>Mode: <code>resource_section</code></p> <p>Output:</p> <p>\"Python documentation is at https://python.org which explains everything.\"</p> <p>Analysis: URL exists but not in a resource section.</p> <p>Final Score: <code>0.0</code> </p> <p>Switch to <code>any_citation</code> mode or add a Resources section.</p>"},{"location":"metric-registry/heuristic/citation_presence/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcda Source Attribution <p>Ensure AI outputs provide proper attribution to sources.</p> \ud83c\udf93 Research Quality <p>Enforce citation standards for academic or research applications.</p> \u2705 Policy Compliance <p>Verify responses meet organizational citation requirements.</p>"},{"location":"metric-registry/heuristic/citation_presence/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Presence = Does the response include citations?</p> <ul> <li>Use it when: Requiring sourced responses or research assistants</li> <li>Score interpretation: 1.0 = citations found, 0.0 = none</li> <li>Key config: <code>mode</code> determines where citations must appear</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.CitationPresence</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Contextual Relevancy \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/heuristic/contains_match/","title":"Contains Match","text":"Check if output contains the expected text as a substring Heuristic Single Turn Binary"},{"location":"metric-registry/heuristic/contains_match/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Substring to find <p>What It Measures</p> <p>Contains Match checks whether the expected output appears as a substring within the actual output. This is the simplest form of text matching\u2014does the response include the required content anywhere?</p> Score Interpretation 1.0  Expected text found in output 0.0  Expected text not found \u2705 Use When <ul> <li>Checking for required keywords/phrases</li> <li>Validating specific content inclusion</li> <li>Simple pass/fail tests</li> <li>Fast sanity checks</li> </ul> \u274c Don't Use When <ul> <li>Exact match required (use Exact String Match)</li> <li>Similarity scoring needed (use BLEU/Levenshtein)</li> <li>Case variations matter</li> <li>Semantic matching needed</li> </ul> <p>See Also: Exact String Match</p> <p>Contains Match checks if expected is a substring of actual. Exact String Match checks if they are identical.</p> How It Works  Computation Logic <p>Simple substring check after stripping whitespace.</p> <pre><code>score = 1.0 if expected.strip() in actual.strip() else 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Strip whitespace]\n        D[\"Check: expected in actual?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        E[\"1.0 = Found\"]\n        F[\"0.0 = Not Found\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt;|Yes| E\n    D --&gt;|No| F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description (none) - - No configuration options <p>Simple by Design</p> <p>Contains Match is intentionally simple with no configuration. For case-insensitive matching or fuzzy matching, use other metrics.</p>"},{"location":"metric-registry/heuristic/contains_match/#code-examples","title":"Code Examples","text":"Basic Usage No Match Example With Runner <pre><code>from axion.metrics import ContainsMatch\nfrom axion.dataset import DatasetItem\n\nmetric = ContainsMatch()\n\nitem = DatasetItem(\n    actual_output=\"The capital of France is Paris, a beautiful city.\",\n    expected_output=\"Paris\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - \"Paris\" is in the output\n</code></pre> <pre><code>from axion.metrics import ContainsMatch\n\nmetric = ContainsMatch()\n\nitem = DatasetItem(\n    actual_output=\"The capital of France is a beautiful city.\",\n    expected_output=\"Paris\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - \"Paris\" not found\n</code></pre> <pre><code>from axion.metrics import ContainsMatch\nfrom axion.runners import MetricRunner\n\nmetric = ContainsMatch()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\npassed = sum(1 for r in results if r.score == 1.0)\nprint(f\"Passed: {passed}/{len(results)}\")\n</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Substring Found (Score: 1.0) <p>Match Found</p> <p>Expected: <code>\"42\"</code></p> <p>Actual: <code>\"The answer to life, the universe, and everything is 42.\"</code></p> <p>Result: <code>1.0</code> </p> <p>The substring \"42\" exists in the output.</p> \u274c Scenario 2: Substring Not Found (Score: 0.0) <p>No Match</p> <p>Expected: <code>\"Python\"</code></p> <p>Actual: <code>\"JavaScript is a popular programming language.\"</code></p> <p>Result: <code>0.0</code> </p> <p>\"Python\" does not appear in the output.</p> \u26a0\ufe0f Scenario 3: Case Sensitivity <p>Case Matters</p> <p>Expected: <code>\"PARIS\"</code></p> <p>Actual: <code>\"The capital is Paris.\"</code></p> <p>Result: <code>0.0</code> </p> <p>\"PARIS\" (uppercase) does not match \"Paris\" (title case).</p>"},{"location":"metric-registry/heuristic/contains_match/#why-it-matters","title":"Why It Matters","text":"\u26a1 Instant Results <p>O(n) substring search. No external dependencies or LLM calls.</p> \u2705 Sanity Checks <p>Quickly verify required content is present in responses.</p> \ud83d\udd0d Keyword Validation <p>Ensure specific terms, codes, or phrases appear in output.</p>"},{"location":"metric-registry/heuristic/contains_match/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contains Match = Does the output contain the expected text?</p> <ul> <li>Use it when: Checking for required keywords or phrases</li> <li>Score interpretation: 1.0 = found, 0.0 = not found</li> <li>Key behavior: Case-sensitive substring match</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContainsMatch</code></p> </li> <li> <p> Related Metrics</p> <p> Exact String Match \u00b7 Sentence BLEU \u00b7 Levenshtein Ratio</p> </li> </ul>"},{"location":"metric-registry/heuristic/exact_string_match/","title":"Exact String Match","text":"Check if output exactly matches expected text Heuristic Single Turn Binary"},{"location":"metric-registry/heuristic/exact_string_match/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Full text comparison <p>What It Measures</p> <p>Exact String Match checks whether the actual output is identical to the expected output (after stripping whitespace). This is the strictest form of text matching\u2014the response must be exactly what was expected.</p> Score Interpretation 1.0  Perfect match\u2014strings are identical 0.0  No match\u2014any difference fails \u2705 Use When <ul> <li>Exact output format is required</li> <li>Testing deterministic transformations</li> <li>Validating code generation</li> <li>Comparing structured outputs (JSON, XML)</li> </ul> \u274c Don't Use When <ul> <li>Minor variations are acceptable</li> <li>Semantic equivalence matters more</li> <li>Paraphrasing is allowed</li> <li>Case differences should be ignored</li> </ul> <p>See Also: Contains Match</p> <p>Exact String Match checks if actual equals expected. Contains Match checks if expected is a substring of actual.</p> <p>Use Exact Match when precision matters; use Contains Match for keyword validation.</p> How It Works  Computation Logic <p>Simple string equality check after stripping whitespace.</p> <pre><code>score = 1.0 if actual.strip() == expected.strip() else 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Strip whitespace]\n        D[\"Check: actual == expected?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        E[\"1.0 = Match\"]\n        F[\"0.0 = No Match\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt;|Yes| E\n    D --&gt;|No| F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description (none) - - No configuration options <p>Simple by Design</p> <p>Exact String Match is intentionally simple with no configuration. For case-insensitive or fuzzy matching, use Levenshtein Ratio.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#code-examples","title":"Code Examples","text":"Basic Usage No Match Example With Runner <pre><code>from axion.metrics import ExactStringMatch\nfrom axion.dataset import DatasetItem\n\nmetric = ExactStringMatch()\n\nitem = DatasetItem(\n    actual_output=\"Hello, World!\",\n    expected_output=\"Hello, World!\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - exact match\n</code></pre> <pre><code>from axion.metrics import ExactStringMatch\n\nmetric = ExactStringMatch()\n\nitem = DatasetItem(\n    actual_output=\"Hello, world!\",  # lowercase 'w'\n    expected_output=\"Hello, World!\",  # uppercase 'W'\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - case mismatch\n</code></pre> <pre><code>from axion.metrics import ExactStringMatch\nfrom axion.runners import MetricRunner\n\nmetric = ExactStringMatch()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nexact_matches = sum(1 for r in results if r.score == 1.0)\nprint(f\"Exact matches: {exact_matches}/{len(results)}\")\n</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>Identical Strings</p> <p>Expected: <code>\"The answer is 42.\"</code></p> <p>Actual: <code>\"The answer is 42.\"</code></p> <p>Result: <code>1.0</code> </p> <p>Strings are character-for-character identical.</p> \u274c Scenario 2: Case Mismatch (Score: 0.0) <p>Different Case</p> <p>Expected: <code>\"PASS\"</code></p> <p>Actual: <code>\"Pass\"</code></p> <p>Result: <code>0.0</code> </p> <p>Case sensitivity causes failure.</p> \u26a0\ufe0f Scenario 3: Whitespace Handling <p>Leading/Trailing Whitespace</p> <p>Expected: <code>\"  Hello  \"</code></p> <p>Actual: <code>\"Hello\"</code></p> <p>Result: <code>1.0</code> </p> <p>Whitespace is stripped before comparison, so these match.</p> \u274c Scenario 4: Extra Content (Score: 0.0) <p>Additional Text</p> <p>Expected: <code>\"Paris\"</code></p> <p>Actual: <code>\"The answer is Paris.\"</code></p> <p>Result: <code>0.0</code> </p> <p>Even containing the expected text, the strings aren't equal.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#why-it-matters","title":"Why It Matters","text":"\u26a1 Instant Results <p>O(n) string comparison. No external dependencies or LLM calls.</p> \ud83c\udfaf Maximum Precision <p>No false positives\u2014only identical strings pass.</p> \ud83d\udd27 Deterministic Testing <p>Perfect for testing deterministic transformations and code generation.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Exact String Match = Is the output identical to the expected text?</p> <ul> <li>Use it when: Exact output format is required (code, JSON, IDs)</li> <li>Score interpretation: 1.0 = identical, 0.0 = any difference</li> <li>Key behavior: Case-sensitive, whitespace-trimmed</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ExactStringMatch</code></p> </li> <li> <p> Related Metrics</p> <p> Contains Match \u00b7 Levenshtein Ratio \u00b7 Sentence BLEU</p> </li> </ul>"},{"location":"metric-registry/heuristic/latency/","title":"Latency","text":"Measure and evaluate execution time performance Heuristic Single Turn Performance"},{"location":"metric-registry/heuristic/latency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>\u221e</code> Seconds (or normalized 0-1) \u26a1 Default Threshold <code>5.0s</code> Target latency \ud83d\udccb Required Inputs <code>latency</code> Execution time in seconds <p>What It Measures</p> <p>The Latency metric evaluates execution time performance. It can return raw latency values or normalize them to a 0-1 scale using various decay functions.</p> Mode Score Interpretation Raw Actual latency in seconds (lower is better) Normalized 0.0-1.0 where 1.0 = instant, 0.0 = very slow \u2705 Use When <ul> <li>Monitoring response times</li> <li>SLA compliance checking</li> <li>Performance regression testing</li> <li>Comparing model latencies</li> </ul> \u274c Don't Use When <ul> <li>Quality metrics are more important</li> <li>Latency isn't being tracked</li> <li>Network conditions are highly variable</li> <li>Cold start effects dominate</li> </ul> <p>Inverse Scoring</p> <p>Unlike most metrics where higher is better, lower latency is better. The metric is marked as <code>inverse_scoring_metric = True</code> for proper aggregation.</p> How It Works  Computation Normalization Methods <p>The metric reads the latency value and optionally normalizes it.</p> <p>Four normalization methods convert raw latency to a 0-1 score:</p> Method Formula Characteristics exponential <code>exp(-latency/threshold)</code> Smooth decay, never reaches 0 sigmoid <code>1/(1 + exp((latency-threshold)/scale))</code> S-curve centered at threshold reciprocal <code>threshold/(threshold + latency)</code> Hyperbolic decay linear <code>max(0, 1 - latency/threshold)</code> Linear drop to 0 <p> \ud83d\udcc8 Exponential Smooth decay. At threshold: ~0.37 </p> <p> \ud83d\udcc9 Sigmoid S-curve. At threshold: 0.5 </p> <p> \ud83d\udcca Reciprocal Hyperbolic. At threshold: 0.5 </p> <p> \ud83d\udcd0 Linear Simple. At threshold: 0.0 </p>"},{"location":"metric-registry/heuristic/latency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Latency Value]\n        B[Threshold Setting]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C{Normalize?}\n        D[Return raw latency]\n        E[Apply normalization function]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"Raw: seconds\"]\n        G[\"Normalized: 0.0-1.0\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt;|No| D\n    C --&gt;|Yes| E\n    D --&gt; F\n    E --&gt; G\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/latency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>threshold</code> <code>float</code> <code>5.0</code> Target latency in seconds <code>normalize</code> <code>bool</code> <code>False</code> Whether to normalize to 0-1 range <code>normalization_method</code> <code>str</code> <code>exponential</code> Method: exponential, sigmoid, reciprocal, linear <p>Choosing a Normalization Method</p> <ul> <li>exponential: Good default, smooth decay</li> <li>sigmoid: Hard cutoff around threshold</li> <li>reciprocal: Balanced decay, never hits 0</li> <li>linear: Simple, goes to 0 at threshold</li> </ul>"},{"location":"metric-registry/heuristic/latency/#code-examples","title":"Code Examples","text":"Basic Usage (Raw) Normalized Scoring With Runner <pre><code>from axion.metrics import Latency\nfrom axion.dataset import DatasetItem\n\nmetric = Latency(threshold=2.0)\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"Paris\",\n    latency=1.5,  # 1.5 seconds\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.5 (raw latency)\nprint(result.explanation)\n# \"Raw latency: 1.500s, below threshold (2.0s).\"\n</code></pre> <pre><code>from axion.metrics import Latency\n\n# Exponential normalization\nmetric = Latency(\n    threshold=2.0,\n    normalize=True,\n    normalization_method='exponential'\n)\n\nitem = DatasetItem(latency=1.0)  # 1 second\nresult = await metric.execute(item)\nprint(f\"{result.score:.3f}\")  # ~0.607 (exp(-1/2))\n\n# Linear normalization\nmetric_linear = Latency(\n    threshold=2.0,\n    normalize=True,\n    normalization_method='linear'\n)\nresult_linear = await metric_linear.execute(item)\nprint(f\"{result_linear.score:.3f}\")  # 0.5 (1 - 1/2)\n</code></pre> <pre><code>from axion.metrics import Latency\nfrom axion.runners import MetricRunner\n\nmetric = Latency(threshold=3.0, normalize=True)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Latency score: {item_result.score:.2f}\")\n    print(f\"  {item_result.explanation}\")\n</code></pre>"},{"location":"metric-registry/heuristic/latency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Excellent Performance <p>Below Half Threshold</p> <p>Threshold: 5.0s</p> <p>Latency: 2.0s (40% of threshold)</p> <p>Raw Score: <code>2.0</code></p> <p>Normalized (exponential): <code>~0.67</code> </p> <p>Explanation: \"Latency: 2.000s. Normalized score: 0.670 (threshold: 5.0s, method: exponential). Performance: excellent.\"</p> \u26a0\ufe0f Scenario 2: At Threshold <p>Exactly at Target</p> <p>Threshold: 5.0s</p> <p>Latency: 5.0s</p> <p>Raw Score: <code>5.0</code></p> <p>Normalized Scores:</p> Method Score exponential 0.37 sigmoid 0.50 reciprocal 0.50 linear 0.00 \u274c Scenario 3: Poor Performance <p>Above Threshold</p> <p>Threshold: 2.0s</p> <p>Latency: 8.0s (4x threshold)</p> <p>Raw Score: <code>8.0</code></p> <p>Normalized (exponential): <code>~0.02</code> </p> <p>Explanation: \"Latency: 8.000s. Normalized score: 0.018 (threshold: 2.0s, method: exponential). Performance: poor.\"</p>"},{"location":"metric-registry/heuristic/latency/#why-it-matters","title":"Why It Matters","text":"\u23f1\ufe0f SLA Compliance <p>Track response times against service level agreements.</p> \ud83d\udcc8 Performance Monitoring <p>Detect regressions and optimize slow endpoints.</p> \u2696\ufe0f Quality vs Speed <p>Balance model quality against response time requirements.</p>"},{"location":"metric-registry/heuristic/latency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Latency = How fast was the response?</p> <ul> <li>Use it when: Monitoring performance or SLA compliance</li> <li>Score interpretation: Raw (seconds) or normalized (0-1, higher = faster)</li> <li>Key config: <code>threshold</code> sets target, <code>normalize</code> enables 0-1 scoring</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.Latency</code></p> </li> <li> <p> Related Concepts</p> <p> MetricRunner \u00b7 Evaluation Strategies</p> </li> </ul>"},{"location":"metric-registry/heuristic/levenshtein_ratio/","title":"Levenshtein Ratio","text":"Calculate character-level string similarity Heuristic Single Turn Fast"},{"location":"metric-registry/heuristic/levenshtein_ratio/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Character-level similarity \u26a1 Default Threshold <code>0.2</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Text comparison <p>What It Measures</p> <p>Levenshtein Ratio calculates the character-level similarity between two strings using the SequenceMatcher algorithm. It measures how many edits (insertions, deletions, substitutions) are needed to transform one string into another.</p> Score Interpretation 1.0  Identical strings 0.8+  Very similar, minor typos 0.5-0.8  Moderate similarity &lt; 0.5  Significant differences \u2705 Use When <ul> <li>Checking for typos or small variations</li> <li>Fuzzy string matching needed</li> <li>Comparing names or identifiers</li> <li>Near-match detection</li> </ul> \u274c Don't Use When <ul> <li>Semantic similarity matters</li> <li>Word-level comparison preferred (use BLEU)</li> <li>Long texts with different structures</li> <li>Exact match required</li> </ul> <p>See Also: Sentence BLEU</p> <p>Levenshtein Ratio measures character-level edit distance. Sentence BLEU measures word-level n-gram precision.</p> <p>Use Levenshtein for typo detection; use BLEU for paraphrase comparison.</p> How It Works  Computation Formula <p>Uses Python's <code>SequenceMatcher</code> to calculate the ratio of matching characters.</p> <p>The SequenceMatcher ratio is calculated as:</p> <pre><code>ratio = 2.0 * M / T\n\nwhere:\nM = number of matches (characters in common)\nT = total number of characters in both strings\n</code></pre> <p>Example: <pre><code>String 1: \"hello\"\nString 2: \"hallo\"\n\nMatches: h, l, l, o = 4 characters match\nTotal: 5 + 5 = 10 characters\nRatio: 2.0 * 4 / 10 = 0.8\n</code></pre></p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Optional: Convert to lowercase]\n        D[Find matching subsequences]\n        E[Calculate similarity ratio]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"Score: 0.0 to 1.0\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>case_sensitive</code> <code>bool</code> <code>False</code> Whether comparison is case-sensitive <p>Case Sensitivity</p> <p>By default, comparison is case-insensitive (both strings converted to lowercase). Set <code>case_sensitive=True</code> for strict character matching.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#code-examples","title":"Code Examples","text":"Basic Usage Case Sensitive With Runner <pre><code>from axion.metrics import LevenshteinRatio\nfrom axion.dataset import DatasetItem\n\nmetric = LevenshteinRatio()\n\nitem = DatasetItem(\n    actual_output=\"The quick brown fox\",\n    expected_output=\"The quick brown fox\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - identical strings\n</code></pre> <pre><code>from axion.metrics import LevenshteinRatio\n\n# Case insensitive (default)\nmetric = LevenshteinRatio(case_sensitive=False)\n\nitem = DatasetItem(\n    actual_output=\"HELLO\",\n    expected_output=\"hello\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - case ignored\n\n# Case sensitive\nmetric_strict = LevenshteinRatio(case_sensitive=True)\nresult_strict = await metric_strict.execute(item)\nprint(result_strict.score)  # 0.0 - case matters\n</code></pre> <pre><code>from axion.metrics import LevenshteinRatio\nfrom axion.runners import MetricRunner\n\nmetric = LevenshteinRatio()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\navg_similarity = sum(r.score for r in results) / len(results)\nprint(f\"Average similarity: {avg_similarity:.2%}\")\n</code></pre>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High Similarity (Score: 0.95) <p>Minor Typo</p> <p>Expected: <code>\"accommodation\"</code></p> <p>Actual: <code>\"accomodation\"</code> (missing 'm')</p> <p>Result: <code>~0.92</code> </p> <p>Single character difference results in high similarity.</p> \u26a0\ufe0f Scenario 2: Moderate Similarity (Score: 0.67) <p>Multiple Differences</p> <p>Expected: <code>\"Hello World\"</code></p> <p>Actual: <code>\"Helo Wrld\"</code> (missing letters)</p> <p>Result: <code>~0.67</code> </p> <p>Several missing characters reduce similarity.</p> \u274c Scenario 3: Low Similarity (Score: 0.2) <p>Very Different Strings</p> <p>Expected: <code>\"The quick brown fox\"</code></p> <p>Actual: <code>\"A lazy dog sleeps\"</code></p> <p>Result: <code>~0.2</code> </p> <p>Completely different content results in low similarity.</p> \u2705 Scenario 4: Case Handling <p>Case Insensitive Match</p> <p>Expected: <code>\"OpenAI\"</code></p> <p>Actual: <code>\"openai\"</code></p> <p>Result (default): <code>1.0</code> </p> <p>Result (case_sensitive=True): <code>~0.67</code> </p> <p>Case sensitivity significantly affects scoring.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Deterministic <p>No LLM calls needed. Instant, reproducible results.</p> \ud83d\udd24 Typo Detection <p>Perfect for detecting spelling errors and near-matches.</p> \ud83d\udcca Gradual Scoring <p>Unlike binary metrics, provides nuanced similarity scores.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Levenshtein Ratio = How similar are the strings at the character level?</p> <ul> <li>Use it when: Checking for typos or fuzzy matching</li> <li>Score interpretation: Higher = more similar characters</li> <li>Key config: <code>case_sensitive</code> controls case handling</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.LevenshteinRatio</code></p> </li> <li> <p> Related Metrics</p> <p> Sentence BLEU \u00b7 Exact String Match \u00b7 Contains Match</p> </li> </ul>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/","title":"PII Leakage (Heuristic)","text":"Detect personally identifiable information using regex patterns Heuristic Single Turn Safety"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Privacy score (1.0 = safe) \u26a1 Default Threshold <code>0.8</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Response to analyze <p>What It Measures</p> <p>PII Leakage (Heuristic) detects personally identifiable information in model outputs using regex patterns and validation rules. It identifies emails, phone numbers, SSNs, credit cards, addresses, and more\u2014without requiring LLM calls.</p> Score Interpretation 1.0  No PII detected\u2014output is safe 0.7-0.9  Low-risk PII (names, zip codes) 0.3-0.7  Medium-risk PII (emails, phones) &lt; 0.3  High-risk PII (SSN, credit cards) \u2705 Use When <ul> <li>Fast, deterministic PII detection needed</li> <li>Production monitoring at scale</li> <li>CI/CD safety gates</li> <li>High-throughput screening</li> </ul> \u274c Don't Use When <ul> <li>Context-aware detection required</li> <li>Non-standard PII formats exist</li> <li>Need semantic understanding</li> <li>International formats dominate</li> </ul> <p>Heuristic vs LLM-based PII Detection</p> <p>PII Leakage (Heuristic) uses regex patterns\u2014fast and deterministic. PII Leakage (LLM) uses language models\u2014slower but more context-aware.</p> <p>Use heuristic for high-throughput screening; use LLM-based for nuanced analysis.</p> How It Works  Computation Detected PII Types Score Calculation <p>The metric scans text using regex patterns, validates matches, and calculates a privacy score.</p> <p> \ud83d\udd34 High Risk <ul> <li>Social Security Numbers (SSN)</li> <li>Credit Card Numbers</li> <li>Passport Numbers</li> </ul> </p> <p> \ud83d\udfe1 Medium Risk <ul> <li>Email Addresses</li> <li>Phone Numbers</li> <li>Street Addresses</li> <li>Date of Birth</li> <li>Driver's License</li> </ul> </p> <p> \ud83d\udfe2 Low Risk <ul> <li>Person Names</li> <li>IP Addresses</li> <li>ZIP Codes</li> </ul> </p> <pre><code>penalty = \u03a3(severity \u00d7 confidence) for each detection\nscore = 1.0 - min(1.0, penalty)\n</code></pre> <p>Severity Weights:</p> PII Type Severity SSN 1.0 Credit Card 1.0 Passport 0.9 Date of Birth 0.8 Email 0.7 Phone 0.7 Street Address 0.6 Driver's License 0.6 Person Name 0.5 IP Address 0.3 ZIP Code 0.2"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Actual Output Text]\n    end\n\n    subgraph DETECT[\"\ud83d\udd0d Step 1: Pattern Detection\"]\n        B[Run regex patterns]\n        C1[\"Email patterns\"]\n        C2[\"Phone patterns\"]\n        C3[\"SSN patterns\"]\n        C4[\"Credit card patterns\"]\n        CN[\"More patterns...\"]\n    end\n\n    subgraph VALIDATE[\"\u2705 Step 2: Validation\"]\n        D[Validate matches]\n        E1[\"Luhn check for CC\"]\n        E2[\"SSN format check\"]\n        E3[\"IP address validation\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        F[Apply severity weights]\n        G[Calculate penalty]\n        H[\"Privacy Score: 1.0 - penalty\"]\n    end\n\n    A --&gt; B\n    B --&gt; C1 &amp; C2 &amp; C3 &amp; C4 &amp; CN\n    C1 &amp; C2 &amp; C3 &amp; C4 &amp; CN --&gt; D\n    D --&gt; E1 &amp; E2 &amp; E3\n    E1 &amp; E2 &amp; E3 --&gt; F\n    F --&gt; G\n    G --&gt; H\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style DETECT stroke:#3b82f6,stroke-width:2px\n    style VALIDATE stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>confidence_threshold</code> <code>float</code> <code>0.6</code> Minimum confidence to count detection <p>Confidence Filtering</p> <p>Detections below the confidence threshold are ignored when calculating the final score. Higher thresholds reduce false positives but may miss some PII.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#code-examples","title":"Code Examples","text":"Basic Usage Detection Example Custom Threshold With Runner <pre><code>from axion.metrics import PIILeakageHeuristic\nfrom axion.dataset import DatasetItem\n\nmetric = PIILeakageHeuristic()\n\nitem = DatasetItem(\n    query=\"What's the weather today?\",\n    actual_output=\"The weather in New York is sunny and 72\u00b0F.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - no PII detected\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\n\nmetric = PIILeakageHeuristic()\n\nitem = DatasetItem(\n    query=\"Contact info?\",\n    actual_output=\"You can reach John Smith at john.smith@email.com or 555-123-4567.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # ~0.3 - email and phone detected\nprint(result.explanation)\n# \"Detected 2 potential PII instances of types: email, phone_us.\"\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\n\n# Higher confidence threshold - fewer false positives\nmetric = PIILeakageHeuristic(confidence_threshold=0.8)\n\nitem = DatasetItem(\n    query=\"What is 123-45-6789?\",\n    actual_output=\"That looks like it could be a social security number format.\",\n)\n\nresult = await metric.execute(item)\n# Only high-confidence SSN detections will affect score\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\nfrom axion.runners import MetricRunner\n\nmetric = PIILeakageHeuristic()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\n# Flag outputs with potential PII\nfor item_result in results:\n    if item_result.score &lt; 0.8:\n        print(f\"PII detected: {item_result.explanation}\")\n        # Access detailed breakdown\n        if item_result.signals:\n            print(f\"High-risk: {item_result.signals.categorized_counts['high_risk']}\")\n            print(f\"Medium-risk: {item_result.signals.categorized_counts['medium_risk']}\")\n</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly what was detected.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca PIIHeuristicResult Structure <pre><code>PIIHeuristicResult(\n{\n    \"final_score\": 0.3,\n    \"total_detections\": 3,\n    \"significant_detections_count\": 2,\n    \"confidence_threshold\": 0.6,\n    \"categorized_counts\": {\n        \"high_risk\": 0,\n        \"medium_risk\": 2,\n        \"low_risk\": 0\n    },\n    \"detections\": [\n        {\n            \"type\": \"email\",\n            \"value\": \"john.smith@email.com\",\n            \"confidence\": 0.95,\n            \"start_pos\": 32,\n            \"end_pos\": 52,\n            \"context\": \"...reach John Smith at john.smith@email.com or 555-123...\"\n        },\n        {\n            \"type\": \"phone_us\",\n            \"value\": \"555-123-4567\",\n            \"confidence\": 0.90,\n            \"start_pos\": 56,\n            \"end_pos\": 68,\n            \"context\": \"...john.smith@email.com or 555-123-4567.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Privacy score (0.0-1.0) <code>total_detections</code> <code>int</code> All potential PII found <code>significant_detections_count</code> <code>int</code> Above confidence threshold <code>categorized_counts</code> <code>Dict</code> Breakdown by risk level <code>detections</code> <code>List</code> Detailed detection info"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#detection-fields","title":"Detection Fields","text":"Field Type Description <code>type</code> <code>str</code> PII type (email, ssn, etc.) <code>value</code> <code>str</code> The detected text <code>confidence</code> <code>float</code> Detection confidence (0-1) <code>start_pos</code> <code>int</code> Start position in text <code>end_pos</code> <code>int</code> End position in text <code>context</code> <code>str</code> Surrounding text"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Clean Output (Score: 1.0) <p>No PII Detected</p> <p>Output:</p> <p>\"The capital of France is Paris. It's known for the Eiffel Tower.\"</p> <p>Analysis:</p> <ul> <li>No email patterns</li> <li>No phone patterns</li> <li>No SSN patterns</li> <li>No addresses</li> </ul> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Medium Risk PII (Score: ~0.5) <p>Email and Phone Detected</p> <p>Output:</p> <p>\"Contact support at help@company.com or call 1-800-555-0199.\"</p> <p>Detections:</p> Type Value Confidence Severity email help@company.com 0.95 0.7 phone_us 1-800-555-0199 0.90 0.7 <p>Penalty: <code>(0.95 \u00d7 0.7) + (0.90 \u00d7 0.7) = 1.295</code> \u2192 capped at 1.0</p> <p>Final Score: <code>1.0 - 1.0 = 0.0</code> </p> <p>Note: Multiple PII instances can quickly reduce the score.</p> \u274c Scenario 3: High Risk PII (Score: ~0.0) <p>SSN Detected</p> <p>Output:</p> <p>\"Your SSN ending in 4567 is associated with account 123-45-6789.\"</p> <p>Detections:</p> Type Value Confidence Severity ssn 123-45-6789 0.95 1.0 <p>Penalty: <code>0.95 \u00d7 1.0 = 0.95</code></p> <p>Final Score: <code>0.05</code> </p> <p>High-risk PII immediately triggers a near-zero score.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Scalable <p>No LLM calls\u2014regex patterns run instantly on millions of outputs.</p> \ud83d\udd12 Privacy Compliance <p>Catch GDPR/CCPA violations before they reach users.</p> \ud83d\ude80 CI/CD Integration <p>Add to pipelines as a safety gate for model outputs.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>PII Leakage (Heuristic) = Does the output contain personally identifiable information?</p> <ul> <li>Use it when: Fast, deterministic PII detection needed</li> <li>Score interpretation: 1.0 = safe, lower = PII detected</li> <li>Key config: <code>confidence_threshold</code> controls sensitivity</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.PIILeakageHeuristic</code></p> </li> <li> <p> Related Metrics</p> <p>[ Bias \u00b7 Toxicity \u00b7 Safety Metrics</p> </li> </ul>"},{"location":"metric-registry/heuristic/retrieval_metrics/","title":"Retrieval Metrics","text":"Evaluate information retrieval quality with standard IR metrics Heuristic Retrieval Multi-K"},{"location":"metric-registry/heuristic/retrieval_metrics/#overview","title":"Overview","text":"<p>Axion provides a comprehensive suite of Information Retrieval (IR) metrics for evaluating search and retrieval systems. These metrics compare retrieved document rankings against ground truth relevance judgments.</p> Metric What It Measures Use Case Hit Rate @ K Any relevant result in top K? Quick relevance check MRR Rank of first relevant result First-result quality NDCG @ K Graded relevance with position discount Ranking quality Precision @ K Fraction of top K that's relevant Result purity Recall @ K Fraction of relevant docs in top K Coverage <p>Multi-K Support</p> <p>All @K metrics support evaluating at multiple K values simultaneously (e.g., <code>k=[5, 10, 20]</code>). This allows comparing retrieval quality at different cutoffs in a single evaluation pass.</p>"},{"location":"metric-registry/heuristic/retrieval_metrics/#required-inputs","title":"Required Inputs","text":"<p>All retrieval metrics require the same input structure:</p> Field Type Description <code>actual_ranking</code> <code>List[Dict]</code> Retrieved documents in order, each with <code>id</code> key <code>expected_reference</code> <code>List[Dict]</code> Ground truth with <code>id</code> and optional <code>relevance</code> score <pre><code>from axion.dataset import DatasetItem\n\nitem = DatasetItem(\n    actual_ranking=[\n        {\"id\": \"doc1\"},  # Position 1\n        {\"id\": \"doc2\"},  # Position 2\n        {\"id\": \"doc3\"},  # Position 3\n    ],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 1.0},  # Relevant\n        {\"id\": \"doc5\", \"relevance\": 1.0},  # Relevant but not retrieved\n    ],\n)\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#hit-rate-k","title":"Hit Rate @ K","text":"Binary check: Was ANY relevant document retrieved in the top K?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \ud83d\udccf Default K <code>10</code> Top results to check"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Retrieved Docs] --&gt; B{Any relevant&lt;br&gt;in top K?}\n    B --&gt;|Yes| C[\"Score: 1.0\"]\n    B --&gt;|No| D[\"Score: 0.0\"]\n\n    style C fill:#10b981,stroke:#059669,color:#fff\n    style D fill:#ef4444,stroke:#dc2626,color:#fff</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage","title":"Usage","text":"<pre><code>from axion.metrics import HitRateAtK\n\n# Single K\nmetric = HitRateAtK(k=10)\n\n# Multiple K values\nmetric = HitRateAtK(k=[5, 10, 20], main_k=10)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 if hit, 0.0 if miss\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#mean-reciprocal-rank-mrr","title":"Mean Reciprocal Rank (MRR)","text":"How early does the first relevant result appear?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_1","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> 1/rank of first relevant \ud83d\udccf K-Independent Evaluates full ranking"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_1","title":"How It Works","text":"<pre><code>MRR = 1 / rank_of_first_relevant_document\n\nExamples:\n- First relevant at position 1 \u2192 MRR = 1.0\n- First relevant at position 2 \u2192 MRR = 0.5\n- First relevant at position 4 \u2192 MRR = 0.25\n- No relevant found \u2192 MRR = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_1","title":"Usage","text":"<pre><code>from axion.metrics import MeanReciprocalRank\n\nmetric = MeanReciprocalRank()\n\nresult = await metric.execute(item)\nprint(result.score)  # 1/rank or 0.0\nprint(result.signals.rank_of_first_relevant)  # e.g., 3\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#ndcg-k","title":"NDCG @ K","text":"Normalized Discounted Cumulative Gain\u2014handles graded relevance with position discounting."},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_2","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Normalized ranking quality \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_2","title":"How It Works","text":"<pre><code>flowchart TD\n    subgraph DCG[\"\ud83d\udcca DCG Calculation\"]\n        A[\"For each position i:\"]\n        B[\"relevance[i] / log\u2082(i + 1)\"]\n        C[\"Sum all values\"]\n    end\n\n    subgraph IDCG[\"\ud83c\udfaf IDCG (Ideal)\"]\n        D[\"Sort by relevance desc\"]\n        E[\"Calculate DCG on ideal order\"]\n    end\n\n    subgraph NDCG[\"\ud83d\udcc8 Final Score\"]\n        F[\"NDCG = DCG / IDCG\"]\n    end\n\n    DCG --&gt; F\n    IDCG --&gt; F\n\n    style NDCG stroke:#f59e0b,stroke-width:2px</code></pre> <p>Formula: <pre><code>DCG@K = \u03a3 (rel_i / log\u2082(i + 1)) for i = 1 to K\nNDCG@K = DCG@K / IDCG@K\n</code></pre></p>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_2","title":"Usage","text":"<pre><code>from axion.metrics import NDCGAtK\n\n# With graded relevance\nitem = DatasetItem(\n    actual_ranking=[{\"id\": \"doc1\"}, {\"id\": \"doc2\"}, {\"id\": \"doc3\"}],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 3.0},  # Highly relevant\n        {\"id\": \"doc2\", \"relevance\": 1.0},  # Marginally relevant\n        {\"id\": \"doc3\", \"relevance\": 2.0},  # Relevant\n    ],\n)\n\nmetric = NDCGAtK(k=[5, 10])\nresult = await metric.execute(item)\nprint(f\"NDCG@10: {result.score:.3f}\")\nprint(f\"DCG: {result.signals.results_by_k[10].dcg:.3f}\")\nprint(f\"IDCG: {result.signals.results_by_k[10].idcg:.3f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#precision-k","title":"Precision @ K","text":"What fraction of the top K results are relevant?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_3","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Relevant / Retrieved \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_3","title":"How It Works","text":"<pre><code>Precision@K = (Relevant docs in top K) / K\n\nExamples (K=5):\n- 5 relevant in top 5 \u2192 Precision = 1.0\n- 3 relevant in top 5 \u2192 Precision = 0.6\n- 0 relevant in top 5 \u2192 Precision = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_3","title":"Usage","text":"<pre><code>from axion.metrics import PrecisionAtK\n\nmetric = PrecisionAtK(k=10)\n\nresult = await metric.execute(item)\nprint(f\"Precision@10: {result.score:.2%}\")\nprint(f\"Hits: {result.signals.results_by_k[10].hits_in_top_k}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#recall-k","title":"Recall @ K","text":"What fraction of ALL relevant documents appear in the top K?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_4","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Found / Total Relevant \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_4","title":"How It Works","text":"<pre><code>Recall@K = (Relevant docs in top K) / (Total relevant docs)\n\nExamples (10 total relevant):\n- 10 relevant in top K \u2192 Recall = 1.0\n- 5 relevant in top K \u2192 Recall = 0.5\n- 0 relevant in top K \u2192 Recall = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_4","title":"Usage","text":"<pre><code>from axion.metrics import RecallAtK\n\nmetric = RecallAtK(k=[5, 10, 20])\n\nresult = await metric.execute(item)\nprint(f\"Recall@10: {result.score:.2%}\")\nprint(f\"Found: {result.signals.results_by_k[10].hits_in_top_k}\")\nprint(f\"Total relevant: {result.signals.results_by_k[10].total_relevant}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#comparison-guide","title":"Comparison Guide","text":""},{"location":"metric-registry/heuristic/retrieval_metrics/#when-to-use-each-metric","title":"When to Use Each Metric","text":"Metric Best For Key Question Hit Rate Quick sanity check \"Did we find anything relevant?\" MRR First-result systems \"How fast do users find what they need?\" NDCG Graded relevance \"Is the ranking order optimal?\" Precision Result quality \"Are results mostly relevant?\" Recall Coverage \"Did we miss relevant docs?\""},{"location":"metric-registry/heuristic/retrieval_metrics/#metric-relationships","title":"Metric Relationships","text":"<pre><code>flowchart TB\n    subgraph COVERAGE[\"Coverage Metrics\"]\n        A[Hit Rate @ K]\n        B[Recall @ K]\n    end\n\n    subgraph QUALITY[\"Quality Metrics\"]\n        C[Precision @ K]\n        D[MRR]\n        E[NDCG @ K]\n    end\n\n    A --&gt;|\"Binary version of\"| B\n    C --&gt;|\"Complementary to\"| B\n    D --&gt;|\"Position-aware\"| E\n\n    style COVERAGE stroke:#3b82f6,stroke-width:2px\n    style QUALITY stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#complete-example","title":"Complete Example","text":"<pre><code>from axion.metrics import (\n    HitRateAtK,\n    MeanReciprocalRank,\n    NDCGAtK,\n    PrecisionAtK,\n    RecallAtK,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import DatasetItem\n\n# Create test item\nitem = DatasetItem(\n    actual_ranking=[\n        {\"id\": \"doc1\"},  # Relevant (relevance: 3)\n        {\"id\": \"doc4\"},  # Not relevant\n        {\"id\": \"doc2\"},  # Relevant (relevance: 2)\n        {\"id\": \"doc5\"},  # Not relevant\n        {\"id\": \"doc3\"},  # Relevant (relevance: 1)\n    ],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 3.0},\n        {\"id\": \"doc2\", \"relevance\": 2.0},\n        {\"id\": \"doc3\", \"relevance\": 1.0},\n    ],\n)\n\n# Evaluate with all metrics\nmetrics = [\n    HitRateAtK(k=5),\n    MeanReciprocalRank(),\n    NDCGAtK(k=5),\n    PrecisionAtK(k=5),\n    RecallAtK(k=5),\n]\n\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run([item])\n\nfor result in results:\n    print(f\"{result.metric_name}: {result.score:.3f}\")\n\n# Output:\n# Hit Rate @ K: 1.000\n# Mean Reciprocal Rank (MRR): 1.000\n# NDCG @ K: 0.876\n# Precision @ K: 0.600\n# Recall @ K: 1.000\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> Metric Formula Score Hit Rate 1 if any relevant in K 0 or 1 MRR 1 / first_relevant_rank 0 to 1 NDCG DCG / IDCG 0 to 1 Precision relevant_in_K / K 0 to 1 Recall relevant_in_K / total_relevant 0 to 1 <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.HitRateAtK</code> <code>axion.metrics.MeanReciprocalRank</code> <code>axion.metrics.NDCGAtK</code> <code>axion.metrics.PrecisionAtK</code> <code>axion.metrics.RecallAtK</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Recall \u00b7 Contextual Ranking</p> </li> </ul>"},{"location":"metric-registry/heuristic/sentence_bleu/","title":"Sentence BLEU","text":"Compute n-gram precision similarity between candidate and reference text Heuristic Single Turn Fast"},{"location":"metric-registry/heuristic/sentence_bleu/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> N-gram precision score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Reference text required <p>What It Measures</p> <p>Sentence BLEU (Bilingual Evaluation Understudy) computes the similarity between a candidate text and reference text using modified n-gram precision with a brevity penalty. Originally designed for machine translation, it's useful for any task where textual similarity to a reference matters.</p> Score Interpretation 1.0  Perfect n-gram match with reference 0.7+  High similarity, minor differences 0.3-0.7  Moderate similarity &lt; 0.3  Low similarity to reference \u2705 Use When <ul> <li>Comparing text to reference translations</li> <li>Evaluating summarization quality</li> <li>Fast, deterministic evaluation needed</li> <li>N-gram overlap is meaningful</li> </ul> \u274c Don't Use When <ul> <li>Semantic similarity matters more than wording</li> <li>Multiple valid phrasings exist</li> <li>Evaluating creative/generative tasks</li> <li>Word order flexibility is expected</li> </ul> <p>See Also: Levenshtein Ratio</p> <p>Sentence BLEU measures n-gram precision (word sequences). Levenshtein Ratio measures character-level edit distance.</p> <p>Use BLEU for word-level comparison; use Levenshtein for character-level.</p> How It Works  Computation BLEU Formula <p>BLEU calculates n-gram precision with clipping and applies a brevity penalty.</p> <p>Modified Precision: <pre><code>p_n = \u03a3 min(count(ngram), max_ref_count(ngram)) / \u03a3 count(ngram)\n</code></pre></p> <p>Brevity Penalty (BP): <pre><code>BP = 1                    if c &gt; r\nBP = exp(1 - r/c)         if c \u2264 r\n\nwhere c = candidate length, r = reference length\n</code></pre></p> <p>Final Score: <pre><code>BLEU = BP \u00d7 exp(\u03a3 w_n \u00d7 log(p_n))\n\nwhere w_n = 1/N (uniform weights)\n</code></pre></p> <p> \ud83d\udcdd Clipping Prevents gaming by repeating words. Each n-gram counted at most as many times as it appears in reference. </p> <p> \ud83d\udccf Brevity Penalty Penalizes outputs shorter than reference. Prevents gaming by outputting only high-confidence words. </p>"},{"location":"metric-registry/heuristic/sentence_bleu/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Candidate Text]\n        B[Reference Text]\n    end\n\n    subgraph NGRAM[\"\ud83d\udd0d Step 1: N-gram Extraction\"]\n        C[Extract 1-grams to n-grams]\n        D1[\"1-gram counts\"]\n        D2[\"2-gram counts\"]\n        D3[\"3-gram counts\"]\n        DN[\"n-gram counts\"]\n    end\n\n    subgraph PRECISION[\"\u2696\ufe0f Step 2: Clipped Precision\"]\n        E[Clip counts to reference max]\n        F[\"Calculate precision per n\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Final Score\"]\n        G[\"Geometric mean of precisions\"]\n        H[\"Apply brevity penalty\"]\n        I[\"Final BLEU Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style NGRAM stroke:#3b82f6,stroke-width:2px\n    style PRECISION stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#f59e0b,stroke:#d97706,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/heuristic/sentence_bleu/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>n_grams</code> <code>int</code> <code>4</code> Maximum n-gram length (e.g., 4 for BLEU-4) <code>case_sensitive</code> <code>bool</code> <code>False</code> Whether comparison is case-sensitive <code>smoothing</code> <code>bool</code> <code>True</code> Apply smoothing for sentence-level BLEU <p>Smoothing</p> <p>Sentence-level BLEU often has zero counts for higher n-grams. Smoothing (add-one) prevents the entire score from becoming zero.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#code-examples","title":"Code Examples","text":"Basic Usage Custom N-grams With Runner <pre><code>from axion.metrics import SentenceBleu\nfrom axion.dataset import DatasetItem\n\nmetric = SentenceBleu()\n\nitem = DatasetItem(\n    actual_output=\"The cat sat on the mat.\",\n    expected_output=\"The cat is sitting on the mat.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: ~0.6 (good n-gram overlap with minor differences)\n</code></pre> <pre><code>from axion.metrics import SentenceBleu\n\n# BLEU-2 for shorter sequences\nmetric = SentenceBleu(n_grams=2)\n\n# Case-sensitive comparison\nmetric = SentenceBleu(case_sensitive=True)\n\n# Without smoothing (corpus-level style)\nmetric = SentenceBleu(smoothing=False)\n</code></pre> <pre><code>from axion.metrics import SentenceBleu\nfrom axion.runners import MetricRunner\n\nmetric = SentenceBleu(n_grams=4)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"BLEU-4: {item_result.score:.3f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/sentence_bleu/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High BLEU Score (~0.9) <p>Near-Perfect Match</p> <p>Reference:</p> <p>\"The quick brown fox jumps over the lazy dog.\"</p> <p>Candidate:</p> <p>\"The quick brown fox jumped over the lazy dog.\"</p> <p>Analysis:</p> <ul> <li>1-grams: 8/9 match (jumped vs jumps)</li> <li>2-grams: 7/8 match</li> <li>3-grams: 6/7 match</li> <li>4-grams: 5/6 match</li> <li>Brevity penalty: ~1.0 (same length)</li> </ul> <p>Final Score: <code>~0.85</code> </p> \u26a0\ufe0f Scenario 2: Moderate BLEU Score (~0.5) <p>Partial Overlap</p> <p>Reference:</p> <p>\"Machine learning models require large datasets for training.\"</p> <p>Candidate:</p> <p>\"Deep learning needs big data to train properly.\"</p> <p>Analysis:</p> <ul> <li>Same meaning, different words</li> <li>Few exact n-gram matches</li> <li>\"learning\" and \"train\" overlap</li> </ul> <p>Final Score: <code>~0.3</code> </p> <p>Semantic similarity high, but n-gram overlap low.</p> \u274c Scenario 3: Low BLEU Score (~0.1) <p>Minimal Overlap</p> <p>Reference:</p> <p>\"Paris is the capital of France.\"</p> <p>Candidate:</p> <p>\"The Eiffel Tower is located in the French capital city.\"</p> <p>Analysis:</p> <ul> <li>Related topic, completely different wording</li> <li>Almost no n-gram matches</li> </ul> <p>Final Score: <code>~0.1</code> </p> <p>Semantically related but lexically different.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Deterministic <p>No LLM calls needed. Instant, reproducible results ideal for CI/CD pipelines.</p> \ud83d\udcca Industry Standard <p>Widely used in NLP research for translation and summarization evaluation.</p> \ud83d\udd22 N-gram Precision <p>Captures phrase-level similarity, not just word overlap.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Sentence BLEU = How much does the candidate text overlap with the reference at the n-gram level?</p> <ul> <li>Use it when: Fast, deterministic text similarity is needed</li> <li>Score interpretation: Higher = more n-gram overlap with reference</li> <li>Key config: <code>n_grams</code> controls phrase length (default 4)</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.SentenceBleu</code></p> </li> <li> <p> Related Metrics</p> <p> Levenshtein Ratio \u00b7 Exact String Match \u00b7 Contains Match</p> </li> </ul>"},{"location":"metric-registry/tool/","title":"Tool Metrics","text":"Evaluate AI agent tool calling correctness and effectiveness 1 Metric Agent <p>Tool metrics evaluate the correctness and effectiveness of tool usage in AI agent workflows. These metrics assess whether agents correctly invoke the right tools with appropriate parameters.</p>"},{"location":"metric-registry/tool/#available-metrics","title":"Available Metrics","text":"Tool Correctness <p>Evaluate if expected tools were correctly called</p> <code>tools_called</code> <code>expected_tools</code>"},{"location":"metric-registry/tool/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Tool Correctness 0.0 \u2013 1.0 0.5 Were the right tools called correctly?"},{"location":"metric-registry/tool/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import ToolCorrectness\nfrom axion.runners import MetricRunner\nfrom axion.dataset import DatasetItem\nfrom axion._core.schema import ToolCall\n\n# Create evaluation item\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"weather in Paris\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={\"query\": \"weather in Paris\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n)\n\n# Initialize metric\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n\n# Run evaluation\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run([item])\n\nprint(f\"Tool Correctness: {results[0].score:.2f}\")\n# Output: Tool Correctness: 1.00\n</code></pre>"},{"location":"metric-registry/tool/#evaluation-modes","title":"Evaluation Modes","text":"<p>Tool Correctness supports multiple evaluation strategies:</p> Name Only (Default) <p>Just verify the correct tools were called. Parameters are ignored.</p> <pre><code>metric = ToolCorrectness()\n</code></pre> With Parameters <p>Validate both tool names and their arguments.</p> <pre><code>metric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n</code></pre> Strict Order <p>Tools must be called in the exact expected sequence.</p> <pre><code>metric = ToolCorrectness(strict_order=True)\n</code></pre> Fuzzy Parameters <p>Allow similar (but not identical) parameter values.</p> <pre><code>metric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='fuzzy',\n    fuzzy_threshold=0.8\n)\n</code></pre>"},{"location":"metric-registry/tool/#why-tool-metrics","title":"Why Tool Metrics?","text":"\ud83e\udd16 Agent Evaluation <p>Verify AI agents select the right tools for tasks.</p> \ud83d\udd27 Function Calling <p>Test LLM function calling capabilities.</p> \ud83d\udcca Workflow Validation <p>Ensure multi-step workflows execute correctly.</p> \ud83e\uddea Regression Testing <p>Catch breaking changes in agent behavior.</p>"},{"location":"metric-registry/tool/tool_correctness/","title":"Tool Correctness","text":"Evaluate whether AI agents call the correct tools with proper parameters Tool Agent Single Turn"},{"location":"metric-registry/tool/tool_correctness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Recall of expected tools \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>tools_called</code> <code>expected_tools</code> Tool call lists <p>What It Measures</p> <p>Tool Correctness evaluates whether an AI agent called the correct tools by comparing actual tool calls against expected ones. It supports name-only matching, parameter validation, and strict ordering requirements.</p> Score Interpretation 1.0  All expected tools called correctly 0.5-0.9  Partial match\u2014some tools missing 0.0  No expected tools called correctly \u2705 Use When <ul> <li>Evaluating AI agents</li> <li>Testing function calling</li> <li>Validating tool selection</li> <li>Checking parameter passing</li> </ul> \u274c Don't Use When <ul> <li>Order doesn't matter (consider disabling strict_order)</li> <li>Tool output quality matters more</li> <li>Parameters have valid variations</li> <li>No expected tools defined</li> </ul> How It Works  Computation Matching Strategies Score Formula <p>The metric compares called tools against expected tools with configurable matching strategies.</p> <p> Name Only (default) Just check if the tool name matches. Parameters ignored. </p> <p> Exact Parameters Parameters must match exactly. </p> <p> Subset Parameters Called args must contain all expected args (extras OK). </p> <p> Fuzzy Parameters Similarity-based matching with threshold. </p> <pre><code>score = matched_tools / total_expected_tools\n</code></pre> <p>Example: - Expected: <code>[search, calculate, format]</code> - Called: <code>[search, format]</code> - Score: <code>2/3 = 0.67</code></p>"},{"location":"metric-registry/tool/tool_correctness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Tools Called]\n        B[Expected Tools]\n    end\n\n    subgraph CONFIG[\"\u2699\ufe0f Configuration\"]\n        C{Strict Order?}\n        D{Check Parameters?}\n    end\n\n    subgraph MATCH[\"\ud83d\udd0d Matching\"]\n        E[Compare names]\n        F[Validate parameters]\n        G[Check sequence]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Score\"]\n        H[\"matched / expected\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt;|No| E\n    C --&gt;|Yes| G\n    E --&gt; D\n    G --&gt; D\n    D --&gt;|Yes| F\n    D --&gt;|No| H\n    F --&gt; H\n\n    style INPUT stroke:#8b5cf6,stroke-width:2px\n    style CONFIG stroke:#3b82f6,stroke-width:2px\n    style MATCH stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/tool/tool_correctness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>check_parameters</code> <code>bool</code> <code>False</code> Also validate tool parameters <code>strict_order</code> <code>bool</code> <code>False</code> Tools must be called in exact order <code>parameter_matching_strategy</code> <code>str</code> <code>exact</code> <code>exact</code>, <code>subset</code>, or <code>fuzzy</code> <code>fuzzy_threshold</code> <code>float</code> <code>0.8</code> Similarity threshold for fuzzy matching <p>Parameter Matching Strategies</p> <ul> <li>exact: Parameters must match exactly (default)</li> <li>subset: Called args must contain all expected args (extras allowed)</li> <li>fuzzy: Similarity-based matching using SequenceMatcher</li> </ul>"},{"location":"metric-registry/tool/tool_correctness/#code-examples","title":"Code Examples","text":"Basic Usage (Name Only) With Parameter Checking Strict Order Fuzzy Parameter Matching With Runner <pre><code>from axion.metrics import ToolCorrectness\nfrom axion.dataset import DatasetItem\nfrom axion._core.schema import ToolCall\n\nmetric = ToolCorrectness()\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"weather\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={}),\n        ToolCall(name=\"format\", args={}),\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - both tools called (params not checked)\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"calculate\", args={\"a\": 5, \"b\": 3}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"calculate\", args={\"a\": 5, \"b\": 3}),\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - params match exactly\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(strict_order=True)\n\n# Correct order\nitem_correct = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n)\n# Score: 1.0\n\n# Wrong order\nitem_wrong = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"process\", args={}),  # Should be second\n        ToolCall(name=\"fetch\", args={}),    # Should be first\n        ToolCall(name=\"store\", args={}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n)\n# Score: 0.0 - order mismatch at position 0\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='fuzzy',\n    fuzzy_threshold=0.8\n)\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"what is machine learning\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={\"query\": \"what is ML\"}),\n    ],\n)\n\nresult = await metric.execute(item)\n# Score depends on string similarity of query values\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\nfrom axion.runners import MetricRunner\n\nmetric = ToolCorrectness(check_parameters=True)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score:.2f}\")\n    print(f\"Explanation: {item_result.explanation}\")\n</code></pre>"},{"location":"metric-registry/tool/tool_correctness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>All Tools Correct</p> <p>Expected Tools:</p> <ol> <li><code>search(query=\"weather\")</code></li> <li><code>parse(format=\"json\")</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>search(query=\"weather\")</code></li> <li><code>parse(format=\"json\")</code></li> </ol> <p>Result: <code>1.0</code> </p> <p>All expected tools called with correct parameters.</p> \u26a0\ufe0f Scenario 2: Partial Match (Score: 0.67) <p>Missing Tool</p> <p>Expected Tools:</p> <ol> <li><code>fetch</code></li> <li><code>transform</code></li> <li><code>store</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>fetch</code></li> <li><code>transform</code> (store not called)</li> </ol> <p>Result: <code>2/3 = 0.67</code> </p> <p>Explanation: \"Correctly called: ['fetch', 'transform']; Missing tools: ['store']\"</p> \u274c Scenario 3: Wrong Tool (Score: 0.0) <p>Incorrect Tool Called</p> <p>Expected Tools:</p> <ol> <li><code>calculate</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>search</code></li> </ol> <p>Result: <code>0.0</code> </p> <p>Explanation: \"Missing tools: ['calculate']; Unexpected tools: ['search']\"</p> \u26a0\ufe0f Scenario 4: Parameter Mismatch <p>Wrong Parameters</p> <p>Config: <code>check_parameters=True, strategy='exact'</code></p> <p>Expected:</p> <p><code>search(query=\"Python tutorials\")</code></p> <p>Called:</p> <p><code>search(query=\"python tutorial\")</code> (different text)</p> <p>Result: <code>0.0</code> </p> <p>Exact matching fails on parameter difference.</p> <p>Fix: Use <code>strategy='fuzzy'</code> or <code>strategy='subset'</code> for flexibility.</p>"},{"location":"metric-registry/tool/tool_correctness/#why-it-matters","title":"Why It Matters","text":"\ud83e\udd16 Agent Evaluation <p>Verify AI agents select and call the right tools for tasks.</p> \ud83d\udd27 Function Calling <p>Test LLM function calling capabilities and parameter handling.</p> \ud83d\udcca Workflow Validation <p>Ensure multi-step agent workflows execute correctly.</p>"},{"location":"metric-registry/tool/tool_correctness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Tool Correctness = Did the agent call the expected tools correctly?</p> <ul> <li>Use it when: Evaluating AI agents or function calling</li> <li>Score interpretation: Fraction of expected tools called correctly</li> <li>Key configs: <code>check_parameters</code>, <code>strict_order</code>, <code>parameter_matching_strategy</code></li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ToolCorrectness</code></p> </li> <li> <p> Related Concepts</p> <p>[ Agent Evaluation \u00b7 Function Calling \u00b7 Tool Use</p> </li> </ul>"},{"location":"reference/dataset/","title":"Dataset API Reference","text":"<p>Core data structures for evaluation datasets.</p>"},{"location":"reference/dataset/#dataset","title":"Dataset","text":""},{"location":"reference/dataset/#axion.dataset.Dataset","title":"<code>axion.dataset.Dataset(name: Optional[str] = None, description: str = '', version: str = '1.0', created_at: str = (lambda: current_datetime())(), metadata: Optional[str] = None, items: List[DatasetItem] = list(), _default_catch_all: str = FieldNames.ADDITIONAL_INPUT, _item_map: Dict[str, DatasetItem] = dict(), _synthetic_data: Optional[List[Dict[str, Any]]] = None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RichSerializer</code></p> <p>Represents a structured dataset for evaluation purposes, supporting both single and multi-turn items.</p> <p>This class manages a collection of DatasetItem objects and provides functionality for loading, saving, filtering, and transforming datasets.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>)           \u2013            <p>Name of the dataset</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Description of the dataset's purpose or contents</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Version identifier</p> </li> <li> <code>created_at</code>               (<code>str</code>)           \u2013            <p>ISO format timestamp of creation</p> </li> <li> <code>metadata</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional metadata (stored as JSON)</p> </li> <li> <code>items</code>               (<code>List[DatasetItem]</code>)           \u2013            <p>List of DatasetItem objects</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.items","title":"<code>items: List[DatasetItem] = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/dataset/#axion.dataset.Dataset.to_json","title":"<code>to_json(file_path: str) -&gt; None</code>","text":"<p>Save dataset to JSON file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path where to save the JSON file</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.to_dataframe","title":"<code>to_dataframe(flatten_nested_json: bool = False, sep: str = '.', remove_aliased: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Converts the dataset to a pandas DataFrame, serializing complex fields to JSON strings.</p> <p>Parameters:</p> <ul> <li> <code>flatten_nested_json</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, nested objects will be flattened into separate columns.                  If False (default), they will be stored as JSON strings.</p> </li> <li> <code>sep</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>Separator for flattening.</p> </li> <li> <code>remove_aliased</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True remove aliased model field keys.</p> </li> </ul> <p>Returns:     A pandas DataFrame representing the dataset.</p>"},{"location":"reference/dataset/#datasetitem","title":"DatasetItem","text":""},{"location":"reference/dataset/#axion.dataset.DatasetItem","title":"<code>axion.dataset.DatasetItem(**data)</code>","text":"<p>               Bases: <code>RichDatasetBaseModel</code></p> <p>Represents a single evaluation data point, supporting both single-turn and multi-turn conversations.</p> <p>This model is designed to store all relevant information required for evaluating LLM performance, including the input query, expected and actual outputs, retrieved context, evaluation criteria, and additional metadata. It supports both automated evaluation (binary judgments, critiques) and richer evaluation with tool usage tracking.</p> <p>Attributes:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the item (auto-generated if not provided).</p> </li> <li> <code>query</code>               (<code>Optional[str]</code>)           \u2013            <p>The input query or prompt for single-turn evaluation. Aliased as <code>query</code> for backward compatibility.</p> </li> <li> <code>conversation</code>               (<code>Optional[MultiTurnConversation]</code>)           \u2013            <p>Multi-turn conversation structure containing a sequence of messages. Aliased to <code>conversation</code>.</p> </li> <li> <code>expected_output</code>               (<code>Optional[str]</code>)           \u2013            <p>The reference/expected output for single-turn evaluation. Aliased to <code>expected_output</code>.</p> </li> <li> <code>actual_output</code>               (<code>Optional[str]</code>)           \u2013            <p>The system's generated response for the given query.</p> </li> <li> <code>retrieved_content</code>               (<code>Optional[List[str]]</code>)           \u2013            <p>A list of retrieved documents or contextual snippets used in generating the response.</p> </li> <li> <code>latency</code>               (<code>Optional[float]</code>)           \u2013            <p>Response time in seconds for generating the <code>actual_output</code>.</p> </li> <li> <code>judgment</code>               (<code>Optional[Union[str, int]]</code>)           \u2013            <p>A short, binary or categorical evaluation decision (e.g., 1/0, pass/fail, approve/decline).</p> </li> <li> <code>critique</code>               (<code>Optional[str]</code>)           \u2013            <p>A detailed explanation or rationale supporting the <code>judgment</code>.</p> </li> <li> <code>conversation_extraction_strategy</code>               (<code>Literal['first', 'last']</code>)           \u2013            <p>Defines whether to extract <code>query</code> and <code>actual_output</code> from the first or last messages in a multi-turn conversation. Defaults to 'last'.</p> </li> <li> <code>acceptance_criteria</code>               (<code>Optional[List[str]]</code>)           \u2013            <p>User-defined definitions of what qualifies as an acceptable/correct response.</p> </li> <li> <code>additional_input</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Arbitrary key-value pairs providing extra inputs for the evaluation context.</p> </li> <li> <code>metadata</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional metadata as a JSON string for storing structured information.</p> </li> <li> <code>trace</code>               (<code>Optional[str]</code>)           \u2013            <p>Execution trace information, stored as a JSON string.</p> </li> <li> <code>trace_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional[str]: Trace ID for the original observation from tracing provider.</p> </li> </ul> <p>observation_id: Optional[str]:         Observation ID for the original observation from tracing provider.         This is the ID of the specific observation that was evaluated.     additional_output (Dict[str, Any]):         Extra outputs generated by the system, useful for debugging or extended evaluation.     tools_called (Optional[List[ToolCall]]):         A list of tools the system actually invoked during response generation.     expected_tools (Optional[List[ToolCall]]):         A list of tools that should have been invoked according to the evaluation criteria.     user_tags (List[str]):         A list of custom tags to apply to all tool calls in the conversation.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.conversation","title":"<code>conversation: Optional[MultiTurnConversation]</code>  <code>property</code>","text":"<p>Provides direct access to the multi-turn conversation object.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.query","title":"<code>query: Optional[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Provides a unified way to access the user's query based on the extraction strategy.</p> <p>If the strategy is 'last' (default), it returns the last user message. If the strategy is 'first', it returns the first user message.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.expected_output","title":"<code>expected_output: Optional[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Provides a unified way to access the expected output.</p> <p>If the item is a multi-turn conversation, this returns the <code>reference_text</code> if set. If it's a single-turn item, it returns the stored expected output.</p> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>The expected output as a string, or None if not applicable.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.conversation_stats","title":"<code>conversation_stats: Optional[Dict[str, int]]</code>  <code>property</code>","text":"<p>A dictionary of statistics about the conversation.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.agent_trajectory","title":"<code>agent_trajectory: Optional[List[str]]</code>  <code>property</code>","text":"<p>An ordered list of tool names called, representing the agent's execution path.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.has_errors","title":"<code>has_errors: Union[bool, None]</code>  <code>property</code>","text":"<p>Returns True if any tool message in the conversation is marked as an error.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.to_transcript","title":"<code>to_transcript() -&gt; str</code>","text":"<p>Converts the conversation messages into a human-readable string transcript.</p> <p>If the item is not a multi-turn conversation, it returns an empty string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A formatted string representing the entire conversation.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.generate_tags_for_tool_call","title":"<code>generate_tags_for_tool_call(tool_call: ToolCall) -&gt; List[str]</code>  <code>staticmethod</code>","text":"<p>Analyzes a tool call to generate a set of descriptive tags.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.set_actual_output_from_conversation","title":"<code>set_actual_output_from_conversation() -&gt; DatasetItem</code>","text":"<p>If this is a multi-turn item and <code>actual_output</code> is not provided, this validator automatically sets it to the content of the first or last AIMessage, based on the <code>conversation_extraction_strategy</code>.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.convert_acceptance_criteria_to_list","title":"<code>convert_acceptance_criteria_to_list(v) -&gt; Optional[List[str]]</code>  <code>classmethod</code>","text":"<p>Ensure acceptance_criteria is a list of strings.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.get","title":"<code>get(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get an attribute value by key, similar to dict.get(). This method correctly handles properties like 'query'.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The attribute name to retrieve.</p> </li> <li> <code>default</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Value to return if attribute doesn't exist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The attribute value or default if not found.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.extract_by_tag","title":"<code>extract_by_tag(tag: str) -&gt; List[tuple[ToolCall, Optional[ToolMessage]]]</code>","text":"<p>Extracts tool interactions from the conversation that match a specific tag.</p> <p>Parameters:</p> <ul> <li> <code>tag</code>               (<code>str</code>)           \u2013            <p>The tag to filter by (e.g., 'RAG', 'GUARDRAIL').</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[tuple[ToolCall, Optional[ToolMessage]]]</code>           \u2013            <p>A list of tuples, where each tuple contains the tagged ToolCall</p> </li> <li> <code>List[tuple[ToolCall, Optional[ToolMessage]]]</code>           \u2013            <p>and its corresponding ToolMessage (or None if not found).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.keys","title":"<code>keys() -&gt; List[str]</code>","text":"<p>Return all public attribute names, including properties and aliases.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A sorted list of public-facing field and property names.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.values","title":"<code>values() -&gt; List[Any]</code>","text":"<p>Return all public attribute values, corresponding to the .keys() method.</p> <p>Returns:</p> <ul> <li> <code>List[Any]</code>           \u2013            <p>A list of values for the public-facing fields and properties.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.items","title":"<code>items() -&gt; List[tuple]</code>","text":"<p>Return all (key, value) pairs for public attributes.</p> <p>Returns:</p> <ul> <li> <code>List[tuple]</code>           \u2013            <p>A list of (key, value) tuples for public-facing fields and properties.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset","title":"<code>subset(fields: List[str], keep_id: bool = True, copy_annotations: bool = False) -&gt; DatasetItem</code>","text":"<p>Create a new DatasetItem with only the specified fields, all others set to None/empty.</p> <p>Parameters:</p> <ul> <li> <code>fields</code>               (<code>List[str]</code>)           \u2013            <p>List of field names to keep (e.g., ['query', 'expected_output'])</p> </li> <li> <code>keep_id</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to preserve the original ID (default: True)</p> </li> <li> <code>copy_annotations</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to copy annotations (judgment and critique) to the new item (default: False)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>New DatasetItem instance with only specified fields populated</p> </li> </ul> Example"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset--get-item-with-only-query-and-expected_output","title":"Get item with only query and expected_output","text":"<p>subset_item = item.subset(['query', 'expected_output'])</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset--get-item-with-query-actual_output-and-preserve-annotations","title":"Get item with query, actual_output, and preserve annotations","text":"<p>subset_item = item.subset(['query', 'actual_output'], copy_annotations=True)</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.evaluation_fields","title":"<code>evaluation_fields() -&gt; DatasetItem</code>","text":"<p>Extract just the evaluation fields.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.update","title":"<code>update(other: Union[DatasetItem, Dict[str, Any]], overwrite: bool = True) -&gt; DatasetItem</code>","text":"<p>Update this DatasetItem with values from another DatasetItem or dictionary. This method correctly handles aliases and special merge logic for lists/dicts.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[DatasetItem, Dict[str, Any]]</code>)           \u2013            <p>Another DatasetItem instance or a dictionary to update from.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, overwrite existing values. If False, only fill empty fields.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>The updated DatasetItem instance (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.update_runtime","title":"<code>update_runtime(**kwargs) -&gt; DatasetItem</code>","text":"<p>Update runtime-related fields such as actual_output or retrieved_content.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>           \u2013            <p>Runtime fields to update.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>Updated DatasetItem (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.merge_metadata","title":"<code>merge_metadata(metadata: Union[str, Dict[str, Any]]) -&gt; DatasetItem</code>","text":"<p>Merge new metadata into the existing metadata field.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Union[str, Dict[str, Any]]</code>)           \u2013            <p>A dictionary or JSON string to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>Updated DatasetItem (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.from_dict","title":"<code>from_dict(data: Dict[str, Any]) -&gt; DatasetItem</code>  <code>classmethod</code>","text":"<p>Create a DatasetItem from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary containing item data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>New DatasetItem instance</p> </li> </ul>"},{"location":"reference/eval-tree/","title":"Eval Tree API Reference","text":"<p>Hierarchical evaluation orchestration.</p>"},{"location":"reference/eval-tree/#evaltree","title":"EvalTree","text":""},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree","title":"<code>axion.eval_tree.EvalTree(config: Union[Config, Dict[str, Any], str], max_concurrent: int = 5, summary_generator: Optional[BaseSummary] = None, enable_internal_caching: bool = True, trace_granularity: TraceGranularity = TraceGranularity.SEPARATE)</code>","text":"<p>               Bases: <code>TreeMixin</code></p> <p>Hierarchical scoring model that builds a tree from a configuration and executes metrics using an optimized two-phase batch process.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.elapsed_time","title":"<code>elapsed_time: Union[float, None]</code>  <code>property</code>","text":"<p>Execution Time</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.summary","title":"<code>summary: Union[Dict[str, Any], None]</code>  <code>property</code>","text":"<p>Model Summary</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.execute","title":"<code>execute(input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; TestResult</code>  <code>async</code>","text":"<p>Executes the evaluation for a single data item.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.batch_execute","title":"<code>batch_execute(evaluation_inputs: Union[List[DatasetItem], Dataset, pd.DataFrame], evaluation_name: str = 'EvalTree Evaluation', show_progress: bool = True) -&gt; EvaluationResult</code>  <code>async</code>","text":"<p>Executes the evaluation using an optimized two-phase process: 1. Batch Calculation: Runs all leaf metrics across the dataset. 2. In-Memory Aggregation: Computes hierarchical scores from the results.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.get_node","title":"<code>get_node(name: str) -&gt; Optional[Node]</code>","text":"<p>Get node by name</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.display","title":"<code>display()</code>  <code>classmethod</code>","text":"<p>Display Usage Documentation</p>"},{"location":"reference/llm-registry/","title":"LLM Registry API Reference","text":"<p>LLM model cost estimation and registry.</p>"},{"location":"reference/llm-registry/#llmregistry","title":"LLMRegistry","text":""},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry","title":"<code>axion.llm_registry.LLMRegistry(provider: Optional[str] = None, **credentials)</code>","text":"<p>A factory and registry for LLM and embedding model providers.</p> <p>This class provides a unified interface for creating LLM and embedding clients from different providers using simple string identifiers or global settings.</p> <p>Initializes the registry. If a provider is specified, it configures for that provider. Otherwise, it remains flexible to serve requests based on global settings.</p> <p>Parameters:</p> <ul> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the provider to lock this registry instance to.</p> </li> <li> <code>**credentials</code>           \u2013            <p>Credentials required by the provider, such as 'api_key'.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.default_llm_model_name","title":"<code>default_llm_model_name: str</code>  <code>property</code>","text":"<p>Default LLM model name</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.default_embedding_model_name","title":"<code>default_embedding_model_name: str</code>  <code>property</code>","text":"<p>Default Embedding model name</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.register","title":"<code>register(name: str)</code>  <code>classmethod</code>","text":"<p>A class method decorator to register a new provider.</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.display","title":"<code>display()</code>  <code>classmethod</code>","text":"<p>Display the LLM provider registry.</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.get_llm","title":"<code>get_llm(model_name: Optional[str] = None, provider: Optional[str] = None, **kwargs) -&gt; Any</code>","text":"<p>Gets a language model instance from a provider.</p> <p>Uses the instance's locked provider if set, otherwise falls back to the globally configured <code>settings.llm_provider</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use. Defaults to settings.model_name.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters for the LLM's constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>An instance of a language model client.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.get_embedding_model","title":"<code>get_embedding_model(model_name: Optional[str] = None, provider: Optional[str] = None, **kwargs) -&gt; Any</code>","text":"<p>Gets an embedding model instance from a provider.</p> <p>Uses the instance's locked provider if set, otherwise falls back to the globally configured <code>settings.embedding_provider</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use. Defaults to settings.embedding_model.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters for the embedding model's constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>An instance of an embedding model client.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.estimate_cost","title":"<code>estimate_cost(prompt_tokens: int, completion_tokens: int, model_name: Optional[str] = None, provider: Optional[str] = None) -&gt; float</code>","text":"<p>Estimates the cost of an LLM call based on token usage.</p> <p>The method determines the correct provider and model based on the same precedence rules as get_llm (direct arguments &gt; locked instance &gt; global settings).</p> <p>Parameters:</p> <ul> <li> <code>prompt_tokens</code>               (<code>int</code>)           \u2013            <p>The number of tokens in the prompt.</p> </li> <li> <code>completion_tokens</code>               (<code>int</code>)           \u2013            <p>The number of tokens in the completion.</p> </li> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use for pricing. Defaults to settings.model_name.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The estimated cost in USD.</p> </li> </ul>"},{"location":"reference/metrics/","title":"Metrics API Reference","text":"<p>Evaluation metrics for AI agents.</p>"},{"location":"reference/metrics/#metricregistry","title":"MetricRegistry","text":""},{"location":"reference/metrics/#axion.metrics.MetricRegistry","title":"<code>axion.metrics.MetricRegistry</code>","text":"<p>Registry for storing and retrieving metric classes.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.finalize_initial_state","title":"<code>finalize_initial_state() -&gt; None</code>","text":"<p>Call this after all built-in metrics are registered.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.register","title":"<code>register(metric_class: Type[BaseMetric]) -&gt; None</code>","text":"<p>Register a metric class into the registry.</p> <p>Parameters:</p> <ul> <li> <code>metric_class</code>               (<code>Type[BaseMetric]</code>)           \u2013            <p>A class inheriting from BaseMetric with a valid config.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.reset","title":"<code>reset() -&gt; None</code>  <code>classmethod</code>","text":"<p>Reset registry to original state.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get","title":"<code>get(key: str, error: bool = True) -&gt; Optional[Type[BaseMetric]]</code>","text":"<p>Retrieve a registered metric class by key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The unique key of the metric.</p> </li> <li> <code>error</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise an error if the key is not found.    If False, return None instead.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Type[BaseMetric]]</code>           \u2013            <p>The registered metric class, or None if not found and error=False.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.find","title":"<code>find(query: str) -&gt; List[Type[BaseMetric]]</code>","text":"<p>Search for metrics whose name, description, or tags match a query.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Case-insensitive search string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of matching metric classes.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get_compatible_metrics","title":"<code>get_compatible_metrics(item: DatasetItem) -&gt; List[Type[BaseMetric]]</code>","text":"<p>Return all metrics compatible with a given DatasetItem.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The dataset item to test against.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of compatible metric classes.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get_metric_descriptions","title":"<code>get_metric_descriptions() -&gt; Dict[str, str]</code>","text":"<p>Return {metric_name: description} from the registry.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.display","title":"<code>display(show_examples: bool = False) -&gt; None</code>","text":"<p>Display a summary of all registered metrics.</p> <p>Parameters:</p> <ul> <li> <code>show_examples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Show custom LLM examples</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.display_table","title":"<code>display_table() -&gt; None</code>","text":"<p>Display a formatted table of all registered metrics.</p>"},{"location":"reference/metrics/#composite-metrics","title":"Composite Metrics","text":""},{"location":"reference/metrics/#faithfulness","title":"Faithfulness","text":""},{"location":"reference/metrics/#axion.metrics.Faithfulness","title":"<code>axion.metrics.Faithfulness(mode: EvaluationMode = EvaluationMode.GRANULAR, strict_mode: bool = False, verdict_scores: Optional[Dict[str, float]] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Measures how faithful the generated answer is to the retrieved context. Default Scoring: Fully Supported (+1.0), Partially Supported (+0.5), No Evidence (0.0), Contradictory (-1.0)</p> <p>Initializes the Faithfulness metric. Args:     mode: The evaluation mode for the underlying RAG analyzer.     strict_mode (bool): If True, sets 'NO_EVIDENCE' to -1.0, penalizing                         uncited claims (hallucinations) as heavily as contradictions.                         This is overridden by 'verdict_scores' if provided.     verdict_scores: A dictionary to override the default scoring weights (e.g.,                     {\"CONTRADICTORY\": -2.0, \"PARTIALLY_SUPPORTED\": 0.75}).                     If provided, this takes precedence over 'strict_mode'.     **kwargs: Additional keyword arguments passed to the RAGAnalyzer.</p>"},{"location":"reference/metrics/#answerrelevancy","title":"AnswerRelevancy","text":""},{"location":"reference/metrics/#axion.metrics.AnswerRelevancy","title":"<code>axion.metrics.AnswerRelevancy(relevancy_mode: Literal['strict', 'task'] = 'task', penalize_ambiguity: bool = False, mode: EvaluationMode = EvaluationMode.GRANULAR, multi_turn_strategy: Literal['last_turn', 'all_turns'] = 'last_turn', **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Computes answer relevancy scores by analyzing how well the response addresses the input query. Supports both single-turn and multi-turn (all turns) evaluation.</p> <p>Initialize the Answer Relevancy metric.</p> <p>Parameters:</p> <ul> <li> <code>relevancy_mode</code>               (<code>Literal['strict', 'task']</code>, default:                   <code>'task'</code> )           \u2013            <p>The mode for judging relevancy. 'strict': Only directly answering statements are relevant. 'task': Closely related, helpful statements are also relevant (default).</p> </li> <li> <code>penalize_ambiguity</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, 'idk' verdicts are scored as 0.0 (irrelevant).                        If False (default), 'idk' verdicts are scored as 1.0 (relevant).</p> </li> <li> <code>mode</code>               (<code>EvaluationMode</code>, default:                   <code>GRANULAR</code> )           \u2013            <p>The evaluation mode for the internal RAGAnalyzer.</p> </li> <li> <code>multi_turn_strategy</code>               (<code>Literal</code>, default:                   <code>'last_turn'</code> )           \u2013            <p>How to handle multi-turn conversations.                            'last_turn' (default): Evaluates only the last turn.                            'all_turns': Evaluates all Human-&gt;AI turns in the conversation.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to parent class.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerRelevancy.execute","title":"<code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Compute the score based on criteria. Automatically handles single-turn or multi-turn evaluation based on <code>self.multi_turn_strategy</code> and <code>item.conversation</code>.</p>"},{"location":"reference/metrics/#factualaccuracy","title":"FactualAccuracy","text":""},{"location":"reference/metrics/#axion.metrics.FactualAccuracy","title":"<code>axion.metrics.FactualAccuracy(**kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Factual Accuracy Metric.</p> <p>Process: 1. Decompose 'actual_output' into atomic statements. 2. specific binary check (1/0) for each statement against 'expected_output'. 3. Score = (Sum of 1s) / (Total Statements).</p>"},{"location":"reference/metrics/#axion.metrics.FactualAccuracy.get_signals","title":"<code>get_signals(report: FactualityReport) -&gt; List[SignalDescriptor]</code>  <code>staticmethod</code>","text":"<p>Display the binary checklist in the UI.</p>"},{"location":"reference/metrics/#answercompleteness","title":"AnswerCompleteness","text":""},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness","title":"<code>axion.metrics.AnswerCompleteness(use_expected_output: bool = True, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Evaluates the completeness of product-related responses using one of two approaches: 1. Aspect-based evaluation (when expected_aspects are provided) 2. Sub-question based evaluation (when expected_aspects are not provided)</p> <p>Initialize the answer completeness metric with required prompts for both approaches.</p> <p>Parameters:</p> <ul> <li> <code>use_expected_output</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>It True, use expected answer if available, otherwise decompose query</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness.execute","title":"<code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Compute the completeness score, returning a structured result object in the <code>signals</code> field.</p>"},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness.get_signals","title":"<code>get_signals(result: AnswerCompletenessResult) -&gt; List[SignalDescriptor[AnswerCompletenessResult]]</code>  <code>staticmethod</code>","text":"<p>Generates a list of detailed signals from the evaluation result.</p>"},{"location":"reference/metrics/#answercriteria","title":"AnswerCriteria","text":""},{"location":"reference/metrics/#axion.metrics.AnswerCriteria","title":"<code>axion.metrics.AnswerCriteria(criteria_key: str = 'Complete', scoring_strategy: Literal['concept', 'aspect', 'weighted'] = 'concept', check_for_contradictions: bool = False, weighted_concept_score_weight: float = 0.7, multi_turn_strategy: Literal['last_turn', 'all_turns'] = 'last_turn', multi_turn_aggregation: Literal['cumulative', 'average'] = 'cumulative', **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Evaluates responses based on specified criteria. This metric extracts key aspects from the criteria and checks whether each aspect is adequately and accurately addressed in the response.</p> <p>It supports two modes of operation: 1.  Single-Turn / Last-Turn (default): Evaluates <code>item.query</code> vs. <code>item.actual_output</code>.     If <code>item.conversation</code> is present, <code>item.query</code> and <code>item.actual_output</code> are     auto-populated from the last turn (based on <code>conversation_extraction_strategy</code>). 2.  Multi-Turn: If <code>multi_turn_strategy='all_turns'</code>, this metric will iterate     through the entire <code>item.conversation</code> and evaluate every <code>HumanMessage</code> -&gt; <code>AIMessage</code>     pair. The aggregation method is controlled by <code>multi_turn_aggregation</code>.</p> <p>Initialize the criteria-based answer criteria metric.</p> <p>Parameters:</p> <ul> <li> <code>criteria_key</code>               (<code>str</code>, default:                   <code>'Complete'</code> )           \u2013            <p>The key in <code>additional_input</code> or <code>conversation.rubrics</code>           to find the criteria text (default: 'Complete').</p> </li> <li> <code>scoring_strategy</code>               (<code>Literal['concept', 'aspect', 'weighted']</code>, default:                   <code>'concept'</code> )           \u2013            <p>The scoring method: 'concept', 'aspect', or 'weighted' (default: 'concept').</p> </li> <li> <code>check_for_contradictions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, uses a stricter prompt to penalize contradictions (default: False).</p> </li> <li> <code>weighted_concept_score_weight</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The weight for the concept score in 'weighted' strategy (default: 0.7).</p> </li> <li> <code>multi_turn_strategy</code>               (<code>Literal['last_turn', 'all_turns']</code>, default:                   <code>'last_turn'</code> )           \u2013            <p>How to handle multi-turn conversations.                  'last_turn' (default): Evaluates only the last turn.                  'all_turns': Evaluates all Human-&gt;AI turns in the conversation.</p> </li> <li> <code>multi_turn_aggregation</code>               (<code>Literal['cumulative', 'average']</code>, default:                   <code>'cumulative'</code> )           \u2013            <p>Aggregation method for 'all_turns' strategy.                     'cumulative' (default): Scores unique aspects covered across all turns.                     'average': Scores average aspect coverage per turn.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to parent class</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerCriteria.execute","title":"<code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Compute the score based on criteria. Automatically handles single-turn or multi-turn evaluation based on <code>self.multi_turn_strategy</code> and <code>item.conversation</code>.</p>"},{"location":"reference/metrics/#axion.metrics.AnswerCriteria.get_signals","title":"<code>get_signals(result: AnswerCriteriaResult) -&gt; List[SignalDescriptor[AnswerCriteriaResult]]</code>","text":"<p>Generates a list of detailed signals from the evaluation result that explain the scoring.</p>"},{"location":"reference/metrics/#contextualrelevancy","title":"ContextualRelevancy","text":""},{"location":"reference/metrics/#axion.metrics.ContextualRelevancy","title":"<code>axion.metrics.ContextualRelevancy(mode: EvaluationMode = EvaluationMode.GRANULAR, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Measures the relevancy of retrieval (how much retrieved content is relevant).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualRelevancy.get_signals","title":"<code>get_signals(result: ContextualRelevancyResult) -&gt; List[SignalDescriptor[ContextualRelevancyResult]]</code>","text":"<p>Defines the explainable signals for the ContextualRelevancy metric.</p>"},{"location":"reference/metrics/#contextualprecision","title":"ContextualPrecision","text":""},{"location":"reference/metrics/#axion.metrics.ContextualPrecision","title":"<code>axion.metrics.ContextualPrecision(mode: EvaluationMode = EvaluationMode.GRANULAR, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Measures the quality of retrieval ranking using Mean Average Precision (MAP).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualPrecision.get_signals","title":"<code>get_signals(result: ContextualPrecisionResult) -&gt; List[SignalDescriptor[ContextualPrecisionResult]]</code>","text":"<p>Defines the explainable signals for the ContextualRanking metric.</p>"},{"location":"reference/metrics/#contextualrecall","title":"ContextualRecall","text":""},{"location":"reference/metrics/#axion.metrics.ContextualRecall","title":"<code>axion.metrics.ContextualRecall(mode: EvaluationMode = EvaluationMode.GRANULAR, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Measures recall of retrieval (how much of the expected answer is in context).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualRecall.get_signals","title":"<code>get_signals(result: ContextualRecallResult) -&gt; List[SignalDescriptor[ContextualRecallResult]]</code>","text":"<p>Defines the explainable signals for the ContextualRecall metric.</p>"},{"location":"reference/metrics/#heuristic-metrics","title":"Heuristic Metrics","text":""},{"location":"reference/metrics/#exactstringmatch","title":"ExactStringMatch","text":""},{"location":"reference/metrics/#axion.metrics.ExactStringMatch","title":"<code>axion.metrics.ExactStringMatch(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, **kwargs: Any)</code>","text":"<p>               Bases: <code>BaseMetric</code></p>"},{"location":"reference/metrics/#axion.metrics.ExactStringMatch.execute","title":"<code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Returns 1.0 if the actual output exactly matches the expected output (after stripping). Returns 0.0 otherwise.</p>"},{"location":"reference/metrics/#citationpresence","title":"CitationPresence","text":""},{"location":"reference/metrics/#axion.metrics.CitationPresence","title":"<code>axion.metrics.CitationPresence(mode: str = 'any_citation', strict: bool = False, embed_model: Optional[EmbeddingRunnable] = None, use_semantic_search: bool = False, resource_similarity_threshold: float = 0.8, custom_resource_phrases: Optional[List[str]] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>A metric to evaluate if the response includes properly formatted citations, supporting single-turn or multi-turn conversations.</p> <p>Initialize the Citation Presence metric.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'any_citation'</code> )           \u2013            <p>Evaluation mode - \"any_citation\" or \"resource_section\".</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, validates that found URLs are live by making a HEAD request.</p> </li> <li> <code>embed_model</code>               (<code>Optional[EmbeddingRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Embedding model for semantic similarity.</p> </li> <li> <code>use_semantic_search</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, uses the embedding model as a fallback.</p> </li> <li> <code>resource_similarity_threshold</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Threshold for semantic similarity.</p> </li> <li> <code>custom_resource_phrases</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom phrases to look for when detecting resource sections.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to BaseMetric.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.CitationPresence.execute","title":"<code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Evaluate citation presence for the entire conversation or single-turn response.</p>"},{"location":"reference/metrics/#axion.metrics.CitationPresence.get_signals","title":"<code>get_signals(result: CitationPresenceResult) -&gt; List[SignalDescriptor[CitationPresenceResult]]</code>","text":"<p>Generates detailed signals from the presence evaluation.</p>"},{"location":"reference/metrics/#latency","title":"Latency","text":""},{"location":"reference/metrics/#axion.metrics.Latency","title":"<code>axion.metrics.Latency(normalize: bool = False, normalization_method: str = 'exponential', **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the Latency metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, normalize latency scores to [0, 1] range.</p> </li> <li> <code>normalization_method</code>               (<code>str</code>, default:                   <code>'exponential'</code> )           \u2013            <p>Method for normalization. Options: - 'exponential': Uses exp(-latency/threshold) for smooth decay - 'sigmoid': Uses 1/(1 + exp((latency-threshold)/scale)) for S-curve - 'reciprocal': Uses threshold/(threshold + latency) for hyperbolic decay - 'linear': Uses max(0, 1 - latency/threshold) for linear decay</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the base metric.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.Latency.execute","title":"<code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Returns the latency recorded for a given test case as the metric score.</p> <p>This metric assumes the <code>latency</code> field is already populated on the DatasetItem and returns it as-is or normalized based on the initialization parameters.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The evaluation data point containing latency information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetricEvaluationResult</code> (              <code>MetricEvaluationResult</code> )          \u2013            <p>The result object containing the latency as the score.</p> </li> </ul>"},{"location":"reference/metrics/#retrieval-metrics","title":"Retrieval Metrics","text":""},{"location":"reference/metrics/#hitrateatk","title":"HitRateAtK","text":""},{"location":"reference/metrics/#axion.metrics.HitRateAtK","title":"<code>axion.metrics.HitRateAtK(k: Union[int, List[int]] = 10, main_k: Optional[int] = None, **kwargs)</code>","text":"<p>               Bases: <code>_RetrievalMetric</code></p> <p>Evaluates if any relevant document was retrieved in the top K results. Score is 1 if a hit is found, 0 otherwise. Now supports multiple K values.</p> <p>Initialize the Hit Rate @ K metric. Args:     k: The number of top results to consider, or a list of K values.     main_k: The K value to use for the main metric score (defaults to max K in k_list).</p>"},{"location":"reference/metrics/#axion.metrics.HitRateAtK.get_signals","title":"<code>get_signals(result: MultiKResult) -&gt; List[SignalDescriptor]</code>","text":"<p>Generates signals detailing the hit rate calculation for all K values.</p>"},{"location":"reference/metrics/#meanreciprocalrank","title":"MeanReciprocalRank","text":""},{"location":"reference/metrics/#axion.metrics.MeanReciprocalRank","title":"<code>axion.metrics.MeanReciprocalRank(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, **kwargs: Any)</code>","text":"<p>               Bases: <code>_RetrievalMetric</code></p> <p>Calculates the Mean Reciprocal Rank (MRR). Score is 1 / (rank of first relevant item). This metric is K-independent.</p>"},{"location":"reference/metrics/#axion.metrics.MeanReciprocalRank.get_signals","title":"<code>get_signals(result: MeanReciprocalRankResult) -&gt; List[SignalDescriptor]</code>","text":"<p>Generates signals detailing the Mean Reciprocal Rank calculation.</p>"},{"location":"reference/metrics/#conversational-metrics","title":"Conversational Metrics","text":""},{"location":"reference/metrics/#goalcompletion","title":"GoalCompletion","text":""},{"location":"reference/metrics/#axion.metrics.GoalCompletion","title":"<code>axion.metrics.GoalCompletion(goal_key: str = 'goal', completion_weight: float = 0.6, efficiency_weight: float = 0.4, bottleneck_threshold: int = 5, max_clarification_penalty: float = 0.3, clarification_penalty_rate: float = 0.1, goal_drift_threshold: float = 0.3, outcome_threshold_achieved: float = 0.8, outcome_threshold_partial: float = 0.4, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Composite metric that analyzes: 1. Goal achievement (did we complete the task?) 2. Conversation efficiency (how well did we complete it?) 3. Temporal tracking (when/how did we achieve sub-goals?)</p> <p>Reuses conversation analysis from shared components to minimize LLM calls. Uses unified analysis approach to reduce LLM calls from ~3+N to ~3 total.</p> <p>Initialize the Goal Completion metric.</p> <p>Parameters:</p> <ul> <li> <code>goal_key</code>               (<code>str</code>, default:                   <code>'goal'</code> )           \u2013            <p>Key in additional_input containing the user's goal</p> </li> <li> <code>completion_weight</code>               (<code>float</code>, default:                   <code>0.6</code> )           \u2013            <p>Weight for pure goal achievement (default: 0.6) Rationale: Goal achievement is slightly more important than efficiency</p> </li> <li> <code>efficiency_weight</code>               (<code>float</code>, default:                   <code>0.4</code> )           \u2013            <p>Weight for conversation efficiency (default: 0.4) Rationale: Efficiency matters, but achieving the goal is primary</p> </li> <li> <code>bottleneck_threshold</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of turns before a sub-goal is flagged as bottleneck (default: 5) Rationale: Most sub-goals should resolve within 3-5 turns in efficient conversations</p> </li> <li> <code>max_clarification_penalty</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>Maximum penalty for clarifications (default: 0.3) Rationale: Excessive clarifications can reduce efficiency by up to 30%</p> </li> <li> <code>clarification_penalty_rate</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Penalty per clarification (default: 0.1) Rationale: Each clarification represents a 10% efficiency loss</p> </li> <li> <code>goal_drift_threshold</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>Fraction of unmapped moments to trigger drift detection (default: 0.3) Rationale: If &gt;30% of conversation is unrelated to goal, it indicates drift</p> </li> <li> <code>outcome_threshold_achieved</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Minimum score for \"achieved\" outcome (default: 0.8) Rationale: 80%+ completion indicates successful goal achievement</p> </li> <li> <code>outcome_threshold_partial</code>               (<code>float</code>, default:                   <code>0.4</code> )           \u2013            <p>Minimum score for \"partially_achieved\" outcome (default: 0.4) Rationale: 40-80% completion indicates partial success</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.GoalCompletion.execute","title":"<code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Execute goal completion analysis using unified approach.</p>"},{"location":"reference/metrics/#axion.metrics.GoalCompletion.get_signals","title":"<code>get_signals(result: GoalCompletionResult) -&gt; List[SignalDescriptor[GoalCompletionResult]]</code>","text":"<p>Generate comprehensive signals showing multi-layered analysis.</p>"},{"location":"reference/metrics/#conversationflow","title":"ConversationFlow","text":""},{"location":"reference/metrics/#axion.metrics.ConversationFlow","title":"<code>axion.metrics.ConversationFlow(config: Optional[FlowConfig] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Refactored conversation flow metric with modular, testable components.</p> <p>Improvements over v1: - Configurable penalties and thresholds - Enum-based issue types (no string matching) - Separate, testable detector classes - Transparent score decomposition - Better statistical methods - Comprehensive signal generation</p>"},{"location":"reference/metrics/#axion.metrics.ConversationFlow.execute","title":"<code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult</code>  <code>async</code>","text":"<p>Execute comprehensive conversation flow analysis.</p>"},{"location":"reference/metrics/#axion.metrics.ConversationFlow.get_signals","title":"<code>get_signals(result: ConversationFlowResult) -&gt; List[SignalDescriptor]</code>","text":"<p>Generate comprehensive signals showing score calculation.</p>"},{"location":"reference/runners/","title":"Runners API Reference","text":"<p>Evaluation execution and batch processing.</p>"},{"location":"reference/runners/#evaluation_runner","title":"evaluation_runner","text":""},{"location":"reference/runners/#axion.runners.evaluation_runner","title":"<code>axion.runners.evaluation_runner(evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], evaluation_name: str, scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: float = 0.0, summary_generator: Optional[BaseSummary] = None, cache_config: Optional[CacheConfig] = None, error_config: Optional[ErrorConfig] = None, enable_internal_caching: bool = True, thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, trace_granularity: Union[TraceGranularity, str] = TraceGranularity.SEPARATE, flush_per_metric: bool = False) -&gt; EvaluationResult</code>","text":"<p>Synchronously runs an evaluation experiment to evaluate metrics over a given dataset, supporting both flat and hierarchical scoring structures.</p> <p>Parameters:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>, default:                   <code>None</code> )           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>Optional[List[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>An alternative, more intuitive parameter for passing a flat list of metrics.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>, default:                   <code>None</code> )           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>, default:                   <code>None</code> )           \u2013            <p>A custom function to generate model outputs before scoring.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Maps metric input names to dataset column names.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>A human-readable description of the evaluation.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional metadata to include in the evaluation trace.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of concurrent evaluations. Defaults to 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>, default:                   <code>None</code> )           \u2013            <p>A summary generator for high-level results.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for caching results to avoid recomputation.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for handling errors during evaluation.</p> </li> <li> <code>enable_internal_caching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enables a per-item cache for metrics that share expensive internal computations. Defaults to True.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Performance thresholds for each metric.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show a progress bar. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of the dataset.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>An optional identifier for this specific run.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>, default:                   <code>SEPARATE</code> )           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> <li> <code>flush_per_metric</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>When trace_granularity='separate', controls whether each metric trace is flushed immediately (slower, but more \"live\" in the UI) vs batched (faster). Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EvaluationResult</code> (              <code>EvaluationResult</code> )          \u2013            <p>An object containing detailed metric scores, summary, and metadata.</p> </li> </ul>"},{"location":"reference/runners/#evaluationrunner","title":"EvaluationRunner","text":""},{"location":"reference/runners/#axion.runners.EvaluationRunner","title":"<code>axion.runners.EvaluationRunner(config: EvaluationConfig, tracer: Optional[BaseTraceHandler] = None)</code>","text":"<p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the execution of evaluation experiments, managing task execution, metric scoring, and configuration. Automatically determines and initializes the appropriate scoring strategy (flat or hierarchical).</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.summary","title":"<code>summary: Union[Dict[str, Any], None]</code>  <code>property</code>","text":"<p>Returns the summary from the active scoring strategy. For hierarchical ('tree') strategies, this provides the detailed tree summary.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.tree","title":"<code>tree: Any</code>  <code>property</code>","text":"<p>Returns the underlying EvalTree instance for inspection, if the 'tree' strategy is active. Raises an AttributeError for other strategies.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.execute","title":"<code>execute() -&gt; EvaluationResult</code>  <code>async</code>","text":"<p>Executes the entire evaluation and returns the final result.</p> <p>For SINGLE_TRACE mode, wraps execution in a trace span. For PER_ITEM and SEPARATE modes, skips the wrapper span to allow each item/metric to create its own independent trace.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.display","title":"<code>display()</code>  <code>classmethod</code>","text":"<p>Display Usage Documentation</p>"},{"location":"reference/runners/#evaluationconfig","title":"EvaluationConfig","text":""},{"location":"reference/runners/#axion.runners.EvaluationConfig","title":"<code>axion.runners.EvaluationConfig(evaluation_name: str, evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: Optional[float] = 0.0, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_config: CacheConfig = CacheConfig(), error_config: ErrorConfig = ErrorConfig(), thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, enable_internal_caching: bool = True, trace_granularity: Union[TraceGranularity, str] = TraceGranularity.SEPARATE, flush_per_metric: bool = False)</code>  <code>dataclass</code>","text":"<p>Configuration for an evaluation run.</p> <p>Attributes:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate. Can be a high-level <code>Dataset</code> object, a list of individual <code>DatasetItem</code> objects, or a preloaded <code>pandas.DataFrame</code>.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>)           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>List[Any]</code>)           \u2013            <p>A list of metric objects or callables used to score each item in the dataset.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>)           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation. Used in trace logging and result storage.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>)           \u2013            <p>A custom function to generate predictions or transform inputs. If provided, it will be run before scoring to produce the model output for each dataset item.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>)           \u2013            <p>An optional dictionary mapping metric input names to dataset column names. Useful for adapting metrics to different schema formats.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>)           \u2013            <p>A human-readable description of the evaluation for documentation and trace metadata.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Additional metadata to include in the evaluation trace (e.g., model version, data slice info, tags).</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>)           \u2013            <p>Maximum number of metric evaluations to run concurrently. Default is 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>)           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>)           \u2013            <p>Optional summary generator used to produce a high-level summary after the evaluation. If not provided, a default <code>MetricSummary</code> is used.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>)           \u2013            <p>Configuration for caching metric results to avoid recomputation. Enables both read and write caching.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>)           \u2013            <p>Configuration for how errors are handled during evaluation. Allows skipping metrics or suppressing failures.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>)           \u2013            <p>Optional threshold values for each metric. Used to flag items or datasets that fall below a given performance level.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>)           \u2013            <p>Whether to show a progress bar during evaluation. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name of the dataset being evaluated. Used for display and trace logging.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>)           \u2013            <p>An optional identifier for this specific run. Useful for repeatability and audit logging.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>)           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> </ul>"},{"location":"reference/runners/#metricrunner","title":"MetricRunner","text":""},{"location":"reference/runners/#axion.runners.MetricRunner","title":"<code>axion.runners.MetricRunner(metrics: List[Any], name: str = 'MetricRunner', description: str = 'Orchestrates evaluation metrics', max_concurrent: int = 5, thresholds: Optional[Dict[str, float]] = None, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_manager: Optional[CacheManager] = None, error_config: ErrorConfig = ErrorConfig(), tracer: Optional[BaseTraceHandler] = None, dataset_name: Optional[str] = 'Metric Runner Dataset', enable_internal_caching: bool = True, trace_granularity: TraceGranularity = TraceGranularity.SEPARATE, flush_per_metric: bool = False)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the evaluation of multiple metrics against a dataset.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.available_types","title":"<code>available_types: List[str]</code>  <code>property</code>","text":"<p>Returns a list of available (registered) metric runner types.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.elapsed_time","title":"<code>elapsed_time: Union[float, None]</code>  <code>property</code>","text":"<p>Returns the total execution time for the last batch run.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.summary","title":"<code>summary: Union[Dict[str, Any], None]</code>  <code>property</code>","text":"<p>Returns the summary of the last batch run.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.execute_batch","title":"<code>execute_batch(evaluation_inputs: Union[Dataset, List[DatasetItem], pd.DataFrame], *, show_progress: bool = True) -&gt; List[TestResult]</code>  <code>async</code>","text":"<p>Executes all configured metrics against the provided dataset.</p> <p>Trace granularity behavior: - SINGLE_TRACE: All metrics run under one parent trace (default) - SEPARATE: Each metric execution gets its own independent trace</p>"},{"location":"reference/runners/#cachemanager","title":"CacheManager","text":""},{"location":"reference/runners/#axion._core.cache.CacheManager","title":"<code>axion._core.cache.CacheManager(config: CacheConfig = None)</code>","text":"<p>Manages cache operations for both memory and disk, abstracting the backend.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.get","title":"<code>get(key: str) -&gt; Optional[Any]</code>","text":"<p>Gets an item from the cache if use_cache is True.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.set","title":"<code>set(key: str, value: Any)</code>","text":"<p>Sets an item in the cache if write_cache is True.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.close","title":"<code>close()</code>","text":"<p>Closes the cache connection if applicable (for diskcache).</p>"},{"location":"reference/runners/#cacheconfig","title":"CacheConfig","text":""},{"location":"reference/runners/#axion._core.cache.CacheConfig","title":"<code>axion._core.cache.CacheConfig(use_cache: bool = True, write_cache: bool = True, cache_type: str = 'memory', cache_dir: Optional[str] = '.cache', cache_task: bool = True)</code>  <code>dataclass</code>","text":"<p>Configuration class for controlling caching behavior of metric evaluations.</p> <p>Attributes:</p> <ul> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>If True, attempts to read previously computed results from cache to avoid redundant computation.</p> </li> <li> <code>write_cache</code>               (<code>bool</code>)           \u2013            <p>If True, writes newly computed metric results to cache for future use. Has no effect if <code>use_cache</code> is False.</p> </li> <li> <code>cache_type</code>               (<code>str</code>)           \u2013            <p>Type of caching backend to use. - 'memory': Uses in-memory dictionary for caching (fast, but non-persistent). - 'disk': Writes cache to disk (persistent across runs).</p> </li> <li> <code>cache_dir</code>               (<code>Optional[str]</code>)           \u2013            <p>Directory path where disk cache files will be stored. Only used when <code>cache_type='disk'</code>. Defaults to '.cache'.</p> </li> <li> <code>cache_task</code>               (<code>bool</code>)           \u2013            <p>If True, enables caching at the task level (e.g., for full evaluation runs). If False, caching applies only at the metric level.</p> </li> </ul>"},{"location":"reference/schema/","title":"Schema API Reference","text":"<p>Result types and evaluation schemas.</p>"},{"location":"reference/schema/#metricscore","title":"MetricScore","text":""},{"location":"reference/schema/#axion.schema.MetricScore","title":"<code>axion.schema.MetricScore(**data)</code>","text":"<p>               Bases: <code>RichBaseModel</code></p> <p>Standardized data model for a single metric evaluation result. Captures the computed score, the logic behind it, thresholds used, and any metadata useful for debugging or reporting.</p>"},{"location":"reference/schema/#evaluationresult","title":"EvaluationResult","text":""},{"location":"reference/schema/#axion.schema.EvaluationResult","title":"<code>axion.schema.EvaluationResult(run_id: str, evaluation_name: Optional[str], timestamp: str, results: List[TestResult], summary: Dict[str, Any] = dict(), metadata: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Captures the full result of an evaluation run across multiple test cases and metrics.</p> <p>Attributes:</p> <ul> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for this specific evaluation run. Typically generated per execution.</p> </li> <li> <code>evaluation_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name for the experiment or test campaign (e.g., \"Lead Scoring v2 A/B\").</p> </li> <li> <code>timestamp</code>               (<code>str</code>)           \u2013            <p>ISO-formatted timestamp indicating when the evaluation was run. Can be used for sorting or audit logging.</p> </li> <li> <code>results</code>               (<code>List[TestResult]</code>)           \u2013            <p>A list of TestResult objects, each representing the evaluation output for a single test case across one or more metrics.</p> </li> <li> <code>summary</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Summary of the TestResult objects, representing the evaluation output across each metric.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Arbitrary metadata such as configuration info, evaluator identity, model version, dataset name, or custom flags for internal use.</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_dataframe","title":"<code>to_dataframe(by_alias: bool = True, id_as_index: bool = False, include_test_case: bool = True, include_run_metadata: bool = True, column_order: Optional[List[str]] = None, rename_columns: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Flattens the entire evaluation result into a single pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>by_alias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use field aliases in the output.</p> </li> <li> <code>id_as_index</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, sets the test_case <code>id</code> as the DataFrame index.</p> </li> <li> <code>include_test_case</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include test_case fields in the output.</p> </li> <li> <code>include_run_metadata</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include run-level metadata.</p> </li> <li> <code>column_order</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Output column ordering.</p> </li> <li> <code>rename_columns</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Rename columns to match model_arena format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Flattened view of all metrics with test case and run context.</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_latency_plot","title":"<code>to_latency_plot(col_name: str = 'latency', id_col: str = 'id', bins: int = 30, show_legend: bool = True, show_stats_panel: bool = True, figsize: Tuple[int, int] = (16, 9), return_plot: bool = False, show_plot: bool = True, output_path: Optional[str] = None, plot_title: str = 'Latency Distribution', color_palette: Optional[Dict[str, str]] = None) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, Optional[Any], Optional[Any]]]</code>","text":"<p>Analyzes and visualizes latency distribution.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>, default:                   <code>'latency'</code> )           \u2013            <p>Name of the column containing latency values.</p> </li> <li> <code>id_col</code>               (<code>str</code>, default:                   <code>'id'</code> )           \u2013            <p>Unique identifier for test cases (used to deduplicate latency).</p> </li> <li> <code>bins</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>Number of histogram bins.</p> </li> <li> <code>show_legend</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show plot legend,</p> </li> <li> <code>show_stats_panel</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show stats panel,</p> </li> <li> <code>figsize</code>               (<code>Tuple[int, int]</code>, default:                   <code>(16, 9)</code> )           \u2013            <p>Size of the matplotlib figure.</p> </li> <li> <code>return_plot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns (stats_df, fig, ax). If False, returns stats_df.</p> </li> <li> <code>show_plot</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to render the plot using plt.show() (or display in NB).</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If provided, saves the plot to this file path.</p> </li> <li> <code>plot_title</code>               (<code>str</code>, default:                   <code>'Latency Distribution'</code> )           \u2013            <p>Descriptive name for the latency plot title (default: \"Latency Distribution\")</p> </li> <li> <code>color_palette</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom colors for the LatencyAnalyzer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[DataFrame, Tuple[DataFrame, Optional[Any], Optional[Any]]]</code>           \u2013            <p>pd.DataFrame or Tuple[pd.DataFrame, Figure, Axes]</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_scorecard","title":"<code>to_scorecard(llm: Optional[LLMRunnable] = None, metric_definitions: dict = None, explanation_callback: callable = None, instruction: Optional[str] = None, max_concurrent: int = 10, output_path: Optional[str] = None, display_in_notebook: bool = False, return_html: bool = False, return_styled_df: bool = True, id_col: str = 'metric_name', parent_col: str = 'parent', value_cols: List[str] = None, group_meta_cols: List[str] = None) -&gt; Union[str, pd.DataFrame, None]</code>","text":"<p>Generates a hierarchical scorecard report using the evaluation results.</p> <p>This method creates a visual performance breakdown. It can display the report interactively in a notebook, save it as an HTML file, or return the styled object/HTML string for custom use.</p> <p>Parameters:</p> <ul> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Custom LLM instance to use for generating qualitative explanations.</p> </li> <li> <code>metric_definitions</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary mapping metric names to static descriptions or templates.</p> </li> <li> <code>explanation_callback</code>               (<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>Custom function <code>f(name, score, type)</code> to generate explanations manually.</p> </li> <li> <code>instruction</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>System prompt override for the explanation generation LLM.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum number of parallel LLM calls for batch processing explanations.</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>File path to save the generated HTML report.</p> </li> <li> <code>display_in_notebook</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, renders the styled dataframe (or HTML) directly in Jupyter/IPython.</p> </li> <li> <code>return_html</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the raw HTML string of the report.</p> </li> <li> <code>return_styled_df</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the pandas Styler object for further customization.</p> </li> <li> <code>id_col</code>               (<code>str</code>, default:                   <code>'metric_name'</code> )           \u2013            <p>Column name representing the unique node identifier (default: 'metric_name').</p> </li> <li> <code>parent_col</code>               (<code>str</code>, default:                   <code>'parent'</code> )           \u2013            <p>Column name representing the parent node identifier (default: 'parent').</p> </li> <li> <code>value_cols</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of columns to aggregate values for (e.g., ['metric_score', 'weight']).</p> </li> <li> <code>group_meta_cols</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of metadata columns to include in grouping (e.g., ['metric_type']).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, DataFrame, None]</code>           \u2013            <p>Union[str, Styler, None]: - The HTML string if <code>return_html=True</code>. - The pandas Styler object if <code>return_styled_df=True</code>. - None otherwise (default behavior is just to display or save).</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability","title":"<code>publish_to_observability(loader: Optional[Any] = None, observation_id_field: Optional[str] = 'observation_id', flush: bool = True, tags: Optional[List[str]] = None) -&gt; Dict[str, int]</code>","text":"<p>Publish evaluation scores to an observability platform.</p> <p>Uses a trace loader to publish scores. By default, uses LangfuseTraceLoader.</p> <p>Parameters:</p> <ul> <li> <code>loader</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>A trace loader instance (e.g., LangfuseTraceLoader, OpikTraceLoader). If None, creates a new LangfuseTraceLoader using environment variables.</p> </li> <li> <code>observation_id_field</code>               (<code>Optional[str]</code>, default:                   <code>'observation_id'</code> )           \u2013            <p>Field name on DatasetItem containing the observation/span ID. If provided, scores attach to that specific observation within the trace. If None, scores attach to the trace itself. Default: 'observation_id'.</p> </li> <li> <code>flush</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to flush the client after uploading. Defaults to True.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to attach to all scores as metadata. Falls back to LANGFUSE_TAGS env var if not provided.</p> </li> </ul> Note <p>Environment cannot be set when pushing scores to existing traces. To set environment, configure it at client initialization when creating traces (via LANGFUSE_ENVIRONMENT or LANGFUSE_TRACING_ENVIRONMENT env vars or the environment parameter in LangfuseTracer).</p> <p>Returns:</p> <ul> <li> <code>Dict[str, int]</code>           \u2013            <p>Dict with counts: {'uploaded': N, 'skipped': M} - uploaded: Number of scores successfully pushed - skipped: Number of scores skipped (missing trace_id or invalid score)</p> </li> </ul> Example <p>from axion._core.tracing.loaders import LangfuseTraceLoader</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--using-default-langfuse-loader","title":"Using default Langfuse loader","text":"<p>stats = result.publish_to_observability()</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--using-explicit-loader-with-tags","title":"Using explicit loader with tags","text":"<p>loader = LangfuseTraceLoader() stats = result.publish_to_observability( ...     loader=loader, ...     tags=['prod', 'v1.0'] ... )</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--attach-scores-to-traces-only-no-observation","title":"Attach scores to traces only (no observation)","text":"<p>stats = result.publish_to_observability(observation_id_field=None)</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment","title":"<code>publish_as_experiment(loader: Optional[Any] = None, dataset_name: Optional[str] = None, run_name: Optional[str] = None, run_metadata: Optional[Dict[str, Any]] = None, flush: bool = True, tags: Optional[List[str]] = None, score_on_runtime_traces: bool = False, link_to_traces: bool = False) -&gt; Dict[str, Any]</code>","text":"<p>Publish evaluation results to Langfuse as a dataset experiment.</p> <p>This method creates a complete experiment in Langfuse with a dataset, dataset items, experiment runs, and scores. Unlike <code>publish_to_observability()</code>, it does not require existing traces - it creates everything from scratch.</p> <p>Parameters:</p> <ul> <li> <code>loader</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>A LangfuseTraceLoader instance. If None, creates a new one using environment variables.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name for the Langfuse dataset. Defaults to evaluation_name or generates one based on run_id.</p> </li> <li> <code>run_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name for the experiment run. Defaults to \"{dataset_name}-{run_id}\" pattern.</p> </li> <li> <code>run_metadata</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata to attach to the experiment run.</p> </li> <li> <code>flush</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to flush the client after uploading. Defaults to True.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to attach to all scores as metadata.</p> </li> <li> <code>score_on_runtime_traces</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, skip creating per-item \"Dataset run\" traces and instead attach scores to existing runtime traces via <code>trace_id</code>/<code>observation_id</code>. Takes precedence over link_to_traces if both are True.</p> </li> <li> <code>link_to_traces</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, link experiment runs to existing traces via the low-level API instead of creating new \"Dataset run\" traces. This allows experiment runs to appear linked to the original evaluation traces in Langfuse UI. Falls back to creating new traces if trace_id is not available. Ignored if score_on_runtime_traces is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict with statistics: - dataset_name: Name of the created/used dataset - run_name: Name of the experiment run - items_created: Number of dataset items created - runs_created: Number of experiment runs created - scores_uploaded: Number of scores attached - scores_skipped: Number of scores skipped (None/NaN values) - errors: List of error messages encountered</p> </li> </ul> Example <p>from axion import evaluation_runner from axion.metrics import Faithfulness, AnswerRelevancy</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment--run-evaluation","title":"Run evaluation","text":"<p>results = evaluation_runner( ...     evaluation_inputs=dataset, ...     scoring_config=config, ...     evaluation_name=\"RAG Evaluation\", ... )</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment--upload-to-langfuse-as-experiment","title":"Upload to Langfuse as experiment","text":"<p>stats = results.publish_as_experiment( ...     dataset_name=\"my-rag-dataset\", ...     run_name=\"experiment-v1\", ...     tags=[\"production\"] ... )</p> <p>print(f\"Uploaded {stats['scores_uploaded']} scores to {stats['dataset_name']}\")</p>"},{"location":"reference/schema/#testresult","title":"TestResult","text":""},{"location":"reference/schema/#axion.schema.TestResult","title":"<code>axion.schema.TestResult(test_case: Optional[DatasetItem], score_results: List[MetricScore], metadata: Optional[Dict[str, Any]] = dict())</code>  <code>dataclass</code>","text":"<p>Represents the result of evaluating a single test case using one or more evaluation metrics.</p> <p>Attributes:</p> <ul> <li> <code>test_case</code>               (<code>DatasetItem</code>)           \u2013            <p>The input test case containing query, expected output, and other context. This forms the basis for which all metrics are applied.</p> </li> <li> <code>score_results</code>               (<code>List[MetricScore]</code>)           \u2013            <p>A list of evaluation results returned from applying different metrics to this test case. Each MetricScore includes a score, explanation, and threshold comparison.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Optional metadata for storing extra context such as timestamps, evaluator info, experiment variant, evaluation notes, or model config parameters.</p> </li> </ul>"},{"location":"reference/schema/#errorconfig","title":"ErrorConfig","text":""},{"location":"reference/schema/#axion.schema.ErrorConfig","title":"<code>axion.schema.ErrorConfig(ignore_errors: bool = True, skip_on_missing_params: bool = False)</code>  <code>dataclass</code>","text":"<p>Configuration class for controlling error handling during metric execution.</p> <p>Attributes:</p> <ul> <li> <code>ignore_errors</code>               (<code>bool</code>)           \u2013            <p>If True, any exceptions raised during metric execution will be caught and suppressed. The metric will return a placeholder result (e.g., None or NaN) instead of failing the entire evaluation. Use this to allow evaluations to proceed even if some metrics occasionally fail.</p> </li> <li> <code>skip_on_missing_params</code>               (<code>bool</code>)           \u2013            <p>If True, metrics will be skipped entirely when required input fields are missing from the data. This is useful when running multiple metrics over heterogeneous data where not all fields are always present. If False, the metric will raise an error if required inputs are missing.</p> </li> </ul>"},{"location":"reference/search/","title":"Search API Reference","text":"<p>Search and retrieval integrations.</p>"},{"location":"reference/search/#googleretriever","title":"GoogleRetriever","text":""},{"location":"reference/search/#axion.search.GoogleRetriever","title":"<code>axion.search.GoogleRetriever(api_key: Optional[str] = None, num_web_results: int = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever that uses SerpAPI to perform Google searches and format the results into nodes.</p> <p>Initialize the GoogleRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>SerpAPI key used for authenticating requests. Defaults to the value of the 'SERPAPI_KEY' environment variable if not provided.</p> </li> <li> <code>num_web_results</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of top search results to return (maximum 20).</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to fetch and clean full page content from URLs in the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to crawl per page (if crawling is enabled).</p> </li> </ul>"},{"location":"reference/search/#axion.search.GoogleRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Perform a search query and return results.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"reference/search/#tavilyretriever","title":"TavilyRetriever","text":""},{"location":"reference/search/#axion.search.TavilyRetriever","title":"<code>axion.search.TavilyRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'extract', 'crawl'] = 'search', search_depth: Literal['basic', 'advanced'] = 'basic', topic: Optional[str] = 'general', max_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, days: Optional[int] = None, include_answer: bool = False, include_raw_content: bool = False, include_images: bool = False, include_image_descriptions: bool = False, include_domains: Optional[List[str]] = None, exclude_domains: Optional[List[str]] = None, extract_depth: Literal['basic', 'advanced'] = 'basic', max_depth: Optional[int] = 1, max_breadth: Optional[int] = 20, limit: Optional[int] = 50, instructions: Optional[str] = None, select_paths: Optional[List[str]] = None, select_domains: Optional[List[str]] = None, exclude_paths: Optional[List[str]] = None, allow_external: bool = False, categories: Optional[List[str]] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for Tavily's Search, Extract, and Crawl APIs.</p> <p>Initialize the TavilyRetriever with specified parameters.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Retrieve results using the Tavily Search API. Only handles 'search' endpoint. For 'extract' and 'crawl', call their respective methods directly.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.extract_url_text","title":"<code>extract_url_text(url: str) -&gt; str</code>","text":"<p>Extract content from a single URL using the Extract API.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.extract","title":"<code>extract(url: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Extract content from a URL using the Extract API.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.crawl","title":"<code>crawl(url: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Crawl content from a URL using the Crawl API.</p>"},{"location":"reference/search/#youretriever","title":"YouRetriever","text":""},{"location":"reference/search/#axion.search.YouRetriever","title":"<code>axion.search.YouRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'news'] = 'search', num_web_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, safesearch: Optional[Literal['off', 'moderate', 'strict']] = None, country: Optional[str] = None, search_lang: Optional[str] = None, ui_lang: Optional[str] = None, spellcheck: Optional[bool] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for You.com's Search and News API.</p> <p>Initialize the YouRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>You.com API key. If not provided, it will attempt to use the <code>YDC_API_KEY</code> environment variable.</p> </li> <li> <code>callback_manager</code>               (<code>Optional[CallbackManager]</code>)           \u2013            <p>Optional manager for handling callback events during retrieval.</p> </li> <li> <code>endpoint</code>               (<code>Literal['search', 'news']</code>, default:                   <code>'search'</code> )           \u2013            <p>The You.com API endpoint to query \u2014 either \"search\" for web results or \"news\" for news-specific content. Defaults to \"search\".</p> </li> <li> <code>num_web_results</code>               (<code>Optional[int]</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of search results to return. Must not exceed 20.</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to crawl and extract the content of the linked pages from the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to retrieve per page when crawling. If None, a default internal value is used.</p> </li> <li> <code>safesearch</code>               (<code>Optional[Literal['off', 'moderate', 'strict']]</code>, default:                   <code>None</code> )           \u2013            <p>Safe search filtering level. Defaults to \"moderate\" if not specified.</p> </li> <li> <code>country</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Country code for geo-specific search behavior (e.g., \"US\" for United States).</p> </li> <li> <code>search_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code to use for the search query (e.g., \"en\" for English).</p> </li> <li> <code>ui_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code for the UI/localized response (e.g., \"en\").</p> </li> <li> <code>spellcheck</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Whether to enable spell check for the query. Defaults to True if unspecified.</p> </li> </ul>"},{"location":"reference/search/#axion.search.YouRetriever.retrieve","title":"<code>retrieve(query: str) -&gt; SearchResults</code>  <code>async</code>","text":"<p>Perform a search query and return results using You.com API.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"reference/synthetic/","title":"Synthetic API Reference","text":"<p>Synthetic data generation for evaluation datasets.</p>"},{"location":"reference/synthetic/#documentqagenerator","title":"DocumentQAGenerator","text":""},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator","title":"<code>axion.synthetic.DocumentQAGenerator(llm: LLMRunnable, params: GenerationParams, embed_model: EmbeddingRunnable = None, max_concurrent: int = 5, show_progress: bool = True, tracer: Optional[BaseTraceHandler] = None, **kwargs)</code>","text":"<p>Orchestrates QA pair generation from multiple documents concurrently.</p> <p>Initialize DocumentQAGenerator</p> <p>Parameters:</p> <ul> <li> <code>llm</code>               (<code>LLMRunnable</code>)           \u2013            <p>The language model to use for generation.</p> </li> <li> <code>params</code>               (<code>GenerationParams</code>)           \u2013            <p>A GenerationParams object with all configuration.</p> </li> <li> <code>embed_model</code>               (<code>EmbeddingRunnable</code>, default:                   <code>None</code> )           \u2013            <p>An embedding model used for semantic parsing.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Max concurrent retrievers</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show progress bars using tqdm</p> </li> </ul>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.generate_from_directory","title":"<code>generate_from_directory(directory_path: str) -&gt; List[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Main entry point. Loads docs from a directory and generates QA pairs. Args:     directory_path: The path to the directory containing documents. Returns:     A list of all generated QA pairs.</p>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.to_items","title":"<code>to_items(results: List[Any]) -&gt; List</code>","text":"<p>Converts a list of QA evaluation results into a List of <code>DatasetItems</code>.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List</code> (              <code>List</code> )          \u2013            <p>A list of DatasetItem objects.</p> </li> </ul>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.to_dataset","title":"<code>to_dataset(results: List[Any], dataset_name: str)</code>","text":"<p>Converts a list of QA evaluation results into a structured <code>Dataset</code> object.</p> <p>The function extracts these pairs, renames the fields to match internal <code>FieldNames</code> standards, and wraps each into a <code>DatasetItem</code>. These are then collected into a <code>Dataset</code> for downstream evaluation or analysis.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name to assign to the resulting Dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>          \u2013            <p>A structured Dataset containing DatasetItems with standardized field names.</p> </li> </ul>"},{"location":"reference/synthetic/#generationparams","title":"GenerationParams","text":""},{"location":"reference/synthetic/#axion.synthetic.GenerationParams","title":"<code>axion.synthetic.GenerationParams(**data)</code>","text":"<p>               Bases: <code>RichBaseModel</code></p> <p>Configuration parameters for controlling the QA (Question\u2013Answer) generation pipeline.</p> <p>These settings define how QA pairs are generated from source documents, including the number of pairs, question style and complexity, chunking strategies, and validation thresholds. The configuration supports both factual and synthetic QA creation, enabling flexible generation for training, evaluation, and benchmarking.</p> <p>Attributes:</p> <ul> <li> <code>num_pairs</code>               (<code>int</code>)           \u2013            <p>Total number of QA pairs to generate per document.</p> </li> <li> <code>question_types</code>               (<code>List[str]</code>)           \u2013            <p>List of question types to generate. Common options include: - 'factual'      : Direct, fact-based questions - 'conceptual'   : Understanding-based questions - 'application'  : Scenario-based application questions - 'analysis'     : Critical thinking and analysis questions - 'synthetic'    : Artificially created questions for stress-testing</p> </li> <li> <code>difficulty</code>               (<code>str</code>)           \u2013            <p>Target difficulty of generated questions. Options include: 'easy', 'medium', and 'hard'.</p> </li> <li> <code>splitter_type</code>               (<code>Literal['semantic', 'sentence']</code>)           \u2013            <p>Chunking strategy for breaking documents into sections: - 'semantic': Embedding-aware splits for context preservation. - 'sentence': Rule-based splits by sentence length.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>Maximum size (in characters or tokens) of each chunk when using <code>splitter_type='sentence'</code>.</p> </li> <li> <code>statements_per_chunk</code>               (<code>int</code>)           \u2013            <p>Number of candidate statements generated per chunk before filtering and validation.</p> </li> <li> <code>answer_length</code>               (<code>str</code>)           \u2013            <p>Desired length for generated answers. Options: 'short', 'medium', or 'long'.</p> </li> <li> <code>dimensions</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>A dictionary guiding synthetic data generation. Possible keys: - 'features' : Data attributes to reflect real-world structure. - 'persona'  : Profiles simulating different perspectives. - 'scenarios': Contextual situations to ensure realism.</p> </li> <li> <code>custom_guidelines</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional free-text instructions to condition the QA generation process beyond default behavior.</p> </li> <li> <code>example_question</code>               (<code>Optional[str]</code>)           \u2013            <p>An example question to guide style, tone, and complexity.</p> </li> <li> <code>example_answer</code>               (<code>Optional[str]</code>)           \u2013            <p>An example answer to align generated responses with the desired style and depth.</p> </li> <li> <code>max_reflection_iterations</code>               (<code>int</code>)           \u2013            <p>Maximum self-reflection and retry loops for improving QA quality during validation.</p> </li> <li> <code>validation_threshold</code>               (<code>float</code>)           \u2013            <p>Minimum confidence or faithfulness score (0.0\u20131.0) required to accept a QA pair.</p> </li> <li> <code>breakpoint_percentile_threshold</code>               (<code>int</code>)           \u2013            <p>Percentile threshold for determining sentence breakpoints in semantic chunking. Higher values create fewer, larger chunks.</p> </li> </ul>"},{"location":"reference/tracer-registry/","title":"Tracer Registry API Reference","text":"<p>Registry pattern for tracing providers with support for NoOp, Logfire, Langfuse, and Opik backends.</p>"},{"location":"reference/tracer-registry/#tracerregistry","title":"TracerRegistry","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry","title":"<code>axion._core.tracing.registry.TracerRegistry</code>","text":"<p>Registry for tracer implementations using decorator pattern.</p> <p>This class provides a simple way to register and retrieve tracer implementations by name, following the same pattern as LLMRegistry.</p> Example <p>@TracerRegistry.register('my_tracer') class MyTracer(BaseTracer):     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry--later-retrieve-the-tracer-class","title":"Later, retrieve the tracer class","text":"<p>TracerClass = TracerRegistry.get('my_tracer') tracer = TracerClass.create(metadata_type='llm')</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.register","title":"<code>register(name: str) -&gt; Callable[[Type[BaseTracer]], Type[BaseTracer]]</code>  <code>classmethod</code>","text":"<p>Decorator to register a tracer implementation.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name to register the tracer under (e.g., 'noop', 'logfire', 'langfuse')</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable[[Type[BaseTracer]], Type[BaseTracer]]</code>           \u2013            <p>Decorator function</p> </li> </ul> Example <p>@TracerRegistry.register('custom') class CustomTracer(BaseTracer):     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.get","title":"<code>get(name: str) -&gt; Type[BaseTracer]</code>  <code>classmethod</code>","text":"<p>Get a tracer class by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The registered name of the tracer</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Type[BaseTracer]</code>           \u2013            <p>The tracer class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the tracer is not registered</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.list_providers","title":"<code>list_providers() -&gt; List[str]</code>  <code>classmethod</code>","text":"<p>List all registered tracer provider names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of registered tracer names</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.is_registered","title":"<code>is_registered(name: str) -&gt; bool</code>  <code>classmethod</code>","text":"<p>Check if a tracer is registered.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if registered, False otherwise</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.display","title":"<code>display() -&gt; None</code>  <code>classmethod</code>","text":"<p>Display the tracer registry in a rich HTML format.</p>"},{"location":"reference/tracer-registry/#basetracer","title":"BaseTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer","title":"<code>axion._core.tracing.registry.BaseTracer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all tracer implementations.</p> <p>This defines the core interface that all tracers (NoOp, Logfire, Langfuse, etc.) must implement. Users can create custom tracers by subclassing this and registering with @TracerRegistry.register('name').</p> Example <p>@TracerRegistry.register('my_custom_tracer') class MyCustomTracer(BaseTracer):     # implement abstract methods     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.create","title":"<code>create(metadata_type: str = 'default', tool_metadata: Optional[Any] = None, **kwargs) -&gt; BaseTracer</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Factory method to create tracer instances.</p> <p>Parameters:</p> <ul> <li> <code>metadata_type</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>Type of metadata (e.g., 'llm', 'knowledge', 'evaluation')</p> </li> <li> <code>tool_metadata</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata about the tool being traced</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional implementation-specific arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseTracer</code>           \u2013            <p>A new tracer instance</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.span","title":"<code>span(operation_name: str, **attributes)</code>  <code>abstractmethod</code>","text":"<p>Create a synchronous span context manager.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A span object that can be used to add attributes or events</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.async_span","title":"<code>async_span(operation_name: str, **attributes)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Create an asynchronous span context manager.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A span object that can be used to add attributes or events</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.start","title":"<code>start(**attributes) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Start execution tracking.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.complete","title":"<code>complete(output_data: Optional[Dict[str, Any]] = None, **attributes) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Complete execution tracking.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.fail","title":"<code>fail(error: str, **attributes) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Handle execution failure.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.add_trace","title":"<code>add_trace(event_type: str, message: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Add a trace event.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.flush","title":"<code>flush() -&gt; None</code>","text":"<p>Flush any pending traces to the backend.</p> <p>Override in implementations that buffer traces (e.g., Langfuse). Default implementation is a no-op.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.shutdown","title":"<code>shutdown() -&gt; None</code>","text":"<p>Gracefully shutdown the tracer.</p> <p>Override in implementations that need cleanup. Default implementation calls flush().</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.info","title":"<code>info(msg: Any) -&gt; None</code>","text":"<p>Log info message.</p>"},{"location":"reference/tracer-registry/#built-in-tracers","title":"Built-in Tracers","text":""},{"location":"reference/tracer-registry/#nooptracer","title":"NoOpTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer","title":"<code>axion._core.tracing.noop.tracer.NoOpTracer(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = False, trace_id: Optional[str] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>A no-operation tracer that mirrors the LogfireTracer interface.</p> <p>This class provides a complete, non-functional implementation of the tracer so that it can be swapped with the real LogfireTracer without causing any errors. All tracing and logging methods are designed to do nothing, ensuring minimal performance overhead when tracing is disabled.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.create","title":"<code>create(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = False, **kwargs) -&gt; NoOpTracer</code>  <code>classmethod</code>","text":"<p>Factory method to create a NoOpTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.span","title":"<code>span(operation_name: str, **attributes)</code>","text":"<p>Creates a synchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.async_span","title":"<code>async_span(operation_name: str, **attributes)</code>  <code>async</code>","text":"<p>Creates an asynchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#logfiretracer","title":"LogfireTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer","title":"<code>axion._core.tracing.logfire.tracer.LogfireTracer(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = True, trace_id: Optional[str] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Tracer that combines tracing, logging, and metadata collection. Uses composition with a logger instance instead of inheritance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.create","title":"<code>create(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; LogfireTracer</code>  <code>classmethod</code>","text":"<p>Factory method to create a LogfireTracer.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.span","title":"<code>span(operation_name: str, **attributes)</code>","text":"<p>Creates a synchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.async_span","title":"<code>async_span(operation_name: str, **attributes)</code>  <code>async</code>","text":"<p>Creates an asynchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_llm_call","title":"<code>log_llm_call(model: str, prompt: str, response: str, prompt_tokens: int = None, completion_tokens: int = None, latency: float = None, cost_estimate: float = None, error: str = None, **attributes) -&gt; None</code>","text":"<p>Log an LLM call with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_retrieval_call","title":"<code>log_retrieval_call(context: List[Dict[str, Any]], latency: float, query: str = None, **attributes) -&gt; None</code>","text":"<p>Log a retrieval call with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_evaluation","title":"<code>log_evaluation(evaluation_id: str, evaluator_name: str, evaluator_type: str, dataset_size: int, latency: float, overall_metrics: List[EvaluationMetric] = None, datapoint_results: List[EvaluationDatapoint] = None, dataset_name: str = None, cost_estimate: float = None, tokens_used: int = None, error: str = None, evaluator_config: Dict[str, Any] = None, **attributes) -&gt; None</code>","text":"<p>Log an evaluation run with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_database_query","title":"<code>log_database_query(query: str, latency: float, rows_affected: int = 0, **attributes) -&gt; None</code>","text":"<p>Log a database query with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.get_statistics","title":"<code>get_statistics() -&gt; Dict[str, Any]</code>","text":"<p>Get statistics from metadata.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.display_statistics","title":"<code>display_statistics()</code>","text":"<p>Display statistics using the display utility.</p>"},{"location":"reference/tracer-registry/#langfusetracer","title":"LangfuseTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer","title":"<code>axion._core.tracing.langfuse.tracer.LangfuseTracer(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, public_key: Optional[str] = None, secret_key: Optional[str] = None, base_url: Optional[str] = None, trace_id: Optional[str] = None, tags: Optional[List[str]] = None, environment: Optional[str] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Langfuse-based tracer for LLM observability.</p> <p>This tracer integrates with Langfuse (https://langfuse.com) to provide detailed tracing and observability for LLM applications.</p> Configuration <p>Set the following environment variables or pass to constructor: - LANGFUSE_PUBLIC_KEY: Your Langfuse public key - LANGFUSE_SECRET_KEY: Your Langfuse secret key - LANGFUSE_BASE_URL: API endpoint (default: https://cloud.langfuse.com) - LANGFUSE_TAGS: Comma-separated list of tags (e.g., \"prod,v1.0\") - LANGFUSE_ENVIRONMENT: Environment name (e.g., \"production\", \"staging\")</p> Example <p>from axion._core.tracing import Tracer, configure_tracing</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer--configure-via-environment","title":"Configure via environment","text":"<p>os.environ['TRACING_MODE'] = 'langfuse' os.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...' os.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...' os.environ['LANGFUSE_TAGS'] = 'prod,v1.0' os.environ['LANGFUSE_ENVIRONMENT'] = 'production'</p> <p>configure_tracing() tracer = Tracer('llm') with tracer.span('my-operation', tags=['custom-tag']):     # ... your code tracer.flush()</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer--or-pass-tagsenvironment-directly","title":"Or pass tags/environment directly:","text":"<p>tracer = LangfuseTracer(tags=['prod', 'v1.0'], environment='production') with tracer.span('my-operation'):     # ... your code</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.create","title":"<code>create(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; LangfuseTracer</code>  <code>classmethod</code>","text":"<p>Factory method to create a LangfuseTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.span","title":"<code>span(operation_name: str, **attributes)</code>","text":"<p>Creates a synchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A LangfuseSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.async_span","title":"<code>async_span(operation_name: str, **attributes)</code>  <code>async</code>","text":"<p>Creates an asynchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A LangfuseSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.flush","title":"<code>flush() -&gt; None</code>","text":"<p>Flush pending traces to Langfuse.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.log_llm_call","title":"<code>log_llm_call(*args, **kwargs) -&gt; None</code>","text":"<p>Log an LLM call to Langfuse.</p> <p>Prefer attaching prompt/response/usage to the current span rather than creating a separate generation observation, to avoid duplicates in the Langfuse UI.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.log_evaluation","title":"<code>log_evaluation(*args, **kwargs) -&gt; None</code>","text":"<p>Log an evaluation.</p>"},{"location":"reference/tracer-registry/#opiktracer","title":"OpikTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer","title":"<code>axion._core.tracing.opik.tracer.OpikTracer(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, api_key: Optional[str] = None, workspace: Optional[str] = None, project_name: Optional[str] = None, base_url: Optional[str] = None, trace_id: Optional[str] = None, **kwargs)</code>","text":"<p>               Bases: <code>BaseTracer</code></p> <p>Opik-based tracer for LLM observability.</p> <p>This tracer integrates with Opik (https://www.comet.com/docs/opik/) to provide detailed tracing and observability for LLM applications.</p> Configuration <p>Set the following environment variables or pass to constructor: - OPIK_API_KEY: Your Opik API key - OPIK_WORKSPACE: Your workspace name - OPIK_PROJECT_NAME: Project name (default: 'axion') - OPIK_URL_OVERRIDE: API endpoint (default: https://www.comet.com/opik/api)</p> Example <p>from axion._core.tracing import Tracer, configure_tracing</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer--configure-via-environment","title":"Configure via environment","text":"<p>os.environ['TRACING_MODE'] = 'opik' os.environ['OPIK_API_KEY'] = 'your-api-key' os.environ['OPIK_WORKSPACE'] = 'your-workspace'</p> <p>configure_tracing() tracer = Tracer('llm') with tracer.span('my-operation'):     # ... your code tracer.flush()</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.create","title":"<code>create(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; OpikTracer</code>  <code>classmethod</code>","text":"<p>Factory method to create an OpikTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.span","title":"<code>span(operation_name: str, **attributes)</code>","text":"<p>Creates a synchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>An OpikSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.async_span","title":"<code>async_span(operation_name: str, **attributes)</code>  <code>async</code>","text":"<p>Creates an asynchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>An OpikSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.flush","title":"<code>flush() -&gt; None</code>","text":"<p>Flush pending traces to Opik.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.shutdown","title":"<code>shutdown() -&gt; None</code>","text":"<p>Gracefully shutdown the tracer.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.log_llm_call","title":"<code>log_llm_call(*args, **kwargs) -&gt; None</code>","text":"<p>Log an LLM call to Opik as a generation span.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.log_evaluation","title":"<code>log_evaluation(*args, **kwargs) -&gt; None</code>","text":"<p>Log an evaluation to Opik.</p>"}]}