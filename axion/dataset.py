import ast
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Literal, Optional, Union

import pandas as pd
from axion._core.asyncio import run_async_function
from axion._core.error import CustomValidationError
from axion._core.logging import get_logger
from axion._core.schema import (
    AIMessage,
    HumanMessage,
    RichBaseModel,
    RichSerializer,
    ToolCall,
    ToolMessage,
)
from axion._core.uuid import uuid7
from axion.dataset_schema import (
    MultiTurnConversation,
    RichDatasetBaseModel,
)
from axion.synthetic import GenerationParams
from axion._core.types import FieldNames
from axion.utils import current_datetime, format_value
from pydantic import (
    ConfigDict,
    Field,
    computed_field,
    field_validator,
    model_validator,
)

logger = get_logger(__name__)


class DatasetItem(RichDatasetBaseModel):
    """
    Represents a single evaluation data point, supporting both single-turn and multi-turn conversations.

    This model is designed to store all relevant information required for evaluating LLM performance,
    including the input query, expected and actual outputs, retrieved context, evaluation criteria,
    and additional metadata. It supports both automated evaluation (binary judgments, critiques)
    and richer evaluation with tool usage tracking.

    Attributes:
        id (str): Unique identifier for the item (auto-generated if not provided).
        query (Optional[str]):
            The input query or prompt for single-turn evaluation.
            Aliased as `query` for backward compatibility.
        conversation (Optional[MultiTurnConversation]):
            Multi-turn conversation structure containing a sequence of messages.
            Aliased to `conversation`.
        expected_output (Optional[str]):
            The reference/expected output for single-turn evaluation.
            Aliased to `expected_output`.
        actual_output (Optional[str]):
            The system's generated response for the given query.
        retrieved_content (Optional[List[str]]):
            A list of retrieved documents or contextual snippets used in generating the response.
        latency (Optional[float]):
            Response time in seconds for generating the `actual_output`.
        judgment (Optional[Union[str, int]]):
            A short, binary or categorical evaluation decision (e.g., 1/0, pass/fail, approve/decline).
        critique (Optional[str]):
            A detailed explanation or rationale supporting the `judgment`.
        conversation_extraction_strategy (Literal['first', 'last']):
            Defines whether to extract `query` and `actual_output` from the first or last messages
            in a multi-turn conversation. Defaults to 'last'.
        acceptance_criteria (Optional[List[str]]):
            User-defined definitions of what qualifies as an acceptable/correct response.
        additional_input (Dict[str, Any]):
            Arbitrary key-value pairs providing extra inputs for the evaluation context.
        metadata (Optional[str]):
            Additional metadata as a JSON string for storing structured information.
        trace (Optional[str]):
            Execution trace information, stored as a JSON string.
        additional_output (Dict[str, Any]):
            Extra outputs generated by the system, useful for debugging or extended evaluation.
        tools_called (Optional[List[ToolCall]]):
            A list of tools the system actually invoked during response generation.
        expected_tools (Optional[List[ToolCall]]):
            A list of tools that should have been invoked according to the evaluation criteria.
        user_tags (List[str]):
            A list of custom tags to apply to all tool calls in the conversation.
    """

    id: str = Field(default_factory=lambda: str(uuid7()))

    # --- Internal Fields ---
    multi_turn_conversation: Optional[MultiTurnConversation] = Field(
        default=None, repr=False
    )
    single_turn_query: Optional[str] = Field(default=None, repr=False)
    single_turn_expected_output: Optional[str] = Field(default=None, repr=False)
    judgment: Optional[Union[str, int]] = Field(
        default=None,
        description='A short, binary decision on the output (e.g., 1/0, pass/fail, approve/decline).',
    )

    critique: Optional[str] = Field(
        default=None,
        description='A detailed explanation or feedback supporting the judgment.',
    )

    # --- Configuration Field ---
    conversation_extraction_strategy: Literal['first', 'last'] = Field(
        default='last',
        description="Determines whether to extract 'query' and 'actual_output' from the 'first' or 'last' messages in a conversation.",
    )

    # --- Standard fields ---
    acceptance_criteria: Optional[List[str]] = None
    additional_input: Dict[str, Any] = Field(default_factory=dict)
    metadata: Optional[str] = Field(None, alias='data_metadata')
    user_tags: List[str] = Field(
        default_factory=list,
        description='A list of custom tags to apply to all tool calls in the conversation.',
    )

    # --- RUNTIME fields ---
    actual_output: Optional[str] = None
    retrieved_content: Optional[List[str]] = None
    latency: Optional[float] = None
    trace: Optional[str] = None
    additional_output: Dict[str, Any] = Field(default_factory=dict)

    # --- RANKING ---
    actual_ranking: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description='Ordered list of retrieved items, e.g., [{"id": "doc1", "score": 0.9}, {"id": "doc2", "score": 0.8}]',
    )
    expected_ranking: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description='Ground truth reference. For IR, e.g., [{"id": "doc1", "relevance": 1.0}, {"id": "doc_abc", "relevance": 0.5}]',
    )

    # --- TOOL fields ---
    tools_called: Optional[List[ToolCall]] = Field(
        default=None, description='Tools that were actually called by the system'
    )
    expected_tools: Optional[List[ToolCall]] = Field(
        default=None, description='Tools that should have been called'
    )

    # --- DATA fields ---
    document_text: Optional[str] = Field(default=None, description='Document Text')

    expected_reference: Optional[List[Dict[str, Any]]] = Field(
        default=None, description='Expected Reference (Alias for expected_ranking)'
    )

    model_config = ConfigDict(arbitrary_types_allowed=True, populate_by_name=True)

    @field_validator(
        'actual_ranking', 'expected_ranking', 'expected_reference', mode='before'
    )
    @classmethod
    def _normalize_ranking_list(
        cls, v: Optional[Union[List[str], List[Dict[str, Any]]]]
    ) -> Optional[List[Dict[str, Any]]]:
        """Converts a simple list of strings (document IDs) into a list of ranking dictionaries with 'id'."""
        if v is None:
            return None
        if not isinstance(v, list):
            # Let Pydantic handle validation for a list of dicts later
            return v

        # If the list contains strings, convert them to {"id": "..."} format
        if v and all(isinstance(item, str) for item in v):
            return [{'id': item} for item in v]

        return v

    @model_validator(mode='before')
    @classmethod
    def _route_convenience_fields(cls, data: Any) -> Any:
        """Move convenience fields to their internal counterparts during initialization."""
        if isinstance(data, dict):
            if FieldNames.QUERY in data and 'single_turn_query' not in data:
                data['single_turn_query'] = data.pop(FieldNames.QUERY)
            if (
                FieldNames.EXPECTED_OUTPUT in data
                and 'single_turn_expected_output' not in data
            ):
                data['single_turn_expected_output'] = data.pop(
                    FieldNames.EXPECTED_OUTPUT
                )
            if (
                FieldNames.CONVERSATION in data
                and 'multi_turn_conversation' not in data
            ):
                data['multi_turn_conversation'] = data.pop(FieldNames.CONVERSATION)
        return data

    @model_validator(mode='after')
    def _sync_ranking_and_reference(self) -> 'DatasetItem':
        """Syncs expected_ranking and expected_reference, prioritizing expected_ranking."""
        if self.expected_ranking is not None and self.expected_reference is None:
            self.expected_reference = self.expected_ranking
        elif self.expected_reference is not None and self.expected_ranking is None:
            self.expected_ranking = self.expected_reference
        return self

    @computed_field
    @property
    def conversation(self) -> Optional[MultiTurnConversation]:
        """Provides direct access to the multi-turn conversation object."""
        return self.multi_turn_conversation

    @computed_field
    @property
    def query(self) -> Optional[str]:
        """
        Provides a unified way to access the user's query based on the extraction strategy.

        If the strategy is 'last' (default), it returns the last user message.
        If the strategy is 'first', it returns the first user message.
        """
        if self.multi_turn_conversation:
            messages_to_iterate = (
                reversed(self.multi_turn_conversation.messages)
                if self.conversation_extraction_strategy == 'last'
                else self.multi_turn_conversation.messages
            )
            for message in messages_to_iterate:
                if isinstance(message, HumanMessage):
                    return message.content
            return None
        return self.single_turn_query

    @query.setter
    def query(self, value: str):
        """Sets the user's query, respecting the extraction strategy for multi-turn items."""
        if self.multi_turn_conversation:
            logger.warning("Setting 'query' on a multi-turn item is not recommended.")
            messages_to_iterate = (
                reversed(self.multi_turn_conversation.messages)
                if self.conversation_extraction_strategy == 'last'
                else self.multi_turn_conversation.messages
            )
            for message in messages_to_iterate:
                if isinstance(message, HumanMessage):
                    message.content = value
                    return
            self.multi_turn_conversation.messages.append(HumanMessage(content=value))
        else:
            self.single_turn_query = value

    @computed_field
    @property
    def expected_output(self) -> Optional[str]:
        """
        Provides a unified way to access the expected output.

        If the item is a multi-turn conversation, this returns the
        `reference_text` if set. If it's a single-turn item, it
        returns the stored expected output.

        Returns:
            The expected output as a string, or None if not applicable.
        """
        if self.multi_turn_conversation and self.multi_turn_conversation.reference_text:
            return self.multi_turn_conversation.reference_text
        return self.single_turn_expected_output

    @expected_output.setter
    def expected_output(self, value: str):
        """
        Sets the expected output.

        For single-turn items, it sets the `single_turn_expected_output` field.
        For multi-turn items, it sets the `reference_text`.

        Args:
            value: The expected output string to set.
        """
        if self.multi_turn_conversation:
            self.multi_turn_conversation.reference_text = value
        else:
            self.single_turn_expected_output = value

    @computed_field
    @property
    def conversation_stats(self) -> Optional[Dict[str, int]]:
        """A dictionary of statistics about the conversation."""
        if not self.multi_turn_conversation:
            return None

        stats = {
            'turn_count': 0,
            'user_message_count': 0,
            'ai_message_count': 0,
            'tool_call_count': len(self.tools_called) if self.tools_called else 0,
        }

        for msg in self.multi_turn_conversation.messages:
            if isinstance(msg, HumanMessage):
                stats['user_message_count'] += 1
            elif isinstance(msg, AIMessage):
                stats['ai_message_count'] += 1

        # A "turn" is typically a user message followed by an AI response
        stats['turn_count'] = max(
            stats['user_message_count'], stats['ai_message_count']
        )

        return stats

    @computed_field
    @property
    def agent_trajectory(self) -> Optional[List[str]]:
        """An ordered list of tool names called, representing the agent's execution path."""
        if not self.tools_called:
            return None
        return [tool.name for tool in self.tools_called]

    @computed_field
    @property
    def has_errors(self) -> Union[bool, None]:
        """Returns True if any tool message in the conversation is marked as an error."""
        if not self.multi_turn_conversation:
            return None
        return any(
            isinstance(msg, ToolMessage) and msg.is_error
            for msg in self.multi_turn_conversation.messages
        )

    def to_transcript(self) -> str:
        """
        Converts the conversation messages into a human-readable string transcript.

        If the item is not a multi-turn conversation, it returns an empty string.

        Returns:
            A formatted string representing the entire conversation.
        """
        if not self.multi_turn_conversation:
            return ''
        lines = []
        for msg in self.multi_turn_conversation.messages:
            lines.append(f"{msg.role.capitalize()}: {msg.content or ''}")
            if isinstance(msg, AIMessage) and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    lines.append(
                        f'  Tool Call: {tool_call.name}({json.dumps(tool_call.args)})'
                    )
        return '\n'.join(lines)

    @staticmethod
    def generate_tags_for_tool_call(tool_call: ToolCall) -> List[str]:
        """Analyzes a tool call to generate a set of descriptive tags."""
        generated_tags = set()
        name = tool_call.name.lower()

        if any(kw in name for kw in ['retriev', 'rag', 'knowledge', 'search']):
            generated_tags.add('RAG')
        if any(kw in name for kw in ['guard', 'check', 'valid', 'safe']):
            generated_tags.add('GUARDRAIL')
        if any(
            kw in name for kw in ['llm', 'generate', 'chat', 'complete', 'summarize']
        ):
            generated_tags.add('LLM')
        if any(kw in name for kw in ['sql', 'database', 'query', 'snowflake', 'trino']):
            generated_tags.add('DATABASE')

        return sorted(list(generated_tags))

    @model_validator(mode='after')
    def _populate_fields_from_conversation(self) -> 'DatasetItem':
        """
        After validation, scan the conversation (if it exists) to
        auto-populate key fields like 'actual_output', 'tools_called',
        and 'retrieved_content', and to tag individual tool calls.
        """
        if not self.multi_turn_conversation:
            return self  # Nothing to do if not multi-turn

        if self.actual_output is None:
            messages_to_iterate = (
                reversed(self.multi_turn_conversation.messages)
                if self.conversation_extraction_strategy == 'last'
                else self.multi_turn_conversation.messages
            )
            for message in messages_to_iterate:
                if isinstance(message, AIMessage) and message.content is not None:
                    self.actual_output = message.content
                    break

        all_tool_calls = []
        for message in self.multi_turn_conversation.messages:
            if isinstance(message, AIMessage) and message.tool_calls:
                for tool_call in message.tool_calls:
                    # Combine auto-generated tags with user-provided tags
                    auto_tags = self.generate_tags_for_tool_call(tool_call)
                    combined_tags = set(auto_tags) | set(self.user_tags)
                    tool_call.tags = sorted(list(combined_tags))
                    all_tool_calls.append(tool_call)

        if all_tool_calls and self.tools_called is None:
            self.tools_called = all_tool_calls

        all_retrieved_content = []
        all_ranked_items = []
        for message in self.multi_turn_conversation.messages:
            if isinstance(message, ToolMessage) and message.tool_output:
                if 'retrieved_content' in message.tool_output and isinstance(
                    message.tool_output['retrieved_content'], list
                ):
                    all_retrieved_content.extend(
                        message.tool_output['retrieved_content']
                    )
                if 'actual_ranking' in message.tool_output and isinstance(
                    message.tool_output['actual_ranking'], list
                ):
                    all_ranked_items.extend(message.tool_output['actual_ranking'])

        if all_retrieved_content and self.retrieved_content is None:
            self.retrieved_content = sorted(list(set(all_retrieved_content)))

        if all_ranked_items and self.actual_ranking is None:
            self.actual_ranking = all_ranked_items

        return self

    @model_validator(mode='after')
    def set_actual_output_from_conversation(self) -> 'DatasetItem':
        """
        If this is a multi-turn item and `actual_output` is not provided,
        this validator automatically sets it to the content of the first or last AIMessage,
        based on the `conversation_extraction_strategy`.
        """
        if self.multi_turn_conversation and self.actual_output is None:
            messages_to_iterate = (
                reversed(self.multi_turn_conversation.messages)
                if self.conversation_extraction_strategy == 'last'
                else self.multi_turn_conversation.messages
            )
            for message in messages_to_iterate:
                if isinstance(message, AIMessage) and message.content is not None:
                    self.actual_output = message.content
                    break  # Stop after finding the correct one
        return self

    @field_validator('acceptance_criteria', mode='before')
    @classmethod
    def convert_acceptance_criteria_to_list(cls, v) -> Optional[List[str]]:
        """Ensure acceptance_criteria is a list of strings."""
        if v is None:
            return None
        if isinstance(v, list):
            return v
        if isinstance(v, str):
            return [v]
        raise TypeError(
            'acceptance_criteria must be a string, list of strings, or None'
        )

    def get(self, key: str, default: Any = None) -> Any:
        """
        Get an attribute value by key, similar to dict.get().
        This method correctly handles properties like 'query'.

        Args:
            key: The attribute name to retrieve.
            default: Value to return if attribute doesn't exist.

        Returns:
            The attribute value or default if not found.
        """
        return getattr(self, key, default)

    def extract_by_tag(self, tag: str) -> List[tuple[ToolCall, Optional[ToolMessage]]]:
        """
        Extracts tool interactions from the conversation that match a specific tag.

        Args:
            tag: The tag to filter by (e.g., 'RAG', 'GUARDRAIL').

        Returns:
            A list of tuples, where each tuple contains the tagged ToolCall
            and its corresponding ToolMessage (or None if not found).
        """
        if not self.multi_turn_conversation:
            return []

        tagged_interactions = []
        tool_message_map = {
            msg.tool_call_id: msg
            for msg in self.multi_turn_conversation.messages
            if isinstance(msg, ToolMessage)
        }

        if not self.tools_called:
            return []

        for tool_call in self.tools_called:
            if tag in tool_call.tags:
                corresponding_message = tool_message_map.get(tool_call.id)
                tagged_interactions.append((tool_call, corresponding_message))

        return tagged_interactions

    def keys(self) -> List[str]:
        """
        Return all public attribute names, including properties and aliases.

        Returns:
            A sorted list of public-facing field and property names.
        """
        model_field_keys = set(self.__class__.model_fields.keys())
        model_field_keys -= FieldNames.get_aliased_model_field_keys()

        # Add the public-facing properties
        public_properties = {
            FieldNames.QUERY,
            FieldNames.EXPECTED_OUTPUT,
            FieldNames.CONVERSATION,
        }

        return sorted(list(model_field_keys | public_properties))

    def values(self) -> List[Any]:
        """
        Return all public attribute values, corresponding to the .keys() method.

        Returns:
            A list of values for the public-facing fields and properties.
        """
        return [self.get(key) for key in self.keys()]

    def items(self) -> List[tuple]:
        """
        Return all (key, value) pairs for public attributes.

        Returns:
            A list of (key, value) tuples for public-facing fields and properties.
        """
        return [(key, self.get(key)) for key in self.keys()]

    def subset(
        self, fields: List[str], keep_id: bool = True, copy_annotations: bool = False
    ) -> 'DatasetItem':
        """
        Create a new DatasetItem with only the specified fields, all others set to None/empty.

        Args:
            fields: List of field names to keep (e.g., ['query', 'expected_output'])
            keep_id: Whether to preserve the original ID (default: True)
            copy_annotations: Whether to copy annotations (judgment and critique) to the new item (default: False)

        Returns:
            New DatasetItem instance with only specified fields populated

        Example:
            # Get item with only query and expected_output
            subset_item = item.subset(['query', 'expected_output'])

            # Get item with query, actual_output, and preserve annotations
            subset_item = item.subset(['query', 'actual_output'], copy_annotations=True)
        """
        new_item_data = {
            field: getattr(self, field) for field in fields if hasattr(self, field)
        }

        if len(new_item_data) != len(fields):
            missing = [f for f in fields if not hasattr(self, f)]
            raise ValueError(f'Field(s) {missing} do not exist on DatasetItem')

        if keep_id:
            new_item_data['id'] = self.id

        if copy_annotations and self.judgment:
            new_item_data[FieldNames.JUDGMENT] = self.judgment
            if self.critique:
                new_item_data[FieldNames.CRITIQUE] = self.critique

        return DatasetItem(**new_item_data)

    def evaluation_fields(self) -> 'DatasetItem':
        """
        Extract just the evaluation fields.
        """
        return self.subset(FieldNames.get_evaluation_fields())

    def update(
        self, other: Union['DatasetItem', Dict[str, Any]], overwrite: bool = True
    ) -> 'DatasetItem':
        """
        Update this DatasetItem with values from another DatasetItem or dictionary.
        This method correctly handles aliases and special merge logic for lists/dicts.

        Args:
            other: Another DatasetItem instance or a dictionary to update from.
            overwrite: If True, overwrite existing values. If False, only fill empty fields.

        Returns:
            The updated DatasetItem instance (self).
        """
        update_data = other.model_dump() if isinstance(other, DatasetItem) else other
        if not isinstance(update_data, dict):
            raise ValueError(f'Unsupported update type: {type(other)}')

        for field_name, new_value in update_data.items():
            # Check if the field exists on the model (works for properties and aliases)
            if not hasattr(self, field_name):
                continue

            current_value = getattr(self, field_name, None)

            should_update = overwrite or (
                current_value is None
                or (isinstance(current_value, (list, dict)) and not current_value)
            )

            if not should_update:
                continue

            # --- Field-specific merge logic ---
            if field_name in {
                FieldNames.ADDITIONAL_INPUT,
                FieldNames.ADDITIONAL_OUTPUT,
            } and isinstance(new_value, dict):
                current_dict = getattr(self, field_name) or {}
                updated = (
                    {**current_dict, **new_value}
                    if overwrite
                    else {**new_value, **current_dict}
                )
                setattr(self, field_name, updated)
            elif field_name == FieldNames.RETRIEVED_CONTENT and isinstance(
                new_value, list
            ):
                current_list = getattr(self, field_name) or []
                merged = new_value if overwrite else list(set(current_list + new_value))
                setattr(self, field_name, merged)
            else:
                # Default behavior: set the attribute. Pydantic handles validation and property setters.
                try:
                    setattr(self, field_name, new_value)
                except CustomValidationError as e:
                    logger.warning(
                        f"Validation error while updating field '{field_name}': {e}"
                    )
        return self

    def update_runtime(self, **kwargs) -> 'DatasetItem':
        """
        Update runtime-related fields such as actual_output or retrieved_content.

        Args:
            **kwargs: Runtime fields to update.

        Returns:
            Updated DatasetItem (self).
        """
        filtered = {
            k: v for k, v in kwargs.items() if k in FieldNames.get_runtime_fields()
        }
        return self.update(filtered, overwrite=True)

    def merge_metadata(self, metadata: Union[str, Dict[str, Any]]) -> 'DatasetItem':
        """
        Merge new metadata into the existing metadata field.

        Args:
            metadata: A dictionary or JSON string to merge.

        Returns:
            Updated DatasetItem (self).
        """

        def parse_json(meta):
            if isinstance(meta, str):
                try:
                    return json.loads(meta)
                except json.JSONDecodeError:
                    return {'raw': meta}
            elif isinstance(meta, dict):
                return meta
            return {'value': str(meta)}

        existing = parse_json(self.metadata or {})
        new_meta = parse_json(metadata)

        existing.update(new_meta)
        self.metadata = json.dumps(existing)

        return self

    def __getitem__(self, key: str) -> Any:
        """
        Allow dictionary-style access to fields, including properties.

        Args:
            key: The field or property name to access.

        Returns:
            The field or property value.

        Raises:
            KeyError: If the key is not a valid attribute or property.
        """
        if not hasattr(self, key):
            raise KeyError(
                f"'{key}' is not a valid attribute or property on this DatasetItem."
            )
        return getattr(self, key)

    def __contains__(self, key: str) -> bool:
        """
        Check if a field or property exists on the item.

        Args:
            key: The field or property name to check.

        Returns:
            True if the field or property exists, False otherwise.
        """
        return hasattr(self, key)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DatasetItem':
        """Create a DatasetItem from a dictionary.

        Args:
            data: Dictionary containing item data

        Returns:
            New DatasetItem instance
        """
        return cls(**data)


@dataclass
class Dataset(RichSerializer):
    """Represents a structured dataset for evaluation purposes, supporting both single and multi-turn items.

    This class manages a collection of DatasetItem objects and provides
    functionality for loading, saving, filtering, and transforming datasets.

    Attributes:
        name: Name of the dataset
        description: Description of the dataset's purpose or contents
        version: Version identifier
        created_at: ISO format timestamp of creation
        metadata: Additional metadata (stored as JSON)
        items: List of DatasetItem objects
    """

    name: Optional[str] = None
    description: str = ''
    version: str = '1.0'
    created_at: str = field(default_factory=lambda: current_datetime())
    metadata: Optional[str] = None  # store as JSON
    items: List[DatasetItem] = field(default_factory=list)

    _default_catch_all: str = FieldNames.ADDITIONAL_INPUT
    _item_map: Dict[str, DatasetItem] = field(default_factory=dict, repr=False)
    _synthetic_data: Optional[List[Dict[str, Any]]] = None

    def __post_init__(self):
        if not self.name:
            self.name = (
                f"dataset-{current_datetime().replace(' ', '_').replace(':', '-')}"
            )
        self._validate_name(self.name)

    @staticmethod
    def _validate_name(name: str = None):
        if not name:
            raise TypeError('Dataset name must be provided')
        if not isinstance(name, str):
            raise TypeError(f"'name' must be a string, got {type(name).__name__}")
        if not name.strip():
            raise ValueError("'name' cannot be empty or whitespace")

    def add_item(self, item: Union[DatasetItem, Dict[str, Any]]) -> DatasetItem:
        """Add an item to the dataset, handling both single-turn and multi-turn items."""
        item_obj = item if isinstance(item, DatasetItem) else DatasetItem(**item)
        self.items.append(item_obj)
        return item_obj

    def add_items(
        self, items: List[Union[DatasetItem, Dict[str, Any]]]
    ) -> List[DatasetItem]:
        """
        Add multiple items to the dataset.

        Args:
            items: List of DatasetItem instances or dictionaries

        Returns:
            List of added DatasetItem instances
        """
        added_items = []
        for item in items:
            added_items.append(self.add_item(item))
        return added_items

    def get_item_by_id(self, item_id: str) -> Optional[DatasetItem]:
        """Retrieve an item by its ID.

        Args:
            item_id: ID of the item to find

        Returns:
            DatasetItem if found, None otherwise
        """
        return next((item for item in self.items if item.id == item_id), None)

    def filter(
        self,
        condition: Callable[[DatasetItem], bool],
        dataset_name: Optional[str] = None,
    ) -> 'Dataset':
        """
        Filters the dataset based on a condition and returns a new Dataset.
        """
        new_dataset = Dataset(
            name=dataset_name or f'{self.name} (filtered)',
            description=self.description,
            version=self.version,
        )
        new_dataset.add_items([item for item in self.items if condition(item)])
        return new_dataset

    def to_json(self, file_path: str) -> None:
        """
        Save dataset to JSON file.

        Args:
            file_path: Path where to save the JSON file
        """
        data = {
            'name': self.name,
            'description': self.description,
            'version': self.version,
            'created_at': self.created_at,
            'metadata': self.metadata,
            'items': [
                item.model_dump(exclude_none=True, by_alias=True) for item in self.items
            ],
        }
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=2)
        logger.info(f'Saved dataset to {file_path} with {len(self.items)} items')

    def to_dataframe(
        self,
        flatten_nested_json: bool = False,
        sep: str = '.',
        remove_aliased: bool = True,
    ) -> pd.DataFrame:
        """
        Converts the dataset to a pandas DataFrame, serializing complex fields to JSON strings.

        Args:
            flatten_nested_json: If True, nested objects will be flattened into separate columns.
                                 If False (default), they will be stored as JSON strings.
            sep: Separator for flattening.
            remove_aliased: If True remove aliased model field keys.
        Returns:
            A pandas DataFrame representing the dataset.
        """
        if not self.items:
            return pd.DataFrame()

        json_fields = [
            FieldNames.CONVERSATION,
            FieldNames.TOOLS_CALLED,
            FieldNames.EXPECTED_TOOLS,
            FieldNames.RETRIEVED_CONTENT,
            FieldNames.USER_TAGS,
            FieldNames.ADDITIONAL_INPUT,
            FieldNames.ADDITIONAL_OUTPUT,
            FieldNames.ACCEPTANCE_CRITERIA,
            FieldNames.EXPECTED_REFERENCE,
        ]

        records = []
        for item in self.items:
            record = item.model_dump(by_alias=True)
            if not flatten_nested_json:
                for _field in json_fields:
                    value = record.get(_field)
                    if value is not None:
                        record[_field] = json.dumps(value)
            records.append(record)

        dataframe = (
            pd.json_normalize(records, sep=sep)
            if flatten_nested_json
            else pd.DataFrame(records)
        )

        if remove_aliased:
            dataframe = dataframe.drop(
                list(FieldNames.get_aliased_model_field_keys()), axis=1
            )

        return dataframe

    def load_dataframe(self, dataframe: pd.DataFrame) -> None:
        """
        Load dataset items from a DataFrame.

        Args:
            dataframe: DataFrame containing dataset items.
        """
        for _, row in dataframe.iterrows():
            item_data: Dict[str, Any] = {self._default_catch_all: {}}
            for key, value in row.items():
                if isinstance(value, float) and pd.isna(value):
                    item_data[key] = None
                elif value is None:
                    item_data[key] = None
                else:
                    # Handle conversation deserialization
                    if key == FieldNames.CONVERSATION and isinstance(value, str):
                        try:
                            convo_dict = json.loads(value)
                            item_data[key] = MultiTurnConversation(**convo_dict)
                            continue
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(
                                'Could not parse conversation JSON for item. Skipping field.'
                            )
                            item_data[key] = None
                            continue

                    if key not in FieldNames.get_full_dataset_fields():
                        item_data[self._default_catch_all][key] = format_value(value)
                    else:
                        item_data[key] = value

            self.add_item(item_data)

        logger.info(f'Loaded {len(self.items)} items from DataFrame')

    def to_csv(self, file_path: str, remove_aliased: bool = True) -> None:
        """
        Save dataset to CSV file.

        Args:
            file_path: Path where to save the CSV file.
            remove_aliased: If True remove aliased model field keys
        """
        dataframe = self.to_dataframe(remove_aliased=remove_aliased)
        dataframe.to_csv(file_path, index=False)
        logger.info(f'Saved dataset to {file_path} with {len(dataframe)} items')

    def synthetic_generate_from_directory(
        self,
        directory_path: str,
        llm,
        params: 'GenerationParams',
        embed_model: None,
        max_concurrent: int = 3,
        show_progress: bool = True,
        **kwargs,
    ):
        """
        Generates synthetic QA data from a directory of documents.

        This method uses the `DocumentQAGenerator` to process documents in the given
        directory, producing synthetic question-answer pairs using the provided language model (LLM)
        and generation parameters. The results are transformed into a format compatible with the
        dataset interface (`query`, `expected_output`) and added to the dataset.

        Args:
            directory_path (str): Path to the directory containing documents to process.
            llm: A language model instance that implements method for generation.
            params (GenerationParams): A configuration object that defines generation settings
                such as number of QA pairs, difficulty, chunking behavior, etc.
            embed_model: An embedding model used for semantic parsing.
            max_concurrent (int, optional): The maximum number of documents to process concurrently.
                Defaults to 3.
            show_progress: Whether to show progress bars using tqdm
        """

        from axion.synthetic import DocumentQAGenerator

        qa_generator = DocumentQAGenerator(
            llm=llm,
            params=params,
            embed_model=embed_model,
            max_concurrent=max_concurrent,
            show_progress=show_progress,
            **kwargs,
        )

        async def _execute():
            return await qa_generator.generate_from_directory(
                directory_path=directory_path
            )

        self._synthetic_data = run_async_function(_execute)
        # Convert to dataset structure
        items = qa_generator.to_items(self._synthetic_data)
        self.add_items(items=items)

    @classmethod
    def create(
        cls,
        name: Optional[str] = None,
        items: Optional[List[Union[Dict[str, Any], str]]] = None,
        **kwargs,
    ) -> 'Dataset':
        """Creates a new dataset with initial items."""
        dataset = cls(name=name, **kwargs)
        for item in items or []:
            if isinstance(item, str):
                dataset.add_item({FieldNames.QUERY: item})
            else:
                dataset.add_item(item)
        return dataset

    @staticmethod
    def merge_response_into_dataset_items(
        items: List[DatasetItem],
        responses: List[RichBaseModel],
        require_success: bool = False,
    ) -> List[DatasetItem]:
        """
        Updates DatasetItem instances with fields from corresponding APIResponseData.

        Args:
            items: List of DatasetItem objects to update.
            responses: List of APIResponseData objects with new runtime values.
            require_success: If True, only keep items when response.status == 'success'.

        Returns:
            List of DatasetItem objects that were successfully processed (if require_success=True)
            or all items (if require_success=False).
        """
        updated_items = []

        for item, response in zip(items, responses):
            status = getattr(
                response, 'status', 'success'
            )  # Default to success if no status

            if require_success and status != 'success':
                logger.info(
                    f"Removing item with query '{item.query}' due to failed status: {status}"
                )
                continue  # Skip this item entirely - it won't be in the returned list

            # Update the item with response data
            item.actual_output = response.actual_output
            if response.retrieved_content:
                item.retrieved_content = response.retrieved_content
            if response.sources:
                item.actual_ranking = response.sources
            item.latency = response.latency
            item.trace = json.dumps(response.trace) if response.trace else None
            item.additional_output.update(response.additional_output or {})

            updated_items.append(item)

        logger.info(f'Processed {len(updated_items)}/{len(items)} items successfully')
        return updated_items

    def execute_dataset_items_from_api(
        self,
        api_name: str,
        config: Union[str, Dict[str, Any], Path],
        max_concurrent: int = 5,
        show_progress: bool = True,
        retry_config: Optional[Union[Any, Dict[str, Any]]] = None,
        require_success: bool = False,
        additional_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        Synchronously executes API calls using the specified API runner and attaches
        responses to the dataset items. Useful for batch-processing queries via a registered API.

        Internally runs async code but exposes a sync interface to the user.

        Args:
            api_name (str): The name of the registered API to use for execution.
            config (str | dict | Path): Config for authenticating with the API.
            max_concurrent (int, optional): Max number of concurrent API requests. Defaults to 5.
            show_progress: Whether to show progress bars using tqdm.
            retry_config (RetryConfig | Dict): Configuration for retrying logic.
            require_success: (bool, optional): If True, remove items from dataset when response.status != 'success'.
            additional_config (dict, optional): Extra configuration options for the runner.
            **kwargs: Extra arguments passed to the executor's `execute_batch` method.
        """
        from axion.runners import APIRunner

        async def _execute():
            executor = APIRunner(
                config=config,
                max_concurrent=max_concurrent,
                show_progress=show_progress,
                retry_config=retry_config,
                additional_config=additional_config or {},
            )
            queries = [item.query for item in self.items]
            response = await executor.execute_batch(api_name, queries=queries, **kwargs)

            # Get the updated items (potentially filtered)
            updated_items = self.merge_response_into_dataset_items(
                self.items, response, require_success
            )

            # Replace the dataset's items with the filtered list
            if require_success:
                removed_count = len(self.items) - len(updated_items)
                self.items = updated_items
                if removed_count > 0:
                    logger.info(
                        f'Removed {removed_count} failed items from dataset. {len(self.items)} items remaining.'
                    )

        run_async_function(_execute)

    @classmethod
    def read_dataframe(
        cls, dataframe: pd.DataFrame, name: Optional[str] = None, **kwargs
    ) -> 'Dataset':
        """
        Creates a Dataset from a pandas DataFrame, safely deserializing JSON and Python literals.
        All fields must be included in DataFrame rows to correctly map to DatasetItem.

        Args:
            dataframe: Input DataFrame to read from.
            name: Optional dataset name.
            **kwargs: Additional parameters passed to the Dataset constructor.

        Returns:
            Dataset: A populated Dataset instance.
        """

        # Drop optional columns if they exist
        dataframe = dataframe.drop(
            columns=list(FieldNames.get_aliased_model_field_keys()), errors='ignore'
        )

        # Clean and normalize DataFrame
        df_cleaned = dataframe.astype(object).where(pd.notna(dataframe), None)
        records = df_cleaned.to_dict(orient='records')

        dataset = cls(name=name, **kwargs)

        json_fields = [
            FieldNames.CONVERSATION,
            FieldNames.TOOLS_CALLED,
            FieldNames.EXPECTED_TOOLS,
            FieldNames.RETRIEVED_CONTENT,
            FieldNames.USER_TAGS,
            FieldNames.ADDITIONAL_INPUT,
            FieldNames.ADDITIONAL_OUTPUT,
            FieldNames.ACCEPTANCE_CRITERIA,
            FieldNames.ACTUAL_RANKING,
            FieldNames.EXPECTED_RANKING,
            FieldNames.EXPECTED_REFERENCE,
        ]

        def _parse_value(_value: Any) -> Any:
            """Safely parse a value that may be JSON, Python literal, or plain text."""
            if not isinstance(_value, str) or not _value.strip():
                return _value

            for parser, _name in ((json.loads, 'JSON'), (ast.literal_eval, 'literal')):
                try:
                    return parser(_value)
                except Exception:
                    continue

            logger.warning(f'Could not parse value: {_value[:80]}... (truncated)')
            return _value

        for record in records:
            for _field in json_fields:
                value = record.get(_field)
                if value is not None:
                    record[_field] = _parse_value(value)

            try:
                dataset.add_item(record)
            except CustomValidationError as e:
                item_id = record.get(FieldNames.ID, 'N/A')
                logger.error(
                    f"Failed to validate item '{item_id}' from DataFrame â€” possible schema mismatch or invalid data. Error: {e}"
                )

        return dataset

    @classmethod
    def read_json(
        cls,
        file_path: Union[str, Path],
        name: Optional[str] = None,
    ) -> 'Dataset':
        """Creates a dataset from a JSON file, correctly parsing multi-turn conversations."""
        with open(file_path, 'r') as f:
            data = json.load(f)

        dataset = cls(
            name=data.get(FieldNames.NAME, name),
            description=data.get('description', ''),
            version=data.get('version', '1.0'),
            created_at=data.get('created_at', current_datetime()),
            metadata=data.get(FieldNames.METADATA),
        )

        items_data = data.get('items', [])
        for item_dict in items_data:
            # This check is for robustness, especially if reading files not created by to_json
            if FieldNames.CONVERSATION in item_dict and isinstance(
                item_dict[FieldNames.CONVERSATION], str
            ):
                try:
                    item_dict[FieldNames.CONVERSATION] = json.loads(
                        item_dict[FieldNames.CONVERSATION]
                    )
                except (json.JSONDecodeError, TypeError):
                    logger.warning(
                        'Could not parse conversation JSON string for an item. Skipping conversation field.'
                    )
                    item_dict[FieldNames.CONVERSATION] = None

            # Only try to add items that have content
            if FieldNames.QUERY in item_dict or FieldNames.CONVERSATION in item_dict:
                try:
                    dataset.add_item(item_dict)
                except CustomValidationError as e:
                    item_id = item_dict.get('id', 'N/A')
                    logger.warning(
                        f"Failed to validate and add item with id '{item_id}'. "
                        f'This may be due to a schema mismatch between the data being read and '
                        f'the current model definitions. Error: {e}'
                    )

        return dataset

    @classmethod
    def read_csv(
        cls,
        file_path: Union[str, Path],
        name: Optional[str] = None,
        column_mapping: Optional[Dict[str, str]] = None,
        **kwargs,
    ) -> 'Dataset':
        """Creates a dataset from a CSV file."""
        dataframe = pd.read_csv(file_path)
        if column_mapping:
            dataframe = dataframe.rename(columns=column_mapping)
        return cls.read_dataframe(dataframe, name=name, **kwargs)

    def get_summary(
        self,
    ) -> Optional[Dict[str, Any]]:
        """
        Return summary statistics about the dataset.
        """
        multi_turn_count = sum(
            1 for item in self.items if item.conversation is not None
        )
        single_turn_count = len(self.items) - multi_turn_count

        return {
            'name': self.name,
            'total_items': len(self.items),
            'single_turn_items': single_turn_count,
            'multi_turn_items': multi_turn_count,
            'has_actual_output': sum(bool(item.actual_output) for item in self.items),
            'has_expected_output': sum(
                bool(item.expected_output) for item in self.items
            ),
            'created_at': self.created_at,
            'version': self.version,
        }

    def get_summary_table(self, title: str = 'Dataset Summary', **kwargs) -> None:
        """
        Return summary statistics about the dataset in rich table format.

        Args:
            title: Title for the log output.
            **kwargs: Additional arguments passed to the logging method.
        """
        summary = self.get_summary()
        if summary:
            logger.log_table(data=[summary], title=title, **kwargs)

    @property
    def synthetic_data(self):
        """Get raw synthetic data if available."""
        return self._synthetic_data

    def __repr__(self) -> str:
        """Return a clean, informative representation of the dataset."""
        # Count items with actual/expected outputs
        has_actual = sum(1 for item in self.items if item.actual_output)
        has_expected = sum(1 for item in self.items if item.expected_output)
        multi_turn_count = sum(
            1 for item in self.items if item.conversation is not None
        )

        desc = (
            self.description[:50] + '...'
            if len(self.description) > 50
            else self.description
        )
        desc_part = f', desc="{desc}"' if self.description else ''

        # Show evaluation status and multi-turn info
        eval_status = ''
        if self.items:
            if has_actual > 0:
                eval_status = f' ({has_actual}/{len(self.items)} with actual)'
            elif has_expected > 0:
                eval_status = f' ({has_expected}/{len(self.items)} with expected)'

            if multi_turn_count > 0:
                eval_status += f', {multi_turn_count} multi-turn'

        return (
            f"Dataset(name='{self.name}', items={len(self.items)}{eval_status}, "
            f'v{self.version}{desc_part})'
        )

    def __len__(self) -> int:
        """Return the number of items in the dataset."""
        return len(self.items)

    def __getitem__(self, index: int) -> DatasetItem:
        """
        Get item by index.

        Args:
            index: Index of the item to retrieve

        Returns:
            DatasetItem at the specified index
        """
        return self.items[index]

    def __iter__(self):
        """Make the dataset iterable."""
        return iter(self.items)


def format_input(input_data: Union[DatasetItem, Dict[str, Any]]) -> DatasetItem:
    """
    Normalize input data into a DatasetItem instance.

    This utility ensures that any input provided for evaluation is converted into
    a standardized `DatasetItem`. If the input is already a `DatasetItem`, it is
    returned unchanged. If the input is a dictionary, it will be unpacked into
    a new `DatasetItem`. Any other type will raise a `TypeError`.

    Args:
        input_data (Union[DatasetItem, Dict[str, Any]]):
            The input to normalize. Can be:
              - A `DatasetItem` (returned as-is).
              - A dictionary that matches the signature of `DatasetItem`.

    Returns:
        DatasetItem: A fully constructed `DatasetItem` object.

    """
    if not isinstance(input_data, (DatasetItem, dict)):
        raise TypeError(
            f"Invalid type '{type(input_data)}' for evaluation_inputs. "
            f'Expected one of: DatasetItem, or Dict'
        )

    return (
        input_data if isinstance(input_data, DatasetItem) else DatasetItem(**input_data)
    )
