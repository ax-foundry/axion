{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Axion","text":"<p>White-box evaluation\u2014Axion empowers builders with actionable signals, automated pipelines, and fully transparent metrics. See exactly why your agent succeeds or fails.</p> Philosophy <p>       Agents are sports cars. Evals are the seatbelt. </p> <p>       It won't make you faster. It won't win you anything. But it will keep a bad release from turning into a public incident. If you skip evals because you're a \"good driver,\" you're not a serious person.     </p> 1 <p>Evals are adult supervision</p> <p>Not rocket science. Simple checks that prevent simple disasters. Start here, stay here, and expand only when the basics are solid.</p> 2 <p>Define \"good\" in one sentence</p> <p>If you can't articulate what success looks like, no framework will save you. Clarity first, tooling second.</p> 3 <p>Start with pass/fail</p> <p>Add nuance only after you've earned it. A binary gate catches more failures than a sophisticated rubric you never run.</p> 4 <p>Calibrate your judges</p> <p>LLM judges are useful. They're also liars with confidence. Calibrate them against humans or don't pretend you measured anything.</p> <p>Evaluation Flywheel  Why Ground Truth Matters </p>"},{"location":"#component-arsenal","title":"Component Arsenal","text":"Core Primitives Structured Handlers &amp; Tool Abstractions <p>Build composable toolchains with pre-defined base classes for structured LLMs, tools, and knowledge retrieval. Eliminate boilerplate, enforce consistency, and focus on your logic.</p> API Integrations Extensible Backend Access <p>Base API classes with built-in tracing and authentication support. Build your own API integrations with ease or extend the provided abstractions.</p> Evaluation Engine &amp; Metric Suite Built-in &amp; Open-Source Friendly <p>Define experiments, run batch evaluations, calibrate judges, and score using our native metrics\u2014or integrate with open libraries for broader experimentation coverage.</p> RAG Toolbox Everything Retrieval\u2014Chunking, Grounding, Response Assembly <p>End-to-end support for grounding pipelines with modular components you can reuse across use cases.</p> Observability at Its Core Trace, Log, Debug with Confidence <p>Native support for Logfire, structured logging, and run tracking gives you production-grade visibility across every step of your AI pipeline.</p> Designed for Scale Async-Native, Pydantic-Validated, Error Resilient <p>Async support everywhere. Predictable, structured I/O with Pydantic validation. Robust error handling out-of-the-box.</p>"},{"location":"#hierarchical-scoring","title":"Hierarchical ScoringWhy \"Axion\"?","text":"<p>What sets Axion apart</p> <p>Our scoring framework is hierarchical by design\u2014moving from a single overall score down into layered sub-scores. This delivers a diagnostic map of quality, not just a number.</p> <pre><code>                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  Overall Score  \u2502\n                \u2502      0.82       \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u25bc                  \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Relevance \u2502      \u2502 Accuracy  \u2502      \u2502   Tone    \u2502\n\u2502   0.91    \u2502      \u2502   0.78    \u2502      \u2502   0.85    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Instant Root Cause Diagnosis</p> <p>Drill down to pinpoint whether issues stem from relevance, accuracy, tone, or other dimensions\u2014no more guessing from flat scores.</p> <p>Strategic Prioritization</p> <p>Forces clarity on what really matters for your business by breaking quality into weighted layers.</p> <p>Actionable Feedback Loop</p> <p>Each layer translates directly into actions\u2014retraining, prompt adjustments, or alignment tuning.</p> <p>Customizable to Business Goals</p> <p>Weight and expand dimensions to match your unique KPIs. Define what \"good AI\" means for you.</p> <pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Define hierarchical scoring configuration\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n    },\n    'model': {\n        'ANSWER_QUALITY': {'Relevance': 1.0},\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=config,  # Or pass path to config.yaml\n)\n\n# Generate scorecard with hierarchical breakdown\nresults.to_scorecard()\n</code></pre> <p>Learn more about Hierarchical Scoring \u2192</p> Origin <p>Agent X-Ray Inspection &amp; Optimization Network</p> <p>The name draws inspiration from the axion\u2014a hypothetical particle in physics proposed to solve the \"strong CP problem\" in quantum chromodynamics. Physicists Frank Wilczek and Steven Weinberg named it after a laundry detergent, hoping it would \"clean up\" their theoretical mess.</p> Particle \u2194 Toolkit \u2736 <p>Incredibly small, immensely powerful</p> <p>Axions may account for the universe's dark matter through sheer numbers. This toolkit offers small, focused tools that combine to tackle AI evaluation at scale.</p> \u2727 <p>Designed to clean things up</p> <p>Named after a detergent to \"clean up\" a theoretical mess. Built to bring clarity and structure to the messy problem of agent evaluation.</p> \u2b22 <p>Modular by nature</p> <p>Lightweight components that work together to solve complex problems. Composable building blocks, not a monolithic framework.</p>"},{"location":"agent_playbook/","title":"Agent Evaluation Playbook","text":"<p>This playbook is a practical guide to evaluating AI agents without the ceremony. The goal isn't \"more dashboards\" \u2014 it's making the agent better in the real world, on the things your users actually do.</p> Philosophy <p>Agents are sports cars.Evals are the seatbelt.</p> <p>Seatbelts don't steer. They don't tune the engine. They don't make you \"better.\" They just keep you from eating the steering wheel when something goes wrong. If your stance is \"we don't need evals because we're careful,\" you're confusing confidence with safety.</p> Mindset <p>This is just data science.</p> <p>Same fundamentals as traditional ML \u2014 dataset, labels, error analysis, iteration. If you're paying $100/seat for an eval tool and still not looking at your data, you're doing it wrong. Expensive tools don't solve the problem \u2014 they just scale whatever process you already have.</p> 1 <p>Evals are adult supervision</p> <p>Not rocket science. Simple checks that prevent simple disasters. Start here, stay here, expand only when the basics are solid.</p> 2 <p>Define \"good\" in one sentence</p> <p>If you can't describe success, no framework will save you. Clarity first, tooling second.</p> 3 <p>Start with pass/fail</p> <p>Add nuance only after you've earned it. A binary gate catches more failures than a sophisticated rubric you never run.</p> 4 <p>Calibrate your judges</p> <p>LLM judges are useful. They're also liars with confidence. Calibrate them against humans or don't pretend you measured anything.</p>"},{"location":"agent_playbook/#the-problem-with-generic-evaluation","title":"The Problem with Generic Evaluation","text":"<p>Evaluating AI agents isn't a traditional machine learning task. It's a context-driven problem\u2014requiring more than generic metrics or off-the-shelf tools. Generative agents are unpredictable and can be highly domain-specific, so success demands a tailored approach rooted in:</p> <ul> <li>Deep domain expertise</li> <li>Context-aware error analysis</li> <li>Metrics tied to real business outcomes</li> </ul> <p>There's no silver bullet. One-size-fits-all evaluation leads to mediocrity. The real question is: Is this agent improving at the job it was designed to do?</p> <p>A customer support agent and an onboarding agent may share similarities, but grading them with the same rubric is like judging a painter and a sculptor by identical standards.</p> <p>Stop confusing activity with progress</p> <p>Shipping a new dashboard is not the same as shipping reliability. If you can't point to specific failure modes that got better, you didn't \"improve the agent\" \u2014 you just looked at it differently.</p>"},{"location":"agent_playbook/#four-common-mistakes","title":"Four Common Mistakes","text":"<p>Before diving into the process, avoid these common pitfalls:</p> 1 <p>Not Looking at Your Data</p> <p>The most common and most damaging mistake. Without reviewing raw examples, you're not truly evaluating.</p> 2 <p>Frameworks Before Fundamentals</p> <p>If your first question is \"What framework?\", you're starting in the wrong place.</p> 3 <p>Overreliance on Generic Evals</p> <p>Generic metrics are not a shortcut. If your evals aren't grounded in known errors, you're wasting time.</p> 4 <p>Misusing LLM-as-a-Judge</p> <p>LLMs can assist evaluation, but relying on them without human validation is a recipe for false confidence.</p>"},{"location":"agent_playbook/#1-not-looking-at-your-data","title":"1. Not Looking at Your Data","text":"<p>The most common\u2014and most damaging\u2014mistake is neglecting to examine your data. Despite being fundamental, this step is often skipped.</p> <p>Effective evaluation begins with frictionless, high-quality data exploration and labeling. Without a deep understanding of your agent's failures, any metrics or conclusions you draw are on shaky ground.</p> <p>If you haven't spent time reviewing raw examples, you're not truly evaluating.</p> <p>If this feels \"manual,\" good</p> <p>Start manually. Use spreadsheets. Label failures. If you can't do it by hand for 30 examples, you're not ready to automate it for 30,000.</p>"},{"location":"agent_playbook/#2-frameworks-before-fundamentals","title":"2. Frameworks Before Fundamentals","text":"<p>Jumping to tools or frameworks before establishing a process is a red flag.</p> <p>If your first question is \"What framework should I use?\", you're starting in the wrong place. Evaluation is not a plug-and-play task. You must define what success looks like for your agent. Frameworks are secondary\u2014they should support your workflow, not define it.</p>"},{"location":"agent_playbook/#3-overreliance-on-generic-evaluations","title":"3. Overreliance on Generic Evaluations","text":"<p>Generic metrics are not a shortcut\u2014they're a risk.</p> <p>If your evals aren't grounded in known errors, you're wasting time. Off-the-shelf metrics (e.g., hallucination, toxicity, relevancy) are easy to reach for\u2014but dangerous when misused. Agents are purpose-built. So should your evaluations be.</p> <p>Generic metrics may be useful after understanding your agent's unique failure modes, but using them prematurely becomes a crutch that masks real issues.</p>"},{"location":"agent_playbook/#4-misusing-llm-as-a-judge","title":"4. Misusing LLM-as-a-Judge","text":"<p>Large language models can assist in evaluation, but they must be used responsibly.</p> <p>Relying on LLMs without first validating them against domain expert feedback is a recipe for false confidence. The correct approach:</p> <ol> <li>Build a lightweight review pipeline</li> <li>Have experts label a few dozen examples</li> <li>Align your LLM judge to match that standard</li> </ol> <p>Until you achieve strong agreement between human and model judgment, LLM outputs should be treated as advisory\u2014not definitive.</p> <p>Don't confuse \"a score\" with \"truth\"</p> <p>LLM judges are great at sounding right. That's the problem. If you didn't calibrate against humans, you're just grading your homework with the same model class.</p>"},{"location":"agent_playbook/#the-analyze-measure-improve-methodology","title":"The Analyze-Measure-Improve Methodology","text":"<p>The AMI cycle isn't new\u2014it's long been used in traditional ML. But with generative agents, the game has changed. We've moved from predictable single-token outputs to freeform systems with far greater complexity.</p> <p>If you approach evaluation as a simple side-by-side comparison or a table of results, you'll miss the deeper nuances in your data. True progress requires going beyond surface-level metrics to uncover root causes and meaningful insights.</p> <p>This is just data science (with a different mindset)</p> <p>The best agent eval workflows look suspiciously like traditional ML evaluation, because they are.</p> <p>Data scientists have been doing this for years:</p> <ul> <li>Curate a representative dataset</li> <li>Label outcomes (often binary)</li> <li>Do error analysis and slice breakdowns</li> <li>Set thresholds, ship changes, measure regressions</li> </ul> <p>What's \"new\" with agents is mostly the mindset shift:</p> <ul> <li>Outputs are messier and more subjective</li> <li>Failures are multi-causal (prompt, retrieval, tools, policy)</li> <li>You need to write down \"good\" explicitly, or your metrics will quietly drift into vibes</li> </ul> <pre><code>graph LR\n    A[\"ANALYZE\"] --&gt;|\"Map what's broken\"| M[\"MEASURE\"]\n    M --&gt;|\"Quantify impact\"| I[\"IMPROVE\"]\n    I --&gt;|\"Validate &amp; iterate\"| A</code></pre> <p>Most AI agent problems aren't solved with a single tweak\u2014they're solved with a disciplined, repeatable process. The AMI lifecycle replaces one-off troubleshooting with a data-driven, continuous improvement loop that:</p> <ul> <li>Cuts failures</li> <li>Speeds time to value</li> <li>Manages risk</li> <li>Drives measurable gains</li> </ul>"},{"location":"agent_playbook/#phase-1-analyze","title":"Phase 1: ANALYZE","text":"<p>Outcome: A clear map of what's broken and why.</p> <ol> <li>Collect representative agent interactions from real-world or test environments</li> <li>Spot and categorize failure patterns (e.g., misunderstood prompts, inconsistent answers)</li> <li>Trace root causes: Specification Issues (prompt ambiguity) vs. Generalization Issues (context mismatch)</li> </ol>"},{"location":"agent_playbook/#phase-2-measure","title":"Phase 2: MEASURE","text":"<p>Outcome: Solid baselines and clear priorities grounded in data.</p> <ol> <li>Build evaluators targeted at each failure type</li> <li>Quantify how often and how badly issues occur across scenarios</li> <li>Prioritize fixes by business impact, surfaced in dashboards everyone can trust</li> </ol>"},{"location":"agent_playbook/#phase-3-improve","title":"Phase 3: IMPROVE","text":"<p>Outcome: Fixes that address root causes\u2014not just symptoms.</p> <ol> <li>Tighten prompts, definitions, and examples to fix Specification Issues</li> <li>Enrich data and refine model/prompt pipelines for Generalization Issues</li> <li>Roll out changes in controlled environments and validate improvements with metrics</li> </ol> <p>And do it again, and again, and again. Once you can run this loop on purpose, then (and only then) scale it with tooling. Don't stop looking at the data just because you hired a dashboard.</p>"},{"location":"agent_playbook/#dataset-best-practices","title":"Dataset Best Practices","text":"<p>A strong evaluation starts with the dataset. Your dataset should be grounded in your agent's errors.</p>"},{"location":"agent_playbook/#diversity-matters","title":"Diversity Matters","text":"<ul> <li>Comprehensive Testing: Covers a wide range of situations so the AI is evaluated fairly</li> <li>Realistic Interactions: Reflects actual user behavior for relevant evaluations</li> <li>Weakness Discovery: Surfaces areas where the AI struggles or produces errors</li> </ul>"},{"location":"agent_playbook/#how-many-examples-do-you-need","title":"How Many Examples Do You Need?","text":"<p>Start with ~30 examples. Continue adding until no new failure modes appear. Stop when additional examples stop revealing new insights.</p> <p>Treat your dataset like a crash report backlog</p> <p>Every new failure case is a free bug report. Collect them. Name them. Fix them. If you can't describe the failure in plain English, you can't evaluate it.</p>"},{"location":"agent_playbook/#coverage-dimensions","title":"Coverage Dimensions","text":"<p>Your golden dataset must be a representative sample of the overall population of queries your end users actually ask:</p> Dimension What to Cover Intent Coverage All major intents + common \"out-of-scope\" questions Utterance Variation Formal, casual, slang, misspellings, abbreviations, verbose Complexity Multi-turn dialogues, corrections, partial information Sentiment &amp; Tone Frustrated, urgent, confused, polite interactions Edge Cases Rare but valid queries, known failure modes"},{"location":"agent_playbook/#dataset-lifecycle","title":"Dataset Lifecycle","text":"Phase Focus Formation Curate high-quality real-world utterances with expert validation Maintenance Continuous review cycles with clear governance Expansion Controlled generation and targeted mining of edge cases"},{"location":"agent_playbook/#defining-evaluation-criteria","title":"Defining Evaluation Criteria","text":"<p>Binary pass/fail judgments are important, but often not enough. Each judgment must be paired with a detailed critique.</p>"},{"location":"agent_playbook/#why-critiques-matter","title":"Why Critiques Matter","text":"<ul> <li>Capture Nuances: Note if something was mostly correct but had areas for improvement</li> <li>Guide Improvement: Provide specific insights into how the AI can be enhanced</li> <li>Balance Simplicity with Depth: Pass/fail offers a clear verdict; critique offers reasoning</li> </ul>"},{"location":"agent_playbook/#example-help-agent-evaluation","title":"Example: Help Agent Evaluation","text":"Interaction Judgment Critique User asks how to create a custom field. Agent provides accurate step-by-step navigation. Pass Technically correct and actionable. However, missed opportunities to check permissions first or ask about field type requirements. Met primary need\u2014passes with enhancement opportunities noted. User asks about lead scoring. Agent says \"Contact your administrator.\" Fail Made assumptions without checking org capabilities. Failed to explore alternatives or explain what lead scoring entails. Dismissive rather than helpful. Due to poor investigation and lack of solutions\u2014fails. <p>The critique should be detailed enough to use in a few-shot prompt for an LLM judge.</p> <p>Pass/fail is the seatbelt latch</p> <p>Pass/fail gives you clarity and velocity. Critiques tell you what to change so tomorrow's run is better than today's.</p>"},{"location":"agent_playbook/#error-classification","title":"Error Classification","text":"<p>Once you know where the errors are, perform an error analysis to identify root causes. The most effective approach is manually classifying examples using a spreadsheet.</p>"},{"location":"agent_playbook/#example-classification","title":"Example Classification","text":"Root Cause Count Percentage Insufficient Workflow Guidance 12 40% Ignoring User Context 8 27% Generic Documentation Links 6 20% Missing Permission Checks 4 13% <p>Now you know where to focus your efforts. This classification helps identify whether the problem lies in:</p> <ul> <li>Prompt engineering</li> <li>Knowledge retrieval</li> <li>Contextual reasoning</li> </ul>"},{"location":"agent_playbook/#metrics-philosophy","title":"Metrics Philosophy","text":""},{"location":"agent_playbook/#metrics-without-actionable-meaning-are-worthless","title":"Metrics Without Actionable Meaning are Worthless","text":"<p>Observability and evaluation are not the same thing. Observability tells you what happened; evaluation is about making agents better\u2014an iterative loop of analyzing, measuring, and improving.</p>"},{"location":"agent_playbook/#why-likert-scales-fail","title":"Why Likert Scales Fail","text":"Problem Explanation Numbers Aren't Actionable A \"3\" doesn't tell you how to improve Subjectivity Evaluators disagree on what qualifies as \"minor inaccuracies\" vs \"some factual errors\" Metrics Rarely Reflect Expert Judgment Domain experts make binary calls\u2014approve vs. reject"},{"location":"agent_playbook/#the-better-approach","title":"The Better Approach","text":"<p>Binary outcomes paired with clear critiques surface what actually matters and provide a direct path for improvement.</p> <p>Rule of Thumb: If you can't clearly explain what a score of 0.8 means\u2014or what action it implies\u2014it's not a useful metric.</p>"},{"location":"agent_playbook/#metric-selection-guide","title":"Metric Selection Guide","text":"<p>The most common failure mode is \"Metric Spaghetti\"\u2014throwing every metric at every test case. This burns tokens, increases latency, and creates noise.</p> <p>Pick the few that matter</p> <p>Start with 2\u20135 metrics that directly map to real failures. If you can't explain why a metric exists in one sentence, delete it.</p>"},{"location":"agent_playbook/#selection-by-dimension","title":"Selection by Dimension","text":""},{"location":"agent_playbook/#retrieval-context-did-we-find-the-right-data","title":"Retrieval &amp; Context (Did we find the right data?)","text":"Metric Type Use When <code>HitRateAtK</code> Math \"Did the right doc appear in the top 5?\" (Binary) <code>MeanReciprocalRank</code> Math You care about position <code>ContextualRelevancy</code> LLM No ground truth ranking\u2014ask \"Is this chunk garbage?\" <code>ContextualSufficiency</code> LLM Check if retrieved chunks actually contain the answer"},{"location":"agent_playbook/#response-quality-is-the-answer-correct","title":"Response Quality (Is the answer correct?)","text":"Metric Type Use When <code>FactualAccuracy</code> LLM You have a Golden Answer <code>Faithfulness</code> LLM RAG essential\u2014checks hallucination <code>AnswerCompleteness</code> LLM User asked 3 sub-questions\u2014did agent answer all 3? <code>AnswerCriteria</code> LLM You have business rules to verify"},{"location":"agent_playbook/#agent-behavior-is-the-interaction-working","title":"Agent Behavior (Is the interaction working?)","text":"Metric Type Use When <code>AnswerRelevancy</code> LLM \"Evasion Detector\"\u2014penalizes off-topic answers <code>ConversationEfficiency</code> LLM Penalizes loops (agent asks for same info twice) <code>GoalCompletion</code> LLM Did user achieve what they came for?"},{"location":"agent_playbook/#style-trust","title":"Style &amp; Trust","text":"Metric Type Use When <code>ToneStyleConsistency</code> LLM Does it match writing style of reference? <code>CitationPresence</code> Heuristic Compliance\u2014checks if citations exist <code>CitationRelevancy</code> LLM Do citations actually support the claim?"},{"location":"agent_playbook/#llm-as-a-judge-best-practices","title":"LLM-as-a-Judge Best Practices","text":""},{"location":"agent_playbook/#the-anti-pattern","title":"The Anti-Pattern","text":"<pre><code>You are an evaluation expert. Judge the correctness based on:\n1. Accuracy &amp; Factual Correctness\n2. Relevance &amp; Completeness\n3. Clarity &amp; Coherence\n\nGive me a score between 1 and 10 and a reason.\n</code></pre> <p>Why this fails:</p> <ul> <li>What's the difference between a 7 and an 8?</li> <li>Five dimensions collapsed into one number</li> <li>Score before reasoning (LLMs reason token-by-token)</li> <li>No examples to ground the model</li> </ul>"},{"location":"agent_playbook/#the-better-pattern","title":"The Better Pattern","text":"<p>Separation of Concerns: Split into two steps.</p> <ol> <li>Extraction: Parse what was said (no judgment yet)</li> <li>Verification: Boolean logic\u2014\"Does X exist in Y?\"</li> </ol> <p>Atomic Statements: Break compound sentences into individual facts.</p> <pre><code># Instead of scoring \"The product has feature A and feature B\"\nstatements = [\n    \"The product has feature A.\",      # Judge: 1 (True)\n    \"The product has feature B.\"       # Judge: 0 (False)\n]\n# Result: 50% accuracy (mathematical, not vibes)\n</code></pre> <p>Chain of Thought: Force analysis before the score.</p> <p>Few-Shot Examples: Always provide examples of good and bad responses with correct scores.</p>"},{"location":"agent_playbook/#judge-model-selection","title":"Judge Model Selection","text":"<p>The teacher must be smarter than the student.</p> <p>You cannot evaluate a GPT-4o agent using a GPT-3.5-Turbo judge. The judge must have reasoning capabilities equal to or greater than the agent being tested.</p> <p>Key principle: \"Cheap on generation, expensive on evaluation.\"</p>"},{"location":"agent_playbook/#summary","title":"Summary","text":"<p>The goal is not to automate evaluation away \u2014 it's to build resilient, domain-aware evaluation systems that measure what truly matters and drive meaningful, measurable improvement. Start simple. Look at your data. Define \"good.\" Then iterate.</p> <p>Evaluation Flywheel  Hierarchical Scoring </p>"},{"location":"evaluation_flywheel/","title":"The Evaluation Flywheel","text":"Lifecycle <p>Build. Test. Deploy.Learn. Repeat.</p> <p>The Evaluation Flywheel is a continuous process for building, testing, deploying, and improving AI models. It's called a \"flywheel\" because feedback from production accelerates improvements in development, building momentum over time.</p> <p>The lifecycle consists of two interconnected loops:</p> 1 <p>Pre-Production (The Lab)</p> <p>Validate before release. Test challengers against baselines using golden datasets and ground truth.</p> 2 <p>Post-Production (The Real World)</p> <p>Confirm value in practice. Monitor drift, measure business impact, and collect user feedback.</p>"},{"location":"evaluation_flywheel/#loop-1-pre-production","title":"Loop 1: Pre-Production","text":"<p>A controlled environment where you test models without affecting real users.</p> <p>Goal: Validate that the new model is better than the current one\u2014and hasn't broken anything that used to work.</p>"},{"location":"evaluation_flywheel/#process","title":"Process","text":"1 <p>Design &amp; Update</p> <p>Create new model versions to address needs or fix problems.</p> 2 <p>Run Experiments</p> <p>Test the \"Challenger\" model against the \"Baseline\" using golden datasets.</p> 3 <p>Measure</p> <p>Quantify results against ground truth with targeted metrics.</p> 4 <p>Analyze</p> <p>Check for improvements, regressions, and safety issues.</p>"},{"location":"evaluation_flywheel/#what-you-need","title":"What You Need","text":"<ul> <li>Golden Datasets \u2014 Curated examples with known correct answers</li> <li>Ground Truth \u2014 The definitive \"right answer\" for each test case</li> </ul>"},{"location":"evaluation_flywheel/#key-metrics","title":"Key Metrics","text":"Metric What It Measures Accuracy How often the model is correct Relevance How well answers match the question Groundedness Whether answers are based on facts Safety Whether outputs avoid harmful content"},{"location":"evaluation_flywheel/#exit-criteria","title":"Exit Criteria","text":"<p>A model leaves this loop only when it:</p> <ul> <li>Passes all safety checks</li> <li>Shows accuracy improvements</li> <li>Has zero regressions on existing capabilities</li> </ul>"},{"location":"evaluation_flywheel/#the-release-gate","title":"The Release Gate","text":"Deployment Decision <p>Between the two loops sits a mandatory checkpoint. Models cannot move to production unless they meet all Loop 1 criteria. Failed models return to the design phase. Passing models get promoted.</p>"},{"location":"evaluation_flywheel/#loop-2-post-production","title":"Loop 2: Post-Production","text":"<p>The live environment where real users interact with your model.</p> <p>Goal: Confirm that Lab results translate to real-world value, and maintain parity between offline and live performance.</p>"},{"location":"evaluation_flywheel/#process_1","title":"Process","text":"1 <p>Deploy &amp; Adapt</p> <p>Release the model and handle real traffic at scale.</p> 2 <p>Monitor</p> <p>Watch for drift when real-world data diverges from training data.</p> 3 <p>Evaluate Value</p> <p>Measure business impact and user outcomes against expectations.</p> 4 <p>Integrate Feedback</p> <p>Collect user signals and analyze usage patterns for the next cycle.</p>"},{"location":"evaluation_flywheel/#what-you-need_1","title":"What You Need","text":"<ul> <li>Session Traces \u2014 Real interaction data from live users</li> <li>Feedback Channels \u2014 User ratings, support tickets, behavioral signals</li> </ul>"},{"location":"evaluation_flywheel/#key-metrics_1","title":"Key Metrics","text":"Metric What It Measures Business KPIs Revenue, conversion, retention impact Usage Adoption, engagement, feature utilization Efficacy Whether users actually solve their problems"},{"location":"evaluation_flywheel/#critical-check-prod-test-parity","title":"Critical Check: Prod-Test Parity","text":"<p>Ask: \"Are live scores matching Lab scores?\"</p> <p>If Lab accuracy was 95% but production accuracy is 70%, something is wrong. This gap signals a problem with your testing methodology or data distribution.</p>"},{"location":"evaluation_flywheel/#success-criteria","title":"Success Criteria","text":"<ul> <li>Positive ROI</li> <li>Production metrics match offline predictions</li> </ul>"},{"location":"evaluation_flywheel/#the-bridge-closing-the-loop","title":"The Bridge: Closing the Loop","text":"<p>The arrow at the bottom of the diagram\u2014The Bridge\u2014is what makes this a flywheel. It feeds real-world data back into the Lab.</p> <pre><code>graph LR\n    A[\"Design &amp; Update\"] --&gt; B[\"Run Experiments\"]\n    B --&gt; C[\"Measure &amp; Analyze\"]\n    C --&gt; D{\"Release Gate\"}\n    D --&gt;|Pass| E[\"Deploy &amp; Monitor\"]\n    D --&gt;|Fail| A\n    E --&gt; F[\"Evaluate &amp; Feedback\"]\n    F --&gt;|\"Bridge\"| A</code></pre> <ul> <li>Sampled logs become training and tuning data</li> <li>Production failures become new test cases</li> </ul> <p>Failures Are Assets</p> <p>Every production failure gets added to your golden datasets. This ensures the next model version is specifically tested against that scenario\u2014preventing the same mistake twice.</p> <p>This continuous feedback loop drives constant improvement. Each cycle through the flywheel makes your evaluation more comprehensive and your models more robust.</p> <p>Agent Evaluation Playbook  Why Ground Truth Matters </p>"},{"location":"why_ground_truth_matters/","title":"Why Ground Truth Matters","text":"Principle <p>Without ground truth,you're grading on vibes.</p> <p>Unlike traditional ML \u2014 where labeled data fuels both training and validation \u2014 AI agents often operate without predefined answers. Curating a high-quality \"golden\" test set isn't just important, it's essential. Ground truth turns subjective performance into an objective, repeatable benchmark.</p> Why It Matters <p>Approximationis not enough.</p> <p>Language models can approximate quality, but in high-stakes and domain-specific environments, approximation is not enough. Ground truth anchors evaluation to a consistent standard: the expected answer (and, when applicable, the expected evidence).</p>"},{"location":"why_ground_truth_matters/#what-goes-wrong-without-ground-truth","title":"What Goes Wrong Without Ground Truth","text":"<p>Even strong evaluation frameworks can fail if they are not anchored to expected outcomes. Three common failure modes:</p> 1 <p>Plausible but wrong</p> <p>The answer sounds correct, but is outdated, incomplete, or subtly incorrect for the domain.</p> 2 <p>The gullible judge</p> <p>LLM judges over-reward \"safe\" answers or fluent answers, scoring linguistic plausibility rather than correctness.</p> 3 <p>The helpful liar</p> <p>Retrieval returns noisy context. The model synthesizes a convincing answer from noise. Similarity-based evals still pass it.</p>"},{"location":"why_ground_truth_matters/#the-core-reasons-ground-truth-matters","title":"The Core Reasons Ground Truth Matters","text":"\u2713 <p>Factual Accuracy</p> <p>Reveals whether an answer is actually correct \u2014 not merely plausible or well-written.</p> \u2713 <p>Relevance &amp; Completeness</p> <p>Clarifies what must be covered and what is irrelevant, preventing \"good-sounding\" partial responses from passing.</p> \u2713 <p>Retrieval Correctness</p> <p>Enables objective checks that the system found and cited the right documents \u2014 separating retrieval failure from generation failure.</p> \u2713 <p>Determinism</p> <p>LLM judging is inherently variable. Ground truth reduces subjectivity and makes scoring repeatable across runs, models, and prompt iterations.</p> \u2713 <p>Benchmarking &amp; Iteration</p> <p>A fixed test set lets you run fair A/B tests, track regressions, and measure improvements with confidence.</p>"},{"location":"why_ground_truth_matters/#ground-truth-dataset-lifecycle","title":"Ground Truth Dataset Lifecycle","text":"1 <p>Formation</p> <p>Curate high-value, real-world utterances. Validate expected outcomes with domain experts.</p> 2 <p>Maintenance</p> <p>Establish review cycles and governance to keep answers current as products, policies, and knowledge evolve.</p> 3 <p>Expansion</p> <p>Grow coverage intentionally \u2014 edge cases, failure clusters, controlled synthetic generation \u2014 without compromising quality.</p>"},{"location":"why_ground_truth_matters/#a-practical-evaluation-approach","title":"A Practical Evaluation Approach","text":"<p>For reliable evaluation, structure matters:</p> <ul> <li>Decompose the workflow into discrete steps (e.g., routing, retrieval, tool-use, generation).</li> <li>Build test cases per step, not just end-to-end: you want to isolate where failures occur.</li> <li>Use hierarchical gates:</li> </ul> <pre><code>graph TD\n    R[\"Retrieval Correctness\"] --&gt;|\"Did we find the right info?\"| G[\"Generation Quality\"]\n    G --&gt;|\"Did we use it correctly?\"| O[\"Overall Correctness\"]\n    O --&gt;|\"Only meaningful if prior gates pass\"| V[\"Validated Result\"]</code></pre>"},{"location":"why_ground_truth_matters/#inconsistent-scoring","title":"Inconsistent Scoring","text":"<p>Why LLM-Only Evaluation Fails</p> <p>LLMs are non-deterministic, and LLM judges can disagree\u2014even on the same answer. Without ground truth, evaluations drift toward subjective heuristics like fluency, verbosity, or \"sounds right.\"</p> <p>Ground truth is the only way to anchor evaluation to objective, expected outcomes. Without it, you're measuring style. With it, you're measuring substance.</p> <p>Evaluation Flywheel  Agent Evaluation Playbook </p>"},{"location":"deep-dives/caliber/example-selector/","title":"Example Selector","text":"<p>The <code>ExampleSelector</code> provides intelligent selection of few-shot examples for LLM-as-judge calibration. Instead of naive slicing (<code>examples[:n]</code>), it offers strategies that improve calibration quality by balancing accept/reject cases, prioritizing misaligned examples, or covering discovered patterns.</p>"},{"location":"deep-dives/caliber/example-selector/#what-youll-learn","title":"What You'll Learn","text":"1 <p>Balanced Selection</p> <p>50/50 accept/reject sampling for unbiased baselines \u2014 the default strategy.</p> 2 <p>Misalignment-Guided</p> <p>Prioritize false positives and false negatives from prior evaluation runs to focus on hard cases.</p> 3 <p>Pattern-Aware</p> <p>Sample from discovered failure patterns to ensure coverage across all failure categories.</p> 4 <p>Metric Integration</p> <p>Convert selected examples to CaliberHQ dict format or Pydantic-based metric format.</p>"},{"location":"deep-dives/caliber/example-selector/#quick-start","title":"Quick Start","text":"<pre><code>from axion.caliber import ExampleSelector, SelectionStrategy\n\n# Initialize with seed for reproducibility\nselector = ExampleSelector(seed=42)\n\n# Basic balanced selection\nresult = selector.select(records, annotations, count=6)\n\nprint(f\"Selected {len(result.examples)} examples\")\nprint(f\"Strategy: {result.strategy_used}\")\nprint(f\"Metadata: {result.metadata}\")\n</code></pre>"},{"location":"deep-dives/caliber/example-selector/#selection-strategies","title":"Selection Strategies","text":"BALANCED (Default) MISALIGNMENT_GUIDED PATTERN_AWARE <p>Selects a 50/50 mix of accept (score=1) and reject (score=0) cases with random sampling. This prevents the LLM judge from being biased toward one outcome.</p> <pre><code>result = selector.select(\n    records=records,\n    annotations=annotations,  # {record_id: 0 or 1}\n    count=6,\n    strategy=SelectionStrategy.BALANCED\n)\n\n# Metadata includes counts\n# {'accepts': 3, 'rejects': 3}\n</code></pre> <p>When to use:</p> <ul> <li>Initial calibration with no prior evaluation data</li> <li>General-purpose few-shot selection</li> <li>When you want unbiased baseline examples</li> </ul> <p>Behavior with imbalanced data:</p> <ul> <li>If all annotations are accepts, returns only accepts</li> <li>If all annotations are rejects, returns only rejects</li> <li>Fills remaining slots from the larger pool when one side is exhausted</li> </ul> <p>Prioritizes cases where the LLM judge disagreed with human annotations. This focuses calibration on the hardest cases \u2014 the examples the judge gets wrong.</p> <pre><code># Requires evaluation results from a prior run\neval_results = [\n    {'id': 'rec_1', 'score': 1},  # LLM predicted 1\n    {'id': 'rec_2', 'score': 0},  # LLM predicted 0\n    # ...\n]\n\nresult = selector.select(\n    records=records,\n    annotations=annotations,\n    count=6,\n    strategy=SelectionStrategy.MISALIGNMENT_GUIDED,\n    eval_results=eval_results\n)\n\n# Metadata shows misalignment stats\n# {\n#     'false_positives_selected': 2,\n#     'false_negatives_selected': 1,\n#     'total_fp_available': 5,\n#     'total_fn_available': 3,\n# }\n</code></pre> <p>When to use:</p> <ul> <li>After running an initial evaluation and seeing low agreement</li> <li>To improve calibration on specific failure modes</li> <li>Iterative refinement workflow</li> </ul> <p>How it works:</p> <ol> <li>Identifies false positives (LLM=1, Human=0) and false negatives (LLM=0, Human=1)</li> <li>Allocates ~1/3 of slots each to FP and FN cases</li> <li>Fills remaining slots with balanced aligned examples</li> </ol> <p>Samples from discovered patterns to ensure coverage across failure categories. Requires results from Pattern Discovery.</p> <pre><code>from axion.caliber import PatternDiscovery\n\n# First, discover patterns in your annotations\ndiscovery = PatternDiscovery(model_name='gpt-4o')\npatterns_result = await discovery.discover(annotated_items)\n\n# Then select examples covering those patterns\nresult = selector.select(\n    records=records,\n    annotations=annotations,\n    count=6,\n    strategy=SelectionStrategy.PATTERN_AWARE,\n    patterns=patterns_result.patterns\n)\n\n# Metadata shows pattern coverage\n# {\n#     'patterns_covered': ['Missing Context', 'Factual Errors', 'Too Brief'],\n#     'total_patterns': 5,\n# }\n</code></pre> <p>When to use:</p> <ul> <li>After Pattern Discovery reveals distinct failure categories</li> <li>To ensure few-shot examples represent all failure types</li> <li>When certain patterns are underrepresented in random selection</li> </ul> <p>How it works:</p> <ol> <li>Takes one example from each discovered pattern (up to <code>count</code>)</li> <li>Fills remaining slots with balanced selection</li> <li>Handles patterns with unknown record IDs gracefully</li> </ol>"},{"location":"deep-dives/caliber/example-selector/#data-format","title":"Data Format","text":"Records Annotations Evaluation Results <p>Records should be a list of dicts with an <code>id</code> or <code>record_id</code> field:</p> <pre><code>records = [\n    {\n        'id': 'rec_1',  # or 'record_id'\n        'query': 'What is Python?',\n        'actual_output': 'A programming language',\n        # ... other fields\n    },\n    # ...\n]\n</code></pre> <p>Annotations map record IDs to binary scores:</p> <pre><code>annotations = {\n    'rec_1': 1,  # Accept\n    'rec_2': 0,  # Reject\n    'rec_3': 1,\n    # ...\n}\n</code></pre> <p>For <code>MISALIGNMENT_GUIDED</code>, provide the LLM's scores:</p> <pre><code>eval_results = [\n    {'id': 'rec_1', 'score': 1},      # 'score' field\n    {'id': 'rec_2', 'llm_score': 0},  # or 'llm_score' field\n    # ...\n]\n</code></pre>"},{"location":"deep-dives/caliber/example-selector/#metric-integration","title":"Metric Integration","text":"<p>ExampleSelector returns generic dicts. The caller converts them to the format expected by their metric.</p>  CaliberHQ (Dict Format) Pydantic-Based Metrics <pre><code>from axion.caliber import ExampleSelector, CaliberMetric\n\nselector = ExampleSelector(seed=42)\nresult = selector.select(records, annotations, count=6)\n\n# Format for CaliberHQ\nexamples = [\n    {\n        'input': {\n            'query': r['query'],\n            'actual_output': r['actual_output'],\n            'expected_output': r.get('expected_output'),\n        },\n        'output': {\n            'score': annotations[r['id']],\n            'reason': r.get('human_reasoning', ''),\n        }\n    }\n    for r in result.examples\n]\n\nevaluator = CaliberMetric(criteria=\"...\", examples=examples)\n</code></pre> <pre><code>from axion.caliber import ExampleSelector\nfrom axion.metrics import Faithfulness\nfrom axion.metrics.faithfulness import FaithfulnessInput\nfrom axion.schema import MetricEvaluationResult\n\nselector = ExampleSelector(seed=42)\nresult = selector.select(records, annotations, count=6)\n\n# Convert to metric-specific Pydantic format\nexamples = [\n    (\n        FaithfulnessInput(\n            query=r['query'],\n            actual_output=r['actual_output'],\n            retrieval_context=r['retrieval_context'],\n        ),\n        MetricEvaluationResult(\n            score=annotations[r['id']],\n            reason=r.get('human_reasoning', ''),\n        )\n    )\n    for r in result.examples\n]\n\nmetric = Faithfulness(examples=examples)\n</code></pre>"},{"location":"deep-dives/caliber/example-selector/#best-practices","title":"Best Practices","text":""},{"location":"deep-dives/caliber/example-selector/#strategy-selection","title":"Strategy Selection","text":"<p>Use this decision tree to pick the right strategy:</p> <pre><code>graph TD\n    Q1[\"Do you have evaluation results from a prior run?\"]\n    Q1 --&gt;|\"Yes\"| Q2[\"Do you have Pattern Discovery results?\"]\n    Q1 --&gt;|\"No\"| B[\"BALANCED&lt;br/&gt;&lt;small&gt;Unbiased baseline&lt;/small&gt;\"]\n    Q2 --&gt;|\"Yes\"| P[\"PATTERN_AWARE&lt;br/&gt;&lt;small&gt;Target failure modes&lt;/small&gt;\"]\n    Q2 --&gt;|\"No\"| M[\"MISALIGNMENT_GUIDED&lt;br/&gt;&lt;small&gt;Focus on hard cases&lt;/small&gt;\"]</code></pre>"},{"location":"deep-dives/caliber/example-selector/#auto-selection-helper","title":"Auto-Selection Helper","text":"<pre><code>def auto_select_strategy(eval_results=None, patterns=None):\n    \"\"\"Automatically choose the best strategy based on available data.\"\"\"\n    if patterns:\n        return SelectionStrategy.PATTERN_AWARE\n    elif eval_results:\n        return SelectionStrategy.MISALIGNMENT_GUIDED\n    else:\n        return SelectionStrategy.BALANCED\n</code></pre>"},{"location":"deep-dives/caliber/example-selector/#reproducibility","title":"Reproducibility","text":"<p>Always set a seed for reproducible results:</p> <pre><code>selector = ExampleSelector(seed=42)\n</code></pre>"},{"location":"deep-dives/caliber/example-selector/#example-count","title":"Example Count","text":"<ul> <li>Start with 4\u20136 examples for initial calibration</li> <li>Increase to 8\u201310 for complex criteria</li> <li>More examples increase cost but may improve calibration</li> </ul> <p>Caliber API Reference  CaliberHQ Guide </p>"},{"location":"deep-dives/caliber/pattern-discovery/","title":"Pattern Discovery","text":"<p>Pattern Discovery clusters free-text evidence into recurring themes and distills them into actionable learning artifacts. It works with any text source \u2014 annotation notes, bug reports, user feedback, support tickets \u2014 and provides a full pipeline from raw text to validated insights.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#what-youll-learn","title":"What You'll Learn","text":"1 <p>Evidence Clustering</p> <p>Cluster any text into semantic themes using LLM, BERTopic, or hybrid methods.</p> 2 <p>Evidence Pipeline</p> <p>Full pipeline: sanitize \u2192 cluster \u2192 distill \u2192 validate \u2192 dedupe \u2192 sink.</p> 3 <p>Learning Artifacts</p> <p>Distill clusters into titled insights with recommended actions and confidence scores.</p> 4 <p>Quality &amp; Safety</p> <p>Validation gates, PII filtering, recurrence checks, and deduplication strategies.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#quick-start","title":"Quick Start","text":""},{"location":"deep-dives/caliber/pattern-discovery/#clustering-only","title":"Clustering Only","text":"<p>Cluster a list of <code>EvidenceItem</code> objects into semantic groups:</p> <pre><code>from axion.caliber import PatternDiscovery, EvidenceItem, ClusteringMethod\n\nevidence = [\n    EvidenceItem(id='bug_1', text='Checkout button unresponsive on mobile Safari'),\n    EvidenceItem(id='bug_2', text='Payment form does not submit on iPhone'),\n    EvidenceItem(id='bug_3', text='Cart total shows wrong currency symbol'),\n    EvidenceItem(id='bug_4', text='Currency formatting broken for EUR prices'),\n]\n\ndiscovery = PatternDiscovery(model_name='gpt-4o-mini', min_category_size=2)\nresult = await discovery.discover_from_evidence(evidence)\n\nfor pattern in result.patterns:\n    print(f'[{pattern.category}] ({pattern.count} items): {pattern.record_ids}')\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#full-pipeline","title":"Full Pipeline","text":"<p>Run the complete pipeline \u2014 clustering, distillation into learning artifacts, validation, and persistence:</p> <pre><code>from axion.caliber import EvidencePipeline, InMemorySink, MetadataConfig\n\nsink = InMemorySink()\n\npipeline = EvidencePipeline(\n    model_name='gpt-4o-mini',\n    recurrence_threshold=2,\n    min_category_size=2,\n    domain_context='E-commerce platform bug triage',\n    sink=sink,\n)\n\nresult = await pipeline.run(evidence)\n\nfor learning in result.learnings:\n    print(f'{learning.title} (confidence={learning.confidence})')\n    for action in learning.recommended_actions:\n        print(f'  - {action}')\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#core-concepts","title":"Core Concepts","text":""},{"location":"deep-dives/caliber/pattern-discovery/#evidenceitem","title":"EvidenceItem","text":"<p>The universal input type. Any text source with an ID, the text itself, optional metadata, and an optional source reference for provenance tracking:</p> <pre><code>EvidenceItem(\n    id='ticket_42',\n    text='Search returns no results for valid product names',\n    metadata={'severity': 'high', 'platform': 'web'},\n    source_ref='jira_PROJ-42',  # for recurrence checking\n)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#clustering-methods","title":"Clustering Methods","text":"Method How it works Best for <code>ClusteringMethod.LLM</code> LLM reads all evidence and outputs semantic clusters Small-medium datasets, best label quality <code>ClusteringMethod.BERTOPIC</code> BERTopic statistical topic modeling, no LLM calls Large datasets, cost-sensitive, 5+ documents <code>ClusteringMethod.HYBRID</code> BERTopic clustering + LLM label refinement Large datasets where you want readable labels <pre><code># LLM-only (default)\nresult = await discovery.discover_from_evidence(evidence, method=ClusteringMethod.LLM)\n\n# BERTopic (no LLM cost)\nresult = await discovery.discover_from_evidence(evidence, method=ClusteringMethod.BERTOPIC)\n\n# Hybrid: BERTopic clusters + LLM-refined labels\nresult = await discovery.discover_from_evidence(evidence, method=ClusteringMethod.HYBRID)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#learning-artifacts","title":"Learning Artifacts","text":"<p>The pipeline distills each cluster into a <code>LearningArtifact</code> \u2014 a structured insight ready for consumption:</p> <pre><code>LearningArtifact(\n    title='Mobile Checkout Failures on iOS',\n    content='Multiple reports of checkout and payment flows...',\n    tags=['ios', 'checkout', 'mobile'],\n    confidence=0.9,\n    supporting_item_ids=['bug_1', 'bug_2', 'feedback_1'],\n    recommended_actions=[\n        'Investigate touch event handling in mobile Safari',\n        'Add fallback submit mechanism for iOS WebKit',\n    ],\n    counterexamples=['bug_3'],       # IDs that look similar but differ\n    scope='iOS mobile checkout flow',\n    when_not_to_apply='Desktop browsers or Android devices',\n)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#evidence-pipeline","title":"Evidence Pipeline","text":"<p>The <code>EvidencePipeline</code> orchestrates the full workflow:</p> <pre><code>graph LR\n    S[\"Sanitize\"] --&gt; C[\"Cluster\"]\n    C --&gt; F[\"Pre-filter\"]\n    F --&gt; D[\"Distill\"]\n    D --&gt; V[\"Validate\"]\n    V --&gt; R[\"Recurrence Check\"]\n    R --&gt; T[\"Tag Normalize\"]\n    T --&gt; DD[\"Dedupe\"]\n    DD --&gt; SK[\"Sink\"]</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#pipeline-stages","title":"Pipeline Stages","text":"Stage What it does Configurable via Sanitize Optional async text cleaning <code>sanitizer=</code> (implements <code>Sanitizer</code> protocol) Cluster Groups evidence into themes <code>method=</code>, <code>clusterer=</code> (implements <code>EvidenceClusterer</code>) Pre-filter Drops clusters below <code>recurrence_threshold</code> <code>recurrence_threshold=</code> Distill LLM converts each cluster into a learning artifact <code>domain_context=</code>, <code>max_concurrent_distillations=</code> Validate Repairs hallucinated IDs, enforces quality gates Automatic Recurrence Ensures learnings span multiple unique sources <code>recurrence_threshold=</code> Tag normalize Lowercase, deduplicate, cap at 10 tags Automatic Dedupe Removes near-duplicate learnings <code>deduper=</code> (implements <code>Deduper</code>) Sink Persists artifacts with provenance metadata <code>sink=</code> (implements <code>ArtifactSink</code>)"},{"location":"deep-dives/caliber/pattern-discovery/#configuration","title":"Configuration","text":"<pre><code>pipeline = EvidencePipeline(\n    # LLM settings\n    model_name='gpt-4o-mini',\n    llm_provider='openai',\n\n    # Clustering\n    method=ClusteringMethod.LLM,\n    min_category_size=2,\n    max_notes=200,\n\n    # Distillation\n    domain_context='Customer support ticket analysis',\n    max_concurrent_distillations=5,\n\n    # Quality gates\n    recurrence_threshold=2,  # need 2+ unique sources per learning\n\n    # Metadata\n    metadata_config=MetadataConfig(\n        include_in_clustering=True,\n        include_in_distillation=True,\n        allowed_keys={'severity', 'platform', 'category'},\n    ),\n\n    # Plugins\n    sink=InMemorySink(),\n    deduper=InMemoryDeduper(),\n)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#pipeline-result","title":"Pipeline Result","text":"<pre><code>result = await pipeline.run(evidence)\n\nresult.clustering_result   # PatternDiscoveryResult \u2014 the raw clusters\nresult.learnings           # list[LearningArtifact] \u2014 distilled insights\nresult.filtered_count      # clusters dropped by pre-filter\nresult.deduplicated_count  # learnings removed by deduper\nresult.validation_repairs  # count of hallucinated-ID repairs\nresult.sink_ids            # IDs returned by the sink after persistence\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#metadata-handling","title":"Metadata Handling","text":""},{"location":"deep-dives/caliber/pattern-discovery/#metadataconfig","title":"MetadataConfig","text":"<p>Control how metadata flows through the pipeline:</p> <pre><code>from axion.caliber import MetadataConfig\n\nconfig = MetadataConfig(\n    include_in_clustering=True,      # attach metadata to clustering prompts\n    include_in_distillation=True,    # attach metadata to distillation prompts\n    allowed_keys={'severity', 'platform'},  # only these keys pass through\n    denied_keys=set(),               # explicit denylist (PII defaults apply)\n)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#pii-safety","title":"PII Safety","text":"<p>By default, metadata keys matching common PII patterns are excluded:</p> <p><code>email</code>, <code>phone</code>, <code>name</code>, <code>address</code>, <code>ssn</code>, <code>password</code>, <code>token</code>, <code>credit_card</code></p> <p>These are filtered before any data reaches an LLM. You can customize the denylist via <code>MetadataConfig.denied_keys</code>.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#metadata-in-clusters","title":"Metadata in Clusters","text":"<p>When <code>include_in_clustering=True</code>, each evidence note sent to the LLM includes a compact header:</p> <pre><code>[meta: severity=high, platform=ios] Checkout button unresponsive on mobile Safari\n</code></pre> <p>During distillation, cluster-level metadata is aggregated into frequency distributions so the LLM sees patterns like \"severity: high (3), medium (1)\".</p>"},{"location":"deep-dives/caliber/pattern-discovery/#plugins","title":"Plugins","text":""},{"location":"deep-dives/caliber/pattern-discovery/#sinks-output-storage","title":"Sinks (Output Storage)","text":"InMemorySink JsonlSink Custom Sink <p>Dict-based storage, ideal for testing and notebooks:</p> <pre><code>from axion.caliber import InMemorySink\n\nsink = InMemorySink()\nresult = await pipeline.run(evidence)\n\nfor sid, entry in sink.artifacts.items():\n    print(entry['artifact'].title, entry['provenance'].timestamp)\n</code></pre> <p>Appends each artifact as a JSON line to a file:</p> <pre><code>from axion.caliber import JsonlSink\n\nsink = JsonlSink(path='learnings.jsonl')\nresult = await pipeline.run(evidence)\n# Each learning is appended as one JSON line\n</code></pre> <p>Implement the <code>ArtifactSink</code> protocol:</p> <pre><code>from axion.caliber import ArtifactSink, LearningArtifact, Provenance\n\nclass DatabaseSink:\n    async def write(\n        self,\n        artifact: LearningArtifact,\n        provenance: Provenance,\n    ) -&gt; str:\n        # persist to your database\n        return record_id\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#dedupers","title":"Dedupers","text":"InMemoryDeduper EmbeddingDeduper <p>Case-insensitive title matching \u2014 lightweight, no external dependencies:</p> <pre><code>from axion.caliber import InMemoryDeduper\n\ndeduper = InMemoryDeduper()\n</code></pre> <p>Cosine similarity on OpenAI embeddings (threshold defaults to 0.85):</p> <pre><code>from axion.caliber import EmbeddingDeduper\n\ndeduper = EmbeddingDeduper(threshold=0.85)  # requires axion[embeddings]\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#validation-quality-gates","title":"Validation &amp; Quality Gates","text":"<p>The pipeline automatically validates every learning artifact:</p>"},{"location":"deep-dives/caliber/pattern-discovery/#hallucinated-id-repair","title":"Hallucinated ID Repair","text":"<p>If the LLM references item IDs that don't exist in the cluster, they are silently removed. If all IDs are invalid, the learning is dropped entirely.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#confidence-demotion","title":"Confidence Demotion","text":"<p>Learnings with <code>confidence &gt;= 0.7</code> but no recommended actions are demoted to <code>confidence = 0.69</code> \u2014 the reasoning being that a high-confidence insight should be actionable.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#recurrence-checking","title":"Recurrence Checking","text":"<p>A learning must be backed by evidence from multiple unique sources (controlled by <code>recurrence_threshold</code>). Two chunks from the same conversation count as one source \u2014 the check uses <code>source_ref</code> to deduplicate.</p> <pre><code># Standalone usage\nfrom axion.caliber.pattern_discovery._utils import validate_learning\n\nrepaired, repair_count = validate_learning(raw_learning, cluster_item_ids)\n</code></pre>"},{"location":"deep-dives/caliber/pattern-discovery/#legacy-api","title":"Legacy API","text":"<p>The original <code>discover()</code> method still works for annotation-based workflows:</p> <pre><code>from axion.caliber import PatternDiscovery, AnnotatedItem\n\nannotations = {\n    'rec_1': AnnotatedItem(record_id='rec_1', score=0, notes='Missing context'),\n    'rec_2': AnnotatedItem(record_id='rec_2', score=0, notes='Too verbose'),\n}\n\ndiscovery = PatternDiscovery(model_name='gpt-4o-mini')\nresult = await discovery.discover(annotations, method=ClusteringMethod.LLM)\n</code></pre> <p>This is equivalent to converting annotations to <code>EvidenceItem</code> objects and calling <code>discover_from_evidence()</code>. The conversion happens automatically via the compatibility layer.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#using-with-evaluation-issues","title":"Using with Evaluation Issues","text":"<p>The <code>InsightExtractor</code> bridges evaluation issues with this pipeline. It converts <code>ExtractedIssue</code> objects (from the Issue Extractor) into <code>EvidenceItem</code> objects and runs them through <code>EvidencePipeline</code> to discover cross-metric patterns:</p> <pre><code>from axion.reporting import IssueExtractor, InsightExtractor\n\nissues = IssueExtractor().extract_from_evaluation(results)\ninsights = await InsightExtractor(model_name='gpt-4o-mini').analyze(issues)\n\nfor pattern in insights.patterns:\n    if pattern.is_cross_metric:\n        print(f\"{pattern.category}: {', '.join(pattern.metrics_involved)}\")\n</code></pre> <p>See the Cross-Metric Insight Extraction Guide for full details.</p>"},{"location":"deep-dives/caliber/pattern-discovery/#api-summary","title":"API Summary","text":"API Input Output Use when <code>PatternDiscovery.discover()</code> <code>Dict[str, AnnotatedItem]</code> <code>PatternDiscoveryResult</code> You have annotation notes from human evaluators <code>PatternDiscovery.discover_from_evidence()</code> <code>Sequence[EvidenceItem]</code> or <code>Dict</code> <code>PatternDiscoveryResult</code> Clustering only, from any text source <code>EvidencePipeline.run()</code> <code>Sequence[EvidenceItem]</code> or <code>Dict</code> <code>PipelineResult</code> Full pipeline: cluster + distill + validate + dedupe + sink <code>InsightExtractor.analyze()</code> <code>IssueExtractionResult</code> <code>InsightResult</code> Cross-metric pattern discovery from evaluation issues <p>Caliber API Reference  CaliberHQ Guide  Example Selector </p>"},{"location":"deep-dives/internals/environment/","title":"Axion Environment &amp; Settings","text":"<p>This guide details the configuration management system for Axion, powered by pydantic-settings.</p>"},{"location":"deep-dives/internals/environment/#core-concepts","title":"Core Concepts","text":"<p>The configuration system is built on several key principles:</p> <p>Schema-First Design: The <code>AxionConfig</code> class serves as the central schema. It's a Pydantic <code>BaseModel</code> that defines the shape of the configuration\u2014all available fields, their types, and default values.</p> <p>Environment Variable Loading: Settings are automatically loaded from environment variables and <code>.env</code> files using pydantic-settings.</p> <p>Centralized Access: A single, global <code>settings</code> object is created at startup. This object is the source of truth for all configuration values throughout the application.</p>"},{"location":"deep-dives/internals/environment/#configuration-loading","title":"Configuration Loading","text":"<p>The system loads settings with a clear order of precedence:</p> <ol> <li>Environment Variables: System environment variables (highest priority)</li> <li>.env File: Values loaded from the <code>.env</code> file</li> <li>Default Values: Default values defined in the <code>AxionConfig</code> schema (lowest priority)</li> </ol>"},{"location":"deep-dives/internals/environment/#env-file-discovery","title":".env File Discovery","text":"<p>The system automatically discovers your <code>.env</code> file:</p> <ol> <li>First checks for an explicit <code>ENV_PATH</code> environment variable</li> <li>Falls back to <code>find_dotenv()</code>, which searches the current and parent directories</li> </ol>"},{"location":"deep-dives/internals/environment/#configuration-schema","title":"Configuration Schema","text":"<p>The following settings are available. Environment variables are case-insensitive.</p>"},{"location":"deep-dives/internals/environment/#general-settings","title":"General Settings","text":"Setting Environment Variable Default Description <code>debug</code> <code>DEBUG</code> <code>False</code> Enable debug mode for verbose logging and diagnostics. <code>port</code> <code>PORT</code> <code>8000</code> The port the application will run on. <code>hosts</code> <code>HOSTS</code> <code>['localhost']</code> A list of allowed hostnames."},{"location":"deep-dives/internals/environment/#llm-settings","title":"LLM Settings","text":"Setting Environment Variable Default Description <code>llm_provider</code> <code>LLM_PROVIDER</code> <code>'openai'</code> Default provider for language models. <code>embedding_provider</code> <code>EMBEDDING_PROVIDER</code> <code>'openai'</code> Default provider for embedding models. <code>llm_model_name</code> <code>LLM_MODEL_NAME</code> <code>'gpt-4o'</code> Default language model name. <code>embedding_model_name</code> <code>EMBEDDING_MODEL_NAME</code> <code>'text-embedding-ada-002'</code> Default embedding model name. <code>api_base_url</code> <code>API_BASE_URL</code> <code>None</code> Optional base URL for OpenAI-compatible APIs. <code>litellm_verbose</code> <code>LITELLM_VERBOSE</code> <code>False</code> Enable verbose logging for LiteLLM debugging."},{"location":"deep-dives/internals/environment/#api-key-settings","title":"API Key Settings","text":"Setting Environment Variable Description <code>openai_api_key</code> <code>OPENAI_API_KEY</code> API key for OpenAI models. <code>anthropic_api_key</code> <code>ANTHROPIC_API_KEY</code> API key for Anthropic Claude models. <code>google_api_key</code> <code>GOOGLE_API_KEY</code> API key for Google Gemini models."},{"location":"deep-dives/internals/environment/#google-vertex-ai-settings","title":"Google Vertex AI Settings","text":"Setting Environment Variable Description <code>vertex_project</code> <code>VERTEXAI_PROJECT</code> GCP project ID for Vertex AI. <code>vertex_location</code> <code>VERTEXAI_LOCATION</code> GCP region for Vertex AI (e.g., <code>us-central1</code>). <code>vertex_credentials</code> <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to GCP service account JSON file."},{"location":"deep-dives/internals/environment/#web-search-api-keys","title":"Web Search API Keys","text":"Setting Environment Variable Description <code>serpapi_key</code> <code>SERPAPI_KEY</code> API key for SerpAPI. <code>ydc_api_key</code> <code>YDC_API_KEY</code> API key for You.com Search API. <code>tavily_api_key</code> <code>TAVILY_API_KEY</code> API key for Tavily Search API."},{"location":"deep-dives/internals/environment/#knowledge-settings","title":"Knowledge Settings","text":"Setting Environment Variable Description <code>llama_parse_api_key</code> <code>LLAMA_PARSE_API_KEY</code> API key for LlamaParse. <code>google_credentials_path</code> <code>GOOGLE_CREDENTIALS_PATH</code> Path to Google Credentials JSON file."},{"location":"deep-dives/internals/environment/#logging-settings","title":"Logging Settings","text":"Setting Environment Variable Default Description <code>log_level</code> <code>LOG_LEVEL</code> <code>'INFO'</code> The minimum logging level (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). <code>log_use_rich</code> <code>LOG_USE_RICH</code> <code>True</code> Use rich for beautiful console output. <code>log_format_string</code> <code>LOG_FORMAT_STRING</code> <code>None</code> A custom format string for the console logger. <code>log_file_path</code> <code>LOG_FILE_PATH</code> <code>None</code> If set, logs will also be written to this file."},{"location":"deep-dives/internals/environment/#tracing-settings","title":"Tracing Settings","text":"Setting Environment Variable Default Description <code>tracing_mode</code> <code>TRACING_MODE</code> <code>'noop'</code> Controls tracing provider (see below). <p>Available Tracing Modes:</p> Mode Description <code>noop</code> Disabled (zero overhead). Default. <code>logfire</code> Pydantic Logfire for OpenTelemetry-based observability. <code>otel</code> Generic OpenTelemetry exporter. <code>langfuse</code> Langfuse LLM observability with cost tracking. <code>opik</code> Comet Opik LLM observability."},{"location":"deep-dives/internals/environment/#logfire-settings","title":"Logfire Settings","text":"Setting Environment Variable Default Description <code>logfire_token</code> <code>LOGFIRE_TOKEN</code> <code>None</code> API token for Logfire hosted mode. <code>logfire_service_name</code> <code>LOGFIRE_SERVICE_NAME</code> <code>'axion'</code> The service name that appears in Logfire. <code>logfire_project_name</code> <code>LOGFIRE_PROJECT</code> <code>None</code> Optional project name for Logfire. <code>logfire_distributed_tracing</code> <code>DISTRIBUTED_TRACING</code> <code>True</code> Toggles distributed tracing. <code>logfire_console_logging</code> <code>CONSOLE_LOGGING</code> <code>False</code> Toggles Logfire's console logging. <code>otel_endpoint</code> <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> <code>None</code> Custom OpenTelemetry endpoint."},{"location":"deep-dives/internals/environment/#langfuse-settings","title":"Langfuse Settings","text":"Setting Environment Variable Default Description <code>langfuse_public_key</code> <code>LANGFUSE_PUBLIC_KEY</code> <code>None</code> Langfuse public key for authentication. <code>langfuse_secret_key</code> <code>LANGFUSE_SECRET_KEY</code> <code>None</code> Langfuse secret key for authentication. <code>langfuse_base_url</code> <code>LANGFUSE_BASE_URL</code> <code>'https://cloud.langfuse.com'</code> Langfuse API endpoint."},{"location":"deep-dives/internals/environment/#opik-settings","title":"Opik Settings","text":"Setting Environment Variable Default Description <code>opik_api_key</code> <code>OPIK_API_KEY</code> <code>None</code> Opik API key for authentication. <code>opik_workspace</code> <code>OPIK_WORKSPACE</code> <code>None</code> Opik workspace name. <code>opik_project_name</code> <code>OPIK_PROJECT_NAME</code> <code>'axion'</code> Opik project name for grouping traces. <code>opik_base_url</code> <code>OPIK_URL_OVERRIDE</code> <code>'https://www.comet.com/opik/api'</code> Opik API endpoint."},{"location":"deep-dives/internals/environment/#usage-in-code","title":"Usage in Code","text":""},{"location":"deep-dives/internals/environment/#accessing-settings","title":"Accessing Settings","text":"<p>Import the global <code>settings</code> object to access configuration values:</p> <pre><code>from axion._core.environment import settings\n\ndef some_function():\n    # Access settings directly\n    if settings.debug:\n        print(\"Debug mode is enabled.\")\n\n    print(f\"Using LLM Provider: {settings.llm_provider}\")\n    print(f\"Default Model: {settings.llm_model_name}\")\n</code></pre>"},{"location":"deep-dives/internals/environment/#resolving-api-keys","title":"Resolving API Keys","text":"<p>Use <code>resolve_api_key()</code> to get API keys with proper fallback:</p> <pre><code>from axion._core.environment import resolve_api_key\n\n# Prioritizes direct argument, falls back to settings\napi_key = resolve_api_key(\n    api_key=None,  # or pass explicit key\n    key_name='tavily_api_key',\n    service_name='Tavily'\n)\n</code></pre>"},{"location":"deep-dives/internals/environment/#auto-detecting-tracing-provider","title":"Auto-Detecting Tracing Provider","text":"<p>The system can auto-detect the tracing provider from environment variables:</p> <pre><code>from axion._core.environment import detect_tracing_provider, list_tracing_providers\n\n# Auto-detect based on which API keys are set\nprovider = detect_tracing_provider()\nprint(f\"Detected provider: {provider}\")\n\n# List all available providers\nproviders = list_tracing_providers()\n# ['noop', 'logfire', 'otel', 'langfuse', 'opik']\n</code></pre> <p>Auto-detection priority:</p> <ol> <li>Explicit <code>TRACING_MODE</code> environment variable</li> <li><code>LANGFUSE_SECRET_KEY</code> set \u2192 <code>'langfuse'</code></li> <li><code>OPIK_API_KEY</code> set \u2192 <code>'opik'</code></li> <li><code>LOGFIRE_TOKEN</code> set \u2192 <code>'logfire'</code></li> <li><code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> set \u2192 <code>'otel'</code></li> <li>Default \u2192 <code>'noop'</code></li> </ol>"},{"location":"deep-dives/internals/environment/#environment-variable-examples","title":"Environment Variable Examples","text":"Basic Langfuse Logfire Multi-Provider Search <pre><code># .env file\nDEBUG=false\nLOG_LEVEL=INFO\nLLM_PROVIDER=openai\nLLM_MODEL_NAME=gpt-4o\nOPENAI_API_KEY=sk-your-api-key\n</code></pre> <pre><code># .env file\nOPENAI_API_KEY=sk-your-api-key\nLANGFUSE_PUBLIC_KEY=pk-lf-xxx\nLANGFUSE_SECRET_KEY=sk-lf-xxx\n# TRACING_MODE is auto-detected from LANGFUSE_SECRET_KEY\n</code></pre> <pre><code># .env file\nOPENAI_API_KEY=sk-your-api-key\nLOGFIRE_TOKEN=your-logfire-token\nLOGFIRE_SERVICE_NAME=my-app\n# TRACING_MODE is auto-detected from LOGFIRE_TOKEN\n</code></pre> <pre><code># .env file\nOPENAI_API_KEY=sk-your-openai-key\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\nGOOGLE_API_KEY=your-google-key\n\n# Vertex AI\nVERTEXAI_PROJECT=my-gcp-project\nVERTEXAI_LOCATION=us-central1\nGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre> <pre><code># .env file\nTAVILY_API_KEY=tvly-xxx\nSERPAPI_KEY=xxx\nYDC_API_KEY=xxx\n</code></pre>"},{"location":"deep-dives/internals/environment/#user-extension-namespace","title":"User Extension Namespace","text":"<p>The settings object includes an <code>ext</code> dictionary for custom user settings:</p> <pre><code>from axion._core.environment import settings\n\n# Access custom settings\ncustom_value = settings.ext.get('my_custom_setting', 'default')\n</code></pre> <p>Logging  Installation  LLM Providers </p>"},{"location":"deep-dives/internals/logging/","title":"Axion Logging","text":"<p>The Axion logging module provides a rich, informative logging experience with beautiful console output powered by the rich library.</p>"},{"location":"deep-dives/internals/logging/#core-features","title":"Core Features","text":"<ul> <li>Beautiful Output: Colorized log levels, emojis, and formatted tables and tracebacks via the <code>rich</code> library</li> <li>High-Level Methods: Convenient methods like <code>logger.success()</code>, <code>logger.log_table()</code>, and the <code>logger.log_operation()</code> context manager</li> <li>Auto-Configuration: Just use <code>get_logger()</code> - it auto-configures from environment variables on first use</li> <li>File Logging: Optionally write logs to a file in addition to the console</li> </ul>"},{"location":"deep-dives/internals/logging/#basic-usage","title":"Basic Usage","text":"<p>For most use cases, simply import the global <code>logger</code> instance or get a module-specific logger:</p> <pre><code># Option 1: Use the global logger\nfrom axion.logging import logger\n\nlogger.info(\"Hello from Axion!\")\nlogger.success(\"Task completed successfully!\")\n\n# Option 2: Get a module-specific logger (recommended)\nfrom axion.logging import get_logger\n\nlogger = get_logger(__name__)\nlogger.warning(\"This warning came from the '%s' module.\", __name__)\n</code></pre>"},{"location":"deep-dives/internals/logging/#configuration","title":"Configuration","text":"<p>Configuration is handled via environment variables or by calling <code>configure_logging()</code> directly.</p>"},{"location":"deep-dives/internals/logging/#environment-variables","title":"Environment Variables","text":"Setting Environment Variable Default Description <code>log_level</code> <code>LOG_LEVEL</code> <code>'INFO'</code> Minimum logging level (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). <code>log_use_rich</code> <code>LOG_USE_RICH</code> <code>True</code> Use rich for beautiful console output. <code>log_format_string</code> <code>LOG_FORMAT_STRING</code> <code>None</code> A custom format string for the logger. <code>log_file_path</code> <code>LOG_FILE_PATH</code> <code>None</code> If set, logs will be written to this file."},{"location":"deep-dives/internals/logging/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Call <code>configure_logging()</code> once at application startup, or let it auto-configure on first use.</p>  Zero-Config (Recommended) Explicit Configuration Reconfiguration <p>Just use <code>get_logger()</code> - it auto-configures from environment variables on first use.</p> <pre><code>from axion.logging import get_logger\n\n# Auto-configures from LOG_LEVEL, LOG_USE_RICH, etc.\nlogger = get_logger(__name__)\nlogger.info(\"Using the configuration from the environment.\")\n</code></pre> <p>Call <code>configure_logging()</code> before getting loggers to override defaults.</p> <pre><code>from axion.logging import configure_logging, get_logger\n\n# Configure first, then get logger\nconfigure_logging(level=\"DEBUG\", use_rich=True)\nlogger = get_logger(__name__)\n\nlogger.debug(\"This debug message is now visible.\")\n</code></pre> <p>Call <code>configure_logging()</code> with new parameters - it applies immediately.</p> <pre><code>from axion.logging import configure_logging, get_logger\n\nlogger = get_logger(__name__)  # Auto-configures to INFO\n\n# Change level on the fly\nconfigure_logging(level=\"DEBUG\")  # Now DEBUG\nconfigure_logging(level=\"ERROR\")  # Now ERROR\n</code></pre> <p>Note</p> <p>Calling <code>configure_logging()</code> with explicit parameters always applies them. Calling with no parameters skips if already configured.</p>"},{"location":"deep-dives/internals/logging/#richlogger-methods","title":"RichLogger Methods","text":"<p>The <code>RichLogger</code> class provides several high-level methods to make your logs more expressive.</p> Method Description <code>logger.success(msg)</code> Logs an info-level message with a checkmark emoji. <code>logger.warning_highlight(msg)</code> Logs a warning-level message with a warning emoji. <code>logger.error_highlight(msg)</code> Logs an error-level message with a cross emoji. <code>logger.log_table(data)</code> Prints a list of dictionaries as a formatted table. <code>logger.log_json(data)</code> Pretty-prints a dictionary or list as JSON. <code>logger.log_performance(operation, duration)</code> Logs performance metrics for an operation. <code>logger.log_exception(e)</code> Logs an exception with traceback. <code>logger.log_operation(name)</code> Context manager that logs start, end, and duration of a code block. <code>logger.async_log_operation(name)</code> Async context manager for logging operations."},{"location":"deep-dives/internals/logging/#examples","title":"Examples","text":""},{"location":"deep-dives/internals/logging/#logging-tables","title":"Logging Tables","text":"<pre><code>from axion.logging import logger\n\nusers = [\n    {\"id\": \"user_1\", \"status\": \"active\", \"last_login\": \"2025-08-02\"},\n    {\"id\": \"user_2\", \"status\": \"inactive\", \"last_login\": \"2025-07-15\"},\n]\nlogger.log_table(users, title=\"User Status\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#timing-operations","title":"Timing Operations","text":"<pre><code>from axion.logging import logger\nimport time\n\nwith logger.log_operation(\"Process User Data\"):\n    # Your code here...\n    time.sleep(0.5)\n\n# Output:\n# \ud83d\udd39 Starting | Process User Data\n# \u2705 Completed | Process User Data in 0.50s\n</code></pre>"},{"location":"deep-dives/internals/logging/#async-operations","title":"Async Operations","text":"<pre><code>from axion.logging import logger\n\nasync def fetch_data():\n    async with logger.async_log_operation(\"Fetch Remote Data\"):\n        # Your async code here...\n        await some_async_operation()\n</code></pre>"},{"location":"deep-dives/internals/logging/#logging-json","title":"Logging JSON","text":"<pre><code>from axion.logging import logger\n\nconfig = {\"model\": \"gpt-4o\", \"temperature\": 0.7}\nlogger.log_json(config, title=\"Model Configuration\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#advanced-usage","title":"Advanced Usage","text":"Module-Specific Loggers Error Handling Conditional Logging Summary <p>For better organization and debugging, create module-specific loggers:</p> <pre><code>from axion.logging import get_logger\n\n# Each module gets its own logger\nlogger = get_logger(__name__)\n\nclass MyService:\n    def __init__(self):\n        self.logger = get_logger(f\"{__name__}.{self.__class__.__name__}\")\n\n    def process_data(self):\n        self.logger.info(\"Starting data processing...\")\n        # Processing logic here\n        self.logger.success(\"Data processing completed!\")\n</code></pre> <pre><code>from axion.logging import logger\n\ntry:\n    risky_operation()\nexcept Exception as e:\n    logger.error_highlight(f\"Operation failed: {e}\")\n    # Rich automatically formats the traceback beautifully\n    logger.exception(\"Full traceback:\")\n</code></pre> <pre><code>import logging\nfrom axion.logging import logger\n\ndef debug_intensive_operation(data):\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.debug(f\"Processing {len(data)} items\")\n        logger.log_json(data[:5])  # Log first 5 items as JSON\n</code></pre> <p>Get a summary of all logging activity during the session:</p> <pre><code>from axion.logging import log_summary\n\n# At the end of your application\nlog_summary()\n\n# Output:\n# --- Logging Summary ---\n# Total Session Runtime: 45.32 seconds\n# Logger 'axion': 25 messages\n#     - INFO: 20\n#     - WARNING: 3\n#     - ERROR: 2\n# Grand Total Messages: 25\n# -----------------------\n</code></pre>"},{"location":"deep-dives/internals/logging/#configuration-state-management","title":"Configuration State Management","text":"<pre><code>from axion.logging import (\n    configure_logging,\n    is_logging_configured,\n    clear_logging_config\n)\n\n# Check if logging is configured\nif is_logging_configured():\n    print(\"Logging already set up\")\n\n# Full reset if needed\nclear_logging_config()\nconfigure_logging(level=\"DEBUG\")\n</code></pre>"},{"location":"deep-dives/internals/logging/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Module-Specific Loggers: Always use <code>get_logger(__name__)</code> for better log organization.</p> </li> <li> <p>Let It Auto-Configure: Just use <code>get_logger()</code> - it configures automatically from environment variables.</p> </li> <li> <p>Configure Before Use: If you need custom settings, call <code>configure_logging()</code> before <code>get_logger()</code>.</p> </li> <li> <p>Use Context Managers: Leverage <code>logger.log_operation()</code> for timing operations.</p> </li> <li> <p>Rich Methods in Development: Use <code>logger.success()</code>, <code>logger.log_table()</code> etc. for better development experience.</p> </li> </ol> <pre><code># Good - Zero config, auto-configures from env vars\nfrom axion.logging import get_logger\nlogger = get_logger(__name__)\n\n# Good - Configure before getting logger\nfrom axion.logging import configure_logging, get_logger\nconfigure_logging(level=\"DEBUG\")\nlogger = get_logger(__name__)\n\n# Good - Using context manager\nwith logger.log_operation(\"Data Migration\"):\n    migrate_users()\n    migrate_products()\n\n# Good - Reconfigure on the fly\nfrom axion.logging import configure_logging\nconfigure_logging(level=\"DEBUG\")  # Always applies with explicit params\n\n# Good - Check configuration state\nfrom axion.logging import is_logging_configured, clear_logging_config\nif is_logging_configured():\n    clear_logging_config()  # Full reset if needed\n</code></pre>"},{"location":"deep-dives/internals/logging/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/internals/logging/#functions","title":"Functions","text":"Function Description <code>get_logger(name)</code> Get a logger instance, auto-configuring on first use. <code>configure_logging(level, use_rich, format_string, file_path)</code> Configure the logging system. <code>is_logging_configured()</code> Check if logging has been configured. <code>clear_logging_config()</code> Clear the logging configuration and reset state. <code>log_summary()</code> Log a summary of all logging activity during the session."},{"location":"deep-dives/internals/logging/#classes","title":"Classes","text":"Class Description <code>RichLogger</code> Custom logger class with enhanced logging methods. <code>LoguruHandler</code> Bridge handler for forwarding logs to loguru (if available). <p>Environment &amp; Settings  Installation </p>"},{"location":"deep-dives/internals/tracing/","title":"Axion Tracing System","text":"<p>Simple observability for AI applications with automatic context management. Supports multiple backends including Logfire (OpenTelemetry), Langfuse, and Opik (Comet) for LLM-specific observability.</p>"},{"location":"deep-dives/internals/tracing/#why-use-axion-tracing","title":"Why Use Axion Tracing?","text":"<ul> <li>Zero setup - Configure once, trace everywhere</li> <li>Automatic context - No manual tracer passing between functions</li> <li>AI-optimized - Built-in support for LLM, evaluation, and knowledge operations</li> <li>Production ready - NOOP mode for zero overhead when tracing is disabled</li> <li>Extensible - Registry pattern makes it easy to add custom tracer providers</li> <li>Multiple backends - Choose between Logfire, Langfuse, or create your own</li> </ul>"},{"location":"deep-dives/internals/tracing/#quick-start","title":"Quick Start","text":"<pre><code>from axion.tracing import init_tracer, trace\n\nclass MyService:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    @trace(name='internal_span', capture_result=True)\n    async def process(self, data: dict):\n        return 100\n\n    @trace(name='span', capture_result=True)\n    async def run(self):\n        # Set manual span for tracing\n        async with self.tracer.async_span(\"manual_span\") as span:\n            # set attribute on span\n            span.set_attribute(\"output_status\", \"success\")\n            return await self.process({\"key\": \"value\"})\n\nawait MyService().run()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#tracing-providers","title":"Tracing Providers","text":"<p>Axion supports four built-in tracing providers, all managed through a unified registry system:</p> Provider Description Use Case <code>noop</code> No-operation tracer with zero overhead Testing, production without tracing <code>logfire</code> OpenTelemetry-based tracing via Logfire General observability, performance monitoring <code>langfuse</code> LLM-specific observability platform LLM cost tracking, prompt management, evaluations <code>opik</code> Comet's open-source LLM observability LLM tracing, cost tracking, evaluations"},{"location":"deep-dives/internals/tracing/#provider-comparison","title":"Provider Comparison","text":"<pre><code>graph TD\n    A[TracerRegistry] --&gt; B[NoOpTracer]\n    A --&gt; C[LogfireTracer]\n    A --&gt; D[LangfuseTracer]\n    A --&gt; E[OpikTracer]\n\n    B --&gt; F[Zero Overhead]\n    C --&gt; G[OpenTelemetry Backend]\n    C --&gt; H[Logfire Cloud/Local UI]\n    D --&gt; I[LLM Observability]\n    D --&gt; J[Cost Tracking]\n    D --&gt; K[Prompt Management]\n    E --&gt; L[Open Source LLM Tracing]\n    E --&gt; M[Comet Integration]</code></pre>"},{"location":"deep-dives/internals/tracing/#configuration","title":"Configuration","text":"<p>Tracing auto-configures from environment variables on first use. Just use <code>Tracer()</code> and it works.</p>"},{"location":"deep-dives/internals/tracing/#environment-variables","title":"Environment Variables","text":"<p>Set <code>TRACING_MODE</code> to select a provider, or let it auto-detect from available credentials:</p> Provider Description Auto-Detection <code>noop</code> Disables all tracing (zero overhead) Default if no credentials found <code>logfire</code> OpenTelemetry via Logfire <code>LOGFIRE_TOKEN</code> present <code>otel</code> Custom OpenTelemetry endpoint <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> present <code>langfuse</code> LLM observability via Langfuse <code>LANGFUSE_SECRET_KEY</code> present <code>opik</code> LLM observability via Opik (Comet) <code>OPIK_API_KEY</code> present <p>Auto-Detection Priority: If <code>TRACING_MODE</code> is not set, the system checks for credentials in this order: 1. <code>LANGFUSE_SECRET_KEY</code> \u2192 uses <code>langfuse</code> 2. <code>OPIK_API_KEY</code> \u2192 uses <code>opik</code> 3. <code>LOGFIRE_TOKEN</code> \u2192 uses <code>logfire</code> 4. <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> \u2192 uses <code>otel</code> 5. Default \u2192 uses <code>noop</code></p>"},{"location":"deep-dives/internals/tracing/#logfire-configuration","title":"Logfire Configuration","text":"<pre><code># For Logfire cloud (recommended)\nTRACING_MODE=logfire\nLOGFIRE_TOKEN=your-logfire-token\n\n# For custom OpenTelemetry endpoint\nTRACING_MODE=otel\nOTEL_EXPORTER_OTLP_ENDPOINT=https://your-otel-endpoint\n</code></pre>"},{"location":"deep-dives/internals/tracing/#langfuse-configuration","title":"Langfuse Configuration","text":"<pre><code>TRACING_MODE=langfuse\nLANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nLANGFUSE_SECRET_KEY=sk-lf-your-secret-key\nLANGFUSE_BASE_URL=https://cloud.langfuse.com  # EU region (default)\n# or https://us.cloud.langfuse.com for US region\n</code></pre>"},{"location":"deep-dives/internals/tracing/#opik-configuration","title":"Opik Configuration","text":"<pre><code>TRACING_MODE=opik\nOPIK_API_KEY=your-opik-api-key\nOPIK_WORKSPACE=your-workspace-name  # Optional\nOPIK_PROJECT_NAME=axion  # Optional, defaults to 'axion'\nOPIK_URL_OVERRIDE=https://www.comet.com/opik/api  # Default (cloud)\n# or http://localhost:5173/api for self-hosted\n</code></pre>"},{"location":"deep-dives/internals/tracing/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Tracing auto-configures on first use of <code>Tracer()</code>. Only call <code>configure_tracing()</code> if you need to override auto-detection.</p> <pre><code>from axion.tracing import configure_tracing, Tracer\n\n# Zero-config - auto-detects from environment variables\ntracer = Tracer('llm')\n\n# Or explicitly configure a provider\nconfigure_tracing(provider='langfuse')\ntracer = Tracer('llm')\n\n# List available providers\nfrom axion.tracing import list_providers\nprint(list_providers())  # ['noop', 'logfire', 'otel', 'langfuse', 'opik']\n\n# Reconfigure (e.g., for testing)\nfrom axion.tracing import clear_tracing_config, is_tracing_configured\n\nif is_tracing_configured():\n    clear_tracing_config()\nconfigure_tracing(provider='noop')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#tracerregistry-architecture","title":"TracerRegistry Architecture","text":"<p>The tracing system uses a decorator-based registry pattern, similar to <code>LLMRegistry</code>. This makes it easy to:</p> <ul> <li>Switch between providers at runtime</li> <li>Add custom tracer implementations</li> <li>Extend functionality without modifying core code</li> </ul>"},{"location":"deep-dives/internals/tracing/#how-it-works","title":"How It Works","text":"<pre><code>from axion.tracing import TracerRegistry, BaseTracer\n\n# List all registered providers\nproviders = TracerRegistry.list_providers()\nprint(providers)  # ['noop', 'logfire', 'langfuse', 'opik']\n\n# Get a specific tracer class\nTracerClass = TracerRegistry.get('langfuse')\ntracer = TracerClass.create(metadata_type='llm')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#creating-custom-tracers","title":"Creating Custom Tracers","text":"<p>You can create custom tracer implementations by subclassing <code>BaseTracer</code> and registering with the <code>@TracerRegistry.register()</code> decorator:</p> <pre><code>from axion.tracing import TracerRegistry, BaseTracer\nfrom contextlib import contextmanager, asynccontextmanager\n\n@TracerRegistry.register('my_custom_tracer')\nclass MyCustomTracer(BaseTracer):\n    \"\"\"Custom tracer implementation.\"\"\"\n\n    def __init__(self, metadata_type: str = 'default', **kwargs):\n        self.metadata_type = metadata_type\n        # Initialize your tracing backend here\n\n    @classmethod\n    def create(cls, metadata_type: str = 'default', **kwargs):\n        return cls(metadata_type=metadata_type, **kwargs)\n\n    @contextmanager\n    def span(self, operation_name: str, **attributes):\n        # Implement span creation\n        print(f\"Starting span: {operation_name}\")\n        try:\n            yield self\n        finally:\n            print(f\"Ending span: {operation_name}\")\n\n    @asynccontextmanager\n    async def async_span(self, operation_name: str, **attributes):\n        # Implement async span creation\n        print(f\"Starting async span: {operation_name}\")\n        try:\n            yield self\n        finally:\n            print(f\"Ending async span: {operation_name}\")\n\n    def start(self, **attributes):\n        pass\n\n    def complete(self, output_data=None, **attributes):\n        pass\n\n    def fail(self, error: str, **attributes):\n        pass\n\n    def add_trace(self, event_type: str, message: str, metadata=None):\n        pass\n\n# Now you can use it\nconfigure_tracing(provider='my_custom_tracer')\n</code></pre>"},{"location":"deep-dives/internals/tracing/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/internals/tracing/#decorator-tracing-recommended","title":"Decorator Tracing (Recommended)","text":"<p>The tracer attribute is required for <code>@trace</code> decorator. The <code>@trace</code> decorator automatically looks for a tracer attribute on the class instance (<code>self</code>) to create and manage spans.</p> <pre><code>from axion.tracing import init_tracer, trace\n\nclass DecoratorService:\n    def __init__(self):\n        self.tracer = init_tracer('base')\n\n    @trace(name=\"process_data\", capture_args=True, capture_result=True)\n    async def process(self, data: dict):\n        await asyncio.sleep(0.1)\n        return {\"status\": \"processed\", \"items\": len(data)}\n\n    @trace  # Simple usage without arguments\n    async def run(self):\n        result = await self.process({\"id\": 123, \"items\": [\"a\", \"b\"]})\n        return result\n\n# Usage\nservice = DecoratorService()\nawait service.run()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#context-aware-function-tracing","title":"Context-Aware Function Tracing","text":"<p>Use <code>init_tracer</code> at the top level of a service or class to start a new trace context. Use <code>get_current_tracer</code> in downstream functions or services that you expect to be called within an existing trace, allowing them to add child spans without needing the tracer to be passed in manually.</p> <pre><code>from axion.tracing import get_current_tracer, init_tracer\n\nclass ServiceA:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    async def process(self):\n        async with self.tracer.async_span(\"service_a_process\"):\n            # Context automatically propagates to ServiceB\n            service_b = ServiceB()\n            await service_b.process()\n\nclass ServiceB:\n    async def process(self):\n        # Get tracer from context - no manual passing needed!\n        tracer = get_current_tracer()\n        async with tracer.async_span(\"service_b_process\"):\n            return \"processed\"\n\n# Usage\nservice = ServiceA()\nawait service.process()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#langfuse-specific-features","title":"Langfuse-Specific Features","text":"<p>When using Langfuse, you get additional LLM-specific features:</p> <pre><code>import os\nos.environ['TRACING_MODE'] = 'langfuse'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\n\nfrom axion.tracing import configure_tracing, Tracer\n\nconfigure_tracing()\ntracer = Tracer('llm')\n\n# Create spans that appear in Langfuse\nwith tracer.span('my-operation') as span:\n    # Your code here\n    span.set_attribute('custom_key', 'custom_value')\n\n# Log LLM calls with token usage\ntracer.log_llm_call(\n    name='chat_completion',\n    model='gpt-4',\n    prompt='Hello, how are you?',\n    response='I am doing well, thank you!',\n    usage={\n        'prompt_tokens': 10,\n        'completion_tokens': 8,\n        'total_tokens': 18\n    }\n)\n\n# Log evaluations as scores\ntracer.log_evaluation(\n    name='relevance_score',\n    score=0.95,\n    comment='Highly relevant response'\n)\n\n# Important: Flush traces before exiting\ntracer.flush()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#opik-specific-features","title":"Opik-Specific Features","text":"<p>Opik (by Comet) provides open-source LLM observability with similar features:</p> <pre><code>import os\nos.environ['TRACING_MODE'] = 'opik'\nos.environ['OPIK_API_KEY'] = 'your-api-key'\nos.environ['OPIK_WORKSPACE'] = 'your-workspace'\n\nfrom axion.tracing import configure_tracing, Tracer\n\nconfigure_tracing()\ntracer = Tracer('llm')\n\n# Create spans that appear in Opik dashboard\nwith tracer.span('my-operation', model='gpt-4') as span:\n    # Your code here\n    span.set_input({'query': 'Hello, how are you?'})\n    span.set_output({'response': 'I am doing well!'})\n    span.set_usage(prompt_tokens=10, completion_tokens=8)\n\n# Log LLM calls with token usage\ntracer.log_llm_call(\n    name='chat_completion',\n    model='gpt-4',\n    provider='openai',\n    prompt='Hello, how are you?',\n    response='I am doing well, thank you!',\n    prompt_tokens=10,\n    completion_tokens=8,\n)\n\n# Important: Flush traces before exiting\ntracer.flush()\n</code></pre> <p>Key Opik features: - Open-source and self-hostable - LLM-specific span types ('llm', 'tool', 'general') - Token usage tracking via <code>usage</code> attribute - Integration with Comet ML platform</p>"},{"location":"deep-dives/internals/tracing/#inputoutput-capture","title":"Input/Output Capture","text":"<p>Spans support <code>set_input()</code> and <code>set_output()</code> methods for capturing data that appears in the Langfuse UI's Input and Output fields:</p> <pre><code>async with tracer.async_span('my-operation') as span:\n    # Capture the input data\n    span.set_input({\n        'query': 'How do I reset my password?',\n        'context': ['doc1', 'doc2'],\n    })\n\n    result = await process_query(query)\n\n    # Capture the output data\n    span.set_output({\n        'response': result.text,\n        'score': result.confidence,\n    })\n</code></pre> <p>The <code>@trace</code> decorator automatically captures input/output when enabled:</p> <pre><code>@trace(name=\"process_data\", capture_args=True, capture_result=True)\nasync def process(self, data: dict):\n    # Input (args/kwargs) automatically captured via span.set_input()\n    result = await do_work(data)\n    # Result automatically captured via span.set_output()\n    return result\n</code></pre> <p>Supported data types for serialization: - Pydantic models (serialized via <code>model_dump()</code>) - Dictionaries, lists, and primitive types - Other objects are converted to string representation</p>"},{"location":"deep-dives/internals/tracing/#custom-span-names","title":"Custom Span Names","text":"<p>Axion components use meaningful span names for better observability:</p> <p>Evaluation Runner: Uses <code>evaluation_name</code> as the trace name in Langfuse/Opik: <pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My RAG Evaluation\"  # This becomes the trace name\n)\n# Appears as \"My RAG Evaluation\" in Langfuse/Opik instead of \"evaluation_runner\"\n</code></pre></p> <p>Trace Granularity: Control how evaluation traces are organized: <pre><code># Single trace (default) - all evaluations under one parent\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Evaluation\",\n    trace_granularity='single_trace'  # or 'single'\n)\n\n# Separate traces - each metric execution gets its own trace\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Evaluation\",\n    trace_granularity='separate'\n)\n</code></pre></p> <p>Evaluation Trace Hierarchy (lean 4-level structure): <pre><code>My RAG Evaluation                             # evaluation_name (root)\n\u251c\u2500 AnswerRelevancy.execute                    # Metric logic\n\u2502  \u2514\u2500 litellm_structured                      # LLM formatting/parsing\n\u2502     \u2514\u2500 llm_call                             # LLM API call (cost/tokens)\n\u2514\u2500 Faithfulness.execute\n   \u2514\u2500 litellm_structured\n      \u2514\u2500 llm_call\n</code></pre></p> <p>LLM Handlers: Use <code>metadata.name</code> or class name as the span name: <pre><code>from axion import LLMHandler\n\nclass SentimentAnalysisHandler(LLMHandler):\n    # ... handler config ...\n    pass\n\nhandler = SentimentAnalysisHandler()\n# Appears as \"SentimentAnalysisHandler\" in Langfuse instead of \"llm_handler\"\n\n# Or set a custom name via metadata\nhandler.metadata.name = \"Sentiment Analysis\"\n# Now appears as \"Sentiment Analysis\" in Langfuse\n</code></pre></p>"},{"location":"deep-dives/internals/tracing/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom axion.tracing import init_tracer, trace\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\nfrom axion._core.metadata.schema import ToolMetadata\n\nclass MetricService:\n    def __init__(self):\n        # Set Context on Span\n        tool_metadata = ToolMetadata(\n            name=\"MetricService\",\n            description='My Service',\n            version='1.0.1',\n        )\n        self.tracer = init_tracer(\n            metadata_type='llm',\n            tool_metadata=tool_metadata,\n        )\n\n    @trace(capture_result=True)\n    async def run_metric(self):\n        # AnswerRelevancy has its own tracer and auto-captured as a child span\n        metric = AnswerRelevancy()\n        data_item = DatasetItem(\n            query=\"How do I reset my password?\",\n            actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n            expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n        )\n        return await metric.execute(data_item)\n\n    @trace(name=\"doing_work\")\n    async def do_some_work(self):\n        await asyncio.sleep(0.5)\n        return \"Work done!\"\n\n    @trace(name='run_main_task')\n    async def run_main_task(self):\n        # Can also set manual spans\n        async with self.tracer.async_span(\"metric_evaluation\") as span:\n            _ = await self.do_some_work()\n            result = await self.run_metric()\n            span.set_attribute(\"operation_status\", \"success\")\n        return result\n\n# Usage\nservice = MetricService()\nresult = await service.run_main_task()\n</code></pre>"},{"location":"deep-dives/internals/tracing/#metadata-types","title":"Metadata Types","text":"<p>Choose the right type for automatic specialized handling:</p> <ul> <li><code>'base'</code> - General operations</li> <li><code>'llm'</code> - Language model calls (captures tokens, model info)</li> <li><code>'knowledge'</code> - Search and retrieval (captures queries, results)</li> <li><code>'database'</code> - Database operations (captures performance)</li> <li><code>'evaluation'</code> - Evaluation metrics (captures scores)</li> </ul>"},{"location":"deep-dives/internals/tracing/#api-reference","title":"API Reference","text":""},{"location":"deep-dives/internals/tracing/#core-functions","title":"Core Functions","text":"Function Description <code>configure_tracing(provider)</code> Configure the tracing provider (auto-configures if not called) <code>is_tracing_configured()</code> Check if tracing has been configured <code>clear_tracing_config()</code> Clear configuration (useful for testing/reconfiguration) <code>list_providers()</code> List available providers: <code>['noop', 'logfire', 'otel', 'langfuse', 'opik']</code> <code>get_tracer()</code> Get the configured tracer class <code>init_tracer(metadata_type, tool_metadata)</code> Initialize a tracer instance <code>Tracer(metadata_type)</code> Factory function for tracer instances (auto-configures)"},{"location":"deep-dives/internals/tracing/#context-management","title":"Context Management","text":"Function Description <code>get_current_tracer()</code> Get active tracer from context <code>set_current_tracer(tracer)</code> Set tracer in context <code>reset_tracer_context(token)</code> Reset tracer context"},{"location":"deep-dives/internals/tracing/#span-methods","title":"Span Methods","text":"Method Description <code>span.set_attribute(key, value)</code> Set a single attribute on the span <code>span.set_attributes(dict)</code> Set multiple attributes at once <code>span.set_input(data)</code> Set input data (appears in Langfuse Input field) <code>span.set_output(data)</code> Set output data (appears in Langfuse Output field) <code>span.add_trace(event_type, message, metadata)</code> Add a trace event to the span"},{"location":"deep-dives/internals/tracing/#registry","title":"Registry","text":"Function Description <code>TracerRegistry.register(name)</code> Decorator to register a tracer class <code>TracerRegistry.get(name)</code> Get a tracer class by name <code>TracerRegistry.list_providers()</code> List all registered providers <code>TracerRegistry.is_registered(name)</code> Check if a provider is registered"},{"location":"deep-dives/internals/tracing/#decorator-options","title":"Decorator Options","text":"Option Description <code>name</code> Custom span name (defaults to function name) <code>capture_args</code> Capture function arguments as span input <code>capture_result</code> Capture function result as span output"},{"location":"deep-dives/internals/tracing/#types","title":"Types","text":"Type Description <code>TracingMode</code> Enumeration of available tracer modes <code>TracerRegistry</code> Registry for tracer implementations <code>BaseTracer</code> Abstract base class for tracer implementations"},{"location":"deep-dives/internals/tracing/#integration","title":"Integration","text":"<p>The tracing system automatically works with other Axion components:</p> <ul> <li>Evaluation metrics are automatically traced</li> <li>API calls include retry and performance data</li> <li>LLM operations capture token usage and model info</li> </ul> <p>Just initialize your tracer and everything else traces automatically.</p>"},{"location":"deep-dives/internals/tracing/#installation","title":"Installation","text":"<p>The tracing providers are optional dependencies. From the project root:</p> <pre><code># Install with Logfire support\npip install -e \".[logfire]\"\n\n# Install with Langfuse support\npip install -e \".[langfuse]\"\n\n# Install with Opik support\npip install -e \".[opik]\"\n\n# Install with all tracing providers\npip install -e \".[tracing]\"\n</code></pre>"},{"location":"deep-dives/metrics/composite-metrics/","title":"Building Composite Evaluation Metrics","text":"<p>Composite metrics chain multiple LLM calls together to evaluate complex properties that no single prompt can capture well. Each sub-metric handles one focused task, and an orchestrator combines their outputs into a final score.</p> <p>This pattern is how Axion implements metrics like Faithfulness, AnswerRelevancy, and FactualAccuracy internally. Below, we re-create the Faithfulness metric from scratch to show the technique.</p>"},{"location":"deep-dives/metrics/composite-metrics/#faithfulness-metric","title":"Faithfulness Metric","text":"<p>The Faithfulness metric evaluates how factually consistent a response is with the retrieved context. It is scored between 0 and 1, where higher values indicate greater consistency. A response is considered faithful if every claim it makes is fully supported by the retrieved context.</p>"},{"location":"deep-dives/metrics/composite-metrics/#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    A[\"LLM Response\"] --&gt; B[\"1. Extract Statements\"]\n    B --&gt; C[\"Atomic claims\"]\n    C --&gt; D[\"2. Evaluate against Context\"]\n    D --&gt; E[\"Per-claim verdicts\"]\n    E --&gt; F[\"3. Compute Score\"]\n    F --&gt; G[\"supported / total\"]</code></pre> Step Sub-metric Input Output 1. Extract <code>StatementGenerator</code> Question + Answer List of atomic statements 2. Evaluate <code>NLIStatement</code> Statements + Context Per-statement verdict (0 or 1) 3. Score <code>Faithfulness</code> Verdicts <code>supported_count / total_count</code>"},{"location":"deep-dives/metrics/composite-metrics/#step-1-statement-generator","title":"Step 1: Statement Generator","text":"<p>The first sub-metric breaks an answer into atomic, self-contained claims. Each statement should be understandable on its own without pronouns or ambiguous references.</p> Statement Generator<pre><code>from pydantic import Field\nfrom typing import List\n\nfrom axion.metrics.base import BaseMetric\nfrom axion.schema import RichBaseModel\n\n\nclass StatementGeneratorInput(RichBaseModel):\n    question: str = Field(description='The question to answer')\n    answer: str = Field(description='The answer to the question')\n\n\nclass StatementGeneratorOutput(RichBaseModel):\n    statements: List[str] = Field(description='The generated statements')\n\n\nclass StatementGenerator(BaseMetric[StatementGeneratorInput, StatementGeneratorOutput]):\n    instruction = \"Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement.\"\n    input_model = StatementGeneratorInput\n    output_model = StatementGeneratorOutput\n    description = 'Statement Generator Prompt'\n    examples = [\n        (\n            StatementGeneratorInput(\n                question='Who was Albert Einstein and what is he best known for?',\n                answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.',\n            ),\n            StatementGeneratorOutput(\n                statements=[\n                    'Albert Einstein was a German-born theoretical physicist.',\n                    'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.',\n                    'Albert Einstein was best known for developing the theory of relativity.',\n                    'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.',\n                ]\n            ),\n        )\n    ]\n</code></pre> <p>The <code>instruction</code> and <code>examples</code> are all you need \u2014 <code>BaseMetric</code> handles the LLM call and response parsing automatically.</p>"},{"location":"deep-dives/metrics/composite-metrics/#step-2-nli-statement-evaluator","title":"Step 2: NLI Statement Evaluator","text":"<p>The second sub-metric judges each statement against the retrieved context using Natural Language Inference (NLI). Each statement gets a binary verdict: <code>1</code> if supported, <code>0</code> if not.</p> NLI Statement Evaluator<pre><code>class StatementFaithfulnessAnswer(RichBaseModel):\n    statement: str = Field(..., description='the original statement, word-by-word')\n    reason: str = Field(..., description='the reason of the verdict')\n    verdict: int = Field(..., description='the verdict(0/1) of the faithfulness.')\n\n\nclass NLIStatementInput(RichBaseModel):\n    retrieved_content: str = Field(\n        ..., description='The retrieved content from the model'\n    )\n    statements: List[str] = Field(..., description='The statements to judge')\n\n\nclass NLIStatementOutput(RichBaseModel):\n    statements: List[StatementFaithfulnessAnswer]\n\n\nclass NLIStatement(BaseMetric[NLIStatementInput, NLIStatementOutput]):\n    instruction = 'Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.'\n    input_model = NLIStatementInput\n    output_model = NLIStatementOutput\n    description = 'NLI Statement Prompt'\n    examples = [\n        (\n            NLIStatementInput(\n                retrieved_content=\"\"\"John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.\"\"\",\n                statements=[\n                    'John is majoring in Biology.',\n                    'John is taking a course on Artificial Intelligence.',\n                    'John is a dedicated student.',\n                    'John has a part-time job.',\n                ],\n            ),\n            NLIStatementOutput(\n                statements=[\n                    StatementFaithfulnessAnswer(\n                        statement='John is majoring in Biology.',\n                        reason=\"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\",\n                        verdict=0,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John is taking a course on Artificial Intelligence.',\n                        reason='The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.',\n                        verdict=0,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John is a dedicated student.',\n                        reason='The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.',\n                        verdict=1,\n                    ),\n                    StatementFaithfulnessAnswer(\n                        statement='John has a part-time job.',\n                        reason='There is no information given in the context about John having a part-time job.',\n                        verdict=0,\n                    ),\n                ]\n            ),\n        ),\n        (\n            NLIStatementInput(\n                retrieved_content='Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.',\n                statements=[\n                    'Albert Einstein was a genius.',\n                ],\n            ),\n            NLIStatementOutput(\n                statements=[\n                    StatementFaithfulnessAnswer(\n                        statement='Albert Einstein was a genius.',\n                        reason='The context and statement are unrelated',\n                        verdict=0,\n                    )\n                ]\n            ),\n        ),\n    ]\n</code></pre> <p>Few-shot examples are critical here \u2014 they calibrate the LLM to apply consistent judgment criteria across different contexts.</p>"},{"location":"deep-dives/metrics/composite-metrics/#step-3-faithfulness-orchestrator","title":"Step 3: Faithfulness Orchestrator","text":"<p>The main <code>Faithfulness</code> class composes the two sub-metrics and adds the scoring logic. It doesn't define its own <code>instruction</code> \u2014 instead, it overrides <code>execute()</code> to orchestrate the pipeline.</p> Faithfulness Orchestrator<pre><code>from typing import Union, Any\nimport numpy as np\n\nfrom axion.metrics.base import MetricEvaluationResult\nfrom axion.dataset import DatasetItem\nfrom axion._core.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass Faithfulness(BaseMetric):\n    \"\"\"\n    Computes faithfulness scores for LLM-generated responses based on retrieved contexts.\n\n    This metric works by:\n    1. Breaking down the LLM response into atomic statements\n    2. Evaluating each statement against the retrieved context\n    3. Computing a faithfulness score as the ratio of supported statements\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.statement_generator = StatementGenerator(**kwargs)\n        self.nli_evaluator = NLIStatement(**kwargs)\n\n    async def _create_statements(self, query: str, output: str) -&gt; List[str]:\n        result = await self.statement_generator.execute(\n            StatementGeneratorInput(question=query, answer=output)\n        )\n        if not result.statements:\n            logger.warning('No statements were generated from the answer.')\n        return result.statements\n\n    async def _evaluate_statements(\n        self, retrieved_content: str, statements: List[str]\n    ) -&gt; NLIStatementOutput:\n        if not statements:\n            logger.warning('No statements provided for evaluation.')\n            return NLIStatementOutput(statements=[])\n        formatted_content = self.convert_retrieved_content_to_string(retrieved_content)\n        return await self.nli_evaluator.execute(\n            NLIStatementInput(\n                retrieved_content=formatted_content, statements=statements\n            )\n        )\n\n    @staticmethod\n    def convert_retrieved_content_to_string(\n        retrieved_content: Union[str, List[str]], separator: str = '\\n'\n    ) -&gt; str:\n        if isinstance(retrieved_content, list):\n            retrieved_content = separator.join(retrieved_content)\n        return retrieved_content\n\n    @staticmethod\n    def _compute_score(evaluation_results: NLIStatementOutput) -&gt; float:\n        statements = evaluation_results.statements\n        if not statements:\n            logger.warning('No statements were evaluated.')\n            return np.nan\n\n        faithful_count = sum(1 for statement in statements if statement.verdict)\n        return faithful_count / len(statements)\n\n    async def execute(self, item: DatasetItem) -&gt; MetricEvaluationResult:\n        statements = await self._create_statements(item.query, item.actual_output)\n        if not statements:\n            return MetricEvaluationResult(\n                score=np.nan,\n            )\n\n        evaluation_results = await self._evaluate_statements(\n            item.retrieved_content, statements\n        )\n        score = self._compute_score(evaluation_results)\n\n        return MetricEvaluationResult(score=score)\n</code></pre>"},{"location":"deep-dives/metrics/composite-metrics/#running-the-metric","title":"Running the Metric","text":"<pre><code>metric = Faithfulness()\n\ndata_item = DatasetItem(\n    query = \"How do I reset my password?\",\n    actual_output = \"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output = \"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    retrieved_content = [\"Password reset available via login page\", \"Reset link sent by email\"],\n    latency = 2.13\n)\nresult = await metric.execute(data_item)\nprint(result.pretty())\n</code></pre> <p>Creating Custom Metrics  Metrics Guide  Metrics Reference </p>"},{"location":"deep-dives/metrics/creating-metrics/","title":"Creating Custom Metrics","text":"<p>Axion provides a flexible framework for creating custom evaluation metrics. Whether you need an LLM to judge nuanced quality, a deterministic algorithm for fast checks, or a hybrid that combines both \u2014 every metric inherits from <code>BaseMetric</code> and plugs directly into the evaluation ecosystem.</p>"},{"location":"deep-dives/metrics/creating-metrics/#what-youll-learn","title":"What You'll Learn","text":"1 <p>LLM-Powered</p> <p>Define an <code>instruction</code> and <code>examples</code> \u2014 the base class handles LLM calls, parsing, and scoring for you.</p> 2 <p>Algorithm-Based</p> <p>Override <code>execute()</code> with deterministic logic. No LLM needed \u2014 tag with <code>'heuristic'</code> and return a <code>MetricEvaluationResult</code>.</p> 3 <p>Hybrid</p> <p>Combine algorithmic pre-processing with LLM judgment and algorithmic aggregation in a single metric.</p> 4 <p>Registration &amp; Usage</p> <p>Register via the <code>@metric</code> decorator or manually through the registry, then run with the evaluation runner.</p>"},{"location":"deep-dives/metrics/creating-metrics/#core-components","title":"Core Components","text":""},{"location":"deep-dives/metrics/creating-metrics/#basemetric-class","title":"BaseMetric Class","text":"<p>All custom metrics inherit from <code>BaseMetric</code>, which provides:</p> <ul> <li>LLM Integration - Built-in LLM handler with configurable models</li> <li>Structured I/O - Type-safe input/output with Pydantic models</li> <li>Execution Framework - Async execution with tracing and logging</li> <li>Configuration Management - Threshold and parameter handling</li> <li>Validation - Automatic field validation for dataset items</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#metric-decorator","title":"Metric Decorator","text":"<p>The <code>@metric</code> decorator provides declarative configuration:</p> <pre><code>@metric(\n    name=\"Human-readable metric name\",\n    description=\"Detailed description of what the metric measures\",\n    required_fields=[\"field1\", \"field2\"],\n    optional_fields=[\"field3\"],\n    metric_category=MetricCategory.SCORE,  # SCORE, ANALYSIS, or CLASSIFICATION\n    default_threshold=0.5,\n    score_range=(0, 1),\n    tags=[\"category\", \"domain\"]\n)\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#metric-categories","title":"Metric Categories","text":"<p>Axion supports three metric categories based on output type. Use <code>MetricCategory</code> to specify what kind of output your metric produces.</p> <pre><code>from axion._core.types import MetricCategory\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    Q1[\"Does your metric produce a numeric score?\"]\n    Q1 --&gt;|\"Yes\"| S[\"MetricCategory.SCORE\"]\n    Q1 --&gt;|\"No\"| Q2[\"Does it output a SINGLE label from a FIXED set?\"]\n    Q2 --&gt;|\"Yes\"| C[\"MetricCategory.CLASSIFICATION\"]\n    Q2 --&gt;|\"No\"| A[\"MetricCategory.ANALYSIS\"]</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#category-comparison","title":"Category Comparison","text":"S <p>SCORE</p> <p>Numeric value (0\u20131) with pass/fail threshold. Aggregated via average or percentile. Example: <code>0.85</code></p> C <p>CLASSIFICATION</p> <p>Single label from a fixed set. Aggregated by label count. No pass/fail. Example: <code>\"Property Condition\"</code></p> A <p>ANALYSIS</p> <p>Structured object with open-ended insights. Custom aggregation. No pass/fail. Example: <code>{ category, reasons[], citations[] }</code></p>"},{"location":"deep-dives/metrics/creating-metrics/#when-to-use-each-category","title":"When to Use Each Category","text":"Category Use When Examples <code>SCORE</code> Metric produces a numeric value that can be compared to a threshold <code>Faithfulness</code>, <code>AnswerRelevancy</code>, <code>Latency</code>, <code>acceptance_rate</code> <code>CLASSIFICATION</code> Metric produces a single categorical label from a fixed set <code>SentimentClassification</code> \u2192 positive/negative/neutral <code>ANALYSIS</code> Metric extracts structured insights without scoring <code>ReferralReasonAnalysis</code> \u2192 reasons, citations, categories"},{"location":"deep-dives/metrics/creating-metrics/#examples","title":"Examples","text":"<p>SCORE Metric (default): <pre><code>@metric(\n    name=\"Answer Quality\",\n    metric_category=MetricCategory.SCORE,  # Default, can be omitted\n    default_threshold=0.7,\n    score_range=(0, 1),\n    ...\n)\nclass AnswerQuality(BaseMetric):\n    async def execute(self, item, **kwargs) -&gt; MetricEvaluationResult:\n        score = await self._evaluate(item)\n        return MetricEvaluationResult(\n            score=score,  # Numeric score required\n            explanation=\"Quality assessment complete\"\n        )\n</code></pre></p> <p>CLASSIFICATION Metric: <pre><code>@metric(\n    name=\"Sentiment Classification\",\n    metric_category=MetricCategory.CLASSIFICATION,\n    required_fields=[\"actual_output\"],\n    ...\n)\nclass SentimentClassification(BaseMetric):\n    async def execute(self, item, **kwargs) -&gt; MetricEvaluationResult:\n        label = await self._classify(item.actual_output)\n        return MetricEvaluationResult(\n            score=None,  # No score for classification\n            explanation=f\"Classified as: {label}\",\n            signals={\"label\": label}  # Single categorical output\n        )\n</code></pre></p> <p>ANALYSIS Metric: <pre><code>@metric(\n    name=\"Referral Reason Analysis\",\n    metric_category=MetricCategory.ANALYSIS,\n    required_fields=[\"actual_output\"],\n    ...\n)\nclass ReferralReasonAnalysis(BaseMetric):\n    async def execute(self, item, **kwargs) -&gt; MetricEvaluationResult:\n        result = await self._analyze(item)\n        return MetricEvaluationResult(\n            score=None,  # No score for analysis\n            explanation=f\"Extracted {len(result.reasons)} reasons\",\n            signals={  # Rich structured output\n                \"primary_category\": result.primary_category,\n                \"all_reasons\": result.reasons,\n                \"citations\": result.citations,\n                \"actionable_type\": result.actionable_type,\n            }\n        )\n</code></pre></p>"},{"location":"deep-dives/metrics/creating-metrics/#downstream-behavior","title":"Downstream Behavior","text":"Behavior <code>SCORE</code> <code>CLASSIFICATION</code> <code>ANALYSIS</code> <code>score</code> field Required numeric <code>None</code> \u2192 <code>np.nan</code> <code>None</code> \u2192 <code>np.nan</code> <code>passed</code> field <code>True</code>/<code>False</code> based on threshold <code>None</code> <code>None</code> <code>threshold</code> field Set from config <code>None</code> <code>None</code> Summary reports Included in averages Excluded from averages Excluded from averages Metadata <code>metric_category: \"score\"</code> <code>metric_category: \"classification\"</code> <code>metric_category: \"analysis\"</code>"},{"location":"deep-dives/metrics/creating-metrics/#field-mapping-for-nested-inputs","title":"Field Mapping for Nested Inputs","text":"<p>If your dataset stores canonical fields under different names or nested paths, you can map them at runtime using <code>field_mapping</code>. Mapped fields are used for validation and lookup, so required fields still pass when they are sourced from alternate locations.</p> <pre><code>from axion.metrics import AnswerCompleteness\n\nmetric = AnswerCompleteness(\n    field_mapping={\n        \"actual_output\": \"additional_output.summary\",\n        \"expected_output\": \"additional_input.reference\",\n    }\n)\n</code></pre> <p>Paths use dot notation and traverse attributes or dict keys. For example, the mapping above resolves <code>actual_output</code> from <code>item.additional_output[\"summary\"]</code>.</p>"},{"location":"deep-dives/metrics/creating-metrics/#building-your-first-metric","title":"Building Your First Metric","text":"<p>The three tabs below walk through each approach \u2014 pick the one that fits your use case.</p>  LLM-Powered Algorithm-Based Hybrid (LLM + Algorithm) <p>For pure LLM metrics, you don't need to override <code>execute()</code> at all. Set the <code>instruction</code> and <code>examples</code> class attributes and the base class takes care of calling the LLM, parsing the response, and returning a <code>MetricEvaluationResult</code>.</p> <p>The <code>instruction</code> tells the LLM what to evaluate and how to score it. The <code>examples</code> list provides few-shot demonstrations that calibrate the LLM's scoring \u2014 include at least one high-score and one low-score example for consistency.</p> <pre><code>from axion.metrics.base import BaseMetric, MetricEvaluationResult, metric\nfrom axion.dataset import DatasetItem\n\n@metric(\n    name=\"Answer Quality\",\n    description=\"Evaluates the overall quality of an answer based on clarity, completeness, and accuracy\",\n    required_fields=[\"actual_output\"],\n    optional_fields=[\"expected_output\", \"query\"],\n    default_threshold=0.7,\n    score_range=(0, 1),\n    tags=[\"quality\", \"general\"]\n)\nclass AnswerQuality(BaseMetric):\n    \"\"\"Evaluates answer quality across multiple dimensions.\"\"\"\n\n    instruction = \"\"\"\n    Evaluate the quality of the given answer based on the following criteria:\n    1. Clarity - Is the answer clear and easy to understand?\n    2. Completeness - Does the answer fully address the question?\n    3. Accuracy - Is the information provided correct?\n    Provide a score from 0 to 1, where:\n    ....\n    Provide a brief explanation for your score.\n    \"\"\"\n\n    examples = [\n        (\n            DatasetItem(\n                query=\"What is photosynthesis?\",\n                actual_output=\"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen. This process occurs in chloroplasts and is essential for plant survival and the oxygen we breathe.\"\n            ),\n            MetricEvaluationResult(\n                score=0.9,\n                explanation=\"Excellent answer that clearly explains photosynthesis, includes key components (sunlight, CO2, water, glucose, oxygen), mentions location (chloroplasts), and explains significance. Very clear and complete.\"\n            )\n        ),\n        (\n            DatasetItem(\n                query=\"How do you bake a cake?\",\n                actual_output=\"Mix ingredients and bake.\"\n            ),\n            MetricEvaluationResult(\n                score=0.2,\n                explanation=\"Poor answer that lacks detail and completeness. Doesn't specify ingredients, quantities, temperatures, or timing. Too vague to be useful for someone wanting to bake a cake.\"\n            )\n        )\n    ]\n</code></pre> <p>No <code>execute()</code> needed</p> <p>When you define <code>instruction</code> and <code>examples</code>, the <code>BaseMetric</code> base class automatically builds the prompt, calls the LLM, and parses the result. Only override <code>execute()</code> when you need custom logic.</p> <p>For deterministic, non-LLM metrics, override <code>execute()</code> directly. This is ideal for pattern matching, coverage checks, latency measurements, or any logic that doesn't need an LLM judgment call.</p> <p>Use <code>self.get_field()</code> to access fields through the <code>field_mapping</code> layer, call <code>self._validate_required_metric_fields(item)</code> up front, and tag with <code>'heuristic'</code> so no LLM is provisioned.</p> <pre><code>import re\nfrom typing import Set\n\nfrom axion.metrics.base import BaseMetric, MetricEvaluationResult, metric\nfrom axion.dataset import DatasetItem\n\n@metric(\n    name=\"Keyword Coverage\",\n    description=\"Measures how many expected keywords appear in the actual output\",\n    required_fields=[\"actual_output\", \"expected_keywords\"],\n    default_threshold=0.6,\n    score_range=(0, 1),\n    tags=[\"coverage\", \"keywords\", \"heuristic\"]\n)\nclass KeywordCoverage(BaseMetric):\n    \"\"\"Calculates the percentage of expected keywords found in the output.\"\"\"\n\n    async def execute(self, item: DatasetItem) -&gt; MetricEvaluationResult:\n        \"\"\"Calculate keyword coverage score.\"\"\"\n        self._validate_required_metric_fields(item)\n\n        actual_output = item.actual_output.lower()\n        expected_keywords = item.expected_keywords\n\n        if isinstance(expected_keywords, str):\n            expected_keywords = [kw.strip() for kw in expected_keywords.split(',')]\n\n        # Find keywords in output\n        found_keywords = []\n        missing_keywords = []\n\n        for keyword in expected_keywords:\n            keyword_lower = keyword.lower()\n            if keyword_lower in actual_output:\n                found_keywords.append(keyword)\n            else:\n                missing_keywords.append(keyword)\n\n        # Calculate score\n        score = len(found_keywords) / len(expected_keywords) if expected_keywords else 0.0\n\n        # Generate explanation\n        explanation = f\"Found {len(found_keywords)}/{len(expected_keywords)} expected keywords. \"\n        if found_keywords:\n            explanation += f\"Found: {', '.join(found_keywords)}. \"\n        if missing_keywords:\n            explanation += f\"Missing: {', '.join(missing_keywords)}.\"\n\n        return MetricEvaluationResult(\n            score=score,\n            explanation=explanation.strip()\n        )\n</code></pre> <p>Key differences from LLM metrics</p> <ul> <li>Override <code>execute()</code> with your own logic instead of setting <code>instruction</code>/<code>examples</code></li> <li>Tag with <code>'heuristic'</code> so no LLM handler is initialized</li> <li>Return <code>MetricEvaluationResult</code> directly with a computed score and explanation</li> </ul> <p>Hybrid metrics combine algorithmic pre-processing with LLM judgment and algorithmic post-processing. This is the pattern used by most metrics in Axion, as it is the bread and butter \u2014 extract structured data deterministically, let the LLM judge nuanced aspects, then aggregate the verdicts into a final score.</p> <p>The example below \u2014 <code>SourceVerification</code> \u2014 checks whether URLs cited in a response actually support the surrounding claims:</p> <ol> <li>Algorithmic step: Extract URLs and citation patterns via regex</li> <li>LLM step: For each citation, judge whether it supports the claim</li> <li>Algorithmic step: Aggregate verdicts into a final score</li> </ol> <pre><code>import re\nfrom typing import List\n\nfrom pydantic import Field\n\nfrom axion._core.schema import RichBaseModel\nfrom axion.dataset import DatasetItem\nfrom axion.metrics.base import BaseMetric, MetricEvaluationResult, metric\n\n# --- Pydantic models for the LLM judge ---\n\nclass SourceVerdict(RichBaseModel):\n    citation: str = Field(description=\"The citation text being judged.\")\n    supports_claim: bool = Field(\n        description=\"True if the source likely supports the surrounding claim.\"\n    )\n    reason: str = Field(description=\"Brief reason for the verdict.\")\n\nclass SourceJudgeInput(RichBaseModel):\n    claim_context: str = Field(description=\"The text surrounding the citation.\")\n    citations: List[str] = Field(description=\"Citations to judge.\")\n\nclass SourceJudgeOutput(RichBaseModel):\n    verdicts: List[SourceVerdict] = Field(\n        description=\"One verdict per citation.\"\n    )\n\n# --- Inner LLM judge (structured output) ---\n\nclass SourceJudge(BaseMetric[SourceJudgeInput, SourceJudgeOutput]):\n    as_structured_llm = True\n    input_model = SourceJudgeInput\n    output_model = SourceJudgeOutput\n    description = \"Judges whether each citation supports its surrounding claim.\"\n    instruction = \"\"\"For each citation, decide whether the source it references\n    is likely to support the claim in which it appears.\n\n    A citation SUPPORTS the claim if:\n    - The title, URL, or identifier clearly relates to the claim topic.\n    - It is a primary or authoritative source for the stated fact.\n\n    A citation DOES NOT SUPPORT the claim if:\n    - It links to an unrelated topic or a generic homepage.\n    - It appears fabricated or nonsensical.\"\"\"\n\n# --- Main hybrid metric ---\n\n@metric(\n    name=\"Source Verification\",\n    description=\"Verifies that cited sources support the claims they accompany\",\n    required_fields=[\"actual_output\", \"query\"],\n    default_threshold=0.7,\n    score_range=(0, 1),\n    tags=[\"citations\", \"verification\", \"hybrid\"]\n)\nclass SourceVerification(BaseMetric):\n    \"\"\"Hybrid metric: regex extraction \u2192 LLM judgment \u2192 algorithmic scoring.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.judge = SourceJudge(**kwargs)\n\n    # Step 1 \u2014 Algorithmic: extract citations from text\n    @staticmethod\n    def _extract_citations(text: str) -&gt; List[str]:\n        citations = []\n        seen = set()\n\n        # Markdown links: [Title](URL)\n        for match in re.finditer(r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)', text):\n            url = match.group(2)\n            if url not in seen:\n                citations.append(match.group(0))\n                seen.add(url)\n\n        # Bare URLs not already captured\n        for match in re.finditer(r'https?://[^\\s&lt;&gt;\"\\')\\]]+', text):\n            url = match.group(0).rstrip('.,;:!?')\n            if url not in seen:\n                citations.append(url)\n                seen.add(url)\n\n        return citations\n\n    async def execute(self, item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult:\n        self._validate_required_metric_fields(item)\n\n        # Step 1 \u2014 Algorithmic extraction\n        citations = self._extract_citations(item.actual_output)\n\n        if not citations:\n            return MetricEvaluationResult(\n                score=0.0,\n                explanation=\"No citations found in the response.\"\n            )\n\n        # Step 2 \u2014 LLM judgment\n        judge_output = await self.judge.execute(\n            claim_context=item.actual_output,\n            citations=citations,\n        )\n\n        # Step 3 \u2014 Algorithmic aggregation\n        supported = sum(1 for v in judge_output.verdicts if v.supports_claim)\n        score = supported / len(citations)\n\n        # Cost tracking (only the LLM component)\n        self.compute_cost_estimate([self.judge])\n\n        explanation = (\n            f\"Source Verification Score: {score:.2f} \"\n            f\"({supported}/{len(citations)} citations support their claims).\"\n        )\n\n        return MetricEvaluationResult(score=score, explanation=explanation)\n</code></pre> <p>Pattern breakdown</p> Step Type What happens Extract citations Algorithm Regex finds URLs and markdown links \u2014 fast, deterministic Judge relevance LLM Structured output model returns per-citation verdicts Aggregate score Algorithm <code>supported / total</code> \u2014 no LLM needed for the math <p>This pattern keeps LLM costs proportional to the number of citations, not the length of the full response, and gives you a deterministic aggregation you can unit-test independently.</p>"},{"location":"deep-dives/metrics/creating-metrics/#registration-methods","title":"Registration Methods","text":""},{"location":"deep-dives/metrics/creating-metrics/#automatic-registration-with-decorator","title":"Automatic Registration with Decorator","text":"<p>The <code>@metric</code> decorator automatically registers metrics:</p> <pre><code>@metric(\n    name=\"Custom Metric\",\n    description=\"Description of the metric\",\n    required_fields=[\"field1\", \"field2\"],\n    default_threshold=0.5\n)\nclass CustomMetric(BaseMetric):\n    # Metric implementation\n    pass\n\n# Metric is automatically available in registry\nfrom axion.metrics import metric_registry\nmetric_class = metric_registry.get(\"custom_metric\")\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#manual-registration","title":"Manual Registration","text":"<p>For dynamic registration or when decorators aren't suitable:</p> <pre><code>from axion.metrics.base import MetricConfig\nfrom axion.metrics import metric_registry\n\nclass DynamicMetric(BaseMetric):\n    \"\"\"Dynamically configured metric.\"\"\"\n    pass\n\n# Create configuration\nconfig = MetricConfig(\n    key=\"dynamic_metric\",\n    name=\"Dynamic Metric\",\n    description=\"A dynamically registered metric\",\n    required_fields=[\"actual_output\"],\n    optional_fields=[],\n    default_threshold=0.6,\n    score_range=(0, 1),\n    tags=[\"dynamic\"]\n)\n\n# Attach config and register\nDynamicMetric.config = config\nmetric_registry.register(DynamicMetric)\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#usage-examples","title":"Usage Examples","text":""},{"location":"deep-dives/metrics/creating-metrics/#using-custom-metrics","title":"Using Custom Metrics","text":"<pre><code>from axion.dataset import DatasetItem\n\n# Initialize your custom metric\nmetric = AnswerQuality()\n\n# Prepare test data\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    retrieved_content=[\"Password reset available via login page\", \"Reset link sent by email\"]\n)\n\n# Execute evaluation\nresult = await metric.execute(data_item)\nprint(f\"Score: {result.score}\")\nprint(f\"Explanation: {result.explanation}\")\n\n# Pretty print results\nprint(result.pretty())\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#running-with-the-evaluation-runner","title":"Running with the Evaluation Runner","text":"<p>For batch evaluation across a full dataset, use <code>evaluation_runner</code>:</p> <pre><code>from axion import Dataset\nfrom axion.runners import evaluation_runner\n\ndataset = Dataset(items=[...])\nmetrics = [AnswerQuality(), KeywordCoverage()]\n\nresults = await evaluation_runner(dataset=dataset, metrics=metrics)\n</code></pre>"},{"location":"deep-dives/metrics/creating-metrics/#best-practices","title":"Best Practices","text":""},{"location":"deep-dives/metrics/creating-metrics/#metric-design","title":"Metric Design","text":"<ul> <li>Single Responsibility - Each metric should evaluate one specific aspect</li> <li>Clear Scoring - Use consistent scoring scales and document ranges</li> <li>Robust Validation - Validate inputs thoroughly and provide helpful error messages</li> <li>Comprehensive Examples - Include diverse examples that cover edge cases</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#error-handling","title":"Error Handling","text":"<ul> <li>Graceful Degradation - Provide fallback scores when computation fails</li> <li>Informative Messages - Return helpful error messages and explanations</li> <li>Input Validation - Validate inputs early and provide clear requirements</li> <li>Logging - Use appropriate logging levels for debugging and monitoring</li> </ul>"},{"location":"deep-dives/metrics/creating-metrics/#performance","title":"Performance","text":"<ul> <li><code>shares_internal_cache</code> - Set to <code>True</code> when multiple metrics can reuse intermediate results (e.g., claim extraction shared between Faithfulness and Citation metrics)</li> <li><code>compute_cost_estimate()</code> - Call with your inner LLM components to surface estimated token costs in evaluation reports</li> <li>Heuristic tagging - Include <code>'heuristic'</code> in <code>tags</code> for algorithm-only metrics so no LLM handler is initialized, reducing startup overhead</li> </ul> <p>Metrics Guide  Running Evaluations  Metrics Reference </p>"},{"location":"deep-dives/metrics/yaml-metrics/","title":"Creating YAML-Based Metrics","text":"<p>Axion supports creating evaluation metrics directly from YAML configuration files, providing a simple alternative to defining metrics as code. YAML metrics support both LLM-powered evaluation and custom heuristic functions.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#overview","title":"Overview","text":"<p>YAML metrics offer two evaluation approaches:</p> <ul> <li>LLM-powered metrics using instruction prompts</li> <li>Heuristic-based metrics using custom Python functions</li> </ul>"},{"location":"deep-dives/metrics/yaml-metrics/#basic-structure","title":"Basic Structure","text":"<p>Every YAML metric requires either an <code>instruction</code> (for LLM evaluation) OR a <code>heuristic</code> (for algorithmic evaluation), but not both:</p> <pre><code># Option 1: LLM-powered metric\nname: 'My Metric'\ninstruction: |\n  Your evaluation prompt here...\n\n# Option 2: Heuristic-based metric\nname: 'My Metric'\nheuristic: |\n  def evaluate(item):\n      # Your logic here\n      return MetricEvaluationResult(score=1.0, explanation=\"...\")\n\n# Optional configuration\nmodel_name: \"gpt-4\"\nthreshold: 0.7\nrequired_fields: [...]\noptional_fields: [...]\nexamples: [...]\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#llm-powered-metrics","title":"LLM-Powered Metrics","text":"<p>Use <code>instruction</code> to define metrics that leverage language models for evaluation.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#basic-llm-metric","title":"Basic LLM Metric","text":"<pre><code># answer_quality.yaml\nname: 'Answer Quality'\ninstruction: |\n  Evaluate the quality of the given answer based on clarity, completeness, and accuracy.\n  Provide a score either of 0 or 1 based on .... and explain your reasoning.\n\n\n# Optional configuration\nmodel_name: \"gpt-4\"\nthreshold: 0.7\nrequired_fields:\n  - \"query\"\n  - \"actual_output\"\n\nexamples:\n  - input:\n      query: \"What is photosynthesis?\"\n      actual_output: \"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen.\"\n    output:\n      score: 1\n      explanation: \"Excellent answer that clearly explains photosynthesis with key components. Very clear and complete.\"\n\n  - input:\n      query: \"How do you bake a cake?\"\n      actual_output: \"Mix ingredients and bake.\"\n    output:\n      score: 0\n      explanation: \"Poor answer lacking detail. Doesn't specify ingredients, quantities, or timing.\"\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#heuristic-based-metrics","title":"Heuristic-Based Metrics","text":"<p>Use <code>heuristic</code> to define metrics with custom Python logic for fast, deterministic evaluation.</p>"},{"location":"deep-dives/metrics/yaml-metrics/#string-matching-heuristic","title":"String Matching Heuristic","text":"<pre><code># contains_match.yaml\nheuristic: |\n  def evaluate(item):\n      expected = item.expected_output.strip()\n      is_contained = expected in item.actual_output\n\n      return MetricEvaluationResult(\n          score=1.0 if is_contained else 0.0,\n          explanation=f\"Expected text {'found' if is_contained else 'not found'} in actual output\"\n      )\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#loading-and-using-yaml-metrics","title":"Loading and Using YAML Metrics","text":"<pre><code>from axion.metrics.yaml_metrics import load_metric_from_yaml\nfrom axion.dataset import DatasetItem\n\n# Load metric from YAML file\nMetricClass = load_metric_from_yaml(\"answer_quality.yaml\")\n\n# Create instance\nmetric = MetricClass(\n    field_mapping={\"actual_output\": \"additional_output.summary\"}\n)\n# Evaluate\ndata_item = DatasetItem(\n    query=\"What is machine learning?\",\n    actual_output=\"Machine learning is a subset of AI that enables computers to learn from data.\",\n    expected_output=\"Machine learning allows computers to learn patterns from data without explicit programming.\"\n)\n\nresult = await metric.execute(data_item)\n</code></pre>"},{"location":"deep-dives/metrics/yaml-metrics/#field-mapping-for-nested-inputs","title":"Field Mapping for Nested Inputs","text":"<p>YAML metrics don't define field mappings in the YAML file yet, but you can pass <code>field_mapping</code> when creating the metric instance to map canonical fields to nested paths or alternate field names.</p> <p>Metrics API Reference  Creating Custom Metrics </p>"},{"location":"deep-dives/runners/api-runner/","title":"API Runner","text":"<p>The API Runner manages multiple API clients through a single registry \u2014 register any endpoint with a decorator, then execute single queries or batches with built-in retry, concurrency control, and standardized response formatting.</p>"},{"location":"deep-dives/runners/api-runner/#what-youll-learn","title":"What You'll Learn","text":"1 <p>Registry-Based</p> <p>Register API clients with <code>@APIRunner.register()</code> and route queries by name at runtime.</p> 2 <p>Batch Processing</p> <p>Execute multiple queries concurrently with semaphore-controlled parallelism and progress tracking.</p> 3 <p>Config-Driven</p> <p>Initialize runners from a dictionary or YAML file \u2014 no hardcoded credentials in code.</p> 4 <p>Standardized Output</p> <p>Every runner returns <code>APIResponseData</code> with query, output, latency, status, and trace info.</p>"},{"location":"deep-dives/runners/api-runner/#creating-custom-runners","title":"Creating Custom Runners","text":"<p>Register your own API implementations to integrate any service into Axion's evaluation pipelines.</p>  Basic Chatbot OpenAI Example <p>Use the <code>@APIRunner.register()</code> decorator and implement <code>execute()</code>. The <code>@api_retry</code> decorator adds automatic retry with exponential backoff.</p> <pre><code>from axion.runners.api import api_retry, BaseAPIRunner, APIResponseData, APIRunner\n\n@APIRunner.register('my_chatbot')\nclass MyChatbotRunner(BaseAPIRunner):\n    \"\"\"Runner for your custom chatbot API.\"\"\"\n    name = 'my_chatbot'\n\n    def __init__(self, config: dict, **kwargs):\n        super().__init__(config, **kwargs)\n        self.api_url = config.get('api_url')\n        self.api_key = config.get('api_key')\n\n    @api_retry('Chatbot API call')\n    def execute(self, query: str, **kwargs) -&gt; APIResponseData:\n        import requests\n        import time\n\n        start_time = time.time()\n\n        response = requests.post(\n            self.api_url,\n            headers={'Authorization': f'Bearer {self.api_key}'},\n            json={'message': query}\n        )\n        result = response.json()\n\n        return APIResponseData(\n            query=query,\n            actual_output=result.get('response', ''),\n            latency=time.time() - start_time,\n            status='success'\n        )\n</code></pre> <p>A real-world example integrating OpenAI's Chat API \u2014 shows how to capture model metadata and token usage in <code>additional_output</code>.</p> <pre><code>from axion.runners.api import api_retry, BaseAPIRunner, APIResponseData, APIRunner\n\n@APIRunner.register('openai_chat')\nclass OpenAIChatRunner(BaseAPIRunner):\n    \"\"\"Custom runner for OpenAI Chat API.\"\"\"\n    name = 'openai_chat'\n\n    def __init__(self, config: dict, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = config.get('model', 'gpt-4')\n\n    @api_retry('OpenAI Chat API call')\n    def execute(self, query: str, **kwargs) -&gt; APIResponseData:\n        from openai import OpenAI\n        import time\n\n        client = OpenAI()\n        start_time = time.time()\n\n        response = client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": query}]\n        )\n\n        return APIResponseData(\n            query=query,\n            actual_output=response.choices[0].message.content,\n            latency=time.time() - start_time,\n            additional_output={'model': self.model, 'usage': dict(response.usage)},\n            status='success'\n        )\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#implementation-requirements","title":"Implementation Requirements","text":"<p>When creating custom runners, ensure your implementation:</p> <ol> <li>Inherits from <code>BaseAPIRunner</code> \u2014 Provides the standard interface and batch processing capabilities</li> <li>Implements the <code>execute()</code> method \u2014 Core method that handles single query execution</li> <li>Returns <code>APIResponseData</code> objects \u2014 Use the standardized response format for consistency</li> <li>Handles errors gracefully \u2014 Use try/except and return appropriate status</li> </ol>"},{"location":"deep-dives/runners/api-runner/#usage-patterns","title":"Usage Patterns","text":"Basic Usage Batch Processing Direct Access <p>Initialize the runner with a config dict, execute queries by runner name, and discover available APIs at runtime.</p> <pre><code>from axion.runners import APIRunner\n\n# Configuration for your registered APIs\nconfig = {\n    'my_chatbot': {\n        'api_url': 'https://api.example.com/chat',\n        'api_key': 'your-api-key'\n    }\n}\n\n# Initialize with configuration\nrunner = APIRunner(config=config, max_concurrent=5)\n\n# Execute single query\nresponse = runner.execute('my_chatbot', \"How do I reset my password?\")\nprint(response.actual_output)\n\n# List available APIs\navailable_apis = runner.list_available_apis()\nprint(f\"Available APIs: {available_apis}\")\n</code></pre> <p>Execute multiple queries concurrently with automatic semaphore control.</p> <pre><code># Prepare multiple queries\nqueries = [\n    \"How do I reset my password?\",\n    \"What are the payment options?\",\n    \"How do I contact support?\"\n]\n\n# Execute batch asynchronously\nresponses = await runner.execute_batch('my_chatbot', queries)\n\n# Process responses\nfor response in responses:\n    print(f\"Query: {response.query}\")\n    print(f\"Response: {response.actual_output}\")\n    print(f\"Latency: {response.latency}s\")\n    print(\"---\")\n</code></pre> <p>Initialize a specific runner directly for fine-grained control.</p> <pre><code># Initialize a specific runner directly\nchatbot_runner = MyChatbotRunner(\n    config={'api_url': '...', 'api_key': '...'},\n    max_concurrent=3\n)\n\n# Execute query\nresponse = chatbot_runner.execute(\"Your query here\")\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#configuration","title":"Configuration","text":"Dictionary YAML <pre><code>config = {\n    'my_chatbot': {\n        'api_url': 'https://api.example.com/chat',\n        'api_key': 'your-api-key'\n    },\n    'openai_chat': {\n        'model': 'gpt-4'\n    }\n}\n\nrunner = APIRunner(config=config)\n</code></pre> <pre><code># config.yaml\nmy_chatbot:\n  api_url: https://api.example.com/chat\n  api_key: ${CHATBOT_API_KEY}  # Environment variable substitution\n\nopenai_chat:\n  model: gpt-4\n</code></pre> <pre><code>runner = APIRunner(config=\"/path/to/config.yaml\")\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#response-format","title":"Response Format","text":"<p>All API runners return standardized <code>APIResponseData</code> objects:</p> Field Type Description <code>query</code> <code>str</code> Original query string <code>actual_output</code> <code>str</code> Primary response content <code>retrieved_content</code> <code>List[str]</code> Retrieved context chunks (if applicable) <code>latency</code> <code>float</code> Response time in seconds <code>trace</code> <code>Dict[str, Any]</code> Debug and trace information <code>additional_output</code> <code>Dict[str, Any]</code> Additional response data <code>status</code> <code>str</code> Execution status (default: <code>'success'</code>) <code>timestamp</code> <code>str</code> ISO-formatted response timestamp"},{"location":"deep-dives/runners/api-runner/#advanced-configuration","title":"Advanced Configuration","text":"Concurrency Retry Control Registry Management <pre><code># Set global concurrency limit\nrunner = APIRunner(config=config, max_concurrent=10)\n\n# Or configure per API when using direct instantiation\napi_runner = MyChatbotRunner(config=api_config, max_concurrent=3)\n</code></pre> <pre><code>from axion.runners.api import RetryConfig\n\n# Set global retry control\nrunner = APIRunner(\n    config=config,\n    retry_config=RetryConfig(max_attempts=5, backoff_factor=2.0)\n)\n\n# Or disable retries for a specific runner\napi_runner = MyChatbotRunner(\n    config=api_config,\n    retry_config=RetryConfig(enabled=False)\n)\n</code></pre> <pre><code># View all registered APIs and their options\nAPIRunner.display()\n\n# Check registered APIs at runtime\navailable = runner.list_available_apis()\n\n# Access specific executor\nchatbot_executor = runner['my_chatbot']\n</code></pre>"},{"location":"deep-dives/runners/api-runner/#integration-with-evaluation-runner","title":"Integration with Evaluation Runner","text":"<p>Use API runners as tasks in the evaluation pipeline \u2014 the evaluation runner generates outputs and scores them in a single workflow:</p> <pre><code>from axion.runners import evaluation_runner, APIRunner\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.dataset import DatasetItem\n\n# Configure your API\nconfig = {'my_chatbot': {'api_url': '...', 'api_key': '...'}}\napi_runner = APIRunner(config=config)\n\n# Create evaluation dataset (without actual_output - the task will generate it)\ndataset = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        expected_output=\"Navigate to login, click 'Forgot Password', follow the email link.\"\n    ),\n    DatasetItem(\n        query=\"What are your business hours?\",\n        expected_output=\"We are open Monday-Friday, 9 AM to 5 PM EST.\"\n    )\n]\n\n# Define task that calls your API\ndef chatbot_task(item: DatasetItem) -&gt; dict:\n    response = api_runner.execute('my_chatbot', item.query)\n    return {\n        'response': response.actual_output,\n        'latency': response.latency\n    }\n\n# Run evaluation with task\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=chatbot_task,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name=\"Chatbot Evaluation\"\n)\n</code></pre> <p>Runners API Reference  Evaluation Runner Deep Dive  Metric Runner Deep Dive </p>"},{"location":"deep-dives/runners/evaluation-runner/","title":"Evaluation Runner","text":"<p>The Evaluation Runner orchestrates end-to-end evaluation experiments \u2014 from generating model predictions to scoring them with multiple metrics. It unifies task execution, metric evaluation, and result aggregation into a single workflow with built-in caching, error handling, tracing, and key remapping.</p>"},{"location":"deep-dives/runners/evaluation-runner/#what-youll-learn","title":"What You'll Learn","text":"1 <p>Task Execution</p> <p>Optionally generate model outputs with a custom task function before scoring \u2014 sync, async, dict, or Pydantic.</p> 2 <p>Metric Evaluation</p> <p>Score outputs with any combination of Axion, Ragas, and DeepEval metrics in parallel.</p> 3 <p>Caching &amp; Errors</p> <p>Cache task outputs and metric results to disk or memory. Configure error handling per experiment.</p> 4 <p>Tracing</p> <p>Automatic integration with Langfuse and Opik for full observability of every LLM call and metric execution.</p>"},{"location":"deep-dives/runners/evaluation-runner/#overview","title":"Overview","text":"<p>The Evaluation Runner combines three phases into a single workflow:</p> <pre><code>flowchart LR\n    A[\"Task Execution&lt;br/&gt;&lt;small&gt;Generate predictions&lt;/small&gt;\"] --&gt; B[\"Metric Evaluation&lt;br/&gt;&lt;small&gt;Score with metrics&lt;/small&gt;\"] --&gt; C[\"Result Aggregation&lt;br/&gt;&lt;small&gt;Collect &amp; summarize&lt;/small&gt;\"]</code></pre> View Documentation &amp; Usage Examples<pre><code>from axion.runners import EvaluationRunner\n\n# Display inline documentation and usage examples\nEvaluationRunner.display()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#getting-started","title":"Getting Started","text":"Basic (No Task) With Task Multi-Library <p>When your dataset already contains model outputs, skip the task phase and go straight to evaluation. Pass <code>evaluation_inputs</code> and <code>scoring_metrics</code> \u2014 the runner handles the rest.</p> <pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Prepare dataset with existing outputs\ndataset = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        actual_output=\"To reset your password, click 'Forgot Password' on the login page...\",\n        expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link.\"\n    ),\n    # More items...\n]\n\n# Run evaluation only\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"Basic Evaluation\"\n)\n\nprint(f\"Evaluation Name: {results.evaluation_name}\")\nprint(f\"Success rate: {results.success_rate}\")\n</code></pre> <p>For end-to-end evaluation, provide a <code>task</code> function that generates outputs from each <code>DatasetItem</code>. The task can return a <code>dict</code>, a Pydantic <code>BaseModel</code>, or use an API runner. Use <code>scoring_key_mapping</code> when task output keys don't match metric input fields.</p> <pre><code>from axion.runners import evaluation_runner, CacheConfig\nfrom axion.metrics import AnswerRelevancy, Latency\nfrom axion.dataset import DatasetItem\n\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link.\",\n)\n\n# Task returns a dictionary (can be async or sync)\ndef my_task(item):\n    return {\n        'response': \"To reset your password, click 'Forgot Password' on the login page.\",\n        'latency': 1.3\n    }\n\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    task=my_task,\n    scoring_metrics=[\n        AnswerRelevancy(model_name='gpt-4o'),\n        Latency(threshold=1.5),\n    ],\n    max_concurrent=5,\n    scoring_key_mapping={'actual_output': 'response'},  # Map task keys \u2192 metric fields\n    evaluation_name='Custom Setup',\n    evaluation_metadata={\"model_version\": \"v2.1\"},\n    cache_config=CacheConfig(use_cache=True),\n)\n\nresults.to_dataframe()   # Pandas DataFrame\nresults.to_scorecard()   # Visual scorecard\n</code></pre> <p>Mix metrics from Axion, Ragas, and DeepEval in a single evaluation run. Each metric is automatically routed to the correct executor.</p> <pre><code>from axion.runners import (\n    EvaluationRunner, EvaluationConfig,\n    CacheConfig, ErrorConfig,\n)\nfrom axion.metrics import AnswerCompleteness, Latency\nfrom axion.integrations.models import LiteLLMRagas, LiteLLMDeepEval\n\nimport pandas as pd\nfrom ragas.metrics import Faithfulness\nfrom deepeval.metrics import AnswerRelevancyMetric\n\n# Create evaluation dataset\ndataframe = pd.DataFrame([\n    {\n        'id': '01',\n        'query': \"How do I reset my password?\",\n        'expected_output': \"Navigate to login, click 'Forgot Password', and follow the reset link.\",\n    },\n    {\n        'id': '02',\n        'query': \"How do I update my billing information?\",\n        'expected_output': \"Go to Account Settings, select Billing, and update your payment method.\",\n    }\n])\n\n# Define evaluation metrics across frameworks\nmetrics = [\n    Faithfulness(llm=LiteLLMRagas()),\n    AnswerCompleteness(model_name='gpt-4'),\n    AnswerRelevancyMetric(model=LiteLLMDeepEval()),\n    Latency(threshold=8)\n]\n\n# Configure the evaluation\nconfig = EvaluationConfig(\n    evaluation_name=\"Advanced Configuration Eval\",\n    evaluation_inputs=dataframe,\n    scoring_metrics=metrics,\n    task=api_runner,\n    max_concurrent=10,\n    cache_config=CacheConfig(use_cache=True, cache_type='memory'),\n    error_config=ErrorConfig(skip_on_missing_params=True),\n    thresholds={\"faithfulness\": 0.8, \"answer_completeness\": 0.7},\n    evaluation_metadata={\"model_version\": \"v2.1\"},\n    dataset_name='Advanced Configuration Dataset'\n)\n\n# Run the evaluation\nrunner = EvaluationRunner(config)\nresults = await runner.execute()\n\nresults.to_dataframe()\nresults.to_scorecard()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#configuration","title":"Configuration","text":"Function Call Config Object <p>Pass parameters directly to <code>evaluation_runner()</code> for quick, inline configuration.</p> <pre><code>from axion.runners import evaluation_runner\n\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"My Experiment\",\n    task=my_task_function,\n    max_concurrent=10,\n    thresholds={\"question_answer_relevance\": 0.75}\n)\n</code></pre> <p>Use <code>EvaluationConfig</code> for reusable, shareable configurations.</p> <pre><code>from axion.runners import EvaluationRunner, EvaluationConfig, CacheConfig\n\nconfig = EvaluationConfig(\n    evaluation_name=\"Advanced Experiment\",\n    evaluation_inputs=dataset,\n    scoring_metrics=metrics,\n    task=generation_task,\n    scoring_key_mapping={'actual_output': 'response'},\n    evaluation_description=\"Evaluating new model version\",\n    evaluation_metadata={\"model_version\": \"v2.1\"},\n    cache_config=CacheConfig(use_cache=True),\n    max_concurrent=5,\n    show_progress=True\n)\n\nrunner = EvaluationRunner(config)\nresults = await runner.execute()\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#task-functions","title":"Task Functions","text":"<p>Task functions generate model outputs for evaluation. They accept a <code>DatasetItem</code> and return output data. Supports both sync and async functions.</p>  Dict Return Pydantic Return API Runner <pre><code>def simple_task(item: DatasetItem) -&gt; Dict[str, Any]:\n    \"\"\"Simple synchronous task function.\"\"\"\n    response = your_model.generate(item.query)\n\n    return {\n        'actual_output': response,\n        'generation_time': time.time()\n    }\n</code></pre> <pre><code>from pydantic import BaseModel\n\nclass Output(BaseModel):\n    response: str\n    latency: float\n\ndef pydantic_task(item: DatasetItem) -&gt; Output:\n    return Output(\n        response=\"To reset your password...\",\n        latency=1.3\n    )\n</code></pre> <pre><code>from axion.runners import APIRunner\n\napi_runner = APIRunner(config={'my_chatbot': {'api_url': '...', 'api_key': '...'}})\n\n# Pass the API runner directly as the task\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=api_runner,\n    scoring_metrics=metrics,\n    scoring_key_mapping={'actual_output': 'response'},\n    evaluation_name=\"API Evaluation\"\n)\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#key-remapping","title":"Key Remapping","text":"<p>Use <code>scoring_key_mapping</code> to bridge the gap between task output keys and metric input fields:</p> <pre><code># Your task returns this structure\ntask_output = {\n    'generated_text': \"The answer is...\",\n    'source_documents': [\"doc1\", \"doc2\"],\n    'model_confidence': 0.95\n}\n\n# But metrics expect this structure\nmetric_expected = {\n    'actual_output': \"The answer is...\",\n    'retrieved_content': [\"doc1\", \"doc2\"]\n}\n\n# Use key mapping to bridge the gap\nscoring_key_mapping = {\n    'actual_output': 'generated_text',\n    'retrieved_content': 'source_documents',\n}\n\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=my_task,\n    scoring_metrics=metrics,\n    scoring_key_mapping=scoring_key_mapping,\n    evaluation_name=\"Mapped Evaluation\"\n)\n</code></pre>"},{"location":"deep-dives/runners/evaluation-runner/#response-format","title":"Response Format","text":"<p>The Evaluation Runner returns an <code>EvaluationResult</code> object:</p> Field Type Description <code>run_id</code> <code>str</code> Unique identifier for this evaluation run <code>evaluation_name</code> <code>str</code> Name of the evaluation <code>timestamp</code> <code>str</code> ISO-formatted execution timestamp <code>results</code> <code>List[TestResult]</code> Detailed results for each evaluation input <code>summary</code> <code>Dict[str, Any]</code> Summary of the TestResult objects <code>metadata</code> <code>Dict[str, Any]</code> Evaluation metadata and configuration <p>Each <code>TestResult</code> contains the same structure as described in the Metric Runner documentation.</p>"},{"location":"deep-dives/runners/evaluation-runner/#advanced-features","title":"Advanced Features","text":"Caching Prompt Caching Metadata &amp; Tracking Tracing <p>Cache task outputs and metric results to avoid re-computation across runs. <code>'memory'</code> is fast but non-persistent; <code>'disk'</code> persists across runs.</p> <pre><code>from axion.runners import CacheConfig\n\ncache_config = CacheConfig(\n    use_cache=True,          # Enable caching\n    cache_task=True,         # Cache task outputs\n    cache_type='disk',\n    cache_dir='cache/',\n)\n\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=expensive_task,\n    scoring_metrics=metrics,\n    cache_config=cache_config,\n    evaluation_name=\"Cached Evaluation\"\n)\n</code></pre> <p>Enable provider-level prompt caching to reduce cost and latency when evaluating many items. When enabled, the system prompt and few-shot examples prefix is marked as cacheable \u2014 providers reuse the cached prefix across items instead of re-processing it on every call.</p> <pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import Faithfulness, AnswerRelevancy\n\n# Option 1: Runner-level (applies to all metrics)\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[Faithfulness(), AnswerRelevancy()],\n    evaluation_name=\"Cached Evaluation\",\n    enable_prompt_caching=True,\n)\n\n# Option 2: Per-metric (granular control)\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[\n        Faithfulness(enable_prompt_caching=True),\n        AnswerRelevancy(),  # caching disabled for this metric\n    ],\n    evaluation_name=\"Selective Caching\",\n)\n</code></pre> <p>Provider behavior:</p> Provider Behavior Anthropic Explicit caching with 5-min TTL; ~90% savings on cached reads OpenAI Automatic caching for prompts &gt;1024 tokens; markers are harmless Others LiteLLM silently ignores markers for unsupported providers <p>Minimum prefix length</p> <p>Caching requires the shared prefix (system prompt + few-shot examples) to exceed the provider's minimum: 1024 tokens for most Anthropic/OpenAI models, 2048 tokens for Claude Haiku. Metrics with short prompts and no examples may not benefit.</p> <p>When tracing is enabled, each LLM span includes a <code>cached_tokens</code> attribute for observability.</p> <p>Attach metadata to evaluation runs for experiment tracking and reproducibility.</p> <pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    task=task,\n    scoring_metrics=metrics,\n    evaluation_name=\"Model Comparison v2.1\",\n    evaluation_description=\"Comparing new model against baseline\",\n    evaluation_metadata={\n        'model_version': 'v2.1.0',\n        'baseline_version': 'v1.9.2',\n        'dataset_version': 'eval_set_march_2024',\n        'environment': 'staging',\n        'researcher': 'data_science_team',\n        'tags': ['comparison', 'monthly_eval', 'production_candidate']\n    },\n    run_id=\"evaluation_2024_03_15_001\"\n)\n\n# Access metadata in results\nprint(f\"Model version: {results.metadata['model_version']}\")\nprint(f\"Environment: {results.metadata['environment']}\")\n</code></pre> <p>The runner automatically integrates with Axion's tracing system. When tracing is enabled (Langfuse or Opik), you get full observability of every LLM call.</p> <p>Trace granularity controls how traces are organized:</p> Mode Description Use Case <code>single_trace</code> All evaluations under one parent trace (default) Viewing entire evaluation as one unit <code>separate</code> Each metric execution gets its own trace Detailed per-item analysis <pre><code>results = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy()],\n    evaluation_name=\"RAG Quality Check\",\n    trace_granularity='single_trace'  # or 'separate'\n)\n</code></pre> <p>Trace hierarchy produced by the runner:</p> <pre><code>RAG Quality Check v2                          # evaluation_name (root span)\n\u251c\u2500 AnswerRelevancy.execute                    # Metric logic span\n\u2502  \u2514\u2500 litellm_structured                      # LLM formatting/parsing\n\u2502     \u2514\u2500 llm_call                             # LLM API call (cost/tokens)\n\u2514\u2500 Faithfulness.execute\n   \u2514\u2500 litellm_structured\n      \u2514\u2500 llm_call\n</code></pre> <p>Enabling tracing:</p> <pre><code>import os\nos.environ['TRACING_MODE'] = 'langfuse'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\nos.environ['LANGFUSE_BASE_URL'] = 'https://us.cloud.langfuse.com'\n\nfrom axion._core.tracing import configure_tracing\nconfigure_tracing()\n\n# Now run your evaluation - traces are automatically captured\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=metrics,\n    evaluation_name=\"My Evaluation\"\n)\n</code></pre> <p>See the Tracing Documentation for more configuration options.</p> <p>Runners API Reference  Metric Runner Deep Dive  API Runner Deep Dive </p>"},{"location":"deep-dives/runners/metric-runner/","title":"Metric Runner","text":"<p>The Metric Runner orchestrates evaluation metrics across multiple frameworks \u2014 Axion, Ragas, and DeepEval \u2014 through a single interface. It handles concurrency, caching, error recovery, and progress tracking so you can focus on choosing the right metrics.</p>"},{"location":"deep-dives/runners/metric-runner/#what-youll-learn","title":"What You'll Learn","text":"1 <p>Batch Processing</p> <p>Run metrics across datasets with configurable concurrency, progress bars, and automatic result collection.</p> 2 <p>Multi-Framework</p> <p>Mix Axion, Ragas, and DeepEval metrics in a single run \u2014 each is automatically routed to the correct executor.</p> 3 <p>Custom Runners</p> <p>Register your own metric framework integrations with a decorator or manual registration.</p> 4 <p>Standardized Output</p> <p>Every runner returns <code>TestResult</code> / <code>MetricScore</code> objects regardless of the underlying framework.</p>"},{"location":"deep-dives/runners/metric-runner/#quick-start","title":"Quick Start","text":"View Available Runners<pre><code>from axion.runners import MetricRunner\n\n# Display all registered Metric runners with their options\nMetricRunner.display()\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#usage-patterns","title":"Usage Patterns","text":"Basic Batch Advanced Configuration Direct Executor <p>Pass a list of metrics and dataset items. The runner handles concurrency and returns a <code>TestResult</code> per item with all metric scores attached.</p> <pre><code>from axion.runners import MetricRunner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize metric and runner\nmetric = AnswerRelevancy()\nrunner = MetricRunner(metrics=[metric], max_concurrent=5)\n\n# Prepare evaluation data\ndata_items = [\n    DatasetItem(\n        query=\"How do I reset my password?\",\n        actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n        expected_output=\"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n        retrieved_content=[\"Password reset is available via the login page. Users receive a reset link by email.\"]\n    ),\n    # Add more data items...\n]\n\n# Execute batch evaluation\nresults = await runner.execute_batch(data_items)\n\n# Process results\nfor result in results:\n    print(f\"Test Case: {result.test_case.query}\")\n    for score in result.score_results:\n        print(f\"  {score.name}: {score.score} (passed: {score.passed})\")\n</code></pre> <p>Configure caching, error handling, custom thresholds, and summary generation. The runner accepts <code>Dataset</code>, <code>List[DatasetItem]</code>, or a Pandas <code>DataFrame</code> as input.</p> <pre><code>from axion.runners import MetricRunner, CacheManager, ErrorConfig, CacheConfig\nfrom axion.runners.summary import MetricSummary\nfrom axion.metrics import AnswerRelevancy, Faithfulness\n\nimport pandas as pd\n\ndataframe = pd.DataFrame({\n    'id': '0000001',\n    'query': \"How do I reset my password?\",\n    'actual_output': \"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    'expected_output': \"Navigate to login, click 'Forgot Password', and follow the reset link sent to your email.\",\n    'retrieved_content': [[\"Password reset is available via the login page. Users receive a reset link by email.\"]]\n})\n\n# Initialize metrics\nmetrics = [\n    AnswerRelevancy(),\n    Faithfulness()\n]\n\n# Advanced configuration\nrunner = MetricRunner(\n    metrics=metrics,\n    max_concurrent=10,                          # Concurrency limit\n    cache_manager=CacheManager(                 # Optional Caching\n        CacheConfig(cache_type='memory')),\n    error_config=ErrorConfig(                   # Optional Error handling\n        ignore_errors=True,\n    ),\n    thresholds={                                # Optional Custom thresholds\n        'AnswerRelevancy': 0.75,\n        'Faithfulness': 0.85\n    },\n    summary_generator=MetricSummary(),\n)\n\n# Execute with progress tracking\nresults = await runner.execute_batch(\n    evaluation_inputs=dataframe, # Can pass Dataset, List of DatasetItems or Pandas Dataframe\n)\n</code></pre> <p>For single-item evaluation, use an executor directly instead of the batch runner.</p> <pre><code>from axion.runners import AxionRunner\nfrom axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize specific executor directly\nmetric = AnswerRelevancy()\nexecutor = AxionRunner(metric=metric, threshold=0.7)\n\n# Execute single evaluation\ndata_item = DatasetItem(\n    query=\"What is machine learning?\",\n    actual_output=\"Machine learning is a subset of AI...\",\n    expected_output=\"ML is a method of data analysis...\"\n)\n\nresult = await executor.execute(data_item)\nprint(f\"Score: {result.score}, Explanation: {result.explanation}\")\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#available-runners","title":"Available Runners","text":"<p>The <code>MetricRunner</code> automatically detects which framework a metric belongs to and routes it to the correct executor. You can also mix metrics from different frameworks in a single run.</p>  Axion Ragas DeepEval <p>Registry Key: <code>axion</code> \u00b7 Class: <code>AxionRunner</code></p> <p>Executes native metrics from the Axion framework. This is the default \u2014 any <code>BaseMetric</code> subclass is automatically routed here.</p> <pre><code>from axion.metrics import AnswerRelevancy\n\nmetric = AnswerRelevancy()\nrunner = MetricRunner(metrics=[metric])\n\n# The runner automatically detects this is an Axion metric\n# and uses AxionRunner internally\n</code></pre> <p>Registry Key: <code>ragas</code> \u00b7 Class: <code>RagasRunner</code></p> <p>Executes metrics from the Ragas evaluation framework.</p> <pre><code>from ragas.metrics import Faithfulness\nfrom axion.integrations.models import LiteLLMRagas\n\n# Ragas metrics are automatically detected and executed with RagasRunner\nmetrics = [Faithfulness(llm=LiteLLMRagas())] # LiteLLMRagas() is optional\nrunner = MetricRunner(metrics=metrics)\n\n# Requires actual_output and optionally retrieved_content\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"Response text...\",\n    retrieved_content=[\"Context 1\", \"Context 2\"]\n)\n</code></pre> <p>Registry Key: <code>deepeval</code> \u00b7 Class: <code>DeepEvalRunner</code></p> <p>Executes metrics from the DeepEval framework.</p> <pre><code>from deepeval.metrics import AnswerRelevancyMetric\nfrom axion.integrations.models import LiteLLMDeepEval\n\n# DeepEval metrics are automatically detected\nmetrics = [\n    AnswerRelevancyMetric(model=LiteLLMDeepEval()), # LiteLLMDeepEval() is optional\n]\nrunner = MetricRunner(metrics=metrics)\n\n# Execute evaluation\nresults = await runner.execute_batch(evaluation_data)\n</code></pre>"},{"location":"deep-dives/runners/metric-runner/#response-format","title":"Response Format","text":"<p>All metric runners return standardized <code>TestResult</code> objects:</p> Field Type Description <code>test_case</code> <code>DatasetItem</code> Original evaluation input <code>score_results</code> <code>List[MetricScore]</code> List of metric evaluation results <p>Each <code>MetricScore</code> contains:</p> Field Type Description <code>id</code> <code>str</code> Unique identifier for the test case <code>name</code> <code>str</code> Metric name <code>score</code> <code>float</code> Numerical score (0.0\u20131.0 typically) <code>threshold</code> <code>float</code> Configured threshold for pass/fail <code>passed</code> <code>bool</code> Whether score meets threshold <code>explanation</code> <code>str</code> Detailed explanation (if available) <code>source</code> <code>str</code> Framework source (axion, ragas, deepeval) <code>timestamp</code> <code>str</code> ISO-formatted execution timestamp"},{"location":"deep-dives/runners/metric-runner/#creating-custom-runners","title":"Creating Custom Runners","text":"<p>Extend the Metric Runner system by registering your own framework integration.</p>  Decorator Registration Manual Registration <p>The recommended approach \u2014 use <code>@MetricRunner.register()</code> to automatically register your runner class.</p> <pre><code>from axion.runners.metric import MetricRunner, BaseMetricRunner\nfrom axion.schema import MetricScore\nfrom axion.dataset import DatasetItem\nfrom typing import Union, Dict, Any\n\n@MetricRunner.register('custom_framework')\nclass CustomFrameworkRunner(BaseMetricRunner):\n    \"\"\"Custom metric framework runner.\"\"\"\n\n    _name = 'custom_framework'\n\n    async def execute(self, input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; MetricScore:\n        \"\"\"Execute metric using custom framework.\"\"\"\n        input_data = self.format_input(input_data)\n\n        try:\n            # Your custom framework integration here\n            score = await self.metric.evaluate(\n                query=input_data.query,\n                response=input_data.actual_output,\n                reference=input_data.expected_output\n            )\n\n            return MetricScore(\n                id=input_data.id,\n                name=self.metric_name,\n                score=score,\n                threshold=self.threshold,\n                passed=self._has_passed(score),\n                source=self.source\n            )\n\n        except Exception as e:\n            return self._create_error_score(input_data.id, e)\n</code></pre> <p>For cases where decorators aren't suitable \u2014 define the class first, then register.</p> <pre><code># Define your custom runner class\nclass AnotherCustomRunner(BaseMetricRunner):\n    _name = 'another_custom'\n\n    async def execute(self, input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; MetricScore:\n        # Implementation here\n        pass\n\n# Manual registration\nMetricRunner.register('another_custom')(AnotherCustomRunner)\n</code></pre> <p>Once registered, custom metrics are automatically detected and routed:</p> <pre><code>from your_custom_framework import CustomMetric\n\ncustom_metric = CustomMetric()\nrunner = MetricRunner(metrics=[custom_metric])\n\n# The runner automatically uses your CustomFrameworkRunner\nresults = await runner.execute_batch(evaluation_data)\n</code></pre> <p>Runners API Reference  Evaluation Runner Deep Dive  API Runner Deep Dive </p>"},{"location":"deep-dives/search/google/","title":"SerpAPI Web Search","text":""},{"location":"deep-dives/search/google/#overview","title":"Overview","text":"<p>SerpAPI is a real-time API that allows developers and businesses to access search engine results from Google, Bing, Baidu, Yahoo, Yandex, eBay, and YouTube. It is known for its fast speed, variety of Google-related APIs, and affordable pricing plans. SerpAPI is a valuable tool for developers, marketers, and data analysts who need to gather search engine data for various purposes</p>"},{"location":"deep-dives/search/google/#api-access","title":"API Access","text":"<p>To use SerpAPI, obtain an API key and configure your application accordingly.</p>"},{"location":"deep-dives/search/google/#steps-to-access-serpapi","title":"Steps to Access SerpAPI","text":"<ol> <li>Sign up and obtain an API key from SerpAPI.</li> <li>Pass the API key in API requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to retrieve structured search results.</li> </ol> <p>Please note that this file will also search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/google/#code-examples","title":"Code Examples","text":"GoogleRetriever Example<pre><code>from axion.search import GoogleRetriever\n\napi_client = GoogleRetriever(num_web_results=3)\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n</code></pre> <p>Search API Reference </p>"},{"location":"deep-dives/search/tavily/","title":"Tavily WebSearch","text":""},{"location":"deep-dives/search/tavily/#overview","title":"Overview","text":"<p>Tavily is an AI-powered search API designed for AI agents and applications that need real-time, accurate, and comprehensive search results. It provides intelligent web search capabilities with content extraction and crawling features to enhance AI applications with up-to-date information.</p>"},{"location":"deep-dives/search/tavily/#api-access","title":"API Access","text":"<p>To integrate Tavily into your application, obtain an API key and configure it accordingly.</p>"},{"location":"deep-dives/search/tavily/#steps-to-access-tavily-api","title":"Steps to Access Tavily API","text":"<ol> <li>Sign up and obtain an API key from Tavily API.</li> <li>Pass the API key in requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to fetch real-time search results.</li> </ol> <p>Please note that this file will search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/tavily/#code-examples","title":"Code Examples","text":"TavilyRetriever Example<pre><code>from axion.search import TavilyRetriever\n\napi_client = TavilyRetriever(num_web_results=3, crawl_pages=False)\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n\n# Crawl specific pages\nresults = await api_client.crawl('https://www.espn.com/')\n\n# Extract content from URLs\nresults = await api_client.extract('https://www.espn.com/')\n</code></pre> <p>Search API Reference </p>"},{"location":"deep-dives/search/you/","title":"You.com WebSearch","text":""},{"location":"deep-dives/search/you/#overview","title":"Overview","text":"<p>You.com is an AI-powered search engine and conversational AI platform that aims to enhance productivity and provide users with a more personalized and interactive search experience, going beyond traditional search and offering tools like AI agents and chatbots</p>"},{"location":"deep-dives/search/you/#api-access","title":"API Access","text":"<p>To integrate You.com into your application, obtain an API key and configure it accordingly.</p>"},{"location":"deep-dives/search/you/#steps-to-access-youcom-api","title":"Steps to Access You.com API","text":"<ol> <li>Sign up and obtain an API key from You.com API.</li> <li>Pass the API key in requests or store it securely in your environment variables.</li> <li>Implement API calls within your AI application to fetch real-time search results.</li> </ol> <p>Please note that this file will search for these credentials in an <code>.env</code> file at the project's root directory.</p>"},{"location":"deep-dives/search/you/#code-examples","title":"Code Examples","text":"YouRetriever Example<pre><code>from axion.search import YouRetriever\n\napi_client = YouRetriever(num_web_results=5, endpoint='news') # supports both \"news\" and \"search\" endpoints\nresults = await api_client.execute('What are Ciena Corp challenges?')\nfor result in results:\n    print(result)\n</code></pre> <p>Search API Reference </p>"},{"location":"deep-dives/synthetic/document-qa/","title":"Document Synthetic QA Generation","text":"<p>The Synthetic QA Generation System provides a user flexible solution for creating question-answer pairs from unstructured documents. This system enables scalable generation of evaluation datasets for RAG models and other conversational systems through an intelligent, multi-stage workflow with built-in quality validation.</p> <p>Note:  This is build as an agentic workflow and will trigger multiple LLM calls, so please be mindful of potential rate limits and associated costs.</p>"},{"location":"deep-dives/synthetic/document-qa/#pipeline-stages","title":"Pipeline Stages","text":"1 <p>Document Ingestion</p> <p>Load and prepare documents from directories.</p> 2 <p>Content Chunking</p> <p>Intelligent splitting while preserving context.</p> 3 <p>Statement Extraction</p> <p>Extract factual, standalone statements from chunks.</p> 4 <p>Question Generation</p> <p>Create diverse questions across multiple types.</p> 5 <p>Answer Generation</p> <p>Generate accurate, grounded answers.</p> 6 <p>Validation &amp; Reflection</p> <p>Quality assessment with iterative improvement.</p>"},{"location":"deep-dives/synthetic/document-qa/#langraph-workflow","title":"Langraph Workflow","text":""},{"location":"deep-dives/synthetic/document-qa/#key-components","title":"Key Components","text":"\u2713 <p>DocumentQAGenerator</p> <p>Main orchestrator for the entire pipeline.</p> \u2713 <p>GenerationParams</p> <p>Configuration object for all generation parameters.</p> \u2713 <p>QAWorkflowGraph</p> <p>A LangGraph-based workflow execution engine.</p> \u2713 <p>Quality Validators</p> <p>Multi-dimensional assessment and feedback.</p>"},{"location":"deep-dives/synthetic/document-qa/#important-note","title":"Important Note","text":"<p>Synthetic data generation with LLMs is not a one-size-fits-all process. The quality and usefulness of generated data depend heavily on the prompts you use\u2014both during the answer generation phase and within your actual production model.</p> <p>Best Practice: Reverse-engineer your real application\u2019s instructions when creating meta-prompts. Provide the generation LLM with a realistic example question and answer that mirrors your production setup.</p> <p>This matters because you\u2019re effectively working with two different prompt contexts:</p> <ol> <li>Generation LLM\u2019s prompt \u2013 used to produce synthetic answers.</li> <li>Your real model\u2019s prompt \u2013 used during inference in your application.</li> </ol> <p>If these prompts differ significantly, the resulting style, tone, and level of detail can vary widely\u2014leading to synthetic data that doesn\u2019t truly represent your production environment. Aligning them ensures consistency and reliability in evaluation and training.</p>"},{"location":"deep-dives/synthetic/document-qa/#configuration","title":"Configuration","text":""},{"location":"deep-dives/synthetic/document-qa/#generationparams","title":"GenerationParams","text":"<p>The <code>GenerationParams</code> class provides comprehensive configuration for the generation process:</p> <pre><code>from axion.synthetic import GenerationParams\n\nparams = GenerationParams(\n    # Content Processing\n    splitter_type=\"semantic\",                    # or \"sentence\"\n    chunk_size=2048,                            # For sentence splitter\n    breakpoint_percentile_threshold=95,         # For semantic splitter\n\n    # Generation Control\n    num_pairs=1,                                # Number of QA pairs to generate\n    statements_per_chunk=5,                     # Statements extracted per chunk\n\n    # Question Configuration\n    question_types=[\"factual\", \"analytical\"],   # Types of questions\n    difficulty=\"medium\",                        # easy, medium, hard\n\n    # Answer Configuration\n    answer_length=\"medium\",                     # short, medium, long\n\n    # Quality Control\n    validation_threshold=0.8,                   # Quality threshold (0-1)\n    max_reflection_iterations=3,                # Max improvement iterations\n\n    # Customization\n    custom_guidelines=\"Focus on technical accuracy\",\n    example_question=\"What is the primary function of X?\",\n    example_answer=\"The primary function of X is...\"\n)\n</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#usage-patterns","title":"Usage Patterns","text":""},{"location":"deep-dives/synthetic/document-qa/#configuration-options","title":"Configuration Options","text":"<pre><code>from axion.synthetic import DocumentQAGenerator, GenerationParams\nimport pandas as pd\nparams = GenerationParams(\n    # Content processing\n    splitter_type=\"semantic\",\n    breakpoint_percentile_threshold=90,\n    statements_per_chunk=8,\n\n    # Generation scope\n    num_pairs=1,\n    question_types=[\"factual\", \"conceptual\", \"analytical\", \"application\"],\n    difficulty=\"easy\",\n    answer_length=\"long\",\n\n    # Quality requirements\n    validation_threshold=0.85,\n    max_reflection_iterations=5,\n\n    # Domain customization\n    custom_guidelines=\"\"\"\n    Focus on technical accuracy and real-world applications.\n    Emphasize practical implementation details.\n    Include architectural considerations where relevant.\n    \"\"\",\n\n    example_question=\"How would you implement X in a production environment?\",\n    example_answer=\"To implement X in production, you would need to consider...\"\n)\n\nqa_generator = DocumentQAGenerator(\n    llm=llm,\n    params=params,\n    embed_model=embed_model, # required for semantic splitting\n    max_concurrent=3  # Conservative for complex processing\n)\n# Generate QA pairs from directory\nresults = await qa_generator.generate_from_directory('path/to/documents/')\npd.DataFrame(results)\n</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#parameter-reference","title":"Parameter Reference","text":""},{"location":"deep-dives/synthetic/document-qa/#content-processing-parameters","title":"Content Processing Parameters","text":"Parameter Type Default Description <code>splitter_type</code> <code>\"semantic\"</code> | <code>\"sentence\"</code> <code>\"sentence\"</code> Text splitting strategy <code>chunk_size</code> <code>int</code> <code>2048</code> Target chunk size for sentence splitter <code>breakpoint_percentile_threshold</code> <code>int</code> <code>95</code> Semantic similarity threshold (80-100) <code>statements_per_chunk</code> <code>int</code> <code>5</code> Number of statements to extract per chunk"},{"location":"deep-dives/synthetic/document-qa/#generation-control-parameters","title":"Generation Control Parameters","text":"Parameter Type Default Description <code>num_pairs</code> <code>int</code> <code>10</code> Total QA pairs to generate (1-100) <code>question_types</code> <code>List[str]</code> <code>[\"factual\", \"analytical\"]</code> Types of questions to generate <code>difficulty</code> <code>\"easy\"</code> | <code>\"medium\"</code> | <code>\"hard\"</code> <code>\"medium\"</code> Question complexity level <code>answer_length</code> <code>\"short\"</code> | <code>\"medium\"</code> | <code>\"long\"</code> <code>\"medium\"</code> Target answer length"},{"location":"deep-dives/synthetic/document-qa/#quality-control-parameters","title":"Quality Control Parameters","text":"Parameter Type Default Description <code>validation_threshold</code> <code>float</code> <code>0.8</code> Minimum quality score (0.0-1.0) <code>max_reflection_iterations</code> <code>int</code> <code>3</code> Maximum improvement iterations (1-10)"},{"location":"deep-dives/synthetic/document-qa/#customization-parameters","title":"Customization Parameters","text":"Parameter Type Default Description <code>custom_guidelines</code> <code>str</code> <code>None</code> Additional generation instructions <code>example_question</code> <code>str</code> <code>None</code> Example question for style guidance <code>example_answer</code> <code>str</code> <code>None</code> Example answer for style guidance"},{"location":"deep-dives/synthetic/document-qa/#quality-validation","title":"Quality Validation","text":""},{"location":"deep-dives/synthetic/document-qa/#validation-dimensions","title":"Validation Dimensions","text":"<p>The system evaluates QA pairs across five dimensions:</p> 1 <p>Accuracy</p> <p>Is the answer factually correct and well-grounded?</p> 2 <p>Completeness</p> <p>Does it fully address the question?</p> 3 <p>Relevance</p> <p>Is it directly aligned with the question?</p> 4 <p>Clarity</p> <p>Is the language clear and understandable?</p> 5 <p>Factual Integrity</p> <p>Does it avoid hallucination or extraneous information?</p>"},{"location":"deep-dives/synthetic/document-qa/#reflection-process","title":"Reflection Process","text":"<p>When QA pairs fall below the validation threshold:</p> <pre><code>graph LR\n    I[\"Identify Issues\"] --&gt; F[\"Generate Feedback\"]\n    F --&gt; P[\"Enhance Prompts\"]\n    P --&gt; R[\"Regenerate QA\"]\n    R --&gt; V[\"Re-validate\"]\n    V --&gt;|\"Below threshold\"| I\n    V --&gt;|\"Passes\"| D[\"Done\"]</code></pre>"},{"location":"deep-dives/synthetic/document-qa/#integration-with-evaluation","title":"Integration with Evaluation","text":"<p>This system is built into the <code>Dataset</code> class within Axion.</p> <pre><code>from axion.synthetic.schema import GenerationParams\nfrom axion.dataset import Dataset\n\ndataset = Dataset(name='Product-Growth-Knowledge')\n\nparams = GenerationParams(\n    num_pairs=1,\n    question_types=[\"factual\", \"conceptual\", \"application\"],\n    difficulty=\"medium\",\n    max_chunk_size=4000,\n    statements_per_chunk=5,\n    answer_length=\"medium\",\n    splitter_type=\"sentence\",\n    custom_guidelines=\"Focus on application scenarios in the questions.\",\n    max_reflection_iterations=3,\n    validation_threshold=0.7\n)\n\ndataset.synthetic_generate_from_directory(\n    directory_path='small_docs/',\n    llm=llm,\n    params=params\n)\n</code></pre> <p>Synthetic API Reference  Datasets Guide </p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get started with Axion in minutes.</p>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/ax-foundry/axion.git\ncd axion\n\n# Create virtual environment (optional)\npython -m venv venv\nsource venv/bin/activate  # On Windows: .\\venv\\Scripts\\activate\n\n# Install the package\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing or running tests:</p> <pre><code># Install with dev dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Axion uses optional dependencies to keep the core installation lightweight. Install extras based on what you need:</p>  Tracing Providers Search Integrations LlamaIndex Extensions Visualization <pre><code># Logfire (OpenTelemetry-based)\npip install -e \".[logfire]\"\n\n# Langfuse (LLM-specific observability)\npip install -e \".[langfuse]\"\n\n# Opik\npip install -e \".[opik]\"\n\n# All tracing providers\npip install -e \".[tracing]\"\n</code></pre> <pre><code># Google Search via SerpAPI\npip install -e \".[search]\"\n</code></pre> <p>Requires <code>SERPAPI_KEY</code> environment variable.</p> <pre><code># HuggingFace embeddings and LLMs\npip install -e \".[huggingface]\"\n\n# Docling document reader (PDF, DOCX, HTML, images)\npip install -e \".[docling]\"\n</code></pre> <pre><code># Matplotlib and Seaborn for plotting\npip install -e \".[plotting]\"\n</code></pre>"},{"location":"getting-started/installation/#combining-extras","title":"Combining Extras","text":"<p>Install multiple extras at once:</p> <pre><code># Example: search + tracing + plotting\npip install -e \".[search,tracing,plotting]\"\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":".env File Programmatic <p>Create a <code>.env</code> file in your project root:</p> <pre><code># Required for LLM-based metrics\nOPENAI_API_KEY=&lt;your-key&gt;\n\n# Optional: Logging settings\nLOG_LEVEL=\"INFO\"\nLOG_RICH=\"true\"\n\n# Optional: Tracing (auto-detects if credentials present)\nTRACING_MODE=\"langfuse\"  # or: noop, logfire, otel, opik\nLANGFUSE_SECRET_KEY=&lt;your-key&gt;\nLANGFUSE_PUBLIC_KEY=&lt;your-key&gt;\n</code></pre> <p>Use <code>axion.init()</code> to configure both logging and tracing at once:</p> <pre><code>import axion\n\n# Initialize with custom settings\naxion.init(\n    tracing='langfuse',  # or: noop, logfire, otel, opik\n    log_level='DEBUG',\n    log_rich=True,\n)\n\n# Or just let it auto-configure from environment variables\n# (no init() call needed - works automatically)\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from axion import Dataset, metric_registry\n\n# Check available metrics\nprint(metric_registry.list_metrics())\n</code></pre> <p>Working with Datasets  Metrics &amp; Evaluation  Agent Evaluation Playbook </p>"},{"location":"guides/caliberhq/","title":"CaliberHQ","text":"<p>CaliberHQ is a toolkit for calibrating LLM-as-a-judge evaluators against a human-labeled baseline. Instead of writing rubrics in the abstract, you work backward from real outputs so the evaluator learns what actually matters for your use case.</p>"},{"location":"guides/caliberhq/#quick-start","title":"Quick start","text":"<pre><code>from axion.caliber import CalibrationSession\n\nsession = CalibrationSession()\n\n# 1) Upload your data\nsession.upload_records(\n    [\n        {\"id\": \"r1\", \"query\": \"\u2026\", \"actual_output\": \"\u2026\"},\n        {\"id\": \"r2\", \"query\": \"\u2026\", \"actual_output\": \"\u2026\"},\n    ]\n)\n\n# 2) Add human annotations\nsession.annotate(\"r1\", score=1, notes=\"Good response\")\nsession.annotate(\"r2\", score=0, notes=\"Factually incorrect\")\n\n# 3) Run LLM evaluation + alignment metrics\nresult = await session.evaluate(\n    criteria=\"Score 1 if accurate and helpful, 0 otherwise\",\n    model_name=\"gpt-4o\",\n    llm_provider=\"openai\",\n)\n\nprint(f\"Accuracy: {result.metrics.accuracy:.1%}\")\nprint(f\"Cohen's Kappa: {result.metrics.cohen_kappa:.3f}\")\n</code></pre>"},{"location":"guides/caliberhq/#the-6-step-workflow","title":"The 6-step workflow","text":"1 <p>Upload</p> <p>Load your evaluation data from CSV or programmatically.</p> 2 <p>Annotate</p> <p>Add human judgments (Accept=1, Reject=0) with optional notes.</p> 3 <p>Evaluate</p> <p>Run the LLM judge and compute alignment metrics (accuracy, kappa, F1).</p> 4 <p>Discover Patterns</p> <p>Cluster any text evidence into themes and distill learning artifacts via a full pipeline.</p> 5 <p>Analyze Misalignments</p> <p>Examine false positives and false negatives (judge vs human).</p> 6 <p>Optimize</p> <p>Produce improved evaluation criteria based on misalignment patterns.</p> <pre><code>graph LR\n    U[\"Upload\"] --&gt; A[\"Annotate\"]\n    A --&gt; E[\"Evaluate\"]\n    E --&gt; D[\"Discover Patterns\"]\n    D --&gt; M[\"Analyze Misalignments\"]\n    M --&gt; O[\"Optimize Prompt\"]\n    O --&gt;|\"Iterate\"| E</code></pre>"},{"location":"guides/caliberhq/#usage-patterns","title":"Usage Patterns","text":"Session-Based (Recommended) Direct Components (Advanced) UI / Web Pipeline <p>Use <code>CalibrationSession</code> for state management and serialization. Good for scripts, web APIs, and notebooks.</p> <pre><code>from axion.caliber import CalibrationSession\n\nsession = CalibrationSession()\nsession.upload_csv(\"data.csv\")\nsession.annotate(\"r1\", score=1, notes=\"Good\")\nresult = await session.evaluate(criteria=\"\u2026\")\n</code></pre> <p>Use individual components for fine-grained control over each step.</p> <pre><code>from axion.caliber import AnnotationManager, EvaluationRunner, UploadHandler\n\nupload = UploadHandler().from_csv(\"data.csv\")\nmanager = AnnotationManager(upload.records)\nmanager.annotate(\"r1\", score=1, notes=\"Good\")\n\n# Run evaluation with your own config (see `EvaluationConfig`)\nrunner = EvaluationRunner()\nresult = await runner.run(upload.records, manager.get_annotations_dict())\n</code></pre> <p>Use <code>WebCaliberHQ</code> for JSON-serializable results with optional progress callbacks \u2014 ideal for web or UI integrations.</p> <pre><code>from axion.align import WebCaliberHQ\n\nweb_eval = WebCaliberHQ(dataset, PassFailMetric())\npayload = web_eval.execute(\n    as_dict=True,\n    on_progress=lambda current, total: print(current, total),\n)\n\n# payload[\"results\"] -&gt; list of row dicts\n# payload[\"metrics\"] -&gt; summary metrics\n# payload[\"confusion_matrix\"] -&gt; confusion matrix dict\n</code></pre> <p>You can also construct a dataset from uploaded records:</p> <pre><code>web_eval = WebCaliberHQ.from_records(records, PassFailMetric())\npayload = web_eval.execute(as_dict=True)\n</code></pre>"},{"location":"guides/caliberhq/#key-components","title":"Key Components","text":"Category Imports Core session <code>CalibrationSession</code> Step components <code>UploadHandler</code>, <code>AnnotationManager</code>, <code>EvaluationRunner</code> Analysis tools <code>PatternDiscovery</code>, <code>EvidencePipeline</code>, <code>MisalignmentAnalyzer</code>, <code>PromptOptimizer</code>, <code>ExampleSelector</code> Renderers <code>ConsoleCaliberRenderer</code>, <code>NotebookCaliberRenderer</code>, <code>JsonCaliberRenderer</code>"},{"location":"guides/caliberhq/#demo","title":"Demo","text":"<p>Run the demo script to see the full workflow in action:</p> <pre><code># Basic demo (no API key needed)\npython examples/caliber_demo.py\n\n# Full end-to-end with LLM calls\nOPENAI_API_KEY=your-key python examples/caliber_demo.py --full\n</code></pre>"},{"location":"guides/caliberhq/#workflow-detail","title":"Workflow Detail","text":"<p>The CaliberHQ workflow progresses through three main phases:</p> <pre><code>graph TD\n    subgraph \"Phase 1: Data Preparation\"\n        U[\"Upload CSV\"] --&gt; V[\"Validate Columns\"]\n        V --&gt; S[\"Load into Store\"]\n    end\n    subgraph \"Phase 2: Human Review\"\n        S --&gt; R[\"Review Records\"]\n        R --&gt; J[\"Accept / Reject\"]\n        J --&gt; N[\"Add Notes\"]\n    end\n    subgraph \"Phase 3: Evaluation &amp; Alignment\"\n        N --&gt; E[\"Run LLM Judge\"]\n        E --&gt; M[\"Compute Alignment Metrics\"]\n        M --&gt; MA[\"Misalignment Analysis\"]\n        MA --&gt; PO[\"Prompt Optimization\"]\n        PO --&gt; PI[\"Pattern Insights\"]\n        PI --&gt;|\"Iterate\"| E\n    end</code></pre>"},{"location":"guides/caliberhq/#alignment-metrics","title":"Alignment Metrics","text":"<p>After evaluation, CaliberHQ computes alignment between the LLM judge and human annotations:</p> Metric What It Measures Accuracy How often the LLM agrees with humans Precision How often LLM \"Accept\" is correct Recall How many human \"Accept\" cases the LLM catches F1 Score Harmonic mean of precision and recall Cohen's Kappa Agreement beyond chance (&gt;0.6 = substantial)"},{"location":"guides/caliberhq/#misalignment-analysis","title":"Misalignment Analysis","text":"<p>The system identifies two types of disagreement:</p> FP <p>False Positives</p> <p>LLM accepts, human rejects. The judge is too lenient \u2014 missing quality issues humans catch.</p> FN <p>False Negatives</p> <p>LLM rejects, human accepts. The judge is too strict \u2014 penalizing acceptable responses.</p>"},{"location":"guides/caliberhq/#feedback-loop","title":"Feedback Loop","text":"<pre><code>graph LR\n    O[\"Optimized Prompt\"] --&gt; R[\"Re-run Evaluation\"]\n    R --&gt; C[\"Compare Metrics\"]\n    C --&gt;|\"Aligned\"| D[\"Done\"]\n    C --&gt;|\"Not aligned\"| O</code></pre>"},{"location":"guides/caliberhq/#defining-a-metric","title":"Defining a Metric","text":"<p>Define an LLM-as-a-judge metric with a clear instruction. The metric can be as simple as a binary pass/fail rubric.</p>  Simple Rubric With CaliberHQ <pre><code>from axion.metrics.base import BaseMetric\n\nclass PassFailMetric(BaseMetric):\n    instruction = (\n        \"Score 1 if the answer is correct and complete. \"\n        \"Otherwise score 0 and explain why.\"\n    )\n</code></pre> <pre><code>from axion.align import CaliberHQ\nfrom axion.dataset import Dataset, DatasetItem\n\nitems = [\n    DatasetItem(\n        id=\"item-1\",\n        query=\"What is the capital of France?\",\n        expected_output=\"Paris\",\n        actual_output=\"Paris.\",\n    ),\n    DatasetItem(\n        id=\"item-2\",\n        query=\"What is 2+2?\",\n        expected_output=\"4\",\n        actual_output=\"5\",\n    ),\n]\n\ndataset = Dataset(items=items)\nevaluator = CaliberHQ(dataset, PassFailMetric())\nevaluator.annotate()  # optional if judgments are already present\nresults_df = evaluator.execute()\n</code></pre>"},{"location":"guides/caliberhq/#intelligent-example-selection","title":"Intelligent Example Selection","text":"<p>When providing few-shot examples to calibrate your LLM judge, use <code>ExampleSelector</code> for smarter selection instead of naive slicing.</p>  Balanced Misalignment-Guided Pattern-Aware <p>Default \u2014 50/50 accept/reject sampling for unbiased baselines.</p> <pre><code>from axion.align import ExampleSelector, SelectionStrategy\n\nselector = ExampleSelector(seed=42)\nresult = selector.select(records, annotations, count=6)\n</code></pre> <p>Prioritize false positive and false negative cases from a prior evaluation run.</p> <pre><code>result = selector.select(\n    records, annotations, count=6,\n    strategy=SelectionStrategy.MISALIGNMENT_GUIDED,\n    eval_results=prior_results\n)\n</code></pre> <p>Cover discovered failure patterns to ensure all categories are represented.</p> <pre><code>result = selector.select(\n    records, annotations, count=6,\n    strategy=SelectionStrategy.PATTERN_AWARE,\n    patterns=discovered_patterns\n)\n</code></pre> Strategy Use Case <code>BALANCED</code> Default \u2014 50/50 accept/reject sampling <code>MISALIGNMENT_GUIDED</code> Prioritize FP/FN cases from prior eval <code>PATTERN_AWARE</code> Cover discovered failure patterns <p>See Example Selector Deep Dive for detailed usage.</p>"},{"location":"guides/caliberhq/#renderers","title":"Renderers","text":"<p>CaliberHQ uses a renderer interface so UIs can plug in without changing core logic.</p>  Notebook Console JSON <pre><code>from axion.align import CaliberHQ, NotebookCaliberHQRenderer\n\nevaluator = CaliberHQ(dataset, PassFailMetric(), renderer=NotebookCaliberHQRenderer())\nevaluator.execute()\n</code></pre> <pre><code>from axion.align import CaliberHQ, ConsoleCaliberHQRenderer\n\nevaluator = CaliberHQ(dataset, PassFailMetric(), renderer=ConsoleCaliberHQRenderer())\nevaluator.execute()\n</code></pre> <pre><code>from axion.align import CaliberHQ, JsonCaliberHQRenderer\n\nevaluator = CaliberHQ(dataset, PassFailMetric(), renderer=JsonCaliberHQRenderer())\nevaluator.execute()\n</code></pre> <p>Caliber API Reference  Pattern Discovery Deep Dive  Example Selector Deep Dive  Metrics Guide </p>"},{"location":"guides/datasets/","title":"Working with Datasets","text":"<p>Axion uses <code>Dataset</code> and <code>DatasetItem</code> classes to manage evaluation data for both single-turn and multi-turn conversations.</p>"},{"location":"guides/datasets/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset, DatasetItem\n\n# Create a single evaluation item\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"The capital of France is Paris.\",\n    expected_output=\"Paris is the capital of France.\",\n    retrieved_content=[\"France is a country in Europe. Paris is its capital.\"]\n)\n\n# Create a dataset\ndataset = Dataset.create(name=\"my-eval-dataset\", items=[item])\n</code></pre>"},{"location":"guides/datasets/#datasetitem-fields","title":"DatasetItem Fields","text":"1 <p>Core Fields</p> <p>Query, actual/expected output, conversation history, and unique identifiers.</p> 2 <p>Retrieval &amp; RAG</p> <p>Retrieved content, actual/expected rankings for IR evaluation.</p> 3 <p>Tool &amp; Agent</p> <p>Tool calls made, expected tool calls, and custom user tags.</p> 4 <p>Evaluation &amp; Metadata</p> <p>Judgments, critiques, acceptance criteria, latency, traces, and metadata.</p>"},{"location":"guides/datasets/#core-fields","title":"Core Fields","text":"Field Description Required <code>id</code> Unique identifier (auto-generated UUID7 if not provided) No <code>query</code> The user's question or input Yes* <code>actual_output</code> The agent's response Yes <code>expected_output</code> Ground truth / expected answer For some metrics <code>conversation</code> Multi-turn conversation history Yes* <p>*Either <code>query</code> or <code>conversation</code> is required.</p>"},{"location":"guides/datasets/#retrieval-rag-fields","title":"Retrieval &amp; RAG Fields","text":"Field Description <code>retrieved_content</code> List of retrieved context strings <code>actual_ranking</code> Ordered list of retrieved items with scores <code>expected_ranking</code> Ground truth ranking for IR evaluation"},{"location":"guides/datasets/#tool-agent-fields","title":"Tool &amp; Agent Fields","text":"Field Description <code>tools_called</code> Tools/functions actually called by the agent <code>expected_tools</code> Expected tool calls for evaluation <code>user_tags</code> Custom tags applied to all tool calls"},{"location":"guides/datasets/#evaluation-fields","title":"Evaluation Fields","text":"Field Description <code>judgment</code> Binary/categorical decision (e.g., pass/fail, 1/0) <code>critique</code> Detailed explanation supporting the judgment <code>acceptance_criteria</code> List of criteria for acceptable responses <code>latency</code> Response time in seconds"},{"location":"guides/datasets/#metadata-tracing","title":"Metadata &amp; Tracing","text":"Field Description <code>metadata</code> Additional metadata as JSON string <code>additional_input</code> Extra key-value pairs for evaluation context <code>additional_output</code> Extra outputs from the system <code>trace</code> Execution trace information (JSON string) <code>trace_id</code> Trace ID from tracing provider <code>observation_id</code> Observation ID from tracing provider"},{"location":"guides/datasets/#creating-datasets","title":"Creating Datasets","text":""},{"location":"guides/datasets/#factory-method","title":"Factory Method","text":"<pre><code># Create with initial items (recommended)\ndataset = Dataset.create(\n    name=\"qa-evaluation\",\n    description=\"Customer support QA pairs\",\n    items=[\n        {\"query\": \"How do I reset my password?\", \"expected_output\": \"...\"},\n        {\"query\": \"What are your hours?\", \"expected_output\": \"...\"},\n    ]\n)\n\n# Create from simple query strings\ndataset = Dataset.create(\n    name=\"queries-only\",\n    items=[\"What is Python?\", \"How does async work?\"]  # Strings become queries\n)\n</code></pre>"},{"location":"guides/datasets/#adding-items","title":"Adding Items","text":"<pre><code>dataset = Dataset(name=\"my-dataset\")\n\n# Add a single item\ndataset.add_item({\n    \"query\": \"What is machine learning?\",\n    \"expected_output\": \"Machine learning is...\"\n})\n\n# Add multiple items at once\ndataset.add_items([\n    {\"query\": \"Question 1\", \"actual_output\": \"Answer 1\"},\n    {\"query\": \"Question 2\", \"actual_output\": \"Answer 2\"},\n])\n\n# Ignore extra keys in your data\ndataset.add_item(\n    {\"query\": \"...\", \"some_extra_field\": \"ignored\"},\n    ignore_extra_keys=True\n)\n</code></pre>"},{"location":"guides/datasets/#loading-datasets","title":"Loading Datasets","text":"<pre><code># From CSV file\ndataset = Dataset.read_csv(\"eval_data.csv\")\n\n# From CSV with column mapping\ndataset = Dataset.read_csv(\n    \"data.csv\",\n    column_mapping={\"question\": \"query\", \"answer\": \"expected_output\"}\n)\n\n# From JSON file\ndataset = Dataset.read_json(\"eval_data.json\")\n\n# From pandas DataFrame\ndataset = Dataset.read_dataframe(df, name=\"from-pandas\")\n</code></pre>"},{"location":"guides/datasets/#saving-datasets","title":"Saving Datasets","text":"<pre><code># Save to JSON (preserves all metadata)\ndataset.to_json(\"output.json\")\n\n# Save to CSV\ndataset.to_csv(\"output.csv\")\n\n# Convert to DataFrame for analysis\ndf = dataset.to_dataframe()\n\n# Flatten nested JSON fields into separate columns\ndf = dataset.to_dataframe(flatten_nested_json=True)\n</code></pre>"},{"location":"guides/datasets/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<pre><code>from axion._core.schema import HumanMessage, AIMessage, ToolCall, ToolMessage\nfrom axion.dataset_schema import MultiTurnConversation\n\n# Using dict format (simple)\nitem = DatasetItem(\n    conversation=[\n        {\"role\": \"user\", \"content\": \"Hello, I need help with my order\"},\n        {\"role\": \"assistant\", \"content\": \"I'd be happy to help! What's your order number?\"},\n        {\"role\": \"user\", \"content\": \"It's #12345\"},\n        {\"role\": \"assistant\", \"content\": \"I found your order. How can I assist?\"}\n    ]\n)\n\n# Using typed messages (with tool calls)\nconversation = MultiTurnConversation(\n    messages=[\n        HumanMessage(content=\"What's the weather in Paris?\"),\n        AIMessage(\n            content=\"Let me check that for you.\",\n            tool_calls=[ToolCall(name=\"get_weather\", args={\"city\": \"Paris\"})]\n        ),\n        ToolMessage(\n            tool_call_id=\"...\",\n            content=\"Sunny, 22C\"\n        ),\n        AIMessage(content=\"The weather in Paris is sunny and 22C.\")\n    ],\n    reference_text=\"Expected: sunny weather information\"  # Optional ground truth\n)\n\nitem = DatasetItem(conversation=conversation)\n</code></pre>"},{"location":"guides/datasets/#conversation-extraction-strategy","title":"Conversation Extraction Strategy","text":"<p>Control how <code>query</code> and <code>actual_output</code> are extracted from multi-turn conversations:</p> <pre><code># Default: extract from last messages\nitem = DatasetItem(\n    conversation=[...],\n    conversation_extraction_strategy=\"last\"  # Default\n)\n\n# Extract from first messages instead\nitem = DatasetItem(\n    conversation=[...],\n    conversation_extraction_strategy=\"first\"\n)\n</code></pre>"},{"location":"guides/datasets/#conversation-utilities","title":"Conversation Utilities","text":"<pre><code># Get conversation statistics\nstats = item.conversation_stats\n# {'turn_count': 4, 'user_message_count': 2, 'ai_message_count': 2, 'tool_call_count': 1}\n\n# Get the agent's execution path (tool names in order)\ntrajectory = item.agent_trajectory  # ['search', 'retrieve', 'summarize']\n\n# Check for errors in tool messages\nif item.has_errors:\n    print(\"Conversation contains tool errors\")\n\n# Convert to readable transcript\ntranscript = item.to_transcript()\n# User: What's the weather?\n# Assistant: Let me check.\n#   Tool Call: get_weather({\"city\": \"Paris\"})\n# ...\n</code></pre>"},{"location":"guides/datasets/#working-with-datasetitems","title":"Working with DatasetItems","text":""},{"location":"guides/datasets/#dict-like-access","title":"Dict-like Access","text":"<pre><code>item = DatasetItem(query=\"Hello\", actual_output=\"Hi there\")\n\n# Access fields like a dictionary\nquery = item[\"query\"]\nquery = item.get(\"query\", default=\"\")\n\n# Check if field exists\nif \"expected_output\" in item:\n    print(item[\"expected_output\"])\n\n# Iterate over fields\nfor key in item.keys():\n    print(f\"{key}: {item[key]}\")\n\n# Get all (key, value) pairs\nfor key, value in item.items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"guides/datasets/#creating-subsets","title":"Creating Subsets","text":"<pre><code># Get item with only specific fields\nsubset = item.subset([\"query\", \"expected_output\"])\n\n# Keep the original ID\nsubset = item.subset([\"query\", \"actual_output\"], keep_id=True)\n\n# Also copy judgment/critique annotations\nsubset = item.subset([\"query\"], copy_annotations=True)\n\n# Get just evaluation-relevant fields\neval_item = item.evaluation_fields()\n</code></pre>"},{"location":"guides/datasets/#updating-items","title":"Updating Items","text":"<pre><code># Update from another item or dictionary\nitem.update({\"actual_output\": \"New response\", \"latency\": 1.5})\n\n# Update without overwriting existing values\nitem.update(other_item, overwrite=False)\n\n# Update only runtime fields (actual_output, latency, retrieved_content, etc.)\nitem.update_runtime(\n    actual_output=\"Response from API\",\n    latency=0.5,\n    retrieved_content=[\"Context 1\", \"Context 2\"]\n)\n\n# Merge additional metadata\nitem.merge_metadata({\"model\": \"gpt-4\", \"temperature\": 0.7})\n</code></pre>"},{"location":"guides/datasets/#tool-extraction-by-tag","title":"Tool Extraction by Tag","text":"<p>Tool calls are automatically tagged based on their names (RAG, GUARDRAIL, LLM, DATABASE):</p> <pre><code># Add custom tags to all tool calls\nitem = DatasetItem(\n    conversation=[...],\n    user_tags=[\"production\", \"v2\"]\n)\n\n# Extract tool interactions by tag\nrag_interactions = item.extract_by_tag(\"RAG\")\nfor tool_call, tool_message in rag_interactions:\n    print(f\"Called: {tool_call.name}\")\n    print(f\"Result: {tool_message.content if tool_message else 'No response'}\")\n</code></pre>"},{"location":"guides/datasets/#dataset-operations","title":"Dataset Operations","text":""},{"location":"guides/datasets/#filtering","title":"Filtering","text":"<pre><code># Filter items based on a condition\nfailed_items = dataset.filter(\n    lambda item: item.judgment == \"fail\",\n    dataset_name=\"failed-cases\"\n)\n\n# Filter items with errors\nerror_dataset = dataset.filter(lambda item: item.has_errors)\n\n# Filter by latency\nslow_items = dataset.filter(lambda item: item.latency and item.latency &gt; 2.0)\n</code></pre>"},{"location":"guides/datasets/#iteration-and-access","title":"Iteration and Access","text":"<pre><code># Get dataset length\nprint(f\"Dataset has {len(dataset)} items\")\n\n# Iterate over items\nfor item in dataset:\n    print(item.query)\n\n# Access by index\nfirst_item = dataset[0]\nlast_item = dataset[-1]\n\n# Get item by ID\nitem = dataset.get_item_by_id(\"some-uuid\")\n</code></pre>"},{"location":"guides/datasets/#summary-statistics","title":"Summary Statistics","text":"<pre><code># Get summary as dictionary\nsummary = dataset.get_summary()\n# {\n#     'name': 'my-dataset',\n#     'total_items': 100,\n#     'single_turn_items': 80,\n#     'multi_turn_items': 20,\n#     'has_actual_output': 100,\n#     'has_expected_output': 75,\n#     'created_at': '2024-01-15 10:30:00',\n#     'version': '1.0'\n# }\n\n# Print formatted summary table\ndataset.get_summary_table(title=\"Evaluation Dataset Summary\")\n</code></pre>"},{"location":"guides/datasets/#batch-api-execution","title":"Batch API Execution","text":"<p>Execute queries against an API and populate <code>actual_output</code>:</p> <pre><code>dataset.execute_dataset_items_from_api(\n    api_name=\"my-rag-api\",\n    config=\"config.yaml\",\n    max_concurrent=5,\n    show_progress=True,\n    require_success=True  # Remove failed items from dataset\n)\n</code></pre>"},{"location":"guides/datasets/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Generate QA pairs from documents:</p> <pre><code>from axion.synthetic import GenerationParams\n\nparams = GenerationParams(\n    num_questions=10,\n    difficulty=\"medium\"\n)\n\ndataset.synthetic_generate_from_directory(\n    directory_path=\"./documents\",\n    llm=my_llm,\n    params=params,\n    embed_model=my_embedder,\n    max_concurrent=3\n)\n\n# Access raw synthetic data\nraw_data = dataset.synthetic_data\n</code></pre> <p>Metrics &amp; Evaluation  Dataset Reference </p>"},{"location":"guides/evaluation/","title":"Running Evaluations","text":"<p>Axion provides evaluation runners for batch processing with caching and parallel execution.</p>"},{"location":"guides/evaluation/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset\nfrom axion.metrics import Faithfulness, AnswerRelevancy\nfrom axion.runners import evaluation_runner\n\n# Load dataset\ndataset = Dataset.from_csv(\"eval_data.csv\")\n\n# Run evaluation\nresults = await evaluation_runner(\n    dataset=dataset,\n    metrics=[Faithfulness(), AnswerRelevancy()]\n)\n\n# View results\ndf = results.to_dataframe()\nresults.to_scorecard(display_in_notebook=True)\n</code></pre>"},{"location":"guides/evaluation/#evaluation-runners","title":"Evaluation Runners","text":"1 <p>evaluation_runner</p> <p>Main entry point for batch evaluations with caching, concurrency, and full result aggregation.</p> 2 <p>MetricRunner</p> <p>Run individual metrics with fine-grained control over single dataset items.</p>"},{"location":"guides/evaluation/#evaluation_runner","title":"evaluation_runner","text":"<p>The main entry point for running evaluations:</p> <pre><code>from axion.runners import evaluation_runner, EvaluationConfig\n\nconfig = EvaluationConfig(\n    max_concurrency=10,\n    cache_enabled=True,\n    cache_dir=\".cache/evaluations\"\n)\n\nresults = await evaluation_runner(\n    dataset=dataset,\n    metrics=metrics,\n    config=config\n)\n</code></pre>"},{"location":"guides/evaluation/#metricrunner","title":"MetricRunner","text":"<p>For running individual metrics with more control:</p> <pre><code>from axion.runners import MetricRunner\n\nrunner = MetricRunner(metric=Faithfulness())\nresult = await runner.run(dataset_item)\n</code></pre>"},{"location":"guides/evaluation/#caching","title":"Caching","text":"<p>Avoid re-running expensive LLM evaluations:</p> <pre><code>from axion._core.cache import CacheManager, CacheConfig\n\ncache = CacheManager(CacheConfig(\n    cache_type=\"disk\",\n    cache_dir=\".cache/metrics\"\n))\n\n# Results are cached by input hash\nresults = await evaluation_runner(dataset, metrics, cache=cache)\n</code></pre>"},{"location":"guides/evaluation/#understanding-results","title":"Understanding Results","text":"<pre><code># Convert to DataFrame for analysis\ndf = results.to_dataframe()\n\n# Generate detailed metric summary report\nfrom axion.runners.summary import MetricSummary\nMetricSummary().execute(results.results, total_time=100)\n\n# Per-test results\nfor test_result in results.results:\n    for score in test_result.score_results:\n        print(f\"{score.name}: {score.score}\")\n\n# Find failures\ndf[df['metric_score'] &lt; 0.5]\n\n# Visual scorecard (Jupyter notebooks)\nresults.to_scorecard(display_in_notebook=True)\n\n# Latency analysis\nresults.to_latency_plot()\n</code></pre>"},{"location":"guides/evaluation/#normalized-dataframes-for-database-loads","title":"Normalized DataFrames for Database Loads","text":"<p>When loading evaluation results into a database, use <code>to_normalized_dataframes()</code> instead of <code>to_dataframe()</code> to avoid data duplication. This returns two separate tables following data engineering best practices:</p> <ul> <li>Dataset Items Table: One row per test case (inputs/ground truth)</li> <li>Metric Results Table: One row per metric score, with foreign key to dataset item</li> </ul> <pre><code># Get normalized tables\ndataset_df, metrics_df = results.to_normalized_dataframes()\n\n# Load into database (example with pandas)\ndataset_df.to_sql('dataset_items', engine, if_exists='append', index=False)\nmetrics_df.to_sql('metric_results', engine, if_exists='append', index=False)\n\n# The 'dataset_id' column in metrics_df is a foreign key to dataset_df['dataset_id']\n# This enables efficient joins and prevents duplicating dataset fields\n</code></pre> <p>Why use normalized tables?</p> Approach Rows for 100 items \u00d7 5 metrics Dataset fields duplicated? <code>to_dataframe()</code> 500 rows Yes (5\u00d7 per item) <code>to_normalized_dataframes()</code> 100 + 500 rows No <p>Merging back to denormalized view:</p> <pre><code># If you need the denormalized view later (by_alias=True is the default)\nmerged_df = metrics_df.merge(dataset_df, on='dataset_id', how='left')\n# This produces the same columns as to_dataframe(), just different column order\n</code></pre> <p>Column naming with <code>by_alias</code>:</p> <pre><code># by_alias=True (default): Uses descriptive aliases to avoid conflicts\n# - dataset_id: DatasetItem's unique identifier\n# - dataset_metadata: DatasetItem's metadata\n# - metric_id: MetricScore's unique identifier\n# - metric_metadata: MetricScore's metadata\ndataset_df, metrics_df = results.to_normalized_dataframes(by_alias=True)\n\n# by_alias=False: Uses original field names\n# - id: Used for both DatasetItem FK (MetricScore's id is removed to avoid conflict)\ndataset_df, metrics_df = results.to_normalized_dataframes(by_alias=False)\nmerged_df = metrics_df.merge(dataset_df, on='id', how='left')\n</code></pre>"},{"location":"guides/evaluation/#summary-classes","title":"Summary Classes","text":"M <p>MetricSummary</p> <p>Detailed metric analysis with performance insights and distribution charts.</p> S <p>SimpleSummary</p> <p>High-level KPIs and business impact dashboard.</p> H <p>HierarchicalSummary</p> <p>Summary for hierarchical evaluation trees with layered breakdowns.</p> <pre><code>from axion.runners.summary import MetricSummary, SimpleSummary\n\n# Detailed analysis\nMetricSummary(show_distribution=True).execute(results.results, total_time=100)\n\n# Simple KPI dashboard\nSimpleSummary().execute(results.results, total_time=100)\n</code></pre>"},{"location":"guides/evaluation/#best-practices","title":"Best Practices","text":"1 <p>Start Small</p> <p>Test with a few items before running the full dataset.</p> 2 <p>Enable Caching</p> <p>Avoid re-running expensive LLM evaluations on unchanged inputs.</p> 3 <p>Tune Concurrency</p> <p>Balance speed vs. API rate limits for your provider.</p> 4 <p>Review Failures</p> <p>Low scores need human analysis, not just numbers.</p> <p>Metric Runner Deep Dive  Evaluation Runner Deep Dive  Runners Reference </p>"},{"location":"guides/hierarchical-scoring/","title":"Hierarchical Scoring","text":"<p>Axion's hierarchical scoring framework moves beyond flat metrics to provide a diagnostic map of AI quality\u2014from a single overall score down into layered, weighted dimensions.</p>"},{"location":"guides/hierarchical-scoring/#why-hierarchical-scoring","title":"Why Hierarchical Scoring?","text":"<p>Traditional evaluation gives you a single number. That's not enough.</p> <p>When an AI agent scores 0.72, what does that mean? Is it struggling with relevance? Accuracy? Tone? Without structure, you're left guessing.</p> <p>Hierarchical scoring solves this by:</p> <ul> <li>Breaking quality into meaningful dimensions</li> <li>Weighting each dimension to reflect business priorities</li> <li>Enabling drill-down from overall score to root cause</li> </ul> <pre><code>graph TD\n    O[\"Overall Score: 0.82\"] --&gt; R[\"Relevance: 0.91\"]\n    O --&gt; A[\"Accuracy: 0.78\"]\n    O --&gt; T[\"Tone: 0.85\"]</code></pre>"},{"location":"guides/hierarchical-scoring/#key-advantages","title":"Key Advantages","text":"\u2713 <p>Instant Root Cause Diagnosis</p> <p>Drill down to pinpoint whether issues stem from relevance, accuracy, tone, or other dimensions.</p> \u2713 <p>Strategic Prioritization</p> <p>Forces clarity on what matters \u2014 break quality into layers that reflect business value.</p> \u2713 <p>Actionable Feedback Loop</p> <p>Each layer maps to specific actions: retraining, prompt adjustments, alignment tuning.</p> \u2713 <p>Customizable to Business Goals</p> <p>Weight dimensions to match your KPIs \u2014 define what \"good AI\" means for you.</p>"},{"location":"guides/hierarchical-scoring/#quick-start","title":"Quick Start","text":"<pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy, Faithfulness, FactualAccuracy\nfrom axion.dataset import DatasetItem\n\n# Define your scoring hierarchy\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n        'Faithfulness': Faithfulness(),\n        'Accuracy': FactualAccuracy(),\n    },\n    'model': {\n        'ANSWER_QUALITY': {\n            'Relevance': 0.4,\n            'Faithfulness': 0.3,\n            'Accuracy': 0.3,\n        },\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\n# Create evaluation data\ndata_item = DatasetItem(\n    query=\"How do I reset my password?\",\n    actual_output=\"To reset your password, click 'Forgot Password' on the login page and follow the email instructions.\",\n    expected_output=\"Click 'Forgot Password' on the login page, enter your email, and follow the reset link sent to your inbox.\",\n    retrieved_content=[\"Password reset is available via the login page. Users receive a reset link by email.\"]\n)\n\n# Run hierarchical evaluation\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=config,\n    evaluation_name=\"RAG Quality Evaluation\"\n)\n\n# View results\nresults.to_dataframe()\n\n# Generate visual scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"guides/hierarchical-scoring/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/hierarchical-scoring/#option-1-python-dictionary","title":"Option 1: Python Dictionary","text":"<p>Define your hierarchy directly in code:</p> <pre><code>config = {\n    'metric': {\n        'Relevance': AnswerRelevancy(metric_name='Relevancy'),\n        'Faithfulness': Faithfulness(),\n    },\n    'model': {\n        'RESPONSE_QUALITY': {\n            'Relevance': 0.6,\n            'Faithfulness': 0.4,\n        },\n    },\n    'weights': {\n        'RESPONSE_QUALITY': 1.0,\n    }\n}\n</code></pre>"},{"location":"guides/hierarchical-scoring/#option-2-yaml-configuration","title":"Option 2: YAML Configuration","text":"<p>For version-controlled, shareable configs:</p> <pre><code># config.yaml\nmetric:\n  Relevance:\n    class: 'answer_relevancy'\n    metric_name: 'Relevancy'\n    model_name: 'gpt-4.1'\n  Faithfulness:\n    class: 'faithfulness'\n\nmodel:\n  RESPONSE_QUALITY:\n    Relevance: 0.6\n    Faithfulness: 0.4\n\nweights:\n  RESPONSE_QUALITY: 1.0\n</code></pre> <pre><code># Load from file\nresults = evaluation_runner(\n    evaluation_inputs=[data_item],\n    scoring_config=\"config.yaml\",\n)\n</code></pre>"},{"location":"guides/hierarchical-scoring/#multi-level-hierarchies","title":"Multi-Level Hierarchies","text":"<p>Build deeper hierarchies for complex evaluation needs:</p> <pre><code>config = {\n    'metric': {\n        'Relevance': AnswerRelevancy(),\n        'Faithfulness': Faithfulness(),\n        'Accuracy': FactualAccuracy(),\n        'Completeness': AnswerCompleteness(),\n        'Tone': ToneConsistency(),\n    },\n    'model': {\n        # First level: group metrics into categories\n        'GROUNDING': {\n            'Faithfulness': 0.6,\n            'Accuracy': 0.4,\n        },\n        'RESPONSE': {\n            'Relevance': 0.5,\n            'Completeness': 0.3,\n            'Tone': 0.2,\n        },\n    },\n    'weights': {\n        # Second level: weight the categories\n        'GROUNDING': 0.5,\n        'RESPONSE': 0.5,\n    }\n}\n</code></pre> <p>This produces:</p> <pre><code>graph TD\n    O[\"Overall Score\"] --&gt; G[\"GROUNDING (50%)\"]\n    O --&gt; R[\"RESPONSE (50%)\"]\n    G --&gt; F[\"Faithfulness (60%)\"]\n    G --&gt; A[\"Accuracy (40%)\"]\n    R --&gt; Re[\"Relevance (50%)\"]\n    R --&gt; C[\"Completeness (30%)\"]\n    R --&gt; T[\"Tone (20%)\"]</code></pre>"},{"location":"guides/hierarchical-scoring/#accessing-results","title":"Accessing Results","text":"<pre><code># Overall score\nprint(results.overall_score)\n\n# Category scores\nfor category, score in results.category_scores.items():\n    print(f\"{category}: {score}\")\n\n# Individual metric scores\nfor metric_name, score in results.metric_scores.items():\n    print(f\"{metric_name}: {score}\")\n\n# Export to DataFrame for analysis\ndf = results.to_dataframe()\n\n# Generate visual scorecard\nresults.to_scorecard()\n</code></pre>"},{"location":"guides/hierarchical-scoring/#best-practices","title":"Best Practices","text":"1 <p>Start Simple</p> <p>Begin with 2-3 key dimensions. Expand as you learn what matters.</p> 2 <p>Align to Business</p> <p>Weights should reflect actual business priorities, not theoretical ideals.</p> 3 <p>Review Failures</p> <p>Low-scoring dimensions indicate where to focus improvement efforts.</p> 4 <p>Iterate &amp; Version</p> <p>Refine weights based on outcomes. Use YAML configs to track scoring criteria over time.</p> <p>Metrics Reference  Creating Custom Metrics  Eval Tree API </p>"},{"location":"guides/insight-extraction/","title":"Cross-Metric Insight Extraction","text":"<p>Discover cross-metric patterns from evaluation issues using semantic clustering. The <code>InsightExtractor</code> bridges the Issue Extractor with the Evidence Pipeline to surface insights that span multiple metrics.</p>"},{"location":"guides/insight-extraction/#overview","title":"Overview","text":"<p>The <code>IssueExtractor</code> groups issues mechanically by metric + signal name. But real problems often cut across metrics \u2014 for example, faithfulness and contextual recall failures can both point to the same retrieval problem. The <code>InsightExtractor</code> uses semantic clustering to find these cross-metric patterns automatically.</p> \u2713 <p>Cross-Metric Patterns</p> <p>Detects issues that span multiple metrics pointing to a shared root cause.</p> \u2713 <p>Actionable Learnings</p> <p>Distills clusters into titled insights with recommended actions and confidence scores.</p> \u2713 <p>Composable &amp; Opt-In</p> <p>Purely additive \u2014 no changes to IssueExtractor or EvidencePipeline.</p> \u2713 <p>Pluggable Pipeline</p> <p>Full control via pipeline overrides, sinks, dedupers, and clustering methods.</p>"},{"location":"guides/insight-extraction/#quick-start","title":"Quick Start","text":"<pre><code>from axion.reporting import IssueExtractor, InsightExtractor\nfrom axion.runners import evaluation_runner\n\n# 1. Run your evaluation\nresults = await evaluation_runner(dataset, metrics)\n\n# 2. Extract issues\nissue_extractor = IssueExtractor()\nissues = issue_extractor.extract_from_evaluation(results)\n\n# 3. Discover cross-metric patterns\ninsight_extractor = InsightExtractor(model_name='gpt-4o-mini')\ninsights = await insight_extractor.analyze(issues)\n\n# 4. Explore results\nfor pattern in insights.patterns:\n    label = \"CROSS-METRIC\" if pattern.is_cross_metric else \"single-metric\"\n    print(f\"[{label}] {pattern.category} ({pattern.count} issues)\")\n    print(f\"  Metrics: {', '.join(pattern.metrics_involved)}\")\n    print(f\"  Test cases: {pattern.distinct_test_cases}\")\n\nfor learning in insights.learnings:\n    print(f\"\\n{learning.title} (confidence={learning.confidence})\")\n    for action in learning.recommended_actions:\n        print(f\"  - {action}\")\n</code></pre>"},{"location":"guides/insight-extraction/#how-it-works","title":"How It Works","text":"<p>The <code>InsightExtractor</code> performs three steps:</p> <pre><code>graph LR\n    I[\"IssueExtractionResult\"] --&gt; A[\"Convert to EvidenceItems\"]\n    A --&gt; P[\"EvidencePipeline.run()\"]\n    P --&gt; R[\"Build InsightPatterns\"]</code></pre> <ol> <li> <p>Convert \u2014 Each <code>ExtractedIssue</code> is mapped to an <code>EvidenceItem</code> with reasoning-first text and structured metadata. Issues with no reasoning and no query are skipped.</p> </li> <li> <p>Cluster &amp; Distill \u2014 The <code>EvidencePipeline</code> clusters evidence semantically and distills each cluster into learning artifacts with recommended actions.</p> </li> <li> <p>Enrich \u2014 Each cluster is enriched with cross-metric metadata: which metrics are involved, whether it spans multiple metrics, and how many distinct test cases it covers.</p> </li> </ol>"},{"location":"guides/insight-extraction/#issue-to-evidence-mapping","title":"Issue-to-Evidence Mapping","text":"<p>Each issue is converted to an <code>EvidenceItem</code> for clustering:</p> Issue Field Maps To Purpose <code>reasoning</code> + <code>signal cue</code> + <code>query</code> <code>EvidenceItem.text</code> Clustering input <code>test_case_id</code> <code>source_ref</code> Recurrence grouping <code>metric_name</code>, <code>signal_name</code>, <code>value</code>, <code>score</code> <code>metadata</code> Cross-metric analysis SHA-256 of key fields <code>id</code> Stable, deterministic IDs <p>The text is structured reasoning-first for better clustering:</p> <pre><code>The claim contradicts the provided context about system requirements.\n[faithfulness / faithfulness_verdict: CONTRADICTORY]\nQuery: What Python versions are supported?\n</code></pre> <p>Issues with no reasoning and no query produce no meaningful text and are automatically filtered out.</p>"},{"location":"guides/insight-extraction/#configuration","title":"Configuration","text":""},{"location":"guides/insight-extraction/#basic-options","title":"Basic Options","text":"<pre><code>insight_extractor = InsightExtractor(\n    model_name='gpt-4o-mini',       # LLM for clustering + distillation\n    method=ClusteringMethod.LLM,     # LLM, BERTOPIC, or HYBRID\n    recurrence_threshold=2,          # min unique test cases per learning\n    max_items=50,                    # max evidence items to process\n    min_category_size=2,             # min items per cluster\n)\n</code></pre>"},{"location":"guides/insight-extraction/#clustering-methods","title":"Clustering Methods","text":"Method Best for LLM calls <code>ClusteringMethod.LLM</code> Small-medium issue sets, best label quality Yes <code>ClusteringMethod.BERTOPIC</code> Large issue sets, cost-sensitive No (clustering only) <code>ClusteringMethod.HYBRID</code> Large sets where you want readable labels Partial <pre><code>from axion.caliber.pattern_discovery.models import ClusteringMethod\n\n# Cost-effective for large evaluations\nextractor = InsightExtractor(\n    model_name='gpt-4o-mini',\n    method=ClusteringMethod.BERTOPIC,\n)\n</code></pre>"},{"location":"guides/insight-extraction/#custom-pipeline","title":"Custom Pipeline","text":"<p>For full control, pass a pre-configured <code>EvidencePipeline</code>:</p> <pre><code>from axion.caliber.pattern_discovery import EvidencePipeline, InMemorySink\n\nsink = InMemorySink()\npipeline = EvidencePipeline(\n    model_name='gpt-4o-mini',\n    recurrence_threshold=2,\n    min_category_size=2,\n    domain_context='RAG system evaluation',\n    sink=sink,\n)\n\nextractor = InsightExtractor(pipeline=pipeline)\ninsights = await extractor.analyze(issues)\n\n# Access persisted artifacts\nfor sid, entry in sink.artifacts.items():\n    print(entry['artifact'].title)\n</code></pre> <p>No mixing</p> <p>Passing both <code>pipeline=</code> and additional keyword arguments raises a <code>ValueError</code>. Use one or the other.</p>"},{"location":"guides/insight-extraction/#forwarding-pipeline-options","title":"Forwarding Pipeline Options","text":"<p>Any extra keyword arguments are forwarded to <code>EvidencePipeline</code>:</p> <pre><code>extractor = InsightExtractor(\n    model_name='gpt-4o-mini',\n    seed=42,                          # reproducible sampling\n    max_concurrent_distillations=3,   # limit parallel LLM calls\n)\n</code></pre>"},{"location":"guides/insight-extraction/#working-with-results","title":"Working with Results","text":""},{"location":"guides/insight-extraction/#insightresult","title":"InsightResult","text":"<pre><code>insights = await extractor.analyze(issues)\n\ninsights.patterns              # List[InsightPattern] \u2014 discovered clusters\ninsights.learnings             # List[LearningArtifact] \u2014 distilled insights\ninsights.total_issues_analyzed # how many issues had meaningful text\ninsights.clustering_method     # the ClusteringMethod enum used\ninsights.pipeline_result       # full PipelineResult for advanced use\n</code></pre>"},{"location":"guides/insight-extraction/#insightpattern","title":"InsightPattern","text":"<p>Each pattern represents a discovered cluster:</p> <pre><code>for pattern in insights.patterns:\n    print(pattern.category)          # \"Retrieval Coverage Gaps\"\n    print(pattern.description)       # \"Issues where context doesn't cover...\"\n    print(pattern.count)             # 8\n    print(pattern.metrics_involved)  # ['faithfulness', 'contextual_recall']\n    print(pattern.is_cross_metric)   # True\n    print(pattern.distinct_test_cases)  # 5\n    print(pattern.examples)          # excerpted evidence text\n    print(pattern.confidence)        # 0.85\n</code></pre> <p>The <code>is_cross_metric</code> flag is <code>True</code> when a cluster contains issues from 2 or more different metrics \u2014 the key signal that a shared root cause exists.</p>"},{"location":"guides/insight-extraction/#learningartifact","title":"LearningArtifact","text":"<p>Each learning is a structured, actionable insight:</p> <pre><code>for learning in insights.learnings:\n    print(learning.title)              # \"Mobile Checkout Failures on iOS\"\n    print(learning.content)            # detailed explanation\n    print(learning.confidence)         # 0.9\n    print(learning.tags)               # ['retrieval', 'context_quality']\n    print(learning.recommended_actions)  # ['Improve chunk overlap...']\n    print(learning.supporting_item_ids)  # evidence IDs backing this\n</code></pre>"},{"location":"guides/insight-extraction/#sync-usage","title":"Sync Usage","text":"<p>For non-async code:</p> <pre><code>insights = extractor.analyze_sync(issues)\n</code></pre>"},{"location":"guides/insight-extraction/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom axion import Dataset\nfrom axion.metrics import Faithfulness, AnswerRelevancy, ContextualRecall\nfrom axion.runners import evaluation_runner\nfrom axion.reporting import IssueExtractor, InsightExtractor\n\nasync def discover_patterns():\n    # 1. Run evaluation\n    dataset = Dataset.from_csv('test_cases.csv')\n    results = await evaluation_runner(\n        dataset=dataset,\n        metrics=[Faithfulness(), AnswerRelevancy(), ContextualRecall()],\n    )\n\n    # 2. Extract issues\n    issue_extractor = IssueExtractor(score_threshold=0.0)\n    issues = issue_extractor.extract_from_evaluation(results)\n    print(f'Found {issues.issues_found} issues across {issues.total_test_cases} test cases')\n\n    if issues.issues_found == 0:\n        print('No issues found.')\n        return\n\n    # 3. Discover cross-metric patterns\n    insight_extractor = InsightExtractor(\n        model_name='gpt-4o-mini',\n        recurrence_threshold=2,\n    )\n    insights = await insight_extractor.analyze(issues)\n\n    # 4. Report cross-metric patterns\n    cross_metric = [p for p in insights.patterns if p.is_cross_metric]\n    print(f'\\n--- {len(cross_metric)} cross-metric patterns found ---')\n    for pattern in cross_metric:\n        print(f'\\n{pattern.category} ({pattern.count} issues)')\n        print(f'  Metrics: {\", \".join(pattern.metrics_involved)}')\n        print(f'  Distinct test cases: {pattern.distinct_test_cases}')\n\n    # 5. Show actionable learnings\n    print(f'\\n--- {len(insights.learnings)} learnings ---')\n    for learning in insights.learnings:\n        print(f'\\n{learning.title} (confidence={learning.confidence})')\n        print(f'  {learning.content[:200]}')\n        for action in learning.recommended_actions:\n            print(f'  -&gt; {action}')\n\nasyncio.run(discover_patterns())\n</code></pre>"},{"location":"guides/insight-extraction/#relationship-to-other-tools","title":"Relationship to Other Tools","text":"Tool What it does When to use <code>IssueExtractor</code> Extracts failing signals by metric + signal Always \u2014 it produces the input for <code>InsightExtractor</code> <code>InsightExtractor</code> Clusters issues semantically across metrics When you want cross-metric root cause analysis <code>EvidencePipeline</code> General-purpose evidence clustering + distillation When you have arbitrary text evidence (not evaluation issues) <p>The <code>InsightExtractor</code> is the bridge: it converts <code>IssueExtractor</code> output into <code>EvidenceItem</code> format and runs it through the <code>EvidencePipeline</code>.</p>"},{"location":"guides/insight-extraction/#api-reference","title":"API Reference","text":"<p>See the full Insight Extractor API Reference for class signatures and data class details.</p>"},{"location":"guides/issue-extraction/","title":"Issue Extraction &amp; Analysis","text":"<p>Extract and analyze low-score signals from evaluation results for debugging, reporting, and LLM-powered issue summarization.</p>"},{"location":"guides/issue-extraction/#overview","title":"Overview","text":"<p>After running evaluations, you often need to understand why certain test cases failed. The <code>IssueExtractor</code> automatically identifies failing signals across all metrics.</p> \u2713 <p>Unified Extraction</p> <p>Works with any metric, built-in or custom.</p> \u2713 <p>Grouped Analysis</p> <p>Similar issues consolidated for pattern detection.</p> \u2713 <p>LLM-Ready Prompts</p> <p>Generate summaries with optional AI analysis.</p> \u2713 <p>Extensible Adapters</p> <p>Register custom signal detection for your metrics.</p>"},{"location":"guides/issue-extraction/#quick-start","title":"Quick Start","text":"<pre><code>from axion.reporting import IssueExtractor\nfrom axion.runners import evaluation_runner\n\n# Run your evaluation\nresults = await evaluation_runner(dataset, metrics)\n\n# Extract issues (signals with score &lt;= 0)\nextractor = IssueExtractor()\nissues = extractor.extract_from_evaluation(results)\n\n# View summary\nprint(f\"Found {issues.issues_found} issues across {issues.total_test_cases} test cases\")\n\n# Generate LLM prompt for analysis\nprompt = extractor.to_prompt_text(issues)\n</code></pre>"},{"location":"guides/issue-extraction/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/issue-extraction/#extract-issues-from-evaluation-results","title":"Extract Issues from Evaluation Results","text":"<pre><code>from axion.reporting import IssueExtractor\n\n# Default: extract signals with score &lt;= 0.0\nextractor = IssueExtractor()\nissues = extractor.extract_from_evaluation(evaluation_result)\n\n# Access extracted issues\nfor issue in issues.all_issues:\n    print(f\"Test: {issue.test_case_id}\")\n    print(f\"Metric: {issue.metric_name}\")\n    print(f\"Signal: {issue.signal_name} = {issue.value}\")\n    print(f\"Score: {issue.score}\")\n    print(f\"Reasoning: {issue.reasoning}\")\n    print(\"---\")\n</code></pre>"},{"location":"guides/issue-extraction/#customize-extraction","title":"Customize Extraction","text":"<pre><code>extractor = IssueExtractor(\n    # Extract signals with score &lt;= 0.5 (not just 0.0)\n    score_threshold=0.5,\n\n    # Include NaN scores as issues\n    include_nan=True,\n\n    # Only extract from specific metrics\n    metric_filters=['Faithfulness', 'Answer Relevancy'],\n\n    # Limit total issues extracted\n    max_issues=100,\n\n    # Sample 10% of test cases (deterministic by ID)\n    sample_rate=0.1,\n\n    # Context fields to include from test cases\n    include_context_fields=['query', 'actual_output', 'expected_output', 'retrieved_content']\n)\n</code></pre>"},{"location":"guides/issue-extraction/#access-issues-by-category","title":"Access Issues by Category","text":"<pre><code>issues = extractor.extract_from_evaluation(results)\n\n# Issues grouped by metric\nfor metric_name, metric_issues in issues.issues_by_metric.items():\n    print(f\"{metric_name}: {len(metric_issues)} issues\")\n\n# Issues grouped by type (metric:signal combination)\nfor issue_type, type_issues in issues.issues_by_type.items():\n    print(f\"{issue_type}: {len(type_issues)} issues\")\n</code></pre>"},{"location":"guides/issue-extraction/#generating-llm-prompts","title":"Generating LLM Prompts","text":""},{"location":"guides/issue-extraction/#basic-prompt-generation","title":"Basic Prompt Generation","text":"<pre><code># Generate a detailed prompt listing all issues\nprompt = extractor.to_prompt_text(issues, max_issues=50)\nprint(prompt)\n</code></pre> <p>Output: <pre><code>## Evaluation Issues Summary\n\n**Evaluation:** RAG Quality Assessment\n**Test Cases Analyzed:** 100\n**Issues Found:** 23\n\n### Issue Breakdown by Metric\n- Faithfulness: 12 issues (8 CONTRADICTORY, 4 NO_EVIDENCE)\n- Answer Relevancy: 7 issues (7 False)\n- Answer Criteria: 4 issues (4 False)\n\n### Detailed Issues\n\n#### Issue 1: Faithfulness - faithfulness_verdict\n- **Test Case:** test_case_42\n- **Signal Group:** claim_0\n- **Value:** CONTRADICTORY\n- **Score:** 0.0\n- **Reasoning:** \"Context states Python 3.8+ required, contradicting the claim about 3.6 support\"\n- **Query:** \"What Python versions are supported?\"\n- **Actual Output:** \"Our product supports Python 3.6 and above...\"\n\n...\n\n## Task\nAnalyze the quality issues found in this evaluation. Provide:\n1. **Critical Failure Patterns:** What are the most common/severe issue types?\n2. **Root Cause Analysis:** What systemic problems might be causing these failures?\n3. **Recommended Improvements:** Specific actions to improve quality\n4. **Priority Ranking:** Which issues should be addressed first?\n</code></pre></p>"},{"location":"guides/issue-extraction/#grouped-prompts-token-efficient","title":"Grouped Prompts (Token-Efficient)","text":"<p>For large evaluations, use grouped prompts to reduce token usage:</p> <pre><code># Group similar issues together with representative examples\ngrouped_prompt = extractor.to_grouped_prompt_text(\n    issues,\n    max_groups=20,           # Limit number of issue groups\n    max_examples_per_group=2  # Examples per group\n)\n</code></pre> <p>Output: <pre><code>## Evaluation Issues Summary (Grouped)\n\n**Total Issues Found:** 156\n**Issue Groups:** 8\n\n### Issue Groups Overview\n\n| Metric | Signal | Count | Values |\n|--------|--------|-------|--------|\n| Faithfulness | faithfulness_verdict | 45 | CONTRADICTORY, NO_EVIDENCE |\n| Answer Criteria | is_covered | 38 | False |\n| Answer Relevancy | is_relevant | 31 | False |\n\n### Detailed Issue Groups\n\n#### Group 1: Faithfulness - faithfulness_verdict\n- **Total Issues:** 45\n- **Failure Values:** CONTRADICTORY, NO_EVIDENCE\n- **Affected Tests:** test_12, test_45, test_67, ... (+42 more)\n\n**Representative Examples:**\n\n*Example 1:*\n- Test: test_12\n- Value: CONTRADICTORY\n- Reasoning: \"Claim contradicts source documentation\"\n- Query: \"What are the system requirements?\"\n\n*Example 2:*\n- Test: test_45\n- Value: NO_EVIDENCE\n- Reasoning: \"No supporting evidence found in context\"\n- Query: \"Does it support Windows?\"\n</code></pre></p>"},{"location":"guides/issue-extraction/#llm-powered-group-summaries","title":"LLM-Powered Group Summaries","text":"<p>Add AI-generated pattern summaries for each issue group:</p> <pre><code>from axion.llm_registry import LLMRegistry\n\n# Get an LLM instance\nreg = LLMRegistry('anthropic')\nllm = reg.get_llm()\n\n# Generate grouped prompt with LLM summaries (async)\ngrouped_prompt = await extractor.to_grouped_prompt_text_async(\n    issues,\n    llm=llm,  # LLM generates 1-2 sentence summary per group\n    max_groups=15\n)\n</code></pre> <p>Output includes AI-generated pattern analysis: <pre><code>#### Group 1: Faithfulness - faithfulness_verdict\n- **Total Issues:** 45\n- **Pattern Summary:** Claims about version compatibility and system requirements\n  consistently contradict the official documentation, suggesting outdated training data\n  or hallucination patterns around technical specifications.\n</code></pre></p>"},{"location":"guides/issue-extraction/#full-llm-analysis-with-summarize","title":"Full LLM Analysis with <code>summarize()</code>","text":"<p>For a complete automated analysis (like copy-pasting to ChatGPT/Gemini, but automated), use the <code>summarize()</code> method:</p> <pre><code>from axion.llm_registry import LLMRegistry\nfrom axion.reporting import IssueExtractor\n\n# Extract issues\nextractor = IssueExtractor()\nissues = extractor.extract_from_evaluation(results)\n\n# Get LLM\nreg = LLMRegistry('anthropic')\nllm = reg.get_llm('claude-sonnet-4-20250514')\n\n# Generate complete analysis\nsummary = await extractor.summarize(issues, llm=llm)\n\nprint(summary.text)\n</code></pre> <p>The default prompt generates: - Executive Summary - 2-3 sentence overview - Missing Concepts - Topics the AI consistently missed - Failure Categories Table - Structured breakdown with counts and examples - Root Cause Analysis - Systemic issues causing failures - Recommended Actions - Prioritized improvements</p>"},{"location":"guides/issue-extraction/#custom-prompts","title":"Custom Prompts","text":"<p>Override the default prompt with your own template:</p> <pre><code>custom_prompt = '''\nAnalyze these evaluation failures:\n\n{overview}\n\n{issue_data}\n\nProvide:\n1. Top 3 failure patterns\n2. Quick wins to fix them\n3. A table with Category, Count, and Example\n'''\n\nsummary = await extractor.summarize(\n    issues,\n    llm=llm,\n    prompt_template=custom_prompt,\n    max_issues=50  # Limit issues in prompt\n)\n</code></pre> <p>The template must include <code>{overview}</code> and <code>{issue_data}</code> placeholders.</p>"},{"location":"guides/issue-extraction/#sync-version","title":"Sync Version","text":"<p>For non-async code:</p> <pre><code>summary = extractor.summarize_sync(issues, llm=llm)\nprint(summary.text)\n</code></pre>"},{"location":"guides/issue-extraction/#structured-output-for-programmatic-use","title":"Structured Output for Programmatic Use","text":"<pre><code># Get structured data instead of text\nllm_input = extractor.to_llm_input(issues, max_issues=50)\n\nprint(llm_input.evaluation_name)      # \"RAG Quality Assessment\"\nprint(llm_input.total_test_cases)     # 100\nprint(llm_input.issues_found)         # 23\nprint(llm_input.issues_by_metric)     # {'Faithfulness': 12, ...}\n\n# Access detailed issue dicts\nfor issue_dict in llm_input.detailed_issues:\n    print(issue_dict['metric'])\n    print(issue_dict['signal_name'])\n    print(issue_dict['value'])\n    print(issue_dict['context']['query'])\n</code></pre>"},{"location":"guides/issue-extraction/#signal-adapter-registry","title":"Signal Adapter Registry","text":"<p>The <code>SignalAdapterRegistry</code> defines how to extract issues from each metric's signals. Axion includes adapters for all built-in metrics, but you can register custom adapters for your own metrics.</p>"},{"location":"guides/issue-extraction/#how-adapters-work","title":"How Adapters Work","text":"<p>Each adapter specifies:</p> Field Description Example <code>headline_signals</code> Signals that indicate pass/fail <code>['is_relevant', 'verdict']</code> <code>issue_values</code> Values that indicate failures <code>{'is_relevant': [False], 'verdict': ['no']}</code> <code>context_signals</code> Related signals for context <code>['statement', 'reason', 'turn_index']</code>"},{"location":"guides/issue-extraction/#built-in-adapters","title":"Built-in Adapters","text":"<pre><code>from axion.reporting import SignalAdapterRegistry\n\n# List all registered adapters\nprint(SignalAdapterRegistry.list_adapters())\n# ['faithfulness', 'answer_criteria', 'answer_relevancy', 'answer_completeness',\n#  'contextual_relevancy', 'contextual_recall', 'contextual_precision',\n#  'factual_accuracy', 'pii_leakage', 'tool_correctness', ...]\n</code></pre>"},{"location":"guides/issue-extraction/#register-a-custom-adapter","title":"Register a Custom Adapter","text":"<p>Best Practice: Define the adapter in the same file as your custom metric. This keeps the signal schema and adapter in sync.</p> <pre><code># my_metrics/quality_checker.py\n\nfrom axion.metrics import BaseMetric\nfrom axion.reporting import SignalAdapterRegistry, MetricSignalAdapter\n\n# 1. Define your metric\nclass QualityChecker(BaseMetric):\n    name = \"Quality Checker\"\n\n    async def a_score(self, item):\n        # Your scoring logic...\n        return MetricScore(\n            name=self.name,\n            score=score,\n            signals={\n                'quality_verdict': {'value': verdict, 'score': 1.0 if verdict == 'PASS' else 0.0},\n                'issues_found': {'value': issues, 'score': 1.0},\n                'reasoning': {'value': reason},\n            }\n        )\n\n# 2. Register adapter alongside the metric\n@SignalAdapterRegistry.register('quality_checker')\ndef _quality_checker_adapter():\n    return MetricSignalAdapter(\n        metric_key='quality_checker',\n        headline_signals=['quality_verdict'],\n        issue_values={'quality_verdict': ['FAIL', 'PARTIAL']},\n        context_signals=['issues_found', 'reasoning']\n    )\n</code></pre> <p>The adapter registers automatically when your metric module is imported.</p>"},{"location":"guides/issue-extraction/#alternative-direct-registration","title":"Alternative: Direct Registration","text":"<p>For quick registration without a decorator:</p> <pre><code>SignalAdapterRegistry.register_adapter(\n    'another_metric',\n    MetricSignalAdapter(\n        metric_key='another_metric',\n        headline_signals=['is_valid'],\n        issue_values={'is_valid': [False, 'INVALID', 'ERROR']},\n        context_signals=['validation_errors', 'field_name']\n    )\n)\n</code></pre>"},{"location":"guides/issue-extraction/#what-if-no-adapter-exists","title":"What If No Adapter Exists?","text":"<p>Unregistered metrics still work - the extractor falls back to score-based extraction:</p> <pre><code># Your custom metric without a registered adapter\n# Signals with score &lt;= threshold are automatically detected as issues\n</code></pre> Feature Without Adapter With Adapter Score-based detection Yes Yes Value-based detection No Yes Explicit headline signals No Yes Context signal extraction Basic Full <p>Register an adapter when you want richer issue detection beyond just scores.</p>"},{"location":"guides/issue-extraction/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom axion import Dataset\nfrom axion.metrics import Faithfulness, AnswerRelevancy, AnswerCriteria\nfrom axion.runners import evaluation_runner\nfrom axion.reporting import IssueExtractor, SignalAdapterRegistry, MetricSignalAdapter\nfrom axion.llm_registry import LLMRegistry\n\nasync def analyze_evaluation():\n    # 1. Run evaluation\n    dataset = Dataset.from_csv(\"test_cases.csv\")\n    results = await evaluation_runner(\n        dataset=dataset,\n        metrics=[Faithfulness(), AnswerRelevancy(), AnswerCriteria()]\n    )\n\n    # 2. Extract issues\n    extractor = IssueExtractor(\n        score_threshold=0.0,\n        include_context_fields=['query', 'actual_output', 'expected_output']\n    )\n    issues = extractor.extract_from_evaluation(results)\n\n    # 3. Quick summary\n    print(f\"Evaluation: {issues.evaluation_name}\")\n    print(f\"Test cases: {issues.total_test_cases}\")\n    print(f\"Issues found: {issues.issues_found}\")\n    print()\n\n    for metric, metric_issues in issues.issues_by_metric.items():\n        print(f\"  {metric}: {len(metric_issues)} issues\")\n\n    # 4. Generate LLM analysis prompt\n    if issues.issues_found &gt; 0:\n        # Use grouped prompt for efficiency\n        reg = LLMRegistry('anthropic')\n        llm = reg.get_llm('claude-sonnet-4-20250514')\n\n        prompt = await extractor.to_grouped_prompt_text_async(\n            issues,\n            llm=llm,\n            max_groups=10\n        )\n\n        # Send to LLM for analysis\n        analysis = await llm.acomplete(prompt)\n        print(\"\\n=== LLM Analysis ===\")\n        print(analysis.text)\n\nasyncio.run(analyze_evaluation())\n</code></pre>"},{"location":"guides/issue-extraction/#api-reference","title":"API Reference","text":""},{"location":"guides/issue-extraction/#issueextractor","title":"IssueExtractor","text":"<pre><code>class IssueExtractor:\n    def __init__(\n        self,\n        score_threshold: float = 0.0,      # Signals &lt;= this are issues\n        include_nan: bool = False,          # Include NaN scores\n        include_context_fields: List[str] = ['query', 'actual_output', 'expected_output'],\n        metric_filters: List[str] = None,   # Only these metrics\n        max_issues: int = None,             # Hard limit\n        sample_rate: float = None,          # 0.0-1.0 sampling\n    ): ...\n\n    def extract_from_evaluation(self, result: EvaluationResult) -&gt; IssueExtractionResult: ...\n    def extract_from_test_result(self, test_result: TestResult, index: int) -&gt; List[ExtractedIssue]: ...\n    def extract_from_metric_score(self, metric_score: MetricScore, ...) -&gt; List[ExtractedIssue]: ...\n\n    def to_llm_input(self, result: IssueExtractionResult, max_issues: int = 50) -&gt; LLMSummaryInput: ...\n    def to_prompt_text(self, result: IssueExtractionResult, max_issues: int = 50) -&gt; str: ...\n    def to_grouped_prompt_text(self, result: IssueExtractionResult, llm=None, max_groups=20, max_examples_per_group=2) -&gt; str: ...\n    async def to_grouped_prompt_text_async(self, result: IssueExtractionResult, llm=None, ...) -&gt; str: ...\n</code></pre>"},{"location":"guides/issue-extraction/#signaladapterregistry","title":"SignalAdapterRegistry","text":"<pre><code>class SignalAdapterRegistry:\n    @classmethod\n    def register(cls, metric_key: str) -&gt; decorator: ...  # Decorator for registration\n\n    @classmethod\n    def register_adapter(cls, metric_key: str, adapter: MetricSignalAdapter) -&gt; None: ...\n\n    @classmethod\n    def get(cls, metric_name: str) -&gt; Optional[MetricSignalAdapter]: ...\n\n    @classmethod\n    def list_adapters(cls) -&gt; List[str]: ...\n</code></pre>"},{"location":"guides/issue-extraction/#data-classes","title":"Data Classes","text":"<pre><code>@dataclass\nclass ExtractedIssue:\n    test_case_id: str\n    metric_name: str\n    signal_group: str\n    signal_name: str\n    value: Any\n    score: float\n    description: Optional[str]\n    reasoning: Optional[str]\n    item_context: Dict[str, Any]\n    source_path: str\n    raw_signal: Dict[str, Any]\n\n@dataclass\nclass IssueExtractionResult:\n    run_id: str\n    evaluation_name: Optional[str]\n    total_test_cases: int\n    total_signals_analyzed: int\n    issues_found: int\n    issues_by_metric: Dict[str, List[ExtractedIssue]]\n    issues_by_type: Dict[str, List[ExtractedIssue]]\n    all_issues: List[ExtractedIssue]\n\n@dataclass\nclass MetricSignalAdapter:\n    metric_key: str\n    headline_signals: List[str]\n    issue_values: Dict[str, List[Any]]\n    context_signals: List[str]\n</code></pre>"},{"location":"guides/issue-extraction/#next-step-cross-metric-insights","title":"Next Step: Cross-Metric Insights","text":"<p>The <code>IssueExtractor</code> groups issues by metric + signal name. To discover patterns that span multiple metrics \u2014 for example, faithfulness and contextual recall failures both pointing to a retrieval problem \u2014 use the <code>InsightExtractor</code>:</p> <pre><code>from axion.reporting import IssueExtractor, InsightExtractor\n\n# Extract issues as usual\nextractor = IssueExtractor()\nissues = extractor.extract_from_evaluation(results)\n\n# Discover cross-metric patterns\ninsight_extractor = InsightExtractor(model_name='gpt-4o-mini')\ninsights = await insight_extractor.analyze(issues)\n\nfor pattern in insights.patterns:\n    if pattern.is_cross_metric:\n        print(f\"{pattern.category}: {', '.join(pattern.metrics_involved)}\")\n</code></pre> <p>Cross-Metric Insight Extraction Guide </p>"},{"location":"guides/issue-extraction/#best-practices","title":"Best Practices","text":"1 <p>Start with Defaults</p> <p><code>IssueExtractor()</code> works well for most cases out of the box.</p> 2 <p>Grouped Prompts</p> <p>Use grouped prompts for large evals \u2014 reduces tokens by 50\u201390%.</p> 3 <p>Register Adapters</p> <p>Custom adapters enable value-based detection for your metrics.</p> 4 <p>Sample Large Datasets</p> <p>Use <code>sample_rate</code> to manage volume on very large evaluations.</p>"},{"location":"guides/llm-providers/","title":"LLM Providers","text":"<p>Axion provides a unified interface for working with multiple LLM providers through the <code>LLMRegistry</code>. This guide covers using built-in providers and registering custom ones.</p>"},{"location":"guides/llm-providers/#quick-start","title":"Quick Start","text":"<pre><code>from axion.llm_registry import LLMRegistry\n\n# Create a registry locked to a provider\nregistry = LLMRegistry(provider='openai')\nllm = registry.get_llm('gpt-4o')\n\n# Or use flexible mode to switch providers\nregistry = LLMRegistry()\nllm_openai = registry.get_llm('gpt-4o', provider='openai')\nllm_claude = registry.get_llm('claude-3-5-sonnet-20241022', provider='anthropic')\n</code></pre>"},{"location":"guides/llm-providers/#built-in-providers","title":"Built-in Providers","text":"\u2713 <p>OpenAI</p> <p>GPT-4o, GPT-4, o1, o3, etc. Embeddings supported. Auth: <code>OPENAI_API_KEY</code></p> \u2713 <p>Anthropic</p> <p>Claude 3.5, Claude 3, etc. No embeddings. Auth: <code>ANTHROPIC_API_KEY</code></p> \u2713 <p>Gemini</p> <p>Gemini 1.5 Pro/Flash, etc. Embeddings supported. Auth: <code>GOOGLE_API_KEY</code></p> \u2713 <p>Vertex AI / HuggingFace</p> <p>Gemini &amp; Claude on Vertex (GCP auth) + any HF Hub model (local inference).</p>"},{"location":"guides/llm-providers/#openai","title":"OpenAI","text":"<pre><code>registry = LLMRegistry(provider='openai')\n\n# LLM\nllm = registry.get_llm('gpt-4o')\nllm = registry.get_llm('gpt-4o-mini', temperature=0.7)\nllm = registry.get_llm('o1-preview')\n\n# Embeddings\nembedding = registry.get_embedding_model('text-embedding-3-small')\n</code></pre>"},{"location":"guides/llm-providers/#anthropic","title":"Anthropic","text":"<pre><code>registry = LLMRegistry(provider='anthropic')\n\n# LLM - automatically prefixed with 'anthropic/' for LiteLLM\nllm = registry.get_llm('claude-3-5-sonnet-20241022')\nllm = registry.get_llm('claude-3-opus-20240229')\n\n# Embeddings - NOT supported (use OpenAI or HuggingFace)\n# registry.get_embedding_model()  # Raises NotImplementedError\n</code></pre>"},{"location":"guides/llm-providers/#gemini","title":"Gemini","text":"<pre><code>registry = LLMRegistry(provider='gemini')\n\n# LLM - automatically prefixed with 'gemini/' for LiteLLM\nllm = registry.get_llm('gemini-1.5-pro')\nllm = registry.get_llm('gemini-1.5-flash')\n\n# Embeddings\nembedding = registry.get_embedding_model('models/embedding-001')\n</code></pre>"},{"location":"guides/llm-providers/#vertex-ai","title":"Vertex AI","text":"<pre><code># Requires GCP service account authentication\nregistry = LLMRegistry(\n    provider='vertex_ai',\n    vertex_project='my-gcp-project',\n    vertex_location='us-central1',\n    vertex_credentials='/path/to/service-account.json'\n)\n\n# Or use environment variables:\n# VERTEXAI_PROJECT, VERTEXAI_LOCATION, GOOGLE_APPLICATION_CREDENTIALS\n\n# LLM - automatically prefixed with 'vertex_ai/' for LiteLLM\nllm = registry.get_llm('gemini-1.5-pro')\n\n# Embeddings\nembedding = registry.get_embedding_model('text-embedding-004')\n</code></pre>"},{"location":"guides/llm-providers/#huggingface","title":"HuggingFace","text":"<pre><code>registry = LLMRegistry(provider='huggingface')\n\n# LLM - uses LlamaIndex for local inference (not LiteLLM)\nllm = registry.get_llm('meta-llama/Llama-2-7b-chat-hf')\n\n# Embeddings\nembedding = registry.get_embedding_model('BAAI/bge-small-en-v1.5')\n</code></pre> <p>HuggingFace Requirements</p> <p>HuggingFace provider requires <code>torch</code> and <code>transformers</code> to be installed. Models run locally, not via API.</p>"},{"location":"guides/llm-providers/#registering-a-custom-provider","title":"Registering a Custom Provider","text":"<p>To add a new provider, create a class that inherits from <code>BaseProvider</code> and use the <code>@LLMRegistry.register()</code> decorator.</p>"},{"location":"guides/llm-providers/#basic-structure","title":"Basic Structure","text":"<pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry, LiteLLMWrapper\n\n@LLMRegistry.register('my_provider')\nclass MyProvider(BaseProvider):\n    \"\"\"Custom provider for My LLM Service.\"\"\"\n\n    # LiteLLM prefix for routing (e.g., 'anthropic/', 'gemini/')\n    LITELLM_PREFIX = 'my_provider/'\n\n    # Whether this provider supports embedding models\n    SUPPORTS_EMBEDDINGS = True\n\n    def __init__(self, api_key: Optional[str] = None, **credentials):\n        super().__init__(api_key=api_key, **credentials)\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        \"\"\"Create an embedding model client.\"\"\"\n        # Return your embedding model instance\n        pass\n</code></pre>"},{"location":"guides/llm-providers/#example-custom-openai-compatible-endpoint","title":"Example: Custom OpenAI-Compatible Endpoint","text":"<pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry, LiteLLMWrapper\n\n@LLMRegistry.register('custom_openai')\nclass CustomOpenAIProvider(BaseProvider):\n    \"\"\"Provider for OpenAI-compatible endpoints (e.g., vLLM, LocalAI).\"\"\"\n\n    LITELLM_PREFIX = 'openai/'  # Use OpenAI routing\n    SUPPORTS_EMBEDDINGS = False\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        **credentials\n    ):\n        super().__init__(api_key=api_key, **credentials)\n        self.api_base = api_base\n\n    def _create_litellm_wrapper(self, model_name: str, **kwargs) -&gt; LiteLLMWrapper:\n        \"\"\"Override to add custom base URL.\"\"\"\n        if self.LITELLM_PREFIX and not model_name.startswith(self.LITELLM_PREFIX):\n            model_name = f'{self.LITELLM_PREFIX}{model_name}'\n\n        temperature = kwargs.pop('temperature', 0.0)\n\n        return LiteLLMWrapper(\n            model=model_name,\n            provider='custom_openai',\n            temperature=temperature,\n            api_key=self.api_key,\n            api_base=self.api_base,\n            **kwargs\n        )\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        raise NotImplementedError('This provider does not support embeddings.')\n\n\n# Usage\nregistry = LLMRegistry(\n    provider='custom_openai',\n    api_key='your-api-key',\n    api_base='http://localhost:8000/v1'\n)\nllm = registry.get_llm('my-local-model')\n</code></pre>"},{"location":"guides/llm-providers/#example-non-litellm-backend","title":"Example: Non-LiteLLM Backend","text":"<p>For providers that don't use LiteLLM (like HuggingFace local inference), override <code>create_llm()</code>:</p> <pre><code>from typing import Any, Optional\nfrom axion.llm_registry import BaseProvider, LLMRegistry\n\n@LLMRegistry.register('ollama')\nclass OllamaProvider(BaseProvider):\n    \"\"\"Provider for Ollama local models.\"\"\"\n\n    LITELLM_PREFIX = ''\n    SUPPORTS_EMBEDDINGS = True\n\n    def __init__(self, base_url: str = 'http://localhost:11434', **credentials):\n        super().__init__(api_key=None, **credentials)\n        self.base_url = base_url\n\n    def create_llm(self, model_name: str, **kwargs) -&gt; Any:\n        \"\"\"Override to use Ollama-specific client instead of LiteLLM.\"\"\"\n        from llama_index.llms.ollama import Ollama\n\n        return Ollama(\n            model=model_name,\n            base_url=self.base_url,\n            **kwargs\n        )\n\n    def create_embedding_model(self, model_name: str, **kwargs) -&gt; Any:\n        from llama_index.embeddings.ollama import OllamaEmbedding\n\n        return OllamaEmbedding(\n            model_name=model_name,\n            base_url=self.base_url,\n            **kwargs\n        )\n\n\n# Usage\nregistry = LLMRegistry(provider='ollama')\nllm = registry.get_llm('llama2')\n</code></pre>"},{"location":"guides/llm-providers/#provider-class-attributes","title":"Provider Class Attributes","text":"Attribute Type Description <code>LITELLM_PREFIX</code> <code>str</code> Prefix added to model names for LiteLLM routing (e.g., <code>'anthropic/'</code>) <code>SUPPORTS_EMBEDDINGS</code> <code>bool</code> Whether the provider offers embedding models (default: <code>True</code>)"},{"location":"guides/llm-providers/#key-methods-to-implement","title":"Key Methods to Implement","text":"Method Required Description <code>__init__()</code> Yes Initialize with credentials <code>create_embedding_model()</code> Yes Return embedding model instance (or raise <code>NotImplementedError</code>) <code>create_llm()</code> No Override only for non-LiteLLM backends <code>_create_litellm_wrapper()</code> No Override to customize LiteLLM wrapper creation"},{"location":"guides/llm-providers/#cost-estimation","title":"Cost Estimation","text":"<p>Add custom model pricing to the cost estimator:</p> <pre><code>from axion.llm_registry import LLMCostEstimator\n\n# Add custom model pricing (price per token)\nLLMCostEstimator.add_custom_model(\n    model='my-custom-model',\n    input_price_per_token=0.001 / 1000,   # $0.001 per 1K tokens\n    output_price_per_token=0.002 / 1000   # $0.002 per 1K tokens\n)\n\n# Estimate cost\ncost = LLMCostEstimator.estimate(\n    model_name='my-custom-model',\n    prompt_tokens=1000,\n    completion_tokens=500\n)\n</code></pre>"},{"location":"guides/llm-providers/#view-registered-providers","title":"View Registered Providers","text":"<pre><code>from axion.llm_registry import LLMRegistry\n\n# Display all registered providers with details\nLLMRegistry.display()\n</code></pre> <p>Environment Configuration  Tracing  LLM Registry Reference </p>"},{"location":"guides/metrics/","title":"Metrics &amp; Evaluation","text":"<p>Axion provides 30+ metrics for evaluating AI agents across multiple dimensions.</p>"},{"location":"guides/metrics/#quick-start","title":"Quick Start","text":"<pre><code>from axion import Dataset, metric_registry\nfrom axion.metrics import Faithfulness, AnswerRelevancy\n\n# Load your dataset\ndataset = Dataset.from_csv(\"eval_data.csv\")\n\n# Select metrics\nmetrics = [Faithfulness(), AnswerRelevancy()]\n\n# Run evaluation\nfrom axion.runners import evaluation_runner\nresults = await evaluation_runner(dataset, metrics)\n</code></pre>"},{"location":"guides/metrics/#metric-output-types","title":"Metric Output Types","text":"S <p>Score</p> <p>Numeric value (0\u20131) with pass/fail threshold. Example: <code>Faithfulness</code> \u2192 <code>0.85</code></p> C <p>Classification</p> <p>Single label from a fixed set. Example: <code>SentimentClassification</code> \u2192 <code>\"positive\"</code></p> A <p>Analysis</p> <p>Structured insights without scoring. Example: <code>ReferralReasonAnalysis</code> \u2192 <code>{reasons[], citations[]}</code></p> <p>See Creating Custom Metrics for details on choosing the right output type.</p>"},{"location":"guides/metrics/#metric-categories","title":"Metric Categories","text":"1 <p>Composite (LLM-based)</p> <p>Nuanced evaluation requiring reasoning. Faithfulness, AnswerRelevancy, FactualAccuracy, AnswerCompleteness, AnswerCriteria.</p> 2 <p>Heuristic (Non-LLM)</p> <p>Fast, deterministic checks. ExactStringMatch, CitationPresence, Latency, ContainsMatch.</p> 3 <p>Retrieval</p> <p>RAG pipeline evaluation. HitRateAtK, MeanReciprocalRank, ContextualRelevancy, ContextualSufficiency.</p> 4 <p>Conversational</p> <p>Multi-turn agent evaluation. GoalCompletion, ConversationEfficiency, ConversationFlow.</p>"},{"location":"guides/metrics/#composite-metrics-llm-based","title":"Composite Metrics (LLM-based)","text":"Metric What it Measures <code>Faithfulness</code> Is the answer grounded in retrieved context? <code>AnswerRelevancy</code> Does the answer address the question? <code>FactualAccuracy</code> Is the answer factually correct vs. expected? <code>AnswerCompleteness</code> Are all parts of the question answered? <code>AnswerCriteria</code> Does the answer meet specific business rules?"},{"location":"guides/metrics/#heuristic-metrics-non-llm","title":"Heuristic Metrics (Non-LLM)","text":"Metric What it Measures <code>ExactStringMatch</code> Exact match between actual and expected <code>CitationPresence</code> Are citations/references present? <code>Latency</code> Response time (pass/fail threshold) <code>ContainsMatch</code> Does output contain required phrases?"},{"location":"guides/metrics/#retrieval-metrics","title":"Retrieval Metrics","text":"Metric What it Measures <code>HitRateAtK</code> Is the right doc in top K results? <code>MeanReciprocalRank</code> Position of first relevant result <code>ContextualRelevancy</code> Are retrieved chunks relevant? <code>ContextualSufficiency</code> Do chunks contain the answer?"},{"location":"guides/metrics/#conversational-metrics","title":"Conversational Metrics","text":"Metric What it Measures <code>GoalCompletion</code> Did user achieve their goal? <code>ConversationEfficiency</code> Were there unnecessary loops? <code>ConversationFlow</code> Is the dialogue logical?"},{"location":"guides/metrics/#using-the-metric-registry","title":"Using the Metric Registry","text":"<pre><code>from axion.metrics import metric_registry\n\n# List all available metrics\nprint(metric_registry.list_metrics())\n\n# Get metric by name\nmetric = metric_registry.get(\"Faithfulness\")\n\n# Filter by category\ncomposite_metrics = metric_registry.filter(category=\"composite\")\n</code></pre>"},{"location":"guides/metrics/#customizing-metrics","title":"Customizing Metrics","text":"<pre><code>from axion.metrics import Faithfulness\n\n# Adjust threshold\nmetric = Faithfulness(threshold=0.8)\n\n# Custom instructions\nmetric = AnswerCriteria(\n    criteria_key=\"my_criteria\",\n    scoring_strategy=\"aspect\"\n)\n</code></pre> <p>Running Evaluations  Creating Custom Metrics  Metrics Reference </p>"},{"location":"guides/search/","title":"Search Integrations","text":"<p>Axion provides retriever implementations for popular search APIs to integrate into evaluation pipelines.</p>"},{"location":"guides/search/#available-retrievers","title":"Available Retrievers","text":"Retriever API Best For <code>GoogleRetriever</code> Google Custom Search Web search, broad coverage <code>TavilyRetriever</code> Tavily AI AI-optimized search results <code>YouRetriever</code> You.com Real-time web data"},{"location":"guides/search/#quick-start","title":"Quick Start","text":"<pre><code>from axion.search import GoogleRetriever\n\nretriever = GoogleRetriever(\n    api_key=\"your-serpapi-key\"\n)\n\nresults = await retriever.search(\"What is RAG in AI?\")\nfor result in results:\n    print(f\"{result.title}: {result.url}\")\n</code></pre>"},{"location":"guides/search/#retrievers","title":"Retrievers","text":"Google Search Tavily Search You.com Search <pre><code>from axion.search import GoogleRetriever\n\nretriever = GoogleRetriever(\n    api_key=\"your-serpapi-key\",\n    num_results=10\n)\n\nresults = await retriever.search(query)\n</code></pre> <p>AI-optimized search with relevance filtering:</p> <pre><code>from axion.search import TavilyRetriever\n\nretriever = TavilyRetriever(\n    api_key=\"your-tavily-key\",\n    search_depth=\"advanced\"\n)\n\nresults = await retriever.search(query)\n</code></pre> <p>Real-time web data with snippet extraction:</p> <pre><code>from axion.search import YouRetriever\n\nretriever = YouRetriever(\n    api_key=\"your-you-key\"\n)\n\nresults = await retriever.search(query)\n</code></pre>"},{"location":"guides/search/#using-with-evaluation","title":"Using with Evaluation","text":"<p>Combine retrievers with evaluation metrics:</p> <pre><code>from axion import DatasetItem\nfrom axion.metrics import ContextualRelevancy\n\n# Get retrieval results\nresults = await retriever.search(query)\ncontent = [r.snippet for r in results]\n\n# Create evaluation item\nitem = DatasetItem(\n    query=query,\n    actual_output=agent_response,\n    retrieved_content=content\n)\n\n# Evaluate retrieval quality\nmetric = ContextualRelevancy()\nscore = await metric.evaluate(item)\n</code></pre> <p>Google Search Deep Dive  Tavily Search Deep Dive  API Reference: Search </p>"},{"location":"guides/synthetic/","title":"Synthetic Data Generation","text":"<p>Generate evaluation datasets from documents or sessions to scale your test coverage.</p>"},{"location":"guides/synthetic/#overview","title":"Overview","text":"<p>Manual dataset curation is essential but doesn't scale. Axion provides synthetic generation methods powered by a graph-based workflow built on <code>pydantic_graph</code>:</p> <ol> <li>Document Q&amp;A Generation - Create question-answer pairs from your knowledge base</li> <li>Session-based Generation - Generate Q&amp;A from conversation transcripts</li> </ol>"},{"location":"guides/synthetic/#generation-methods","title":"Generation Methods","text":"Document Q&amp;A Session-based <p>Generate evaluation data from your documents using <code>DocumentQAGenerator</code>:</p> <pre><code>from axion.synthetic import DocumentQAGenerator, GenerationParams\n\n# Initialize with your LLM\ngenerator = DocumentQAGenerator(\n    llm=your_llm,  # LLMRunnable-compatible object\n    params=GenerationParams(\n        num_pairs=10,\n        question_types=[\"factual\", \"conceptual\", \"application\"],\n        difficulty=\"medium\",\n        answer_length=\"medium\",\n        validation_threshold=0.7,\n    ),\n)\n\n# Generate from a directory of documents\nresults = await generator.generate_from_directory(\"./documents\")\n\n# Convert to Dataset for evaluation\ndataset = generator.to_dataset(results, dataset_name=\"my_synthetic_dataset\")\n</code></pre> <p>Direct Workflow Usage</p> <p>For more control, use <code>QAWorkflowGraph</code> directly:</p> <pre><code>from axion.synthetic.workflow import QAWorkflowGraph\n\nworkflow = QAWorkflowGraph(llm=your_llm)\n\n# Run with document content\nresult = await workflow.run_from_documents(\n    content=\"Your document text here...\",\n    num_pairs=5,\n    question_types=[\"factual\", \"analytical\"],\n    difficulty=\"medium\",\n    splitter_type=\"sentence\",\n    chunk_size=2048,\n    statements_per_chunk=5,\n    validation_threshold=0.8,\n    max_reflection_iterations=3,\n)\n\n# Access results\nqa_pairs = result[\"validated_qa_pairs\"]\nstatements = result[\"statements\"]\nquality = result[\"average_quality\"]\n</code></pre> <p>Generate Q&amp;A pairs from conversation transcripts:</p> <pre><code>from axion.synthetic.workflow import QAWorkflowGraph\n\nworkflow = QAWorkflowGraph(llm=your_llm)\n\nresult = await workflow.run_from_sessions(\n    session_messages=[\n        {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n        {\"role\": \"assistant\", \"content\": \"Go to Settings &gt; Security &gt; Reset Password\"},\n    ],\n    session_metadata={\"topic\": \"account_management\"},\n    num_pairs=3,\n)\n</code></pre>"},{"location":"guides/synthetic/#how-it-works","title":"How It Works","text":"<p>The workflow is implemented as a directed graph with the following nodes:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; InitializeWorkflow\n    InitializeWorkflow --&gt; ProcessDocuments: documents\n    InitializeWorkflow --&gt; ProcessSessions: sessions\n    ProcessDocuments --&gt; ChunkContent\n    ProcessSessions --&gt; ChunkContent\n    ChunkContent --&gt; ExtractStatements\n    ExtractStatements --&gt; GenerateQuestions\n    GenerateQuestions --&gt; GenerateAnswers\n    GenerateAnswers --&gt; ValidateQAPairs\n    ValidateQAPairs --&gt; PrepareEnhancement: quality below threshold\n    ValidateQAPairs --&gt; [*]: quality met or max iterations\n    PrepareEnhancement --&gt; GenerateQuestions</code></pre>"},{"location":"guides/synthetic/#pipeline-steps","title":"Pipeline Steps","text":"Step Description Initialize Validate inputs and configure processors Process Apply transformations to documents or sessions Chunk Split content using sentence or semantic chunking Extract Statements Parse content into factual claims with source indices Generate Questions Create diverse questions from statements Generate Answers Create ground-truth answers from source content Validate Score Q&amp;A pairs and filter low-quality results Enhance Iteratively improve questions below threshold"},{"location":"guides/synthetic/#configuration-options","title":"Configuration Options","text":""},{"location":"guides/synthetic/#generationparams","title":"GenerationParams","text":"Parameter Type Default Description <code>num_pairs</code> int 1 Number of Q&amp;A pairs to generate <code>question_types</code> list factual, conceptual, application, analysis Types of questions to generate <code>difficulty</code> str medium Question difficulty (easy, medium, hard) <code>splitter_type</code> str semantic Chunking strategy (semantic, sentence) <code>chunk_size</code> int 4000 Max chunk size for sentence splitter <code>statements_per_chunk</code> int 5 Statements to extract per chunk <code>answer_length</code> str medium Answer length (short, medium, long) <code>validation_threshold</code> float 0.7 Minimum quality score to accept <code>max_reflection_iterations</code> int 3 Max enhancement iterations <code>custom_guidelines</code> str None Custom instructions for generation <code>example_question</code> str None Example question for style guidance <code>example_answer</code> str None Example answer for style guidance"},{"location":"guides/synthetic/#dimensional-generation","title":"Dimensional Generation","text":"<p>Guide generation with structured dimensions:</p> <pre><code>result = await workflow.run_from_documents(\n    content=content,\n    num_pairs=10,\n    dimensions={\n        \"features\": \"Product pricing, subscription tiers, enterprise features\",\n        \"persona\": \"Technical decision maker evaluating SaaS solutions\",\n        \"scenarios\": \"Comparing vendor options during procurement process\",\n    },\n)\n</code></pre>"},{"location":"guides/synthetic/#key-considerations","title":"Key Considerations","text":"<p>Quality Over Quantity</p> <p>Synthetic data quality varies significantly. Always review a sample before using at scale, and use <code>validation_threshold</code> to filter aggressively.</p>"},{"location":"guides/synthetic/#iterative-refinement","title":"Iterative Refinement","text":"<p>The workflow automatically refines low-quality Q&amp;A pairs:</p> <pre><code>result = await workflow.run_from_documents(\n    content=content,\n    validation_threshold=0.85,  # High bar\n    max_reflection_iterations=5,  # More attempts to meet threshold\n)\n\nprint(f\"Final quality: {result['average_quality']:.2f}\")\nprint(f\"Iterations used: {result['current_iteration']}\")\n</code></pre>"},{"location":"guides/synthetic/#coverage-strategy","title":"Coverage Strategy","text":"Phase Focus Formation Curate real-world examples with expert validation Expansion Augment with synthetic data for edge cases Maintenance Continuously add new failure modes"},{"location":"guides/synthetic/#when-to-use-synthetic-data","title":"When to Use Synthetic Data","text":"<p>Good for:</p> <ul> <li>Expanding coverage of edge cases</li> <li>Testing rare scenarios</li> <li>Scaling regression testing</li> <li>Bootstrapping evaluation datasets</li> </ul> <p>Not a replacement for:</p> <ul> <li>Real user data</li> <li>Expert-labeled ground truth</li> <li>Domain-specific nuance</li> </ul>"},{"location":"guides/synthetic/#graph-visualization","title":"Graph Visualization","text":"<p>View the workflow structure in notebooks:</p> <pre><code>workflow = QAWorkflowGraph(llm=your_llm)\nworkflow.visualize_graph()  # Displays mermaid diagram\n</code></pre> <p>Agent Evaluation Playbook  Metrics Guide  Synthetic Data Reference </p>"},{"location":"guides/langfuse/configuration/","title":"Langfuse Configuration","text":"<p>This guide covers all setup and configuration options for integrating Axion with Langfuse.</p>"},{"location":"guides/langfuse/configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Langfuse account (cloud or self-hosted)</li> <li>Langfuse API keys (public and secret)</li> <li>Python 3.12+</li> <li>Axion installed with Langfuse support: <code>pip install axion[langfuse]</code></li> </ul>"},{"location":"guides/langfuse/configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure Langfuse credentials via environment variables:</p> Variable Required Description Default <code>LANGFUSE_PUBLIC_KEY</code> Yes Your Langfuse public key (<code>pk-lf-...</code>) - <code>LANGFUSE_SECRET_KEY</code> Yes Your Langfuse secret key (<code>sk-lf-...</code>) - <code>LANGFUSE_BASE_URL</code> No Langfuse host URL <code>https://us.cloud.langfuse.com</code> <code>LANGFUSE_TAGS</code> No Comma-separated default tags for traces - <code>LANGFUSE_ENVIRONMENT</code> No Environment name (e.g., <code>production</code>) - <code>LANGFUSE_TRACING_ENVIRONMENT</code> No Environment name (Langfuse SDK standard) - <code>LANGFUSE_DEFAULT_TAGS</code> No Comma-separated default tags for scores - <code>TRACING_MODE</code> No Set to <code>langfuse</code> to force Langfuse provider Auto-detected"},{"location":"guides/langfuse/configuration/#example-env-file","title":"Example <code>.env</code> File","text":"<pre><code>LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nLANGFUSE_SECRET_KEY=sk-lf-your-secret-key\nLANGFUSE_BASE_URL=https://us.cloud.langfuse.com\nLANGFUSE_ENVIRONMENT=production\nLANGFUSE_TAGS=prod,v2.0\nLANGFUSE_DEFAULT_TAGS=evaluation,automated\n</code></pre>"},{"location":"guides/langfuse/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"Auto-DetectionExplicit ConfigurationLoader with Custom CredentialsDirect LangfuseTracer <pre><code>from axion._core.tracing import Tracer\n\n# Tracer auto-detects Langfuse from LANGFUSE_SECRET_KEY\ntracer = Tracer('llm')\n</code></pre> <pre><code>from axion._core.tracing import configure_tracing, Tracer\n\n# Explicitly configure Langfuse\nconfigure_tracing(provider='langfuse')\ntracer = Tracer('llm')\n</code></pre> <pre><code>from axion.tracing import LangfuseTraceLoader\n\n# Override credentials for loader\nloader = LangfuseTraceLoader(\n    public_key='pk-lf-...',\n    secret_key='sk-lf-...',\n    host='https://cloud.langfuse.com',\n    default_tags=['evaluation', 'automated']\n)\n</code></pre> <pre><code>from axion._core.tracing.langfuse.tracer import LangfuseTracer\n\n# Direct initialization with all options\ntracer = LangfuseTracer(\n    tags=['prod', 'v1.0'],\n    environment='production'\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#auto-detection-behavior","title":"Auto-Detection Behavior","text":"<p>When you create a <code>Tracer()</code> instance, Axion automatically detects the appropriate backend:</p> <ol> <li>Check <code>TRACING_MODE</code>: If set to <code>langfuse</code>, use Langfuse</li> <li>Check credentials: If <code>LANGFUSE_SECRET_KEY</code> is set, use Langfuse</li> <li>Check Logfire: If Logfire is configured, use OpenTelemetry</li> <li>Default: Use NOOP tracer (no overhead)</li> </ol> <pre><code>import os\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\n\nfrom axion._core.tracing import Tracer\ntracer = Tracer('llm')  # Automatically uses Langfuse\n</code></pre>"},{"location":"guides/langfuse/configuration/#langfusetraceloader-initialization","title":"LangfuseTraceLoader Initialization","text":"<p>The <code>LangfuseTraceLoader</code> class accepts these initialization options:</p> Parameter Type Default Description <code>public_key</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_PUBLIC_KEY</code> <code>secret_key</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_SECRET_KEY</code> <code>host</code> <code>str</code> <code>None</code> Override <code>LANGFUSE_BASE_URL</code> <code>default_tags</code> <code>list[str]</code> <code>None</code> Tags applied to all scores <code>request_pacing</code> <code>float</code> <code>0.05</code> Delay between API requests <code>max_retries</code> <code>int</code> <code>3</code> Max retry attempts <code>base_delay</code> <code>float</code> <code>0.5</code> Initial retry delay <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader(\n    public_key='pk-lf-...',\n    secret_key='sk-lf-...',\n    host='https://us.cloud.langfuse.com',\n    default_tags=['evaluation'],\n    request_pacing=0.1,  # Slower for rate limit prone accounts\n    max_retries=5,\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#region-configuration","title":"Region Configuration","text":"<p>Langfuse offers different regional endpoints:</p> <pre><code>import os\n\n# US region (default)\nos.environ['LANGFUSE_BASE_URL'] = 'https://us.cloud.langfuse.com'\n\n# EU region\nos.environ['LANGFUSE_BASE_URL'] = 'https://cloud.langfuse.com'\n\n# Self-hosted\nos.environ['LANGFUSE_BASE_URL'] = 'https://your-langfuse-instance.com'\n</code></pre>"},{"location":"guides/langfuse/configuration/#tags-and-environment","title":"Tags and Environment","text":""},{"location":"guides/langfuse/configuration/#tags","title":"Tags","text":"<p>Tags help filter and organize traces. They can be set at multiple levels:</p> <pre><code>import os\n\n# Default tags for all traces (comma-separated)\nos.environ['LANGFUSE_TAGS'] = 'prod,v1.0'\n\n# Default tags for scores\nos.environ['LANGFUSE_DEFAULT_TAGS'] = 'evaluation,automated'\n</code></pre> <p>Tag precedence:</p> <ol> <li>Per-call tags (passed to method)</li> <li>Loader default tags (set at initialization)</li> <li>Environment variable fallback (<code>LANGFUSE_TAGS</code>)</li> </ol>"},{"location":"guides/langfuse/configuration/#environment","title":"Environment","text":"<p>Environment identifies which deployment created a trace:</p> <pre><code>import os\nos.environ['LANGFUSE_ENVIRONMENT'] = 'production'\n</code></pre> <p>Environment Limitation</p> <p>Environment cannot be set when pushing scores to existing traces. It must be configured at tracer initialization when creating the original traces.</p>"},{"location":"guides/langfuse/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/langfuse/configuration/#connection-issues","title":"Connection Issues","text":"<p>If the Langfuse client fails to initialize:</p> <ol> <li> <p>Verify credentials: <pre><code>import os\nprint(f\"Public key: {os.environ.get('LANGFUSE_PUBLIC_KEY', 'NOT SET')}\")\nprint(f\"Secret key set: {bool(os.environ.get('LANGFUSE_SECRET_KEY'))}\")\n</code></pre></p> </li> <li> <p>Check the base URL: <pre><code>print(f\"Base URL: {os.environ.get('LANGFUSE_BASE_URL', 'default')}\")\n</code></pre></p> </li> <li> <p>Test connection: <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader()\ntraces = loader.fetch_traces(limit=1)\nprint(f\"Connection successful: {len(traces)} traces\")\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/configuration/#rate-limiting-429-errors","title":"Rate Limiting (429 Errors)","text":"<p>If you encounter rate limiting when fetching many traces:</p> <pre><code># Increase delay between requests\nloader = LangfuseTraceLoader(\n    request_pacing=0.1,  # Increase from default 0.05\n    max_retries=5,\n    base_delay=1.0,\n)\n\n# Or fetch summaries only (fewer API calls)\ntraces = loader.fetch_traces(\n    limit=1000,\n    fetch_full_traces=False\n)\n</code></pre>"},{"location":"guides/langfuse/configuration/#credentials-not-found","title":"Credentials Not Found","text":"<p>Ensure environment variables are set before importing Axion modules:</p> <pre><code>import os\nos.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...'\nos.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...'\n\n# Now import\nfrom axion._core.tracing import Tracer\n</code></pre> <p>Or use a <code>.env</code> file with <code>python-dotenv</code>:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Load before imports\n\nfrom axion._core.tracing import Tracer\n</code></pre>"},{"location":"guides/langfuse/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Tracing: Create and manage traces</li> <li>Publishing: Publish evaluation results</li> <li>Overview: Complete workflow example</li> </ul>"},{"location":"guides/langfuse/overview/","title":"Langfuse Integration","text":"<p>Integrate Axion evaluation with Langfuse observability to close the feedback loop between production LLM operations and quality metrics.</p>"},{"location":"guides/langfuse/overview/#why-use-langfuse-with-axion","title":"Why Use Langfuse with Axion?","text":"<ul> <li>Close the feedback loop: Connect evaluation results directly to production traces</li> <li>Production-grade observability: Track LLM performance, costs, and quality metrics in one place</li> <li>Continuous evaluation: Automate evaluation pipelines on real production data</li> </ul>"},{"location":"guides/langfuse/overview/#evaluation-workflows","title":"Evaluation Workflows","text":"<p>Axion supports three distinct evaluation workflows, each designed for different use cases. Understanding when to use each workflow is critical for effective LLM evaluation.</p>"},{"location":"guides/langfuse/overview/#workflow-overview","title":"Workflow Overview","text":"Workflow Testing Style Primary Use Case Publishing Method API-Driven Black-box Regression testing, CI/CD <code>publish_as_experiment()</code> Trace-Based White-box Historical analysis, debugging <code>publish_as_experiment()</code> or <code>publish_to_observability()</code> Online Production Continuous Real-time quality monitoring <code>publish_to_observability()</code>"},{"location":"guides/langfuse/overview/#offline-api-driven","title":"Offline: API-Driven","text":"<p>Best for: Regression testing, CI/CD pipelines, comparing API versions</p> <p>This workflow treats your agent as a black-box. You provide inputs from a golden dataset, call your API endpoint, and evaluate the responses. You don't need access to internal traces or spans.</p> <pre><code>flowchart LR\n    subgraph Source[\"Source\"]\n        direction TB\n        A[(Langfuse)] --&gt;|read_from_langfuse| B[/Golden Dataset/]\n    end\n\n    subgraph Execute[\"Execute\"]\n        direction TB\n        B --&gt;|execute_dataset_items_from_api| C[[APIRunner]]\n        C --&gt;|calls| D([Agent API])\n        D --&gt;|responses| B\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        B --&gt; E[[evaluation_runner]]\n        E --&gt; F{{EvaluationResult}}\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        F --&gt;|new experiment| G[publish_as_experiment]\n        G --&gt; A\n    end</code></pre> <p>When to use:</p> <ul> <li>You want to test API behavior without trace instrumentation</li> <li>Running automated regression tests in CI/CD</li> <li>Comparing responses between different API versions or models</li> <li>The agent is deployed as a service and you only care about input/output behavior</li> </ul> <p>Example:</p> <pre><code>from axion import Dataset\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.runners import evaluation_runner\n\n# 1. Load golden dataset from Langfuse\ndataset = Dataset.read_from_langfuse(golden, name='my_eval_dataset')\n\n# 2. Execute API calls to populate actual_output\ndataset.execute_dataset_items_from_api('Agent API', 'config.yaml')\n\n# 3. Run evaluation\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name='API Regression Test',\n)\n\n# 4. Publish as experiment for version comparison\nresult.publish_as_experiment(\n    dataset_name='api-regression-tests',\n    run_name='v2.1-release',\n)\n</code></pre>"},{"location":"guides/langfuse/overview/#offline-trace-based","title":"Offline: Trace-Based","text":"<p>Best for: Historical analysis, A/B experiments, debugging with span-level insights</p> <p>This workflow evaluates agent runs where you have access to internal traces. You trigger agent runs from a golden dataset, collect the traces (with span-level details), then evaluate. This gives you white-box visibility into agent internals.</p> <pre><code>flowchart LR\n    subgraph LF[\"Langfuse\"]\n        direction TB\n        A[/Golden Dataset/]\n        C[(Traces)]\n    end\n\n    subgraph Run[\"Run Agent\"]\n        direction TB\n        B([Agent])\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        D[LangfuseTraceLoader]\n        E[/Dataset with Actuals/]\n        D --&gt; E\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        F[[evaluation_runner]]\n        G{{EvaluationResult}}\n        F --&gt; G\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        H[publish_to_observability]\n        I[publish_as_experiment]\n    end\n\n    A --&gt;|for each item| B\n    B --&gt;|traces| C\n    C --&gt;|fetch_traces| D\n    E --&gt; F\n    G --&gt;|attach to traces| H\n    G --&gt;|new experiment| I\n    H --&gt; C\n    I --&gt; C</code></pre> <p>When to use:</p> <ul> <li>You need span-level insights (LLM calls, tool usage, retrieval steps)</li> <li>Debugging why specific responses failed</li> <li>Running A/B experiments with different agent configurations</li> <li>Evaluating historical production runs</li> </ul> <p>Key difference from API-Driven: The <code>actual_output</code> is extracted from traces, giving you access to intermediate steps, not just final responses.</p> <p>Example:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\n# 1. Run your agent against golden dataset items (externally)\n# This creates traces in Langfuse with tags ['experiment-v1']\n\n# 2. Fetch traces with outputs\nloader = LangfuseTraceLoader()\ntraces = loader.fetch_traces(tags=['experiment-v1'])\n\n# 3. Convert to Dataset (preserving trace_id for linking)\nitems = [\n    DatasetItem(\n        id=t.id,\n        query=t.input.get('query', ''),\n        actual_output=t.output.get('response', ''),\n        trace_id=t.id,  # Preserves link to original trace\n    )\n    for t in traces if t.input and t.output\n]\ndataset = Dataset(items=items)\n\n# 4. Run evaluation\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name='Trace-Based Evaluation',\n)\n\n# 5. Publish - choose based on your needs:\n# Option A: Attach scores to existing traces (for debugging)\nresult.publish_to_observability()\n\n# Option B: Create experiment (for comparison UI)\nresult.publish_as_experiment(\n    dataset_name='agent-experiments',\n    run_name='config-v2',\n    link_to_traces=True,  # Links experiment runs to original traces\n)\n</code></pre>"},{"location":"guides/langfuse/overview/#online-production","title":"Online: Production","text":"<p>Best for: Continuous quality monitoring, real-time alerts, production health tracking</p> <p>This workflow evaluates live production traces. You fetch recent traces from Langfuse, run evaluation metrics, and attach scores back to those traces for monitoring dashboards.</p> <pre><code>flowchart LR\n    subgraph Production[\"Production\"]\n        direction TB\n        A([Agent]) --&gt;|traces| B[(Langfuse)]\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        B --&gt;|fetch_traces| C[LangfuseTraceLoader]\n        C --&gt; D[/Dataset/]\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        D --&gt; E[[evaluation_runner]]\n        E --&gt; F{{EvaluationResult}}\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        F --&gt;|attach scores| G[publish_to_observability]\n        G --&gt; B\n    end</code></pre> <p>When to use:</p> <ul> <li>Monitoring production quality in real-time</li> <li>Setting up quality alerts and dashboards</li> <li>Tracking quality drift over time</li> <li>Evaluating a sample of production traffic</li> </ul> <p>Example:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, Toxicity\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\nasync def evaluate_production_traces():\n    # 1. Fetch recent production traces\n    loader = LangfuseTraceLoader()\n    traces = loader.fetch_traces(\n        limit=100,\n        tags=['production'],\n    )\n\n    # 2. Convert to Dataset\n    items = [\n        DatasetItem(\n            id=t.id,\n            query=t.input.get('query', ''),\n            actual_output=t.output.get('response', ''),\n            trace_id=t.id,\n        )\n        for t in traces if t.input and t.output\n    ]\n    dataset = Dataset(items=items)\n\n    # 3. Run lightweight evaluation metrics\n    result = await evaluation_runner(\n        evaluation_inputs=dataset,\n        scoring_metrics=[AnswerRelevancy(), Toxicity()],\n        evaluation_name='Production Monitoring',\n    )\n\n    # 4. Attach scores to production traces\n    stats = result.publish_to_observability(tags=['automated-eval'])\n    print(f\"Evaluated {stats['uploaded']} production traces\")\n\n# Run periodically (e.g., every hour via cron)\n</code></pre> <p>Cost Considerations for Online Evaluation</p> <p>For high-volume production systems, consider:</p> <ul> <li>Sampling: Evaluate a random sample (e.g., 1-5%) of traces instead of all</li> <li>Lightweight metrics: Use heuristic metrics instead of LLM-based metrics for high-frequency evaluation</li> <li>Batching: Aggregate traces and evaluate in batches during off-peak hours</li> </ul>"},{"location":"guides/langfuse/overview/#disabling-evaluation-tracing","title":"Disabling Evaluation Tracing","text":"<p>By default, <code>evaluation_runner</code> creates traces for each metric execution. If you don't need these evaluation traces (most publishing workflows only use source traces from <code>DatasetItem.trace_id</code>), you can disable them to reduce overhead.</p> <p>Important: Configure tracing to NOOP before creating metric instances, since tracers are cached at instantiation time.</p> <pre><code>from axion.tracing import configure_tracing\nfrom axion.runners import evaluation_runner\nfrom axion.metrics import AnswerRelevancy, ExactStringMatch\n\n# 1. Disable tracing BEFORE creating metrics\nconfigure_tracing('noop')\n\n# 2. Create metrics (they will use NOOP tracers)\nconfig = {\n    'metric': {\n        'Relevance': AnswerRelevancy(model_name='gpt-4o'),\n        'ExactStringMatch': ExactStringMatch(),\n    },\n    'model': {\n        'ANSWER_QUALITY': {\n            'Relevance': 1.0,\n            'ExactStringMatch': 1.0,\n        },\n    },\n    'weights': {\n        'ANSWER_QUALITY': 1.0,\n    }\n}\n\n# 3. Run evaluation (no evaluation traces created)\nresults = evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_config=config,\n    evaluation_name='My Evaluation',\n)\n\n# 4. Restore tracing for publishing\nconfigure_tracing('langfuse')\n\n# 5. Publish results (uses source traces from DatasetItem.trace_id)\nresults.publish_to_observability()\n</code></pre> <p>When to keep evaluation tracing enabled</p> <p>Keep tracing enabled (default) when:</p> <ul> <li>Using <code>publish_as_experiment(score_on_runtime_traces=True)</code></li> <li>You need to debug metric execution in Langfuse</li> <li>You want visibility into LLM calls made by metrics</li> </ul>"},{"location":"guides/langfuse/overview/#choosing-the-right-workflow","title":"Choosing the Right Workflow","text":"<p>Use this decision tree to select the appropriate workflow:</p> <pre><code>Do you have existing traces in Langfuse?\n\u251c\u2500\u2500 No \u2192 Do you need span-level insights?\n\u2502         \u251c\u2500\u2500 No \u2192 Use API-Driven (black-box testing)\n\u2502         \u2514\u2500\u2500 Yes \u2192 Run agent with tracing, then use Trace-Based\n\u2514\u2500\u2500 Yes \u2192 Are these production traces?\n          \u251c\u2500\u2500 Yes \u2192 Use Online Production (monitoring)\n          \u2514\u2500\u2500 No \u2192 Use Trace-Based (historical analysis)\n</code></pre> Question API-Driven Trace-Based Online Need span-level debugging? No Yes Depends Requires trace instrumentation? No Yes Yes Creates new traces? Optional Yes No Best for CI/CD? Yes Possible No Best for monitoring? No No Yes"},{"location":"guides/langfuse/overview/#quick-summary","title":"Quick Summary","text":"<pre><code>flowchart LR\n    A[Agent] --&gt;|traces| B[Langfuse]\n    B --&gt;|fetch| C[LangfuseTraceLoader]\n    C --&gt;|convert| D[Dataset]\n    D --&gt;|evaluate| E[evaluation_runner]\n    E --&gt;|scores| F[EvaluationResult]\n    F --&gt;|publish| B</code></pre> Detailed Workflow <pre><code>flowchart LR\n    subgraph Production[\"Production\"]\n        direction TB\n        A([Agent]) --&gt;|traces| B[(Langfuse)]\n    end\n\n    subgraph Fetch[\"Fetch\"]\n        direction TB\n        B --&gt;|fetch_traces| C[LangfuseTraceLoader]\n        C --&gt; D[/Dataset/]\n        E[/Local Data/] --&gt; D\n    end\n\n    subgraph Evaluate[\"Evaluate\"]\n        direction TB\n        D --&gt; F[[evaluation_runner]]\n        F --&gt; G{{EvaluationResult}}\n    end\n\n    subgraph Analyze[\"Analyze\"]\n        direction TB\n        G --&gt; H[summary]\n        G --&gt; I[to_dataframe]\n        G --&gt; J[to_scorecard]\n    end\n\n    subgraph Publish[\"Publish\"]\n        direction TB\n        G --&gt;|existing traces| K[publish_to_observability]\n        G --&gt;|new experiment| L[publish_as_experiment]\n        K --&gt; B\n        L --&gt; B\n    end</code></pre>"},{"location":"guides/langfuse/overview/#complete-example","title":"Complete Example","text":"<p>This example demonstrates the full workflow: fetching traces, running evaluation, viewing results, and publishing back to Langfuse.</p> <pre><code>import asyncio\nfrom axion.tracing import LangfuseTraceLoader\nfrom axion.metrics import AnswerRelevancy, AnswerCompleteness\nfrom axion.runners import evaluation_runner\nfrom axion import Dataset, DatasetItem\n\nasync def main():\n    # 1. Fetch traces from Langfuse\n    loader = LangfuseTraceLoader()\n    traces = loader.fetch_traces(limit=50, tags=['production'])\n\n    # 2. Convert to Dataset\n    items = [\n        DatasetItem(\n            id=t.id,\n            query=t.input.get('query', ''),\n            actual_output=t.output.get('response', ''),\n            trace_id=t.id,\n        )\n        for t in traces if t.input and t.output\n    ]\n    dataset = Dataset(items=items)\n\n    # 3. Run evaluation\n    result = await evaluation_runner(\n        evaluation_inputs=dataset,\n        scoring_metrics=[AnswerRelevancy(), AnswerCompleteness()],\n        evaluation_name='Production Evaluation',\n    )\n\n    # 4. View results\n    from axion.runners.summary import MetricSummary\n    MetricSummary().execute(result.results, total_time=100)\n    result.to_scorecard(display_in_notebook=True)\n\n    # 5. Publish back to Langfuse\n    stats = result.publish_as_experiment(\n        dataset_name='my-eval-dataset',\n        run_name='experiment-v1',\n        tags=['production']\n    )\n    print(f\"Published {stats['scores_uploaded']} scores\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/langfuse/overview/#understanding-results","title":"Understanding Results","text":"<p>After running <code>evaluation_runner</code>, use these methods to analyze results:</p> Method Description <code>MetricSummary().execute(result.results, total_time)</code> Generate detailed metric analysis report <code>result.to_dataframe()</code> Convert results to a pandas DataFrame for analysis <code>result.to_scorecard(display_in_notebook=True)</code> Display an interactive scorecard visualization <code>result.to_latency_plot()</code> Visualize metric latency distributions"},{"location":"guides/langfuse/overview/#quick-analysis","title":"Quick Analysis","text":"<pre><code>from axion.runners.summary import MetricSummary\n\n# Generate detailed summary report\nMetricSummary().execute(result.results, total_time=100)\n\n# Export to DataFrame for custom analysis\ndf = result.to_dataframe()\nprint(df.describe())\n\n# Visual scorecard (in Jupyter notebooks)\nresult.to_scorecard(display_in_notebook=True)\n</code></pre>"},{"location":"guides/langfuse/overview/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Configure Langfuse: Set up credentials and environment</li> <li>Create Traces: Instrument your LLM operations with <code>@trace</code> decorator</li> <li>Explore Traces: Navigate and filter traces with <code>TraceCollection</code></li> <li>Fetch &amp; Evaluate: Retrieve traces and run metrics</li> <li>Publish Results: Send scores back to Langfuse</li> </ol>"},{"location":"guides/langfuse/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration: Set up Langfuse credentials and options</li> <li>Tracing: Learn how to create and manage traces</li> <li>Trace Collection: Rich trace exploration, filtering, and dataset conversion</li> <li>Publishing: Publish scores to existing traces or create experiments</li> <li>Metrics Guide: Available metrics and customization options</li> </ul>"},{"location":"guides/langfuse/publishing/","title":"Publishing Evaluation Results","text":"<p>After running evaluations, publish results back to Langfuse. Axion provides two methods depending on whether you're scoring existing traces or creating new experiments.</p>"},{"location":"guides/langfuse/publishing/#two-publishing-paths","title":"Two Publishing Paths","text":"1 <p>publish_to_observability()</p> <p>Attach scores to existing production traces. Requires <code>trace_id</code> on each item.</p> 2 <p>publish_as_experiment()</p> <p>Create a complete experiment from scratch \u2014 datasets, items, runs, and scores. No existing traces needed.</p>"},{"location":"guides/langfuse/publishing/#publishing-to-existing-traces","title":"Publishing to Existing Traces","text":"<p>Use <code>publish_to_observability()</code> when you have existing traces in Langfuse and want to attach evaluation scores to them.</p>"},{"location":"guides/langfuse/publishing/#basic-usage","title":"Basic Usage","text":"<pre><code># Publish with default settings\nstats = result.publish_to_observability()\nprint(f\"Uploaded: {stats['uploaded']}, Skipped: {stats['skipped']}\")\n</code></pre>"},{"location":"guides/langfuse/publishing/#with-tags","title":"With Tags","text":"<pre><code>stats = result.publish_to_observability(\n    tags=['experiment-v1', 'automated']\n)\n</code></pre>"},{"location":"guides/langfuse/publishing/#trace-level-only","title":"Trace-Level Only","text":"<pre><code># Scores attach to traces, not observations\nstats = result.publish_to_observability(observation_id_field=None)\n</code></pre>"},{"location":"guides/langfuse/publishing/#using-langfusetraceloader-directly","title":"Using LangfuseTraceLoader Directly","text":"<p>For more control, use the loader's method:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader(default_tags=['evaluation'])\n\nstats = loader.push_scores_to_langfuse(\n    evaluation_result=result,\n    observation_id_field='observation_id',\n    flush=True,\n    tags=['prod', 'v1.0']  # Merged with default_tags\n)\n</code></pre>"},{"location":"guides/langfuse/publishing/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>loader</code> <code>BaseTraceLoader</code> <code>None</code> Loader instance (creates one if None) <code>observation_id_field</code> <code>str</code> <code>'observation_id'</code> Field for granular scoring <code>flush</code> <code>bool</code> <code>True</code> Flush client after uploading <code>tags</code> <code>list[str]</code> <code>None</code> Tags to attach to scores"},{"location":"guides/langfuse/publishing/#return-statistics","title":"Return Statistics","text":"<pre><code>stats = result.publish_to_observability()\n# stats = {\n#     'uploaded': 45,  # Successfully uploaded scores\n#     'skipped': 5,    # Skipped (missing trace_id, NaN scores)\n# }\n</code></pre>"},{"location":"guides/langfuse/publishing/#granular-vs-trace-level-scoring","title":"Granular vs Trace-Level Scoring","text":"<p>Trace-level scoring attaches scores to the entire trace:</p> <pre><code>stats = result.publish_to_observability(observation_id_field=None)\n</code></pre> <p>Observation-level scoring attaches scores to specific spans:</p> <pre><code># Ensure DatasetItems have observation_id set\nstats = result.publish_to_observability(observation_id_field='observation_id')\n</code></pre>"},{"location":"guides/langfuse/publishing/#publishing-as-experiments","title":"Publishing as Experiments","text":"<p>Use <code>publish_as_experiment()</code> for evaluation workflows that don't start with existing traces. This creates a complete experiment in Langfuse: datasets, dataset items, experiment runs, and scores.</p>"},{"location":"guides/langfuse/publishing/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from axion.runners import evaluation_runner\nfrom axion.metrics import Faithfulness, AnswerRelevancy\n\n# Run evaluation on a local dataset\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[Faithfulness(), AnswerRelevancy()],\n    evaluation_name='Offline RAG Evaluation',\n)\n\n# Upload as a Langfuse experiment\nstats = result.publish_as_experiment(\n    dataset_name='my-rag-eval-dataset',\n    run_name='experiment-v1',\n    run_metadata={'model': 'gpt-4o', 'version': '2.0'},\n    tags=['offline', 'baseline'],\n)\n\nprint(f\"Dataset: {stats['dataset_name']}\")\nprint(f\"Run: {stats['run_name']}\")\nprint(f\"Items created: {stats['items_created']}\")\nprint(f\"Scores uploaded: {stats['scores_uploaded']}\")\n</code></pre>"},{"location":"guides/langfuse/publishing/#how-it-works","title":"How It Works","text":"<pre><code>graph TD\n    N[\"1. Determine Names\"] --&gt; D[\"2. Create/Get Dataset\"]\n    D --&gt; P1[\"3. Create Dataset Items\"]\n    P1 --&gt; P2[\"4. Create Experiment Runs\"]\n    P2 --&gt; F[\"5. Final Flush\"]\n    N -.- N1[\"dataset_name: provided OR evaluation_name OR auto-generated\"]\n    N -.- N2[\"run_name: provided OR dataset_name-run_id[:8]\"]\n    D -.- D1[\"client.create_dataset() &amp;mdash; upserts, safe if exists\"]\n    P1 -.- P1a[\"Serialize input, expected_output, create_dataset_item()\"]\n    P2 -.- P2a[\"Create trace, link to dataset item, attach scores\"]</code></pre>"},{"location":"guides/langfuse/publishing/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>loader</code> <code>LangfuseTraceLoader</code> <code>None</code> Loader instance (creates one if None) <code>dataset_name</code> <code>str</code> <code>None</code> Name for the Langfuse dataset <code>run_name</code> <code>str</code> <code>None</code> Name for the experiment run <code>run_metadata</code> <code>dict</code> <code>None</code> Metadata for the experiment run <code>flush</code> <code>bool</code> <code>True</code> Flush client after uploading <code>tags</code> <code>list[str]</code> <code>None</code> Tags to attach to scores <code>score_on_runtime_traces</code> <code>bool</code> <code>False</code> Skip creating runs, just add scores to existing traces <code>link_to_traces</code> <code>bool</code> <code>False</code> Link experiment runs to existing traces instead of creating new ones"},{"location":"guides/langfuse/publishing/#experiment-modes","title":"Experiment Modes","text":"<p><code>publish_as_experiment()</code> supports three modes for different use cases:</p> Mode Creates Runs Links to Traces Use Case Default (both <code>False</code>)  new traces Standard experiment - creates fresh traces for each run <code>score_on_runtime_traces=True</code> N/A (scores only) Just add scores to existing traces without creating experiment runs <code>link_to_traces=True</code>  linked Experiment UI linked to original evaluation traces <p>When to use each mode</p> <ul> <li>Default: Use when you don't have existing traces and want a self-contained experiment</li> <li>score_on_runtime_traces: Use when you only care about scores on existing traces, not the experiment UI</li> <li>link_to_traces: Use when you ran evaluations with tracing enabled and want experiment runs to link back to those original traces</li> </ul>"},{"location":"guides/langfuse/publishing/#default-mode","title":"Default Mode","text":"<p>Creates new traces for each dataset item run:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n)\n# Creates: dataset items + new \"Dataset run\" traces + scores\n</code></pre>"},{"location":"guides/langfuse/publishing/#score-on-runtime-traces","title":"Score on Runtime Traces","text":"<p>Attaches scores directly to existing traces without creating experiment runs:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n    score_on_runtime_traces=True,\n)\n# Creates: dataset items + scores on existing traces\n# Does NOT create: experiment runs\n</code></pre> <p>This is useful when:</p> <ul> <li>You only need scores visible on your production traces</li> <li>You don't need the experiment comparison UI</li> <li>Your <code>DatasetItem</code> objects have <code>trace_id</code> set from prior tracing</li> </ul>"},{"location":"guides/langfuse/publishing/#link-to-traces","title":"Link to Traces","text":"<p>Creates experiment runs that link to your existing evaluation traces:</p> <pre><code>stats = result.publish_as_experiment(\n    dataset_name='my-dataset',\n    run_name='experiment-v1',\n    link_to_traces=True,\n)\n# Creates: dataset items + experiment runs linked to existing traces + scores\n</code></pre> <p>This is useful when:</p> <ul> <li>You ran <code>evaluation_runner</code> with Langfuse tracing enabled</li> <li>You want experiment runs in the Langfuse UI to link back to those original traces</li> <li>You need both the experiment comparison view AND visibility into the original trace details</li> </ul> <p>Fallback behavior</p> <p>When <code>link_to_traces=True</code> but a <code>DatasetItem</code> doesn't have a <code>trace_id</code>, that item falls back to default mode (creates a new trace).</p> <p>Precedence</p> <p>If both <code>score_on_runtime_traces=True</code> and <code>link_to_traces=True</code> are set, <code>score_on_runtime_traces</code> takes precedence.</p>"},{"location":"guides/langfuse/publishing/#return-statistics_1","title":"Return Statistics","text":"<pre><code>stats = result.publish_as_experiment(...)\n# stats = {\n#     'dataset_name': 'my-rag-eval-dataset',\n#     'run_name': 'experiment-v1',\n#     'items_created': 50,\n#     'runs_created': 50,\n#     'scores_uploaded': 100,\n#     'scores_skipped': 0,\n#     'errors': [],\n# }\n</code></pre>"},{"location":"guides/langfuse/publishing/#behavior-with-existing-names","title":"Behavior with Existing Names","text":"<p>Understanding how the method handles existing datasets and runs:</p> Scenario Behavior Dataset already exists <code>create_dataset()</code> upserts - retrieves existing, no error Item ID already exists Caught as \"already exists\" error, item is reused Run name already exists Creates a new run under the same name (distinguished by timestamp)"},{"location":"guides/langfuse/publishing/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Item IDs come from Axion's <code>DatasetItem.id</code> - Enables deduplication. Running the same evaluation twice won't duplicate items.</p> </li> <li> <p>Runs are always created fresh - Each call creates new experiment runs, even with the same <code>run_name</code>. This lets you compare multiple runs.</p> </li> <li> <p>Dataset items are append-only - New items are added, existing items (by ID) are reused.</p> </li> </ol>"},{"location":"guides/langfuse/publishing/#example-scenarios","title":"Example Scenarios","text":"First RunSame Dataset, Different RunRe-running Same Experiment <pre><code># Creates: dataset \"my-rag-eval\", 100 items, run \"baseline-v1\"\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',\n    run_name='baseline-v1'\n)\n</code></pre> <pre><code># Reuses: dataset \"my-rag-eval\", existing items (by ID)\n# Creates: new run \"improved-v2\"\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',  # Same dataset\n    run_name='improved-v2'       # New run name\n)\n</code></pre> <pre><code># Reuses: dataset, items\n# Creates: NEW run also named \"baseline-v1\" (Langfuse shows both)\nresult.publish_as_experiment(\n    dataset_name='my-rag-eval',\n    run_name='baseline-v1'  # Same run name\n)\n</code></pre> <p>Comparing Experiments</p> <p>Use the same <code>dataset_name</code> with different <code>run_name</code> values to compare multiple experiments (different models, prompts, or configurations) in Langfuse's experiment comparison view.</p>"},{"location":"guides/langfuse/publishing/#choosing-the-right-method","title":"Choosing the Right Method","text":"<p>See Also: Evaluation Workflows</p> <p>The choice of publishing method depends on your evaluation workflow. See Evaluation Workflows for guidance on choosing between API-Driven, Trace-Based, and Online Production workflows.</p>"},{"location":"guides/langfuse/publishing/#by-workflow","title":"By Workflow","text":"Workflow Primary Method Alternative API-Driven (black-box testing) <code>publish_as_experiment()</code> - Trace-Based (white-box testing) <code>publish_as_experiment(link_to_traces=True)</code> <code>publish_to_observability()</code> Online Production (monitoring) <code>publish_to_observability()</code> -"},{"location":"guides/langfuse/publishing/#by-scenario","title":"By Scenario","text":"Scenario Use This Method Scoring production traces <code>publish_to_observability()</code> A/B testing with existing traces <code>publish_to_observability()</code> Offline evaluation (no traces) <code>publish_as_experiment()</code> Comparing model versions <code>publish_as_experiment()</code> Creating baseline datasets <code>publish_as_experiment()</code> Continuous monitoring <code>publish_to_observability()</code> Experiment UI + link to evaluation traces <code>publish_as_experiment(link_to_traces=True)</code> Scores on traces + no experiment runs <code>publish_as_experiment(score_on_runtime_traces=True)</code>"},{"location":"guides/langfuse/publishing/#quick-reference","title":"Quick Reference","text":"<pre><code># For existing traces (from production):\nresult.publish_to_observability()  # Attaches scores to existing traces\n\n# For new experiments (no existing traces):\nresult.publish_as_experiment()  # Creates everything from scratch\n\n# For experiments linked to evaluation traces:\nresult.publish_as_experiment(link_to_traces=True)  # Links runs to existing traces\n\n# For scores only (no experiment runs):\nresult.publish_as_experiment(score_on_runtime_traces=True)  # Scores only\n</code></pre>"},{"location":"guides/langfuse/publishing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/langfuse/publishing/#scores-not-appearing","title":"Scores Not Appearing","text":"<p>If scores don't appear in the Langfuse UI:</p> <ol> <li> <p>Check return stats: <pre><code>stats = result.publish_to_observability()\nprint(f\"Uploaded: {stats['uploaded']}, Skipped: {stats['skipped']}\")\n</code></pre></p> </li> <li> <p>Ensure flush completed: <pre><code>stats = result.publish_to_observability(flush=True)\n</code></pre></p> </li> <li> <p>Verify trace_id matches: <pre><code># trace_id must match an existing trace\nfor item in dataset.items:\n    print(f\"Item {item.id} -&gt; trace_id: {item.trace_id}\")\n</code></pre></p> </li> <li> <p>Check for NaN scores (these are skipped): <pre><code>import math\nfor test_result in result.results:\n    for score in test_result.score_results:\n        if score.score is None or math.isnan(score.score):\n            print(f\"Invalid score: {score.name}\")\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/publishing/#missing-trace_id-warnings","title":"Missing trace_id Warnings","text":"<p>Scores are skipped if <code>trace_id</code> is missing:</p> <pre><code># Ensure trace_id is preserved during conversion\nitems.append(DatasetItem(\n    id=trace.id,\n    query=query,\n    actual_output=output,\n    trace_id=trace.id,  # Required!\n))\n</code></pre> <p>Check your dataset items: <pre><code>for item in dataset.items:\n    if not item.trace_id:\n        print(f\"Missing trace_id: {item.id}\")\n</code></pre></p>"},{"location":"guides/langfuse/publishing/#rate-limiting","title":"Rate Limiting","text":"<p>For large evaluations, consider batching:</p> <pre><code># Increase delay between requests\nloader = LangfuseTraceLoader(request_pacing=0.1)\nstats = result.publish_to_observability(loader=loader)\n</code></pre> <p>Overview  Tracing  Evaluation Guide </p>"},{"location":"guides/langfuse/trace-collection/","title":"Trace Collection","text":"<p><code>TraceCollection</code> provides a rich exploration layer between fetching raw traces and converting them to a <code>Dataset</code> for evaluation. It wraps raw Langfuse traces with dot-notation access, step-based navigation, filtering, serialization, and dataset conversion.</p> <ul> <li>Dot-notation access: Navigate nested trace data with attribute syntax</li> <li>Step-based navigation: Access named observations (spans, generations) as logical steps</li> <li>Prompt variable extraction: Extract structured variables from generation prompts via regex patterns</li> <li>Filtering and serialization: Filter traces by attributes, save/load JSON snapshots</li> <li>Dataset conversion: Convert directly to an axion <code>Dataset</code> with custom extraction logic</li> </ul>"},{"location":"guides/langfuse/trace-collection/#quick-start","title":"Quick Start","text":"<pre><code>from axion.tracing import TraceCollection, LangfuseTraceLoader\n\nloader = LangfuseTraceLoader()\n\n# Fetch and wrap traces in one step\ncollection = TraceCollection.from_langfuse(\n    trace_ids=['abc123', 'def456'],\n    loader=loader,\n)\n\n# Explore traces with dot-notation\ntrace = collection[0]\nprint(trace.id)\nprint(trace.name)\nprint(trace.step_names)\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#loading-traces","title":"Loading Traces","text":""},{"location":"guides/langfuse/trace-collection/#from-langfuse","title":"From Langfuse","text":"<p>The <code>from_langfuse()</code> factory fetches traces and wraps them in a single call:</p> <pre><code>from axion.tracing import TraceCollection, LangfuseTraceLoader\n\nloader = LangfuseTraceLoader(\n    public_key='pk-lf-...',\n    secret_key='sk-lf-...',\n)\n\n# Fetch by trace IDs\ncollection = TraceCollection.from_langfuse(\n    trace_ids=['abc123', 'def456'],\n    loader=loader,\n)\n\n# Or fetch by filters\ncollection = TraceCollection.from_langfuse(\n    limit=100,\n    days_back=7,\n    tags=['production'],\n    name='baseball-rules-agent',\n    loader=loader,\n)\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#from-pre-fetched-traces","title":"From Pre-fetched Traces","text":"<p>If you already have raw traces from <code>loader.fetch_traces()</code> or <code>loader.fetch_trace()</code>:</p> <pre><code>traces = loader.fetch_traces(limit=50)\ncollection = TraceCollection.from_raw_traces(traces)\n\n# Or wrap a single trace\ntrace = loader.fetch_trace('abc123')\ncollection = TraceCollection([trace])\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#from-json","title":"From JSON","text":"<p>Load a previously saved snapshot:</p> <pre><code>collection = TraceCollection.load_json('traces.json')\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#exploring-traces","title":"Exploring Traces","text":""},{"location":"guides/langfuse/trace-collection/#dot-notation-access","title":"Dot-Notation Access","text":"<p><code>TraceCollection</code> wraps each trace in a <code>Trace</code> object that supports attribute-style access to both trace-level fields and named observation steps:</p> <pre><code>trace = collection[0]\n\n# Trace-level attributes\ntrace.id            # Trace ID\ntrace.name          # Trace name\ntrace.tags          # Tags list\ntrace.latency       # Response latency\n\n# Fuzzy matching: snake_case and camelCase resolve to the same key\ntrace.created_at    # Resolves to 'createdAt' if present\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#step-navigation","title":"Step Navigation","text":"<p>Observations are grouped by name into steps. Access them by name directly on the trace:</p> <pre><code># List all step names\ntrace.step_names    # ['rule-lookup', 'play-analysis', 'ruling']\n\n# Access a step by name\nstep = trace.ruling\n\n# Step properties\nstep.count          # Number of observations in this step\nstep.first          # First observation\nstep.last           # Last observation\n\n# Access specific observation types within a step\nstep.generation     # The GENERATION observation\nstep.context        # Alias for the SPAN observation\nstep.generation.input   # Generation prompt input\nstep.generation.output  # Generation output\n</code></pre> <p>Bracket access also works, which is useful for step names with special characters:</p> <pre><code>step = trace['rule-lookup']\nstep.generation.output\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#fuzzy-step-names","title":"Fuzzy Step Names","text":"<p>Step name resolution is case and separator insensitive:</p> <pre><code># All of these resolve to the same step\ntrace.ruling            # Exact match\ntrace.Ruling            # Case-insensitive\ntrace.play_analysis     # Matches 'playAnalysis'\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#prompt-variable-extraction","title":"Prompt Variable Extraction","text":"<p>For traces with structured prompts, <code>PromptPatternsBase</code> lets you define regex patterns to extract variables from generation inputs.</p>"},{"location":"guides/langfuse/trace-collection/#defining-patterns","title":"Defining Patterns","text":"<p>Subclass <code>PromptPatternsBase</code> and define <code>_patterns_&lt;step_name&gt;()</code> methods:</p> <pre><code>import re\nfrom typing import Dict\nfrom axion.tracing import PromptPatternsBase\nfrom axion._core.tracing.collection import create_extraction_pattern\n\nclass BaseballRulesPatterns(PromptPatternsBase):\n    @classmethod\n    def _patterns_ruling(cls) -&gt; Dict[str, str]:\n        h_situation = 'GAME SITUATION'\n        h_rules = 'APPLICABLE RULES'\n        h_precedents = 'HISTORICAL PRECEDENTS'\n        return {\n            'game_situation': create_extraction_pattern(\n                h_situation, re.escape(h_rules)\n            ),\n            'applicable_rules': create_extraction_pattern(\n                h_rules, re.escape(h_precedents)\n            ),\n            'historical_precedents': create_extraction_pattern(\n                h_precedents, r'$'\n            ),\n        }\n</code></pre> <p><code>create_extraction_pattern(start_text, end_pattern)</code> builds a regex that captures text between a labelled start and a terminating pattern: <code>StartText:\\s*(.*?)\\s*(?:EndPattern)</code>.</p>"},{"location":"guides/langfuse/trace-collection/#using-patterns","title":"Using Patterns","text":"<p>Pass your patterns class when creating the collection:</p> <pre><code>collection = TraceCollection.from_langfuse(\n    trace_ids=['abc123'],\n    loader=loader,\n    prompt_patterns=BaseballRulesPatterns,\n)\n\n# Extract variables from a step\nvariables = collection[0].ruling.extract_variables()\n# {'game_situation': 'runners on first and second, one out', 'applicable_rules': '...', ...}\n\n# Or access via dot-notation\ncollection[0].ruling.variables\n</code></pre> <p>Hyphenated Step Names</p> <p>Step names with hyphens or special characters are normalized when looking up pattern methods. For example, <code>rule-lookup</code> maps to <code>_patterns_rule_lookup()</code>.</p>"},{"location":"guides/langfuse/trace-collection/#observation-tree","title":"Observation Tree","text":"<p>While steps group observations by name, the observation tree reconstructs the parent/child hierarchy from <code>parent_observation_id</code> fields -- the same hierarchy visible in the Langfuse timeline UI.</p>"},{"location":"guides/langfuse/trace-collection/#trace-level-metadata","title":"Trace-Level Metadata","text":"<p>Traces represent workflows or pipelines and carry top-level metadata that is always accessible, regardless of how many roots exist:</p> <pre><code>trace = collection[0]\n\ntrace.name      # Workflow / pipeline name\ntrace.input     # Trace-level input\ntrace.output    # Trace-level output\n</code></pre> <p>These properties read directly from the underlying trace object and are never shadowed by step names.</p>"},{"location":"guides/langfuse/trace-collection/#accessing-the-tree","title":"Accessing the Tree","text":"<pre><code>trace = collection[0]\n\n# All root nodes (list, may have multiple roots)\nroots = trace.tree_roots\n\n# Convenience: single root when exactly one exists, else None\nroot = trace.tree\n\n# Walk the entire trace (pre-order depth-first, works with any number of roots)\nfor node in trace.walk():\n    print(\"  \" * node.depth + node.name)\n</code></pre> <p><code>trace.walk()</code> traverses all roots in order, so it works whether the trace has one root or many. You can also walk a single node's subtree with <code>node.walk()</code>.</p>"},{"location":"guides/langfuse/trace-collection/#observationnode-properties","title":"ObservationNode Properties","text":"<p>Each node wraps an observation and adds tree structure:</p> <pre><code>node = trace.tree\n\n# Tree structure\nnode.parent          # Parent node (None for roots)\nnode.children        # List of child nodes (sorted by start_time)\nnode.is_root         # True if no parent\nnode.is_leaf         # True if no children\nnode.depth           # Distance from root (0 for roots)\n\n# Timing\nnode.start_time      # datetime (parsed from ISO string if needed)\nnode.end_time        # datetime\nnode.duration        # timedelta (end_time - start_time)\n\n# Observation data (SmartAccess delegation)\nnode.name            # Observation name\nnode.type            # SPAN, GENERATION, etc.\nnode.input           # Observation input\nnode.output          # Observation output\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#searching-and-navigating","title":"Searching and Navigating","text":""},{"location":"guides/langfuse/trace-collection/#trace-level-search","title":"Trace-Level Search","text":"<p><code>trace.find()</code> searches across all roots and returns the first match:</p> <pre><code># Find by name (across all roots)\ngen = trace.find(name='recommendation:ai.generateText')\n\n# Find by type\nfirst_gen = trace.find(type='GENERATION')\n\n# Find by both (AND)\nspecific = trace.find(name='ruling', type='GENERATION')\n\n# Returns None when no match\ntrace.find(name='nonexistent')  # None\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#node-level-search","title":"Node-Level Search","text":"<p><code>node.find()</code> searches within a single node's subtree:</p> <pre><code>root = trace.tree  # only works when a single root exists\n\n# Find by name within this subtree\ngen = root.find(name='recommendation:ai.generateText')\n\n# Find by type\nfirst_gen = root.find(type='GENERATION')\n\n# Find by both (AND)\nspecific = root.find(name='ruling', type='GENERATION')\n\n# Returns None when no match\nroot.find(name='nonexistent')  # None\n</code></pre> <p>Bracket access searches descendants by name first, then falls back to observation field lookup:</p> <pre><code># Find a descendant node by name\ngen_node = root['recommendation:ai.generateText']\n\n# Falls back to observation field when no descendant matches\ntrace_id = root['id']\n\n# Raises KeyError when neither found\nroot['nonexistent']  # KeyError\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#iteration-and-containment","title":"Iteration and Containment","text":"<p>Nodes support standard Python iteration protocols:</p> <pre><code>root = trace.tree\n\n# Iterate direct children\nfor child in root:\n    print(child.name, child.type)\n\n# Number of direct children\nlen(root)            # 3\n\n# Check if a name exists anywhere in the subtree\n'recommendation:ai.generateText' in root  # True\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#collection-level-access","title":"Collection-Level Access","text":"<pre><code># Get tree_roots for every trace in the collection\nfor roots in collection.trees:\n    for root in roots:\n        print(root.name, [c.name for c in root.children])\n</code></pre> <p><code>from_langfuse()</code> filter behavior</p> <p>When using <code>from_langfuse()</code>, the <code>name=</code> filter parameter is ignored when <code>trace_ids</code> is provided. Trace IDs take precedence and bypass all other filters.</p>"},{"location":"guides/langfuse/trace-collection/#filtering","title":"Filtering","text":""},{"location":"guides/langfuse/trace-collection/#lambda-filter","title":"Lambda Filter","text":"<pre><code># Filter by any condition\nprod_traces = collection.filter(lambda t: 'production' in (t.tags or []))\nlong_traces = collection.filter(lambda t: t.latency &gt; 5.0)\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#attribute-filter","title":"Attribute Filter","text":"<pre><code># Simple attribute equality\nby_name = collection.filter_by(name='baseball-rules-agent')\n</code></pre> <p>Both methods return a new <code>TraceCollection</code>.</p>"},{"location":"guides/langfuse/trace-collection/#serialization","title":"Serialization","text":""},{"location":"guides/langfuse/trace-collection/#save-and-load-json","title":"Save and Load JSON","text":"<pre><code># Save to disk\ncollection.save_json('traces/snapshot.json')\n\n# Load later (with optional patterns)\nloaded = TraceCollection.load_json(\n    'traces/snapshot.json',\n    prompt_patterns=BaseballRulesPatterns,\n)\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#raw-access","title":"Raw Access","text":"<pre><code># Get the underlying raw trace objects\nraw_list = collection.to_list()\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#converting-to-dataset","title":"Converting to Dataset","text":"<p><code>to_dataset()</code> converts a <code>TraceCollection</code> into an axion <code>Dataset</code> for evaluation.</p>"},{"location":"guides/langfuse/trace-collection/#default-conversion","title":"Default Conversion","text":"<p>Without a transform, <code>to_dataset()</code> extracts <code>query</code> from trace input and <code>actual_output</code> from trace output using standard key detection (<code>query</code>, <code>question</code>, <code>input</code>, <code>response</code>, <code>answer</code>, etc.):</p> <pre><code>dataset = collection.to_dataset(name='my-eval')\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#custom-transform","title":"Custom Transform","text":"<p>For complex trace structures, pass a transform function. The transform receives a <code>Trace</code> and returns either a <code>DatasetItem</code> or a <code>dict</code> of fields:</p> Returning DatasetItemReturning dict <pre><code>from axion.dataset import DatasetItem\n\ndef extract_ruling(trace):\n    step = trace.ruling\n    gen = step.generation\n\n    return DatasetItem(\n        id=str(trace.id),\n        query=f'What is the correct ruling for play {trace.id}?',\n        actual_output=gen.output.get('ruling', ''),\n        trace_id=str(trace.id),\n        observation_id=str(gen.id),\n        additional_output={\n            'explanation': gen.output.get('explanation', ''),\n            'rule_citations': gen.output.get('rule_citations', []),\n        },\n    )\n\ndataset = collection.to_dataset(\n    name='rulings-eval',\n    transform=extract_ruling,\n)\n</code></pre> <pre><code>def simple_transform(trace):\n    return {\n        'query': str(trace.input),\n        'actual_output': str(trace.output),\n        'trace_id': str(trace.id),\n    }\n\ndataset = collection.to_dataset(\n    name='simple-eval',\n    transform=simple_transform,\n)\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#end-to-end-example","title":"End-to-End Example","text":"<p>This example demonstrates the full workflow: fetch traces from a baseball rules agent, explore with patterns, convert to dataset, evaluate, and publish.</p> <pre><code>import re\nfrom typing import Dict\n\nfrom axion.tracing import (\n    LangfuseTraceLoader,\n    PromptPatternsBase,\n    TraceCollection,\n)\nfrom axion._core.tracing.collection import create_extraction_pattern\nfrom axion.dataset import DatasetItem\nfrom axion.metrics import AnswerRelevancy, Faithfulness\nfrom axion.runners import evaluation_runner\n\n\n# 1. Define prompt patterns for variable extraction\nclass BaseballRulesPatterns(PromptPatternsBase):\n    @classmethod\n    def _patterns_ruling(cls) -&gt; Dict[str, str]:\n        h_situation = 'GAME SITUATION'\n        h_rules = 'APPLICABLE RULES'\n        return {\n            'game_situation': create_extraction_pattern(\n                h_situation, re.escape(h_rules)\n            ),\n            'applicable_rules': create_extraction_pattern(\n                h_rules, r'$'\n            ),\n        }\n\n\n# 2. Fetch traces\nloader = LangfuseTraceLoader()\ncollection = TraceCollection.from_langfuse(\n    trace_ids=['abc123', 'def456'],\n    loader=loader,\n    prompt_patterns=BaseballRulesPatterns,\n)\n\n# 3. Explore\nfor trace in collection:\n    print(trace.id, trace.step_names)\n    if 'ruling' in trace.step_names:\n        print('  variables:', trace.ruling.variables)\n\n\n# 4. Define extraction transform\ndef extract_ruling(trace):\n    return DatasetItem(\n        id=str(trace.id),\n        query=str(trace.input),\n        actual_output=str(trace.output),\n        trace_id=str(trace.id),\n    )\n\n\n# 5. Convert to Dataset and evaluate\ndataset = collection.to_dataset(name='baseball-rules-eval', transform=extract_ruling)\n\nresult = await evaluation_runner(\n    evaluation_inputs=dataset,\n    scoring_metrics=[AnswerRelevancy(), Faithfulness()],\n    evaluation_name='Baseball Rules Evaluation',\n)\n\n# 6. Publish scores back to Langfuse\nresult.publish_to_observability()\n</code></pre>"},{"location":"guides/langfuse/trace-collection/#api-reference","title":"API Reference","text":""},{"location":"guides/langfuse/trace-collection/#tracecollection","title":"TraceCollection","text":"Method Description <code>from_langfuse(trace_ids, limit, days_back, tags, name, loader, prompt_patterns)</code> Fetch from Langfuse and wrap <code>from_raw_traces(raw_traces, prompt_patterns)</code> Wrap pre-fetched trace objects <code>load_json(path, prompt_patterns)</code> Load from a JSON file <code>filter(condition)</code> Filter by lambda, returns new <code>TraceCollection</code> <code>filter_by(**kwargs)</code> Filter by attribute equality <code>to_dataset(name, transform)</code> Convert to axion <code>Dataset</code> <code>save_json(path)</code> Serialize to JSON file <code>to_list()</code> Return raw trace objects <code>len(collection)</code> Number of traces <code>collection[i]</code> Access by index"},{"location":"guides/langfuse/trace-collection/#trace","title":"Trace","text":"Property / Method Description <code>trace.name</code> Trace-level name (workflow / pipeline name) <code>trace.input</code> Trace-level input <code>trace.output</code> Trace-level output <code>trace.step_names</code> List of observation group names <code>trace.steps</code> Dict of step name to <code>TraceStep</code> <code>trace.observations</code> Flat list of all observations <code>trace.raw</code> Underlying raw trace object <code>trace.tree_roots</code> List of root <code>ObservationNode</code>s (hierarchy) <code>trace.tree</code> Single root node if exactly one root, else <code>None</code> <code>trace.walk()</code> Pre-order depth-first generator across all roots <code>trace.find(name, type)</code> First node matching name and/or type across all roots, or <code>None</code> <code>trace.&lt;step_name&gt;</code> Access a step by name (fuzzy matching) <code>trace.&lt;attribute&gt;</code> Access trace-level attributes (fuzzy matching)"},{"location":"guides/langfuse/trace-collection/#tracestep","title":"TraceStep","text":"Property / Method Description <code>step.count</code> Number of observations <code>step.first</code> First observation <code>step.last</code> Last observation <code>step.generation</code> GENERATION observation <code>step.context</code> SPAN observation (alias) <code>step.extract_variables()</code> Extract prompt variables via patterns <code>step.variables</code> Shorthand for <code>extract_variables()</code>"},{"location":"guides/langfuse/trace-collection/#observationnode","title":"ObservationNode","text":"Property / Method Description <code>node.observation</code> Underlying <code>ObservationsView</code> <code>node.parent</code> Parent node (<code>None</code> for roots) <code>node.children</code> Child nodes (sorted by <code>start_time</code>) <code>node.is_root</code> <code>True</code> if no parent <code>node.is_leaf</code> <code>True</code> if no children <code>node.depth</code> Distance from root <code>node.start_time</code> Parsed <code>datetime</code> <code>node.end_time</code> Parsed <code>datetime</code> <code>node.duration</code> <code>timedelta</code> (end - start) <code>node.walk()</code> Pre-order depth-first generator <code>node.find(name, type)</code> First descendant matching name and/or type, or <code>None</code> <code>node['name']</code> Descendant by name; falls back to observation field; raises <code>KeyError</code> <code>for child in node</code> Iterate direct children <code>len(node)</code> Number of direct children <code>'name' in node</code> <code>True</code> if any descendant has that name"},{"location":"guides/langfuse/trace-collection/#promptpatternsbase","title":"PromptPatternsBase","text":"Method Description <code>get_for(step_name)</code> Look up extraction patterns for a step <code>_patterns_&lt;name&gt;()</code> Override in subclass to define patterns"},{"location":"guides/langfuse/trace-collection/#create_extraction_pattern","title":"create_extraction_pattern","text":"<pre><code>create_extraction_pattern(start_text: str, end_pattern: str) -&gt; str\n</code></pre> <p>Builds a regex: <code>StartText:\\s*(.*?)\\s*(?:EndPattern)</code></p>"},{"location":"guides/langfuse/trace-collection/#next-steps","title":"Next Steps","text":"<ul> <li>Tracing: Creating traces with <code>@trace</code> and <code>fetch_traces()</code></li> <li>Publishing: Publish evaluation scores back to Langfuse</li> <li>Overview: Complete evaluation workflow examples</li> </ul>"},{"location":"guides/langfuse/tracing/","title":"Creating Traces","text":"<p>This guide covers how to create and manage traces in Langfuse using Axion's tracing system.</p>"},{"location":"guides/langfuse/tracing/#using-the-trace-decorator","title":"Using the @trace Decorator","text":"<p>The <code>@trace</code> decorator automatically captures function inputs and outputs:</p> <pre><code>from axion._core.tracing import init_tracer, trace\n\nclass RAGService:\n    def __init__(self):\n        self.tracer = init_tracer('llm')\n\n    @trace(name='rag-query', capture_args=True, capture_result=True)\n    async def query(self, question: str, context: list[str]) -&gt; str:\n        # Your RAG logic here\n        response = await self.llm.generate(question, context)\n        return response\n\n# Usage\nservice = RAGService()\nanswer = await service.query(\n    question='What is the return policy?',\n    context=['Returns accepted within 30 days...']\n)\nservice.tracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#decorator-parameters","title":"Decorator Parameters","text":"Parameter Type Default Description <code>name</code> <code>str</code> Function name Name for the span <code>capture_args</code> <code>bool</code> <code>False</code> Capture function arguments as input <code>capture_result</code> <code>bool</code> <code>False</code> Capture return value as output"},{"location":"guides/langfuse/tracing/#manual-span-creation","title":"Manual Span Creation","text":"<p>For more control over what gets captured, create spans manually:</p>"},{"location":"guides/langfuse/tracing/#synchronous-context-manager","title":"Synchronous Context Manager","text":"<pre><code>from axion._core.tracing import Tracer\nfrom openai import OpenAI\n\ntracer = Tracer('llm')\nclient = OpenAI()\n\nwith tracer.span('my-operation') as span:\n    span.set_input({'query': 'How do I upgrade my plan?'})\n\n    response = client.chat.completions.create(\n        model='gpt-4o',\n        messages=[{'role': 'user', 'content': 'How do I upgrade my plan?'}]\n    )\n\n    span.set_output({'response': response.choices[0].message.content})\n    span.set_attribute('model', 'gpt-4o')\n    span.set_attribute('tokens', response.usage.total_tokens)\n\ntracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#asynchronous-context-manager","title":"Asynchronous Context Manager","text":"<p>For async code, use <code>async_span</code>:</p> <pre><code>async with tracer.async_span('async-operation') as span:\n    span.set_input({'query': user_query})\n    result = await process_query(user_query)\n    span.set_output({'result': result})\n</code></pre>"},{"location":"guides/langfuse/tracing/#inputoutput-capture","title":"Input/Output Capture","text":""},{"location":"guides/langfuse/tracing/#set_input-and-set_output","title":"set_input() and set_output()","text":"<p>Use these methods to explicitly capture data on spans:</p> <pre><code>with tracer.span('llm-call') as span:\n    # Set input data (dict)\n    span.set_input({\n        'query': user_question,\n        'context': retrieved_chunks,\n        'model': 'gpt-4o'\n    })\n\n    # Your LLM call\n    response = llm.generate(...)\n\n    # Set output data (dict)\n    span.set_output({\n        'response': response.text,\n        'tokens_used': response.usage.total_tokens\n    })\n</code></pre>"},{"location":"guides/langfuse/tracing/#set_attribute","title":"set_attribute()","text":"<p>Add additional metadata to spans:</p> <pre><code>with tracer.span('retrieval') as span:\n    span.set_attribute('num_chunks', 5)\n    span.set_attribute('search_type', 'semantic')\n    span.set_attribute('latency_ms', 142)\n</code></pre>"},{"location":"guides/langfuse/tracing/#nested-spans","title":"Nested Spans","text":"<p>Create hierarchical traces with nested spans:</p> <pre><code>with tracer.span('rag-pipeline') as parent:\n    parent.set_input({'query': query})\n\n    # Child span for retrieval\n    with tracer.span('retrieval') as retrieval_span:\n        chunks = retriever.search(query)\n        retrieval_span.set_output({'chunks': len(chunks)})\n\n    # Child span for generation\n    with tracer.span('generation') as gen_span:\n        gen_span.set_input({'context': chunks})\n        response = llm.generate(query, chunks)\n        gen_span.set_output({'response': response})\n\n    parent.set_output({'answer': response})\n</code></pre>"},{"location":"guides/langfuse/tracing/#flushing-traces","title":"Flushing Traces","text":"<p>Always Flush Before Exit</p> <p>Call <code>tracer.flush()</code> before your application exits to ensure all traces are sent to Langfuse. This is especially important in scripts and short-lived processes.</p> <pre><code>tracer = Tracer('llm')\n\n# Your tracing operations\nwith tracer.span('operation') as span:\n    span.set_input({'query': 'test'})\n    # ...\n\n# Ensure traces are sent\ntracer.flush()\n</code></pre>"},{"location":"guides/langfuse/tracing/#fetching-traces","title":"Fetching Traces","text":"<p>Use <code>LangfuseTraceLoader</code> to retrieve traces from Langfuse:</p> <pre><code>from axion.tracing import LangfuseTraceLoader\n\nloader = LangfuseTraceLoader()\n\n# Fetch recent traces\ntraces = loader.fetch_traces(\n    limit=100,          # Maximum traces to fetch\n    days_back=7,        # Time window in days\n    tags=['prod'],      # Filter by tags (optional)\n    name='rag-query',   # Filter by trace name (optional)\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#fetch_traces-parameters","title":"fetch_traces() Parameters","text":"Parameter Type Default Description <code>limit</code> <code>int</code> <code>50</code> Maximum number of traces to fetch <code>mode</code> <code>str</code> <code>'days_back'</code> Time window mode: <code>days_back</code>, <code>hours_back</code>, <code>absolute</code> <code>days_back</code> <code>int</code> <code>7</code> Number of days to look back (days_back mode) <code>hours_back</code> <code>int</code> <code>24</code> Number of hours to look back (hours_back mode) <code>from_timestamp</code> <code>datetime \\| str \\| None</code> <code>None</code> Start timestamp (absolute mode, ISO string supported) <code>to_timestamp</code> <code>datetime \\| str \\| None</code> <code>None</code> End timestamp (absolute mode, ISO string supported) <code>tags</code> <code>list[str]</code> <code>None</code> Filter by specific tags <code>name</code> <code>str</code> <code>None</code> Filter by trace name <code>fetch_full_traces</code> <code>bool</code> <code>True</code> Fetch full details vs. summaries <code>**trace_list_kwargs</code> <code>dict</code> <code>{}</code> Extra kwargs passed to <code>langfuse_client.api.trace.list(...)</code>"},{"location":"guides/langfuse/tracing/#filtering-examples","title":"Filtering Examples","text":"<pre><code># Filter by multiple tags (AND logic)\nprod_traces = loader.fetch_traces(\n    limit=100,\n    tags=['production', 'v2.0']\n)\n\n# Filter by trace name\nrag_traces = loader.fetch_traces(\n    limit=100,\n    name='rag-query'\n)\n\n# Combine filters\ntraces = loader.fetch_traces(\n    limit=50,\n    days_back=3,\n    tags=['production'],\n    name='chat-completion'\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#absolute-window-example","title":"Absolute Window Example","text":"<pre><code>from datetime import datetime, timezone\n\ntraces = loader.fetch_traces(\n    mode='absolute',\n    from_timestamp=datetime(2026, 1, 1, tzinfo=timezone.utc),\n    to_timestamp=datetime(2026, 1, 2, tzinfo=timezone.utc),\n    tags=['prod'],\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#converting-traces-to-dataset","title":"Converting Traces to Dataset","text":"<p>Traces must be converted to <code>DatasetItem</code> objects for evaluation. The key is to preserve <code>trace_id</code> and optionally <code>observation_id</code> for score publishing.</p> <p>Use TraceCollection for Rich Exploration</p> <p>For step-based navigation, dot-notation access, prompt variable extraction, and built-in dataset conversion, see Trace Collection. The manual approach below works for simple cases.</p>"},{"location":"guides/langfuse/tracing/#understanding-trace-structure","title":"Understanding Trace Structure","text":"<p>Langfuse traces contain:</p> <ul> <li><code>id</code>: Unique trace identifier</li> <li><code>input</code>: The input data (dict or string)</li> <li><code>output</code>: The output data (dict or string)</li> <li><code>observations</code>: List of spans within the trace</li> <li><code>tags</code>: Associated tags</li> <li><code>metadata</code>: Additional metadata</li> </ul>"},{"location":"guides/langfuse/tracing/#manual-conversion","title":"Manual Conversion","text":"<pre><code>from axion import Dataset, DatasetItem\n\nitems = []\nfor trace in traces:\n    # Extract query from input\n    query = ''\n    if trace.input:\n        if isinstance(trace.input, dict):\n            query = trace.input.get('query', trace.input.get('question', ''))\n        else:\n            query = str(trace.input)\n\n    # Extract response from output\n    actual_output = ''\n    if trace.output:\n        if isinstance(trace.output, dict):\n            actual_output = trace.output.get('response', trace.output.get('answer', ''))\n        else:\n            actual_output = str(trace.output)\n\n    # Create DatasetItem with trace_id for score publishing\n    items.append(DatasetItem(\n        id=trace.id,\n        query=query,\n        actual_output=actual_output,\n        trace_id=trace.id,  # Required for publish_to_observability()\n    ))\n\ndataset = Dataset(items=items)\n</code></pre>"},{"location":"guides/langfuse/tracing/#using-dataframe-conversion","title":"Using DataFrame Conversion","text":"<p>For more complex conversions, use <code>Dataset.read_dataframe()</code>:</p> <pre><code>import pandas as pd\nfrom axion import Dataset\n\n# Convert traces to DataFrame\ndata = []\nfor trace in traces:\n    data.append({\n        'id': trace.id,\n        'query': trace.input.get('query', '') if trace.input else '',\n        'actual_output': trace.output.get('response', '') if trace.output else '',\n        'trace_id': trace.id,\n        'retrieved_content': trace.input.get('context', []) if trace.input else [],\n    })\n\ndf = pd.DataFrame(data)\n\n# Convert to Dataset\ndataset = Dataset.read_dataframe(df, ignore_extra_keys=True)\n</code></pre>"},{"location":"guides/langfuse/tracing/#preserving-observation-ids","title":"Preserving Observation IDs","text":"<p>For granular scoring at the span level, extract observation IDs:</p> <pre><code>items = []\nfor trace in traces:\n    # Find the generation span for granular scoring\n    obs_id = None\n    for obs in trace.observations or []:\n        if obs.type == 'GENERATION':\n            obs_id = obs.id\n            break\n\n    items.append(DatasetItem(\n        id=trace.id,\n        query=extract_query(trace.input),\n        actual_output=extract_output(trace.output),\n        trace_id=trace.id,\n        observation_id=obs_id,  # Scores attach to this span\n    ))\n</code></pre>"},{"location":"guides/langfuse/tracing/#performance-tips","title":"Performance Tips","text":"<p>Fetching Large Volumes</p> <p>Set <code>fetch_full_traces=False</code> when fetching large volumes of traces. This returns trace summaries instead of full details, significantly reducing API calls and avoiding rate limits.</p> <pre><code># Fast fetch for large volumes\ntraces = loader.fetch_traces(\n    limit=1000,\n    fetch_full_traces=False  # Returns summaries only\n)\n</code></pre>"},{"location":"guides/langfuse/tracing/#empty-traces","title":"Empty Traces","text":"<p>If <code>fetch_traces()</code> returns an empty list:</p> <ol> <li> <p>Extend time window: <pre><code>traces = loader.fetch_traces(days_back=30)\n</code></pre></p> </li> <li> <p>Verify tags exist: <pre><code># Fetch without tag filter first\nall_traces = loader.fetch_traces(limit=10, tags=None)\nprint(f\"All traces: {len(all_traces)}\")\n</code></pre></p> </li> <li> <p>Ensure traces were flushed: <pre><code>tracer.flush()  # Call after tracing operations\n</code></pre></p> </li> </ol>"},{"location":"guides/langfuse/tracing/#next-steps","title":"Next Steps","text":"<ul> <li>Trace Collection: Rich trace exploration with dot-notation, step navigation, and dataset conversion</li> <li>Publishing: Publish evaluation scores to Langfuse</li> <li>Configuration: Advanced configuration options</li> <li>Overview: Complete workflow example</li> </ul>"},{"location":"metric-registry/composite/","title":"Composite Metrics","text":"LLM-powered evaluation metrics for comprehensive AI response analysis 14 Metrics LLM-Powered <p>Composite metrics use language models to perform nuanced reasoning and analysis. These metrics evaluate complex aspects of AI responses including factual accuracy, relevance, grounding, and style\u2014things that require understanding context, semantics, and intent.</p>"},{"location":"metric-registry/composite/#rag-retrieval-metrics","title":"RAG &amp; Retrieval Metrics","text":"<p>Evaluate the quality of retrieval-augmented generation systems.</p> Faithfulness <p>Verify claims against retrieved context</p> <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Contextual Relevancy <p>Check if retrieved chunks are relevant</p> <code>query</code> <code>retrieved_content</code> Contextual Recall <p>Check if context supports expected answer</p> <code>expected_output</code> <code>retrieved_content</code> Contextual Precision <p>Measure useful chunk ranking (MAP)</p> <code>query</code> <code>expected_output</code> <code>retrieved_content</code> Contextual Ranking <p>Check if relevant chunks rank higher</p> <code>query</code> <code>retrieved_content</code> Contextual Sufficiency <p>Binary check for enough context</p> <code>query</code> <code>retrieved_content</code> Contextual Utilization <p>Measure context usage efficiency</p> <code>query</code> <code>actual_output</code> <code>retrieved_content</code>"},{"location":"metric-registry/composite/#answer-quality-metrics","title":"Answer Quality Metrics","text":"<p>Evaluate the quality and correctness of AI-generated answers.</p> Answer Relevancy <p>Check if response addresses the query</p> <code>query</code> <code>actual_output</code> Factual Accuracy <p>Verify against ground truth</p> <code>query</code> <code>actual_output</code> <code>expected_output</code> Answer Completeness <p>Check coverage of expected content</p> <code>query</code> <code>actual_output</code> <code>expected_output</code> Answer Criteria <p>Evaluate against custom criteria</p> <code>query</code> <code>actual_output</code> + <code>acceptance_criteria</code>"},{"location":"metric-registry/composite/#style-safety-metrics","title":"Style &amp; Safety Metrics","text":"<p>Evaluate tone, citations, and privacy compliance.</p> Tone &amp; Style Consistency <p>Match expected voice and formatting</p> <code>actual_output</code> <code>expected_output</code> Citation Relevancy <p>Validate citation quality</p> <code>query</code> <code>actual_output</code> PII Leakage <p>Detect privacy violations</p> <code>query</code> <code>actual_output</code>"},{"location":"metric-registry/composite/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Faithfulness 0.0 \u2013 1.0 0.5 Are claims grounded in context? Answer Relevancy 0.0 \u2013 1.0 0.5 Does response address the query? Factual Accuracy 0.0 \u2013 1.0 0.8 Does it match ground truth? Answer Completeness 0.0 \u2013 1.0 0.5 Are all expected aspects covered? Answer Criteria 0.0 \u2013 1.0 0.5 Does it meet custom criteria? Tone &amp; Style 0.0 \u2013 1.0 0.8 Does it match expected voice? Citation Relevancy 0.0 \u2013 1.0 0.8 Are citations relevant? PII Leakage 0.0 \u2013 1.0 0.5 Is output privacy-safe? (1.0 = safe) Contextual Relevancy 0.0 \u2013 1.0 0.5 Are chunks relevant to query? Contextual Recall 0.0 \u2013 1.0 0.5 Is expected answer in context? Contextual Precision 0.0 \u2013 1.0 0.5 Are useful chunks ranked first? Contextual Ranking 0.0 \u2013 1.0 0.5 Are relevant chunks ranked first? Contextual Sufficiency 0.0 or 1.0 0.5 Is context sufficient? (binary) Contextual Utilization 0.0 \u2013 1.0 0.5 Was relevant context used?"},{"location":"metric-registry/composite/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import (\n    Faithfulness,\n    AnswerRelevancy,\n    ContextualPrecision,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import Dataset\n\n# Initialize metrics\nmetrics = [\n    Faithfulness(strict_mode=True),\n    AnswerRelevancy(),\n    ContextualPrecision(),\n]\n\n# Run evaluation\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run(dataset)\n\n# Analyze results\nfor item in results:\n    print(f\"Faithfulness: {item.scores['faithfulness']:.2f}\")\n    print(f\"Relevancy: {item.scores['answer_relevancy']:.2f}\")\n    print(f\"Precision: {item.scores['contextual_precision']:.2f}\")\n</code></pre>"},{"location":"metric-registry/composite/#choosing-the-right-metrics","title":"Choosing the Right Metrics","text":"<p>Evaluation Strategy</p> <p>For RAG Systems:</p> <ul> <li>Start with Faithfulness (hallucination detection)</li> <li>Add Contextual Relevancy (retrieval quality)</li> <li>Use Contextual Precision/Ranking (ranking quality)</li> </ul> <p>For Q&amp;A Systems:</p> <ul> <li>Use Answer Relevancy (topical alignment)</li> <li>Add Factual Accuracy if you have ground truth</li> <li>Add Answer Completeness for comprehensive responses</li> </ul> <p>For Customer Service:</p> <ul> <li>Use Tone &amp; Style Consistency (brand voice)</li> <li>Add Answer Criteria (policy compliance)</li> <li>Include PII Leakage (privacy protection)</li> </ul> <p>For Research Assistants:</p> <ul> <li>Use Citation Relevancy (source quality)</li> <li>Add Faithfulness (grounding)</li> <li>Include Answer Completeness (thoroughness)</li> </ul>"},{"location":"metric-registry/composite/answer_completeness/","title":"Answer Completeness","text":"Measure how completely the response covers expected content LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/answer_completeness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Coverage ratio \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>expected_output</code> Reference answer required <p>What It Measures</p> <p>Answer Completeness evaluates whether the response covers all the key aspects from the expected output. It answers: \"Did the AI mention everything important from the reference answer?\"</p> Score Interpretation 1.0  All aspects from expected output covered 0.7+  Most aspects covered, minor omissions 0.5  Half the expected content covered &lt; 0.5  Significant content missing \u2705 Use When <ul> <li>You have reference answers</li> <li>Completeness matters more than brevity</li> <li>Testing comprehensive responses</li> <li>Evaluating educational content</li> </ul> \u274c Don't Use When <ul> <li>Brevity is preferred</li> <li>Multiple valid answer formats</li> <li>No expected_output available</li> <li>Creative/generative tasks</li> </ul> <p>See Also: Answer Criteria</p> <p>Answer Completeness checks coverage of expected output aspects. Answer Criteria checks coverage of custom acceptance criteria.</p> <p>Use Completeness when you have a reference answer; use Criteria for custom requirements.</p> How It Works  Computation Scoring System <p>The metric extracts key aspects from the expected output and checks if each is covered in the actual response.</p> <p> \u2705 COVERED 1 Aspect from expected output is present in the response. </p> <p> \u274c NOT COVERED 0 Aspect from expected output is missing from the response. </p> <p>Score Formula</p> <pre><code>score = covered_aspects / total_aspects\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Expected Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Aspect Extraction\"]\n        D[Extract Aspects from Expected]\n        E[\"Key Aspects List\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Coverage Check\"]\n        F[Check Each Aspect in Response]\n        G[\"Covered / Not Covered\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Covered Aspects\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>use_expected_output</code> <code>bool</code> <code>True</code> Use expected_output for aspect extraction <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Alternative Mode</p> <p>When <code>use_expected_output=False</code>, the metric uses sub-question decomposition instead of aspect extraction.</p>"},{"location":"metric-registry/composite/answer_completeness/#code-examples","title":"Code Examples","text":"Basic Usage With Runner <pre><code>from axion.metrics import AnswerCompleteness\nfrom axion.dataset import DatasetItem\n\nmetric = AnswerCompleteness()\n\nitem = DatasetItem(\n    query=\"What are the benefits of exercise?\",\n    actual_output=\"Exercise improves cardiovascular health and boosts mood.\",\n    expected_output=\"Exercise improves cardiovascular health, strengthens muscles, boosts mood, and helps with weight management.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (2 of 4 aspects covered)\n</code></pre> <pre><code>from axion.metrics import AnswerCompleteness\nfrom axion.runners import MetricRunner\n\nmetric = AnswerCompleteness()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Covered: {item_result.signals.covered_aspects_count}/{item_result.signals.total_aspects_count}\")\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerCompletenessResult Structure <pre><code>AnswerCompletenessResult(\n{\n    \"score\": 0.5,\n    \"covered_aspects_count\": 2,\n    \"total_aspects_count\": 4,\n    \"concept_coverage_score\": 0.5,\n    \"aspect_breakdown\": [\n        {\n            \"aspect\": \"cardiovascular health improvement\",\n            \"covered\": true,\n            \"concepts_covered\": [\"cardiovascular health\"],\n            \"reason\": \"Mentioned in response\"\n        },\n        {\n            \"aspect\": \"muscle strengthening\",\n            \"covered\": false,\n            \"concepts_missing\": [\"muscles\", \"strength\"],\n            \"reason\": \"Not mentioned in response\"\n        },\n        {\n            \"aspect\": \"mood improvement\",\n            \"covered\": true,\n            \"concepts_covered\": [\"mood\", \"boosts\"],\n            \"reason\": \"Mentioned in response\"\n        },\n        {\n            \"aspect\": \"weight management\",\n            \"covered\": false,\n            \"concepts_missing\": [\"weight\"],\n            \"reason\": \"Not mentioned in response\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_completeness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>score</code> <code>float</code> Overall completeness score <code>covered_aspects_count</code> <code>int</code> Aspects found in response <code>total_aspects_count</code> <code>int</code> Total aspects from expected output <code>aspect_breakdown</code> <code>List</code> Per-aspect coverage details"},{"location":"metric-registry/composite/answer_completeness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Complete Coverage (Score: 1.0) <p>All Aspects Covered</p> <p>Expected Output:</p> <p>\"Python is a high-level programming language known for readability, extensive libraries, and cross-platform support.\"</p> <p>AI Response:</p> <p>\"Python is a high-level language with clean, readable syntax. It has a vast ecosystem of libraries and runs on Windows, Mac, and Linux.\"</p> <p>Analysis:</p> Aspect Covered High-level language \u2705 Readability \u2705 Extensive libraries \u2705 Cross-platform \u2705 <p>Final Score: <code>4 / 4 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Coverage (Score: 0.6) <p>Some Aspects Missing</p> <p>Expected Output:</p> <p>\"Our product offers: free shipping, 30-day returns, 24/7 support, price matching, and warranty.\"</p> <p>AI Response:</p> <p>\"We provide free shipping on all orders and a 30-day return policy. Our support team is available around the clock.\"</p> <p>Analysis:</p> Aspect Covered Free shipping \u2705 30-day returns \u2705 24/7 support \u2705 Price matching \u274c Warranty \u274c <p>Final Score: <code>3 / 5 = 0.6</code> </p> \u274c Scenario 3: Poor Coverage (Score: 0.25) <p>Most Aspects Missing</p> <p>Expected Output:</p> <p>\"The recipe requires flour, sugar, eggs, and butter. Preheat oven to 350\u00b0F. Mix ingredients, pour into pan, bake 25 minutes.\"</p> <p>AI Response:</p> <p>\"You'll need flour and sugar.\"</p> <p>Analysis:</p> Aspect Covered Flour \u2705 Sugar \u2705 Eggs \u274c Butter \u274c Oven temperature \u274c Mixing instructions \u274c Baking time \u274c <p>Final Score: <code>2 / 7 = 0.29</code> </p>"},{"location":"metric-registry/composite/answer_completeness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcdd Content Coverage <p>Ensures AI responses include all important information, not just some of it.</p> \ud83c\udf93 Educational Quality <p>Critical for tutoring systems where incomplete answers leave knowledge gaps.</p> \ud83d\udccb Requirements Coverage <p>Verify that responses address all parts of complex queries.</p>"},{"location":"metric-registry/composite/answer_completeness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Completeness = Does the response cover all aspects from the expected answer?</p> <ul> <li>Use it when: You have reference answers and need comprehensive coverage</li> <li>Score interpretation: Higher = more aspects from expected output covered</li> <li>Key difference: Measures coverage, not accuracy</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerCompleteness</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Criteria \u00b7 Factual Accuracy \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/answer_criteria/","title":"Answer Criteria","text":"Evaluate responses against user-defined acceptance criteria LLM-Powered Knowledge Single Turn Multi-Turn"},{"location":"metric-registry/composite/answer_criteria/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Criteria coverage ratio \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>acceptance_criteria</code> <p>What It Measures</p> <p>Answer Criteria evaluates whether a response meets user-defined acceptance criteria. It decomposes criteria into aspects and concepts, then checks coverage. This is ideal for custom evaluation requirements that don't fit standard metrics.</p> Score Interpretation 1.0  All criteria aspects fully covered 0.7+  Most criteria met, minor gaps 0.5  Half the criteria covered &lt; 0.5  Significant criteria not met \u2705 Use When <ul> <li>Custom acceptance criteria exist</li> <li>Domain-specific requirements</li> <li>Multi-aspect evaluation needed</li> <li>Testing conversational agents</li> </ul> \u274c Don't Use When <ul> <li>Standard metrics suffice</li> <li>No clear acceptance criteria</li> <li>Purely factual evaluation</li> <li>Simple pass/fail needed</li> </ul> <p>See Also: Answer Completeness</p> <p>Answer Criteria evaluates against custom acceptance criteria. Answer Completeness evaluates against expected output aspects.</p> <p>Use Criteria for custom requirements; use Completeness when you have a reference answer.</p> How It Works  Computation Scoring Strategies <p>The metric decomposes acceptance criteria into aspects, identifies key concepts per aspect, then checks if the response covers them.</p> <p>Choose how to calculate the final score based on aspect and concept coverage.</p> <p> \ud83d\udcca CONCEPT Score = total_concepts_covered / total_conceptsDefault. Granular concept-level coverage. </p> <p> \ud83d\udccb ASPECT Score = covered_aspects / total_aspectsBinary per-aspect (all-or-nothing). </p> <p> \u2696\ufe0f WEIGHTED Score = 0.7 \u00d7 concept_score + 0.3 \u00d7 aspect_scoreBlend of both approaches. </p>"},{"location":"metric-registry/composite/answer_criteria/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Acceptance Criteria]\n    end\n\n    subgraph DECOMPOSE[\"\ud83d\udd0d Step 1: Criteria Decomposition\"]\n        D[Extract Aspects]\n        E[\"Aspects with Key Concepts\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Coverage Check\"]\n        F[Check Each Aspect]\n        G[\"Covered / Missing Concepts\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Apply Scoring Strategy\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style DECOMPOSE stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#configuration","title":"Configuration","text":"Parameters Scoring Strategies Parameter Type Default Description <code>criteria_key</code> <code>str</code> <code>'Complete'</code> Key to look up criteria <code>scoring_strategy</code> <code>'concept'</code> | <code>'aspect'</code> | <code>'weighted'</code> <code>'concept'</code> How to calculate score <code>check_for_contradictions</code> <code>bool</code> <code>False</code> Check if response contradicts criteria <code>weighted_concept_score_weight</code> <code>float</code> <code>0.7</code> Weight for concept score in weighted strategy <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>multi_turn_aggregation</code> <code>'cumulative'</code> | <code>'average'</code> <code>'cumulative'</code> How to aggregate multi-turn scores <pre><code>from axion.metrics import AnswerCriteria\n\n# Concept-level (default, most granular)\nmetric = AnswerCriteria(scoring_strategy='concept')\n\n# Aspect-level (binary per aspect)\nmetric = AnswerCriteria(scoring_strategy='aspect')\n\n# Weighted blend\nmetric = AnswerCriteria(\n    scoring_strategy='weighted',\n    weighted_concept_score_weight=0.7  # 70% concept, 30% aspect\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#code-examples","title":"Code Examples","text":"Basic Usage Custom Criteria Multi-Turn <pre><code>from axion.metrics import AnswerCriteria\nfrom axion.dataset import DatasetItem\n\nmetric = AnswerCriteria()\n\nitem = DatasetItem(\n    query=\"Explain how to make a good cup of coffee\",\n    actual_output=\"Use fresh beans, grind just before brewing, use water at 200\u00b0F, and brew for 4 minutes.\",\n    acceptance_criteria=\"Must mention: bean freshness, grind timing, water temperature, brew time\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import AnswerCriteria\n\n# Strict aspect-level scoring\nmetric = AnswerCriteria(\n    scoring_strategy='aspect',\n    check_for_contradictions=True\n)\n\nitem = DatasetItem(\n    query=\"What's your return policy?\",\n    actual_output=\"You can return items within 30 days with receipt.\",\n    acceptance_criteria=\"\"\"\n    Must cover:\n    1. Return window (30 days)\n    2. Receipt requirement\n    3. Condition of items\n    4. Refund method\n    \"\"\",\n)\n</code></pre> <pre><code>from axion.metrics import AnswerCriteria\n\nmetric = AnswerCriteria(\n    multi_turn_strategy='all_turns',\n    multi_turn_aggregation='cumulative'  # Criteria can be met across turns\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerCriteriaResult Structure <pre><code>AnswerCriteriaResult(\n{\n    \"scoring_strategy\": \"concept\",\n    \"covered_aspects_count\": 3,\n    \"total_aspects_count\": 4,\n    \"total_concepts_covered\": 5,\n    \"total_concepts\": 7,\n    \"concept_coverage_score\": 0.71,\n    \"aspect_breakdown\": [\n        {\n            \"aspect\": \"Bean freshness\",\n            \"covered\": true,\n            \"concepts_covered\": [\"fresh beans\", \"quality\"],\n            \"concepts_missing\": [],\n            \"reason\": \"Response mentions using fresh beans\"\n        },\n        {\n            \"aspect\": \"Water temperature\",\n            \"covered\": true,\n            \"concepts_covered\": [\"200\u00b0F\"],\n            \"concepts_missing\": [\"optimal range\"],\n            \"reason\": \"Specific temperature provided\"\n        }\n    ],\n    \"evaluated_turns_count\": 1\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_criteria/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>scoring_strategy</code> <code>str</code> Strategy used (concept/aspect/weighted) <code>covered_aspects_count</code> <code>int</code> Aspects fully covered <code>total_aspects_count</code> <code>int</code> Total aspects in criteria <code>total_concepts_covered</code> <code>int</code> Concepts found in response <code>total_concepts</code> <code>int</code> Total concepts across all aspects <code>concept_coverage_score</code> <code>float</code> Concept-level coverage ratio <code>aspect_breakdown</code> <code>List</code> Per-aspect coverage details"},{"location":"metric-registry/composite/answer_criteria/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Full Coverage (Score: 1.0) <p>All Criteria Met</p> <p>Criteria:</p> <p>\"Must mention: greeting, issue acknowledgment, solution, follow-up offer\"</p> <p>AI Response:</p> <p>\"Hello! I understand you're having trouble with your order. I've issued a full refund which will appear in 3-5 days. Is there anything else I can help with?\"</p> <p>Analysis:</p> Aspect Covered Concepts Greeting \u2705 \"Hello\" Issue acknowledgment \u2705 \"trouble with your order\" Solution \u2705 \"full refund\", \"3-5 days\" Follow-up offer \u2705 \"anything else I can help\" <p>Final Score: <code>4 / 4 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Coverage (Score: 0.5) <p>Some Criteria Missing</p> <p>Criteria:</p> <p>\"Must include: product name, price, availability, shipping info\"</p> <p>AI Response:</p> <p>\"The Widget Pro costs $49.99 and is currently in stock.\"</p> <p>Analysis:</p> Aspect Covered Concepts Product name \u2705 \"Widget Pro\" Price \u2705 \"$49.99\" Availability \u2705 \"in stock\" Shipping info \u274c missing <p>Final Score (aspect): <code>3 / 4 = 0.75</code></p> <p>No shipping information provided.</p> \u274c Scenario 3: Poor Coverage (Score: 0.25) <p>Most Criteria Not Met</p> <p>Criteria:</p> <p>\"Must cover: apology, explanation, compensation, prevention steps\"</p> <p>AI Response:</p> <p>\"We apologize for the inconvenience.\"</p> <p>Analysis:</p> Aspect Covered Concepts Apology \u2705 \"apologize\" Explanation \u274c missing Compensation \u274c missing Prevention steps \u274c missing <p>Final Score: <code>1 / 4 = 0.25</code> </p>"},{"location":"metric-registry/composite/answer_criteria/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Custom Requirements <p>Define exactly what a good response looks like for your specific use case.</p> \ud83d\udccb Policy Compliance <p>Ensure AI responses follow company guidelines, scripts, or regulatory requirements.</p> \ud83d\udcac Agent Quality <p>Evaluate customer service agents against expected response patterns.</p>"},{"location":"metric-registry/composite/answer_criteria/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Criteria = Does the response meet your custom acceptance criteria?</p> <ul> <li>Use it when: You have specific requirements beyond standard metrics</li> <li>Score interpretation: Higher = more criteria aspects covered</li> <li>Key config: Choose <code>scoring_strategy</code> based on granularity needs</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerCriteria</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Completeness \u00b7 Answer Relevancy \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/answer_relevancy/","title":"Answer Relevancy","text":"Evaluate how well an AI response addresses the input query LLM-Powered Knowledge Single Turn Multi-Turn"},{"location":"metric-registry/composite/answer_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant statements \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>conversation</code> <p>What It Measures</p> <p>Answer Relevancy evaluates whether each statement in the AI's response directly addresses the user's query. Unlike Faithfulness (which checks factual grounding), this metric measures topical alignment\u2014did the AI stay on topic or go off on tangents?</p> Score Interpretation 1.0  Every statement directly addresses the query 0.7+  Mostly relevant with minor tangents 0.5  Threshold\u2014mix of relevant and off-topic content &lt; 0.5  Significant off-topic or irrelevant content \u2705 Use When <ul> <li>Q&amp;A systems &amp; chatbots</li> <li>Customer support agents</li> <li>Search result evaluation</li> <li>Any query-response system</li> </ul> \u274c Don't Use When <ul> <li>Open-ended conversations</li> <li>Exploratory discussions</li> <li>No clear query/question</li> <li>Tasks where tangents are valuable</li> </ul> <p>See Also: Faithfulness</p> <p>Answer Relevancy checks if statements address the user's query (topical alignment). Faithfulness checks if claims are grounded in the source context (factual accuracy).</p> <p>Use both together for comprehensive RAG evaluation.</p> How It Works  Computation Verdict System <p>The metric uses an Evaluator LLM to decompose the response into atomic statements, then judge each statement's relevance to the query.</p> <p>Each extracted statement receives a verdict indicating its relevance to the query.</p> <p> \u2705 YES 1.0 Statement directly addresses the query. Clearly relevant. </p> <p> \u2753 IDK 1.0 Ambiguous relevance. Configurable\u2014can be 0.0 with <code>penalize_ambiguity=True</code> <p> \u274c NO 0.0 Statement is off-topic or doesn't address the query at all. </p> <p>Score Formula</p> <pre><code>score = (yes_count + idk_count*) / total_statements\n\n* idk_count included only if penalize_ambiguity=False (default)\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        C[StatementExtractor LLM]\n        D[\"Atomic Statements&lt;br/&gt;&lt;small&gt;Self-contained facts&lt;/small&gt;\"]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Step 2: Relevancy Judgment\"]\n        E[RelevancyJudge LLM]\n        F[\"Verdict per Statement&lt;br/&gt;&lt;small&gt;yes / no / idk&lt;/small&gt;\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Relevant\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    A --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#configuration","title":"Configuration","text":"Parameters Strict Configuration Multi-Turn Parameter Type Default Description <code>relevancy_mode</code> <code>'strict'</code> | <code>'task'</code> <code>'task'</code> strict: Only direct answers count. task: Helpful related info also counts <code>penalize_ambiguity</code> <code>bool</code> <code>False</code> When <code>True</code>, ambiguous (<code>idk</code>) verdicts score 0.0 instead of 1.0 <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Relevancy Modes</p> <ul> <li><code>task</code> mode (default): More lenient\u2014counts closely related, helpful information as relevant</li> <li><code>strict</code> mode: Only statements that directly answer the question count as relevant</li> </ul> <p>For high-precision evaluation where tangential information should be penalized:</p> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Strict evaluation: only direct answers, penalize ambiguity\nmetric = AnswerRelevancy(\n    relevancy_mode='strict',\n    penalize_ambiguity=True\n)\n</code></pre> <p>For conversational AI evaluation:</p> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Evaluate all turns in a conversation\nmetric = AnswerRelevancy(\n    multi_turn_strategy='all_turns'  # or 'last_turn' (default)\n)\n</code></pre> <ul> <li><code>last_turn</code>: Only evaluates the final Human\u2192AI exchange</li> <li><code>all_turns</code>: Evaluates every turn and aggregates via micro-averaging</li> </ul>"},{"location":"metric-registry/composite/answer_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Strict Mode Multi-Turn Conversation <pre><code>from axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem\n\n# Initialize with defaults (task mode, lenient)\nmetric = AnswerRelevancy()\n\nitem = DatasetItem(\n    query=\"What features does this laptop have?\",\n    actual_output=(\n        \"The laptop has a 15-inch Retina display and 16GB of RAM. \"\n        \"It also comes with a 1-year warranty. \"\n        \"Our company was founded in 2010.\"\n    ),\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score ~0.67: warranty is borderline, founding year is irrelevant\n</code></pre> <pre><code>from axion.metrics import AnswerRelevancy\n\n# Strict: only direct answers count\nmetric = AnswerRelevancy(\n    relevancy_mode='strict',\n    penalize_ambiguity=True\n)\n\n# Now only \"15-inch display\" and \"16GB RAM\" statements count\n# Warranty = ambiguous (0.0), founding year = no (0.0)\n</code></pre> <pre><code>from axion.metrics import AnswerRelevancy\nfrom axion.dataset import DatasetItem, MultiTurnConversation\nfrom axion.schema import HumanMessage, AIMessage\n\nconversation = MultiTurnConversation(messages=[\n    HumanMessage(content=\"What is Python?\"),\n    AIMessage(content=\"Python is a programming language known for readability.\"),\n    HumanMessage(content=\"What are its main uses?\"),\n    AIMessage(content=\"Python is used for web dev, data science, and automation.\"),\n])\n\nmetric = AnswerRelevancy(multi_turn_strategy='all_turns')\nitem = DatasetItem(conversation=conversation)\n\nresult = await metric.execute(item)\nprint(f\"Evaluated {result.signals.evaluated_turns_count} turns\")\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca AnswerRelevancyResult Structure <pre><code>AnswerRelevancyResult(\n{\n    \"overall_score\": 1.0,\n    \"explanation\": \"The score is 1.0 because the response fully and accurately explains...\",\n    \"relevant_statements_count\": 2,\n    \"irrelevant_statements_count\": 0,\n    \"ambiguous_statements_count\": 0,\n    \"total_statements_count\": 2,\n    \"statement_breakdown\": [\n        {\n            \"statement\": \"The infield fly rule prevents the defense from dropping a fly ball.\",\n            \"verdict\": \"yes\",\n            \"is_relevant\": true,\n            \"turn_index\": 0\n        },\n        {\n            \"statement\": \"The rule prevents an easy double play when runners are on base.\",\n            \"verdict\": \"yes\",\n            \"is_relevant\": true,\n            \"turn_index\": 0\n        }\n    ],\n    \"evaluated_turns_count\": 1\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/answer_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> The 0-1 relevancy score <code>explanation</code> <code>str</code> Human-readable summary of why the score was given <code>relevant_statements_count</code> <code>int</code> Count of <code>yes</code> verdicts <code>irrelevant_statements_count</code> <code>int</code> Count of <code>no</code> verdicts <code>ambiguous_statements_count</code> <code>int</code> Count of <code>idk</code> verdicts <code>total_statements_count</code> <code>int</code> Total statements extracted <code>statement_breakdown</code> <code>List</code> Per-statement verdict details <code>evaluated_turns_count</code> <code>int</code> Number of conversation turns evaluated"},{"location":"metric-registry/composite/answer_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Relevancy (Score: 1.0) <p>All Statements Relevant</p> <p>Query:</p> <p>\"What are the health benefits of green tea?\"</p> <p>AI Response:</p> <p>\"Green tea contains antioxidants that may reduce inflammation. It also has caffeine which can improve alertness.\"</p> <p>Analysis:</p> Statement Verdict Score Green tea contains antioxidants that may reduce inflammation yes 1.0 Green tea has caffeine which can improve alertness yes 1.0 <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Relevancy (Score: 0.67) <p>Mixed Verdicts</p> <p>Query:</p> <p>\"What features does this laptop have?\"</p> <p>AI Response:</p> <p>\"The laptop has a 15-inch display. It has 16GB RAM. Our company has excellent customer service.\"</p> <p>Analysis:</p> Statement Verdict Score The laptop has a 15-inch display yes 1.0 The laptop has 16GB RAM yes 1.0 Our company has excellent customer service no 0.0 <p>Final Score: <code>2 / 3 = 0.67</code> </p> <p>The customer service statement doesn't address laptop features.</p> \u274c Scenario 3: Poor Relevancy (Score: 0.25) <p>Mostly Off-Topic</p> <p>Query:</p> <p>\"How do I reset my password?\"</p> <p>AI Response:</p> <p>\"Our platform uses industry-standard encryption. We were founded in 2015. Password resets can be done via email. We have offices in 3 countries.\"</p> <p>Analysis:</p> Statement Verdict Score Our platform uses industry-standard encryption no 0.0 We were founded in 2015 no 0.0 Password resets can be done via email yes 1.0 We have offices in 3 countries no 0.0 <p>Final Score: <code>1 / 4 = 0.25</code> </p> <p>Only one statement actually answers the question.</p>"},{"location":"metric-registry/composite/answer_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf User Experience <p>Users expect direct answers. Off-topic responses frustrate users and reduce trust in your AI system.</p> \ud83d\udcac Conversation Quality <p>For chatbots and assistants, staying on topic is crucial. Tangential responses break conversational flow.</p> \ud83d\udd0d Debug Generation <p>Identifies when your model goes off-topic\u2014separate from retrieval issues (Faithfulness) or factual errors.</p>"},{"location":"metric-registry/composite/answer_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Answer Relevancy = Does the AI's response actually address what the user asked?</p> <ul> <li>Use it when: You need to ensure responses stay on topic</li> <li>Score interpretation: Higher = more statements directly address the query</li> <li>Key config: Use <code>relevancy_mode='strict'</code> for precision, <code>'task'</code> for lenient evaluation</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.AnswerRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Completeness \u00b7 Context Precision</p> </li> </ul>"},{"location":"metric-registry/composite/citation_relevancy/","title":"Citation Relevancy","text":"Measure the quality and relevance of citations in AI responses LLM-Powered Knowledge Multi-Turn Citation"},{"location":"metric-registry/composite/citation_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant citations \u26a1 Default Threshold <code>0.8</code> High bar for citation quality \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>conversation</code> <p>What It Measures</p> <p>Citation Relevancy evaluates whether the citations included in an AI response are actually relevant to the user's query. It extracts citations using pattern matching, then judges each citation's relevance using an LLM. Essential for research assistants and fact-checking systems.</p> Score Interpretation 1.0  All citations directly relevant to query 0.8+  Most citations relevant, minor tangents 0.5  Mixed relevance\u2014some helpful, some off-topic &lt; 0.5  Mostly irrelevant or unrelated citations \u2705 Use When <ul> <li>Building research assistants</li> <li>Fact-checking systems</li> <li>Academic writing tools</li> <li>Any system that generates citations</li> </ul> \u274c Don't Use When <ul> <li>Responses don't include citations</li> <li>Citation format is non-standard</li> <li>Internal linking (not external sources)</li> <li>Pure conversational AI</li> </ul> <p>See Also: Faithfulness</p> <p>Citation Relevancy checks if cited sources are relevant to the query. Faithfulness checks if claims are grounded in retrieved context.</p> <p>Use Citation Relevancy for output validation; use Faithfulness for RAG grounding.</p> How It Works  Computation Supported Citation Formats Verdict System <p>The metric uses regex-based extraction followed by LLM-based relevance judgment.</p> <p>The metric extracts citations using multiple regex patterns:</p> <p> \ud83d\udcdd Markdown Links <code>Title</code> </p> <p> \ud83d\udd17 HTTP/HTTPS URLs <code>https://example.com/article</code> </p> <p> \ud83c\udf10 WWW URLs <code>www.example.com/page</code> </p> <p> \ud83d\udcda DOI Patterns <code>doi:10.1234/example</code> </p> <p> \ud83c\udf93 Academic Format <code>(Smith et al., 2023)</code> or <code>(Smith, 2023)</code> </p> <p>Each citation receives a binary relevance verdict.</p> <p> \u2705 RELEVANT 1 Citation directly supports answering the user's query. </p> <p> \u274c IRRELEVANT 0 Citation is off-topic or doesn't help answer the question. </p> <p>Score Formula</p> <pre><code>score = relevant_citations / total_citations\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response with Citations]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Citation Extraction\"]\n        C[Regex Pattern Matching]\n        D[\"Extracted Citations\"]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Step 2: Relevance Judgment\"]\n        E[CitationRelevanceJudge LLM]\n        F[\"Verdict per Citation\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Relevant\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    A --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>multi_turn_strategy</code> <code>'last_turn'</code> | <code>'all_turns'</code> <code>'last_turn'</code> How to evaluate conversations <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Multi-Turn Support</p> <p>In multi-turn conversations, citations are associated with their corresponding query context:</p> <ul> <li><code>last_turn</code>: Only evaluates citations in the final response</li> <li><code>all_turns</code>: Evaluates citations across all turns, matching each to its original query</li> </ul>"},{"location":"metric-registry/composite/citation_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Multi-Turn With Runner <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.dataset import DatasetItem\n\nmetric = CitationRelevancy()\n\nitem = DatasetItem(\n    query=\"What are the health benefits of green tea?\",\n    actual_output=\"\"\"\n    Green tea has numerous health benefits:\n\n    1. Rich in antioxidants [Source](https://healthline.com/green-tea-benefits)\n    2. May improve brain function (Smith et al., 2020)\n    3. Great for parties! [Party Guide](https://party-planning.com)\n    \"\"\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.67 (2 of 3 citations relevant)\n</code></pre> <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.dataset import DatasetItem, MultiTurnConversation\nfrom axion.schema import HumanMessage, AIMessage\n\nconversation = MultiTurnConversation(messages=[\n    HumanMessage(content=\"What causes climate change?\"),\n    AIMessage(content=\"Climate change is primarily caused by greenhouse gases. [IPCC Report](https://ipcc.ch/report)\"),\n    HumanMessage(content=\"How can I reduce my carbon footprint?\"),\n    AIMessage(content=\"You can reduce emissions by using public transport. [EPA Guide](https://epa.gov/guide)\"),\n])\n\nmetric = CitationRelevancy(multi_turn_strategy='all_turns')\nitem = DatasetItem(conversation=conversation)\n\nresult = await metric.execute(item)\nprint(f\"Evaluated {result.signals.total_citations} citations across turns\")\n</code></pre> <pre><code>from axion.metrics import CitationRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = CitationRelevancy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_citations_count}/{item_result.signals.total_citations}\")\n    for citation in item_result.signals.citation_breakdown:\n        status = \"\u2705\" if citation.relevance_verdict else \"\u274c\"\n        print(f\"  {status} {citation.citation_text[:50]}...\")\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationRelevancyResult Structure <pre><code>CitationRelevancyResult(\n{\n    \"relevance_score\": 0.67,\n    \"total_citations\": 3,\n    \"relevant_citations_count\": 2,\n    \"irrelevant_citations_count\": 1,\n    \"citation_breakdown\": [\n        {\n            \"citation_text\": \"[Source](https://healthline.com/green-tea-benefits)\",\n            \"relevance_verdict\": true,\n            \"relevance_reason\": \"Directly addresses health benefits of green tea\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        },\n        {\n            \"citation_text\": \"(Smith et al., 2020)\",\n            \"relevance_verdict\": true,\n            \"relevance_reason\": \"Academic source on tea and brain function\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        },\n        {\n            \"citation_text\": \"[Party Guide](https://party-planning.com)\",\n            \"relevance_verdict\": false,\n            \"relevance_reason\": \"Party planning is unrelated to health benefits\",\n            \"turn_index\": 0,\n            \"original_query\": \"What are the health benefits of green tea?\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/citation_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>relevance_score</code> <code>float</code> Ratio of relevant citations (0.0-1.0) <code>total_citations</code> <code>int</code> Total citations extracted <code>relevant_citations_count</code> <code>int</code> Count of relevant citations <code>irrelevant_citations_count</code> <code>int</code> Count of irrelevant citations <code>citation_breakdown</code> <code>List</code> Per-citation verdict details"},{"location":"metric-registry/composite/citation_relevancy/#citation-breakdown-fields","title":"Citation Breakdown Fields","text":"Field Type Description <code>citation_text</code> <code>str</code> The extracted citation <code>relevance_verdict</code> <code>bool</code> Whether citation is relevant <code>relevance_reason</code> <code>str</code> Explanation for verdict <code>turn_index</code> <code>int</code> Conversation turn (for multi-turn) <code>original_query</code> <code>str</code> Query that prompted this citation"},{"location":"metric-registry/composite/citation_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: All Citations Relevant (Score: 1.0) <p>High-Quality Citations</p> <p>Query:</p> <p>\"What are effective treatments for type 2 diabetes?\"</p> <p>AI Response:</p> <p>\"Type 2 diabetes can be managed through:</p> <ol> <li>Lifestyle changes including diet and exercise ADA Guidelines</li> <li>Metformin as first-line medication (Johnson et al., 2021)</li> <li>Regular blood glucose monitoring CDC Recommendations\"</li> </ol> <p>Analysis:</p> Citation Verdict Reason ADA Guidelines \u2705 Authoritative diabetes organization Johnson et al., 2021 \u2705 Academic source on medication CDC Recommendations \u2705 Government health authority <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Relevance (Score: 0.5) <p>Some Citations Off-Topic</p> <p>Query:</p> <p>\"How does machine learning work?\"</p> <p>AI Response:</p> <p>\"Machine learning uses algorithms to learn from data:</p> <ul> <li>Neural networks mimic brain structure ML Textbook</li> <li>Training requires large datasets Data Science Blog</li> <li>My favorite coffee shop uses ML! Best Coffee</li> <li>Check out this unrelated video Cat Video\"</li> </ul> <p>Analysis:</p> Citation Verdict Reason ML Textbook \u2705 Directly about machine learning Data Science Blog \u2705 Relevant to ML data requirements Best Coffee \u274c Coffee reviews unrelated to ML Cat Video \u274c Entertainment, not educational <p>Final Score: <code>2 / 4 = 0.5</code> </p> \u274c Scenario 3: Mostly Irrelevant (Score: 0.25) <p>Citation Spam</p> <p>Query:</p> <p>\"What is the capital of France?\"</p> <p>AI Response:</p> <p>\"Paris is the capital of France. Here are some links:</p> <ul> <li>My Portfolio</li> <li>Buy Cheap Flights</li> <li>Wikipedia - France</li> <li>Dating Site\"</li> </ul> <p>Analysis:</p> Citation Verdict Reason My Portfolio \u274c Self-promotion, irrelevant Buy Cheap Flights \u274c Commercial, off-topic Wikipedia - France \u2705 Relevant geographic source Dating Site \u274c Completely unrelated <p>Final Score: <code>1 / 4 = 0.25</code> </p>"},{"location":"metric-registry/composite/citation_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Source Quality <p>Ensures AI-generated citations actually support the response, not random links or self-promotion.</p> \ud83c\udf93 Research Integrity <p>Critical for academic and research tools where citations must be relevant and authoritative.</p> \u2705 User Trust <p>Users expect citations to be helpful. Irrelevant citations damage credibility and waste time.</p>"},{"location":"metric-registry/composite/citation_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Relevancy = Are the citations actually relevant to the user's question?</p> <ul> <li>Use it when: AI responses include citations that need quality validation</li> <li>Score interpretation: Higher = more citations are relevant</li> <li>Key feature: Supports multiple citation formats (URLs, DOIs, academic)</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.CitationRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Relevancy \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_precision/","title":"Contextual Precision","text":"Evaluate if useful context chunks are ranked higher LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_precision/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Mean Average Precision \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>expected_output</code> <code>retrieved_content</code> Ground truth + context <p>What It Measures</p> <p>Contextual Precision evaluates whether useful chunks are ranked higher in the retrieval results. Using Mean Average Precision (MAP), it rewards retrieval systems that place the most helpful documents at the top. A useful chunk is one that contributes to generating the expected answer.</p> Score Interpretation 1.0  All useful chunks at the top 0.7+  Good ranking, useful chunks near top 0.5  Mixed ranking quality &lt; 0.5  Useful chunks buried low in results \u2705 Use When <ul> <li>Evaluating retrieval ranking quality</li> <li>Tuning re-ranking algorithms</li> <li>Testing with limited context windows</li> <li>Optimizing for top-k retrieval</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Chunk order doesn't matter</li> <li>Using all retrieved chunks equally</li> <li>Single-chunk retrieval</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Precision asks: \"Are the most useful chunks ranked first?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant to the query?</li> <li>Contextual Recall: Do chunks cover the expected answer?</li> <li>Contextual Ranking: Are relevant chunks ranked higher?</li> </ul> How It Works  Computation MAP Calculation <p>The metric evaluates chunk usefulness for generating the expected answer, then calculates MAP.</p> <p>Mean Average Precision rewards useful chunks appearing early in the ranking.</p> <p>Example with 5 chunks (U = useful, X = not useful):</p> <pre><code>Position:  1    2    3    4    5\nChunks:   [U]  [X]  [U]  [X]  [U]\n\nPrecision@1 = 1/1 = 1.0   (first useful at position 1)\nPrecision@3 = 2/3 = 0.67  (second useful at position 3)\nPrecision@5 = 3/5 = 0.6   (third useful at position 5)\n\nMAP = (1.0 + 0.67 + 0.6) / 3 = 0.76\n</code></pre> <p> \u2705 USEFUL +P@k Chunk helps generate the expected answer. Contributes to MAP score. </p> <p> \u274c NOT USEFUL 0 Chunk doesn't contribute to the answer. Dilutes precision. </p> <p>Score Formula</p> <pre><code>MAP = sum(Precision@k for each useful chunk) / total_useful_chunks\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Expected Output]\n        C[Retrieved Chunks in Order]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Usefulness Check\"]\n        D[RAGAnalyzer Engine]\n        E1[\"Chunk 1: U/\u2717\"]\n        E2[\"Chunk 2: U/\u2717\"]\n        E3[\"Chunk 3: U/\u2717\"]\n        EN[\"...\"]\n    end\n\n    subgraph MAP[\"\ud83d\udcca Step 2: Calculate MAP\"]\n        F[\"For each useful chunk at position k\"]\n        G[\"Precision@k = useful_seen / k\"]\n        H[\"Average all Precision@k values\"]\n        I[\"Final MAP Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E1 &amp; E2 &amp; E3 &amp; EN\n    E1 &amp; E2 &amp; E3 &amp; EN --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style MAP stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Usefulness vs Relevancy</p> <ul> <li>Relevancy (Contextual Relevancy): \"Is this chunk about the topic?\"</li> <li>Usefulness (Contextual Precision): \"Does this chunk help generate the correct answer?\"</li> </ul> <p>A chunk can be relevant but not useful (e.g., background info that isn't needed).</p>"},{"location":"metric-registry/composite/contextual_precision/#code-examples","title":"Code Examples","text":"Basic Usage Perfect vs Poor Ranking With Runner <pre><code>from axion.metrics import ContextualPrecision\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualPrecision()\n\nitem = DatasetItem(\n    query=\"Who invented the telephone?\",\n    expected_output=\"Alexander Graham Bell invented the telephone in 1876.\",\n    retrieved_content=[\n        \"Alexander Graham Bell invented the telephone.\",  # Useful\n        \"The telephone revolutionized communication.\",    # Not useful\n        \"Bell patented it in 1876.\",                      # Useful\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# MAP: (1/1 + 2/3) / 2 = 0.83\n</code></pre> <pre><code>from axion.metrics import ContextualPrecision\n\nmetric = ContextualPrecision()\n\n# Perfect ranking: useful chunks first\nperfect_order = DatasetItem(\n    query=\"What is Python?\",\n    expected_output=\"Python is a programming language created by Guido van Rossum.\",\n    retrieved_content=[\n        \"Python is a high-level programming language.\",   # Useful (pos 1)\n        \"Guido van Rossum created Python.\",               # Useful (pos 2)\n        \"Programming is fun.\",                            # Not useful\n    ],\n)\n# MAP = (1/1 + 2/2) / 2 = 1.0\n\n# Poor ranking: useful chunks last\npoor_order = DatasetItem(\n    query=\"What is Python?\",\n    expected_output=\"Python is a programming language created by Guido van Rossum.\",\n    retrieved_content=[\n        \"Programming is fun.\",                            # Not useful\n        \"Python is a high-level programming language.\",   # Useful (pos 2)\n        \"Guido van Rossum created Python.\",               # Useful (pos 3)\n    ],\n)\n# MAP = (1/2 + 2/3) / 2 = 0.58\n</code></pre> <pre><code>from axion.metrics import ContextualPrecision\nfrom axion.runners import MetricRunner\n\nmetric = ContextualPrecision()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"MAP Score: {item_result.score}\")\n    print(f\"Useful chunks: {item_result.signals.useful_chunks}/{item_result.signals.total_chunks}\")\n    print(f\"First useful at position: {item_result.signals.first_useful_position}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_useful else \"\u274c\"\n        print(f\"  {i+1}. {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualPrecisionResult Structure <pre><code>ContextualPrecisionResult(\n{\n    \"map_score\": 0.83,\n    \"total_chunks\": 3,\n    \"useful_chunks\": 2,\n    \"first_useful_position\": 1,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Alexander Graham Bell invented the telephone.\",\n            \"is_useful\": true,\n            \"position\": 1\n        },\n        {\n            \"chunk_text\": \"The telephone revolutionized communication.\",\n            \"is_useful\": false,\n            \"position\": 2\n        },\n        {\n            \"chunk_text\": \"Bell patented it in 1876.\",\n            \"is_useful\": true,\n            \"position\": 3\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_precision/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>map_score</code> <code>float</code> Mean Average Precision (0.0-1.0) <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>useful_chunks</code> <code>int</code> Chunks useful for generating answer <code>first_useful_position</code> <code>int</code> Rank of first useful chunk <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_precision/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_useful</code> <code>bool</code> Whether chunk helps generate expected answer <code>position</code> <code>int</code> Rank position in retrieval results"},{"location":"metric-registry/composite/contextual_precision/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Precision (Score: 1.0) <p>Useful Chunks Ranked First</p> <p>Query:</p> <p>\"What are the three states of matter?\"</p> <p>Expected Output:</p> <p>\"The three states of matter are solid, liquid, and gas.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Matter exists in three states: solid, liquid, and gas.\" \u2705</li> <li>\"Solids have fixed shape, liquids take container shape.\" \u2705</li> <li>\"Gases expand to fill available space.\" \u2705</li> <li>\"Matter is anything that has mass.\" \u274c</li> <li>\"Chemistry is the study of matter.\" \u274c</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 1, 2, 3\nP@1 = 1/1 = 1.0\nP@2 = 2/2 = 1.0\nP@3 = 3/3 = 1.0\nMAP = (1.0 + 1.0 + 1.0) / 3 = 1.0\n</code></pre> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Precision (Score: 0.58) <p>Useful Chunks Buried</p> <p>Query:</p> <p>\"Who wrote Romeo and Juliet?\"</p> <p>Expected Output:</p> <p>\"William Shakespeare wrote Romeo and Juliet.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Shakespeare was born in Stratford-upon-Avon.\" \u274c</li> <li>\"Romeo and Juliet is a famous tragedy.\" \u274c</li> <li>\"William Shakespeare wrote Romeo and Juliet.\" \u2705</li> <li>\"The play was written in the 1590s.\" \u2705</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 3, 4\nP@3 = 1/3 = 0.33\nP@4 = 2/4 = 0.50\nMAP = (0.33 + 0.50) / 2 = 0.42\n</code></pre> <p>Final Score: <code>0.42</code> </p> <p>Key information buried at positions 3-4 instead of 1-2.</p> \u274c Scenario 3: Poor Precision (Score: 0.2) <p>Useful Chunk at Bottom</p> <p>Query:</p> <p>\"What is the speed of light?\"</p> <p>Expected Output:</p> <p>\"The speed of light is approximately 299,792 km/s.\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Light is a form of electromagnetic radiation.\" \u274c</li> <li>\"Light travels in waves.\" \u274c</li> <li>\"Light can be refracted through prisms.\" \u274c</li> <li>\"Light behaves as both particles and waves.\" \u274c</li> <li>\"The speed of light is about 300,000 km/s in vacuum.\" \u2705</li> </ol> <p>MAP Calculation:</p> <pre><code>Useful at positions: 5\nP@5 = 1/5 = 0.2\nMAP = 0.2 / 1 = 0.2\n</code></pre> <p>Final Score: <code>0.2</code> </p> <p>The only useful chunk is at the very bottom.</p>"},{"location":"metric-registry/composite/contextual_precision/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcca Ranking Quality <p>Measures not just what you retrieve, but how well you rank it. Critical for top-k systems.</p> \u26a1 Efficiency <p>When context windows are limited, having useful chunks first means better answers faster.</p> \ud83d\udd27 Re-ranking Tuning <p>Directly measures re-ranking model performance. Low MAP = improve your re-ranker.</p>"},{"location":"metric-registry/composite/contextual_precision/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Precision = Are the most useful chunks ranked at the top?</p> <ul> <li>Use it when: Evaluating retrieval ranking, especially with limited context windows</li> <li>Score interpretation: Higher MAP = useful chunks appear earlier</li> <li>Key formula: Mean Average Precision over useful chunk positions</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualPrecision</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Ranking \u00b7 Contextual Recall \u00b7 Contextual Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_ranking/","title":"Contextual Ranking","text":"Evaluate if relevant context chunks are ranked higher LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_ranking/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Precision-weighted ranking score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No expected_output needed <p>What It Measures</p> <p>Contextual Ranking evaluates whether relevant chunks are positioned higher in retrieval results. Unlike Contextual Precision (which checks usefulness for generating an answer), Ranking simply checks query relevance\u2014making it usable without ground truth.</p> Score Interpretation \u2265 0.9  Excellent ranking\u2014relevant chunks at top \u2265 0.7  Good ranking quality 0.5  Mediocre\u2014relevant chunks scattered &lt; 0.5  Poor ranking\u2014relevant chunks buried \u2705 Use When <ul> <li>No expected_output available</li> <li>Evaluating retrieval ranking</li> <li>Comparing re-ranking algorithms</li> <li>Testing search relevance</li> </ul> \u274c Don't Use When <ul> <li>You have expected_output (use Precision)</li> <li>Chunk order doesn't matter</li> <li>Single-chunk retrieval</li> <li>Evaluating answer quality</li> </ul> <p>Ranking vs Precision</p> <p>Contextual Ranking checks: \"Are relevant chunks ranked higher?\" (based on query) Contextual Precision checks: \"Are useful chunks ranked higher?\" (based on expected answer)</p> <p>Use Ranking when you don't have ground truth; use Precision when you do.</p> How It Works  Computation Ranking Calculation <p>The metric evaluates chunk relevance to the query, then calculates a precision-weighted ranking score.</p> <p>The score heavily penalizes relevant chunks ranked low.</p> <p>Example with 5 chunks (R = relevant, X = not relevant):</p> <pre><code>Position:  1    2    3    4    5\nChunks:   [R]  [X]  [R]  [X]  [R]\n\nPrecision@1 = 1/1 = 1.0   (first relevant at position 1)\nPrecision@3 = 2/3 = 0.67  (second relevant at position 3)\nPrecision@5 = 3/5 = 0.6   (third relevant at position 5)\n\nScore = (1.0 + 0.67 + 0.6) / 3 = 0.76\n</code></pre> <p> \u2705 RELEVANT +P@k Chunk is relevant to the query. Contributes to ranking score. </p> <p> \u274c NOT RELEVANT 0 Chunk is off-topic. Dilutes precision at each position. </p> <p>Score Formula</p> <pre><code>score = sum(Precision@k for each relevant chunk) / total_relevant_chunks\nscore = clamp(score, 0.0, 1.0)\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks in Order]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Relevancy Check\"]\n        C[RAGAnalyzer Engine]\n        D1[\"Chunk 1: R/\u2717\"]\n        D2[\"Chunk 2: R/\u2717\"]\n        D3[\"Chunk 3: R/\u2717\"]\n        DN[\"...\"]\n    end\n\n    subgraph RANK[\"\ud83d\udcca Step 2: Calculate Ranking Score\"]\n        E[\"For each relevant chunk at position k\"]\n        F[\"Precision@k = relevant_seen / k\"]\n        G[\"Sum all Precision@k values\"]\n        H[\"Divide by total relevant chunks\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style RANK stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Interpretation Guide</p> Score Range Quality Recommendation \u2265 0.9 Excellent Ranking is optimal \u2265 0.7 Good Acceptable for most use cases &lt; 0.7 Poor Consider improving re-ranking 0.0 None No relevant chunks found"},{"location":"metric-registry/composite/contextual_ranking/#code-examples","title":"Code Examples","text":"Basic Usage Compare Rankings With Runner <pre><code>from axion.metrics import ContextualRanking\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRanking()\n\nitem = DatasetItem(\n    query=\"What is machine learning?\",\n    retrieved_content=[\n        \"Machine learning is a subset of AI.\",       # Relevant\n        \"Python is a programming language.\",         # Not relevant\n        \"ML models learn from data.\",                # Relevant\n        \"The weather is nice today.\",                # Not relevant\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: (1/1 + 2/3) / 2 = 0.83\n</code></pre> <pre><code>from axion.metrics import ContextualRanking\n\nmetric = ContextualRanking()\n\n# Good ranking: relevant first\ngood_order = DatasetItem(\n    query=\"Benefits of exercise\",\n    retrieved_content=[\n        \"Exercise improves cardiovascular health.\",   # Relevant\n        \"Regular workouts boost energy levels.\",      # Relevant\n        \"Cooking is a useful skill.\",                 # Not relevant\n    ],\n)\n# Score: (1/1 + 2/2) / 2 = 1.0\n\n# Bad ranking: relevant last\nbad_order = DatasetItem(\n    query=\"Benefits of exercise\",\n    retrieved_content=[\n        \"Cooking is a useful skill.\",                 # Not relevant\n        \"Exercise improves cardiovascular health.\",   # Relevant\n        \"Regular workouts boost energy levels.\",      # Relevant\n    ],\n)\n# Score: (1/2 + 2/3) / 2 = 0.58\n</code></pre> <pre><code>from axion.metrics import ContextualRanking\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRanking()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Ranking Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_chunks}/{item_result.signals.total_chunks}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_relevant else \"\u274c\"\n        print(f\"  {i+1}. {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRankingResult Structure <pre><code>ContextualRankingResult(\n{\n    \"final_score\": 0.83,\n    \"relevant_chunks\": 2,\n    \"total_chunks\": 4,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Machine learning is a subset of AI.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"Python is a programming language.\",\n            \"is_relevant\": false\n        },\n        {\n            \"chunk_text\": \"ML models learn from data.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"The weather is nice today.\",\n            \"is_relevant\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_ranking/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Ranking score (0.0-1.0) <code>relevant_chunks</code> <code>int</code> Number of relevant chunks <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_ranking/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_relevant</code> <code>bool</code> Whether chunk is relevant to query"},{"location":"metric-registry/composite/contextual_ranking/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Excellent Ranking (Score: 1.0) <p>All Relevant Chunks First</p> <p>Query:</p> <p>\"How does photosynthesis work?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Photosynthesis converts light energy into chemical energy.\" \u2705</li> <li>\"Plants use chlorophyll to absorb sunlight.\" \u2705</li> <li>\"Photosynthesis produces glucose and oxygen.\" \u2705</li> <li>\"The ocean covers 71% of Earth.\" \u274c</li> <li>\"Volcanic eruptions release gases.\" \u274c</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 1, 2, 3\nP@1 = 1/1 = 1.0\nP@2 = 2/2 = 1.0\nP@3 = 3/3 = 1.0\nScore = (1.0 + 1.0 + 1.0) / 3 = 1.0\n</code></pre> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Mediocre Ranking (Score: 0.5) <p>Relevant Chunks Scattered</p> <p>Query:</p> <p>\"What are the benefits of meditation?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Yoga is an ancient practice.\" \u274c</li> <li>\"Meditation reduces stress and anxiety.\" \u2705</li> <li>\"Cooking can be therapeutic.\" \u274c</li> <li>\"Mindfulness improves focus.\" \u2705</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 2, 4\nP@2 = 1/2 = 0.5\nP@4 = 2/4 = 0.5\nScore = (0.5 + 0.5) / 2 = 0.5\n</code></pre> <p>Final Score: <code>0.5</code> </p> <p>Relevant content not prioritized at top positions.</p> \u274c Scenario 3: Poor Ranking (Score: 0.33) <p>Relevant Chunks at Bottom</p> <p>Query:</p> <p>\"What is the capital of Japan?\"</p> <p>Retrieved Context (in order):</p> <ol> <li>\"Japan has a population of 125 million.\" \u274c</li> <li>\"Japanese cuisine includes sushi.\" \u274c</li> <li>\"Tokyo is the capital of Japan.\" \u2705</li> </ol> <p>Ranking Calculation:</p> <pre><code>Relevant at positions: 3\nP@3 = 1/3 = 0.33\nScore = 0.33 / 1 = 0.33\n</code></pre> <p>Final Score: <code>0.33</code> </p> <p>The only relevant chunk is last\u2014poor ranking quality.</p>"},{"location":"metric-registry/composite/contextual_ranking/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf No Ground Truth Needed <p>Evaluate ranking quality without expected answers\u2014ideal for production monitoring.</p> \ud83d\udcca Re-ranker Evaluation <p>Directly measures whether your re-ranking model improves result ordering.</p> \u26a1 Context Window Efficiency <p>When using top-k results, good ranking ensures the best content is included.</p>"},{"location":"metric-registry/composite/contextual_ranking/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Ranking = Are relevant chunks positioned at the top of results?</p> <ul> <li>Use it when: Evaluating retrieval ranking without ground truth</li> <li>Score interpretation: Higher = relevant chunks appear earlier</li> <li>Key difference: Uses query relevance, not answer usefulness</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRanking</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Relevancy \u00b7 Faithfulness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_recall/","title":"Contextual Recall","text":"Measure if retrieved context supports the expected answer LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_recall/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of supported statements \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>expected_output</code> <code>retrieved_content</code> Ground truth required <p>What It Measures</p> <p>Contextual Recall evaluates whether the retrieved context contains sufficient information to support the expected answer. It extracts statements from the ground truth and checks if each is supported by the retrieved chunks. High recall means the retrieval didn't miss important information.</p> Score Interpretation 1.0  All expected facts are in retrieved context 0.7+  Most expected facts supported, minor gaps 0.5  Half the expected facts missing from context &lt; 0.5  Significant information not retrieved \u2705 Use When <ul> <li>You have ground truth answers</li> <li>Evaluating retrieval completeness</li> <li>Testing if critical info is retrieved</li> <li>Debugging \"information not found\" errors</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Multiple valid answers exist</li> <li>Testing retrieval ranking (use Precision)</li> <li>Evaluating generation quality</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Recall asks: \"Does the context contain everything needed to answer correctly?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant to the query?</li> <li>Contextual Precision: Are useful chunks ranked higher?</li> <li>Contextual Sufficiency: Is there enough info overall?</li> </ul> How It Works  Computation Verdict System <p>The metric extracts factual statements from the expected answer and checks context support.</p> <p>Each ground truth statement receives a support verdict.</p> <p> \u2705 SUPPORTED 1 Statement from expected answer is found in retrieved context. </p> <p> \u274c NOT SUPPORTED 0 Statement from expected answer is missing from retrieved context. </p> <p>Score Formula</p> <pre><code>score = supported_statements / total_statements\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Expected Output]\n        B[Retrieved Context]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        C[Extract Factual Statements]\n        D[\"Ground Truth Statements\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Support Check\"]\n        E[Check Against Context]\n        F1[\"Stmt 1: \u2713/\u2717\"]\n        F2[\"Stmt 2: \u2713/\u2717\"]\n        F3[\"Stmt 3: \u2713/\u2717\"]\n        FN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Supported\"]\n        H[\"Calculate Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A --&gt; C\n    C --&gt; D\n    D --&gt; E\n    B --&gt; E\n    E --&gt; F1 &amp; F2 &amp; F3 &amp; FN\n    F1 &amp; F2 &amp; F3 &amp; FN --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Ground Truth Focus</p> <p>Unlike Contextual Relevancy (which asks \"is this chunk relevant?\"), Recall asks \"is this expected fact present?\" It measures retrieval from the answer's perspective.</p>"},{"location":"metric-registry/composite/contextual_recall/#code-examples","title":"Code Examples","text":"Basic Usage Complete Recall With Runner <pre><code>from axion.metrics import ContextualRecall\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRecall()\n\nitem = DatasetItem(\n    expected_output=\"Paris is the capital of France. It has a population of about 2 million.\",\n    retrieved_content=[\n        \"Paris is the capital and largest city of France.\",\n        \"The Eiffel Tower is located in Paris.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (capital fact supported, population fact missing)\n</code></pre> <pre><code>from axion.metrics import ContextualRecall\n\nmetric = ContextualRecall()\n\nitem = DatasetItem(\n    expected_output=\"Python was created by Guido van Rossum in 1991.\",\n    retrieved_content=[\n        \"Python is a programming language created by Guido van Rossum.\",\n        \"Python was first released in 1991.\",\n        \"Python emphasizes code readability.\",\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (both creator and year facts are in context)\n</code></pre> <pre><code>from axion.metrics import ContextualRecall\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRecall()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Supported: {item_result.signals.supported_gt_statements}/{item_result.signals.total_gt_statements}\")\n    for stmt in item_result.signals.statement_breakdown:\n        status = \"\u2705\" if stmt.is_supported else \"\u274c\"\n        print(f\"  {status} {stmt.statement_text}\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRecallResult Structure <pre><code>ContextualRecallResult(\n{\n    \"recall_score\": 0.5,\n    \"total_gt_statements\": 2,\n    \"supported_gt_statements\": 1,\n    \"statement_breakdown\": [\n        {\n            \"statement_text\": \"Paris is the capital of France\",\n            \"is_supported\": true\n        },\n        {\n            \"statement_text\": \"Paris has a population of about 2 million\",\n            \"is_supported\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_recall/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>recall_score</code> <code>float</code> Ratio of supported statements (0.0-1.0) <code>total_gt_statements</code> <code>int</code> Factual statements from expected output <code>supported_gt_statements</code> <code>int</code> Statements found in context <code>statement_breakdown</code> <code>List</code> Per-statement verdict details"},{"location":"metric-registry/composite/contextual_recall/#statement-breakdown-fields","title":"Statement Breakdown Fields","text":"Field Type Description <code>statement_text</code> <code>str</code> The ground truth statement <code>is_supported</code> <code>bool</code> Whether statement is in context"},{"location":"metric-registry/composite/contextual_recall/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Recall (Score: 1.0) <p>All Facts Retrieved</p> <p>Expected Output:</p> <p>\"The Great Wall of China is over 13,000 miles long. It was built over many centuries, starting in the 7th century BC.\"</p> <p>Retrieved Context:</p> <ol> <li>\"The Great Wall of China stretches over 13,000 miles.\"</li> <li>\"Construction began in the 7th century BC.\"</li> <li>\"Multiple dynasties contributed to building the wall over centuries.\"</li> </ol> <p>Analysis:</p> Statement Verdict Over 13,000 miles long \u2705 Supported Built over many centuries \u2705 Supported Starting in 7th century BC \u2705 Supported <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Recall (Score: 0.5) <p>Missing Information</p> <p>Expected Output:</p> <p>\"Water boils at 100\u00b0C at sea level. At higher altitudes, it boils at lower temperatures due to reduced pressure.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Water boils at 100 degrees Celsius under standard conditions.\"</li> <li>\"Water is essential for life on Earth.\"</li> </ol> <p>Analysis:</p> Statement Verdict Boils at 100\u00b0C at sea level \u2705 Supported Higher altitudes = lower boiling point \u274c Not found Due to reduced pressure \u274c Not found <p>Final Score: <code>1 / 3 = 0.33</code> </p> <p>The altitude/pressure relationship wasn't retrieved.</p> \u274c Scenario 3: Poor Recall (Score: 0.0) <p>Critical Information Missing</p> <p>Expected Output:</p> <p>\"Einstein developed the theory of relativity and won the Nobel Prize for the photoelectric effect.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Albert Einstein was a famous physicist.\"</li> <li>\"Einstein was born in Germany in 1879.\"</li> </ol> <p>Analysis:</p> Statement Verdict Developed theory of relativity \u274c Not found Won Nobel Prize \u274c Not found For photoelectric effect \u274c Not found <p>Final Score: <code>0 / 3 = 0.0</code> </p> <p>None of the key facts from the expected answer were retrieved.</p>"},{"location":"metric-registry/composite/contextual_recall/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Completeness Check <p>Ensures retrieval captures all necessary information, not just some of it.</p> \ud83c\udfaf Answer-Focused <p>Evaluates retrieval from the answer's perspective\u2014did we get what's needed to answer correctly?</p> \ud83d\udc1b Debug Missing Info <p>Identifies exactly which expected facts weren't retrieved, guiding retrieval improvements.</p>"},{"location":"metric-registry/composite/contextual_recall/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Recall = Does the retrieved context contain all facts from the expected answer?</p> <ul> <li>Use it when: You have ground truth and want to measure retrieval completeness</li> <li>Score interpretation: Higher = more expected facts found in context</li> <li>Key insight: Low recall means the retriever missed important information</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRecall</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Relevancy \u00b7 Answer Completeness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_relevancy/","title":"Contextual Relevancy","text":"Evaluate if retrieved context is relevant to the user's query LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_relevancy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of relevant chunks \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No answer required <p>What It Measures</p> <p>Contextual Relevancy evaluates whether the retrieved context chunks are relevant to the user's query. It measures retrieval quality independent of generation\u2014answering: \"Did we retrieve the right documents?\"</p> Score Interpretation 1.0  All retrieved chunks are relevant 0.7+  Most chunks relevant, some noise 0.5  Mixed relevance\u2014half helpful &lt; 0.5  Mostly irrelevant retrieval \u2705 Use When <ul> <li>Evaluating RAG retrieval quality</li> <li>Tuning vector search parameters</li> <li>Debugging poor answer quality</li> <li>Comparing retrieval strategies</li> </ul> \u274c Don't Use When <ul> <li>No retrieval component exists</li> <li>Evaluating answer quality (use Faithfulness)</li> <li>All chunks are from same document</li> <li>Retrieval is keyword-based only</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Relevancy asks: \"Are the retrieved chunks relevant to the query?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Precision: Are relevant chunks ranked higher?</li> <li>Contextual Recall: Do chunks cover the expected answer?</li> <li>Contextual Sufficiency: Is there enough info to answer?</li> </ul> How It Works  Computation Verdict System <p>The metric evaluates each retrieved chunk's relevance to the query.</p> <p>Each chunk receives a binary relevance verdict.</p> <p> \u2705 RELEVANT 1 Chunk contains information useful for answering the query. </p> <p> \u274c IRRELEVANT 0 Chunk is off-topic or doesn't help answer the query. </p> <p>Score Formula</p> <pre><code>score = relevant_chunks / total_chunks\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 1: Relevancy Check\"]\n        C[RAGAnalyzer Engine]\n        D1[\"Chunk 1: \u2713/\u2717\"]\n        D2[\"Chunk 2: \u2713/\u2717\"]\n        D3[\"Chunk 3: \u2713/\u2717\"]\n        DN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 2: Scoring\"]\n        E[\"Count Relevant Chunks\"]\n        F[\"Calculate Ratio\"]\n        G[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style G fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Shared Cache</p> <p>Contextual Relevancy shares an internal cache with other contextual metrics. Running multiple retrieval metrics together is efficient.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#code-examples","title":"Code Examples","text":"Basic Usage Comparing Retrieval With Runner <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualRelevancy()\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    retrieved_content=[\n        \"Paris is the capital and largest city of France.\",\n        \"France is known for its wine and cuisine.\",\n        \"The Eiffel Tower was built in 1889.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.67 (2 of 3 chunks relevant)\n</code></pre> <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRelevancy()\nrunner = MetricRunner(metrics=[metric])\n\n# Compare two retrieval strategies\nresults_v1 = await runner.run(dataset_with_bm25)\nresults_v2 = await runner.run(dataset_with_embeddings)\n\navg_v1 = sum(r.score for r in results_v1) / len(results_v1)\navg_v2 = sum(r.score for r in results_v2) / len(results_v2)\n\nprint(f\"BM25 Relevancy: {avg_v1:.2f}\")\nprint(f\"Embedding Relevancy: {avg_v2:.2f}\")\n</code></pre> <pre><code>from axion.metrics import ContextualRelevancy\nfrom axion.runners import MetricRunner\n\nmetric = ContextualRelevancy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Relevant: {item_result.signals.relevant_chunks}/{item_result.signals.total_chunks}\")\n    for i, chunk in enumerate(item_result.signals.chunk_breakdown):\n        status = \"\u2705\" if chunk.is_relevant else \"\u274c\"\n        print(f\"  {status} Chunk {i+1}: {chunk.chunk_text[:50]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualRelevancyResult Structure <pre><code>ContextualRelevancyResult(\n{\n    \"relevancy_score\": 0.67,\n    \"total_chunks\": 3,\n    \"relevant_chunks\": 2,\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Paris is the capital and largest city of France.\",\n            \"is_relevant\": true\n        },\n        {\n            \"chunk_text\": \"France is known for its wine and cuisine.\",\n            \"is_relevant\": false\n        },\n        {\n            \"chunk_text\": \"The Eiffel Tower was built in 1889.\",\n            \"is_relevant\": true\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_relevancy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>relevancy_score</code> <code>float</code> Ratio of relevant chunks (0.0-1.0) <code>total_chunks</code> <code>int</code> Total chunks retrieved <code>relevant_chunks</code> <code>int</code> Number of relevant chunks <code>chunk_breakdown</code> <code>List</code> Per-chunk verdict details"},{"location":"metric-registry/composite/contextual_relevancy/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The retrieved chunk content <code>is_relevant</code> <code>bool</code> Whether chunk is relevant to query"},{"location":"metric-registry/composite/contextual_relevancy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High Relevancy (Score: 1.0) <p>All Chunks Relevant</p> <p>Query:</p> <p>\"How does photosynthesis work?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Photosynthesis converts light energy into chemical energy.\"</li> <li>\"Plants use chlorophyll to absorb sunlight.\"</li> <li>\"The process produces glucose and oxygen from CO2 and water.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Light energy conversion \u2705 Core concept Chlorophyll absorption \u2705 Key mechanism Glucose/oxygen production \u2705 Process outputs <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Mixed Relevancy (Score: 0.5) <p>Retrieval Noise</p> <p>Query:</p> <p>\"What are the symptoms of diabetes?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Diabetes symptoms include increased thirst and frequent urination.\"</li> <li>\"Exercise is important for overall health.\"</li> <li>\"Blurred vision and fatigue are common in diabetic patients.\"</li> <li>\"Healthy eating includes fruits and vegetables.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Thirst and urination \u2705 Direct symptoms Exercise importance \u274c General health, not symptoms Blurred vision, fatigue \u2705 Diabetes symptoms Fruits and vegetables \u274c Diet info, not symptoms <p>Final Score: <code>2 / 4 = 0.5</code> </p> \u274c Scenario 3: Poor Relevancy (Score: 0.0) <p>Retrieval Failure</p> <p>Query:</p> <p>\"What is quantum computing?\"</p> <p>Retrieved Chunks:</p> <ol> <li>\"Classical computers use binary bits.\"</li> <li>\"The internet was invented in the 1960s.\"</li> <li>\"Programming languages include Python and Java.\"</li> </ol> <p>Analysis:</p> Chunk Verdict Binary bits \u274c Classical computing, not quantum Internet history \u274c Completely off-topic Programming languages \u274c Unrelated to quantum concepts <p>Final Score: <code>0 / 3 = 0.0</code> </p> <p>Retrieval completely failed to find quantum computing content.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Retrieval Quality <p>Identifies when your retrieval system returns irrelevant documents, causing poor answer quality.</p> \ud83c\udfaf Debug Isolation <p>Separates retrieval problems from generation problems. Low relevancy = fix retrieval, not the LLM.</p> \u26a1 Efficiency <p>Irrelevant chunks waste context window space and can confuse the generator.</p>"},{"location":"metric-registry/composite/contextual_relevancy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Relevancy = Are the retrieved chunks relevant to the query?</p> <ul> <li>Use it when: Evaluating or tuning RAG retrieval</li> <li>Score interpretation: Higher = more relevant retrieval</li> <li>Key insight: Measures retrieval, not generation</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualRelevancy</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Recall \u00b7 Faithfulness</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_sufficiency/","title":"Contextual Sufficiency","text":"Evaluate if retrieved context contains enough information to answer the query LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_sufficiency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary sufficiency verdict \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>retrieved_content</code> No answer required <p>What It Measures</p> <p>Contextual Sufficiency evaluates whether the retrieved context contains enough information to fully answer the user's query. Unlike other metrics that measure partial coverage, this is a binary judgment: either the context is sufficient or it isn't.</p> Score Interpretation 1.0  Context is sufficient to answer the query 0.0  Context is insufficient\u2014information missing \u2705 Use When <ul> <li>Diagnosing retrieval quality</li> <li>Testing retrieval before generation</li> <li>Identifying information gaps</li> <li>Deciding when to expand search</li> </ul> \u274c Don't Use When <ul> <li>Need granular coverage scores</li> <li>Evaluating answer quality</li> <li>Comparing retrieval strategies</li> <li>Need partial credit</li> </ul> <p>RAG Evaluation Suite</p> <p>Contextual Sufficiency asks: \"Is there enough context to answer this question?\"</p> <p>Related retrieval metrics:</p> <ul> <li>Contextual Relevancy: Are chunks relevant?</li> <li>Contextual Recall: Are expected facts present?</li> <li>Contextual Utilization: Was the context actually used?</li> </ul> How It Works  Computation Verdict System <p>The metric uses an LLM to make a binary judgment about context sufficiency.</p> <p>A single binary verdict for the entire context.</p> <p> \u2705 SUFFICIENT 1.0 Context contains all necessary information to answer the query completely. </p> <p> \u274c INSUFFICIENT 0.0 Context is missing critical information needed to answer the query. </p> <p>Diagnostic Purpose</p> <p>This metric helps diagnose retrieval issues independent of generation. If sufficiency is low but faithfulness is high, your retriever needs improvement.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Context]\n    end\n\n    subgraph JUDGE[\"\u2696\ufe0f Sufficiency Judgment\"]\n        C[RAGAnalyzer Engine]\n        D[\"Can this context answer the query?\"]\n        E[\"Binary Verdict\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"1.0 = Sufficient\"]\n        G[\"0.0 = Insufficient\"]\n        H[\"Reasoning Provided\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F &amp; G\n    F &amp; G --&gt; H\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style JUDGE stroke:#f59e0b,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px\n    style E fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Binary by Design</p> <p>Unlike other metrics that provide granular scores, Sufficiency is intentionally binary. For partial coverage scores, use Contextual Recall or Contextual Relevancy.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#code-examples","title":"Code Examples","text":"Basic Usage Insufficient Example With Runner <pre><code>from axion.metrics import ContextualSufficiency\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualSufficiency()\n\nitem = DatasetItem(\n    query=\"What is the boiling point of water?\",\n    retrieved_content=[\n        \"Water boils at 100 degrees Celsius at sea level.\",\n        \"This is equivalent to 212 degrees Fahrenheit.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (context is sufficient)\n</code></pre> <pre><code>from axion.metrics import ContextualSufficiency\n\nmetric = ContextualSufficiency()\n\nitem = DatasetItem(\n    query=\"What is the boiling point of water at high altitude?\",\n    retrieved_content=[\n        \"Water boils at 100 degrees Celsius at sea level.\",\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 0.0 (missing altitude information)\nprint(result.signals.reasoning)\n# \"Context only mentions sea level; no information about altitude effects.\"\n</code></pre> <pre><code>from axion.metrics import ContextualSufficiency\nfrom axion.runners import MetricRunner\n\nmetric = ContextualSufficiency()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nsufficient_count = sum(1 for r in results if r.score == 1.0)\nprint(f\"Sufficient: {sufficient_count}/{len(results)}\")\n\nfor item_result in results:\n    if item_result.score == 0.0:\n        print(f\"\u26a0\ufe0f Insufficient for: {item_result.signals.query[:50]}...\")\n        print(f\"   Reason: {item_result.signals.reasoning}\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualSufficiencyResult Structure <pre><code>ContextualSufficiencyResult(\n{\n    \"sufficiency_score\": 1.0,\n    \"is_sufficient\": true,\n    \"reasoning\": \"The context fully addresses the query by providing the boiling point of water (100\u00b0C) and its Fahrenheit equivalent (212\u00b0F).\",\n    \"query\": \"What is the boiling point of water?\",\n    \"context\": \"Water boils at 100 degrees Celsius at sea level. This is equivalent to 212 degrees Fahrenheit.\"\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_sufficiency/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>sufficiency_score</code> <code>float</code> Binary score (1.0 or 0.0) <code>is_sufficient</code> <code>bool</code> Whether context is sufficient <code>reasoning</code> <code>str</code> Explanation for the verdict <code>query</code> <code>str</code> The user query (preview) <code>context</code> <code>str</code> The retrieved context (preview)"},{"location":"metric-registry/composite/contextual_sufficiency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Sufficient Context (Score: 1.0) <p>Complete Information</p> <p>Query:</p> <p>\"Who invented the telephone and when?\"</p> <p>Retrieved Context:</p> <p>\"Alexander Graham Bell invented the telephone in 1876. He was granted the patent on March 7th of that year.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Inventor identified: Alexander Graham Bell</li> <li>\u2705 Year provided: 1876</li> <li>\u2705 Additional detail: Patent date</li> </ul> <p>Verdict: Sufficient</p> <p>Reasoning: \"The context directly answers both parts of the query\u2014who (Alexander Graham Bell) and when (1876).\"</p> <p>Final Score: <code>1.0</code> </p> \u274c Scenario 2: Insufficient - Missing Key Info (Score: 0.0) <p>Critical Information Missing</p> <p>Query:</p> <p>\"What are the side effects of aspirin?\"</p> <p>Retrieved Context:</p> <p>\"Aspirin is a common pain reliever. It belongs to a class of drugs called NSAIDs. It can be purchased over the counter.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Drug identification: Correct</li> <li>\u2705 Drug class: NSAIDs</li> <li>\u274c Side effects: Not mentioned</li> </ul> <p>Verdict: Insufficient</p> <p>Reasoning: \"The context describes what aspirin is but does not mention any side effects, which is the core of the query.\"</p> <p>Final Score: <code>0.0</code> </p> \u274c Scenario 3: Insufficient - Partial Answer (Score: 0.0) <p>Incomplete Coverage</p> <p>Query:</p> <p>\"Compare the populations of Tokyo and New York City.\"</p> <p>Retrieved Context:</p> <p>\"Tokyo is the capital of Japan with a metropolitan population of over 37 million people, making it the world's most populous metropolitan area.\"</p> <p>Analysis:</p> <ul> <li>\u2705 Tokyo population: Provided</li> <li>\u274c NYC population: Missing</li> <li>\u274c Comparison: Cannot be made</li> </ul> <p>Verdict: Insufficient</p> <p>Reasoning: \"Context only provides Tokyo's population. NYC population is missing, making a comparison impossible.\"</p> <p>Final Score: <code>0.0</code> </p>"},{"location":"metric-registry/composite/contextual_sufficiency/#why-it-matters","title":"Why It Matters","text":"\ud83d\udd0d Retrieval Diagnosis <p>Quickly identify if poor answers stem from insufficient retrieval, not generation quality.</p> \ud83d\udd04 Adaptive Search <p>Use as a signal to expand search or trigger alternative retrieval strategies.</p> \u26a1 Pre-Generation Check <p>Evaluate context before generating\u2014don't waste tokens on insufficient information.</p>"},{"location":"metric-registry/composite/contextual_sufficiency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Sufficiency = Is there enough context to fully answer the query?</p> <ul> <li>Use it when: Diagnosing retrieval gaps or deciding to expand search</li> <li>Score interpretation: 1.0 = sufficient, 0.0 = insufficient (binary)</li> <li>Key insight: Identifies \"missing information\" problems in retrieval</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualSufficiency</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Recall \u00b7 Contextual Relevancy \u00b7 Contextual Utilization</p> </li> </ul>"},{"location":"metric-registry/composite/contextual_utilization/","title":"Contextual Utilization","text":"Measure the efficiency of context usage in generation LLM-Powered Knowledge Single Turn Retrieval"},{"location":"metric-registry/composite/contextual_utilization/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of utilized chunks \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Answer + context required <p>What It Measures</p> <p>Contextual Utilization measures the efficiency of context usage\u2014what proportion of relevant retrieved chunks were actually used in the generated answer. Low utilization means relevant information was retrieved but ignored.</p> Score Interpretation 1.0  All relevant chunks were utilized 0.7+  Good utilization, minor waste 0.5  Half of relevant chunks unused &lt; 0.5  Significant waste\u2014relevant info ignored \u2705 Use When <ul> <li>Optimizing context window usage</li> <li>Debugging incomplete answers</li> <li>Identifying generation issues</li> <li>Measuring retrieval efficiency</li> </ul> \u274c Don't Use When <ul> <li>No actual_output available</li> <li>Evaluating retrieval only</li> <li>Testing factual correctness</li> <li>All chunks are equally important</li> </ul> <p>Utilization vs Faithfulness</p> <p>Contextual Utilization asks: \"Was the relevant context actually used?\" Faithfulness asks: \"Is the answer grounded in context?\"</p> <p>High Faithfulness + Low Utilization = Answer is correct but incomplete (missed relevant info).</p> How It Works  Computation Verdict System <p>The metric evaluates which relevant chunks were actually utilized in the answer.</p> <p>Each relevant chunk is checked for utilization.</p> <p> \u2705 UTILIZED 1 Information from this chunk appears in the generated answer. </p> <p> \u274c NOT UTILIZED 0 Relevant chunk was ignored\u2014information not used in answer. </p> <p>Score Formula</p> <p><pre><code>score = utilized_chunks / total_relevant_chunks\n</code></pre> Only relevant chunks are counted\u2014irrelevant chunks don't affect the score.</p>"},{"location":"metric-registry/composite/contextual_utilization/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Chunks]\n        C[Generated Answer]\n    end\n\n    subgraph FILTER[\"\ud83d\udd0d Step 1: Identify Relevant\"]\n        D[Check Chunk Relevancy]\n        E[\"Relevant Chunks Only\"]\n    end\n\n    subgraph CHECK[\"\u2696\ufe0f Step 2: Check Utilization\"]\n        F[Compare to Answer]\n        G1[\"Chunk 1: \u2713/\u2717\"]\n        G2[\"Chunk 2: \u2713/\u2717\"]\n        G3[\"Chunk 3: \u2713/\u2717\"]\n        GN[\"...\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Utilized\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; D\n    D --&gt; E\n    E --&gt; F\n    C --&gt; F\n    F --&gt; G1 &amp; G2 &amp; G3 &amp; GN\n    G1 &amp; G2 &amp; G3 &amp; GN --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style FILTER stroke:#3b82f6,stroke-width:2px\n    style CHECK stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Relevance Filtering</p> <p>The metric first filters to only relevant chunks (using the same logic as Contextual Relevancy), then checks which of those were utilized. This means irrelevant chunks don't penalize or inflate the score.</p>"},{"location":"metric-registry/composite/contextual_utilization/#code-examples","title":"Code Examples","text":"Basic Usage Full Utilization With Runner <pre><code>from axion.metrics import ContextualUtilization\nfrom axion.dataset import DatasetItem\n\nmetric = ContextualUtilization()\n\nitem = DatasetItem(\n    query=\"What are the health benefits of green tea?\",\n    actual_output=\"Green tea contains antioxidants that reduce inflammation.\",\n    retrieved_content=[\n        \"Green tea is rich in antioxidants.\",                    # Relevant, utilized\n        \"Antioxidants help reduce inflammation.\",                # Relevant, utilized\n        \"Green tea can boost metabolism.\",                       # Relevant, NOT utilized\n        \"Tea originated in China thousands of years ago.\",       # Not relevant\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 2/3 = 0.67 (2 of 3 relevant chunks utilized)\n</code></pre> <pre><code>from axion.metrics import ContextualUtilization\n\nmetric = ContextualUtilization()\n\nitem = DatasetItem(\n    query=\"What is Python?\",\n    actual_output=\"Python is a high-level programming language created by Guido van Rossum, known for its readability.\",\n    retrieved_content=[\n        \"Python is a high-level programming language.\",          # Utilized\n        \"Guido van Rossum created Python.\",                      # Utilized\n        \"Python emphasizes code readability.\",                   # Utilized\n    ],\n)\n\nresult = await metric.execute(item)\n# Score: 1.0 (all relevant chunks utilized)\n</code></pre> <pre><code>from axion.metrics import ContextualUtilization\nfrom axion.runners import MetricRunner\n\nmetric = ContextualUtilization()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Utilization: {item_result.score:.0%}\")\n    print(f\"Used: {item_result.signals.utilized_chunks}/{item_result.signals.total_relevant_chunks}\")\n    for chunk in item_result.signals.chunk_breakdown:\n        status = \"\u2705\" if chunk.is_utilized else \"\u274c\"\n        print(f\"  {status} {chunk.chunk_text[:40]}...\")\n</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ContextualUtilizationResult Structure <pre><code>ContextualUtilizationResult(\n{\n    \"utilization_score\": 0.67,\n    \"total_relevant_chunks\": 3,\n    \"utilized_chunks\": 2,\n    \"utilization_rate\": \"66.7%\",\n    \"chunk_breakdown\": [\n        {\n            \"chunk_text\": \"Green tea is rich in antioxidants.\",\n            \"is_utilized\": true\n        },\n        {\n            \"chunk_text\": \"Antioxidants help reduce inflammation.\",\n            \"is_utilized\": true\n        },\n        {\n            \"chunk_text\": \"Green tea can boost metabolism.\",\n            \"is_utilized\": false\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/contextual_utilization/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>utilization_score</code> <code>float</code> Ratio of utilized chunks (0.0-1.0) <code>total_relevant_chunks</code> <code>int</code> Relevant chunks in context <code>utilized_chunks</code> <code>int</code> Chunks actually used in answer <code>utilization_rate</code> <code>str</code> Human-readable percentage <code>chunk_breakdown</code> <code>List</code> Per-chunk (relevant only) details"},{"location":"metric-registry/composite/contextual_utilization/#chunk-breakdown-fields","title":"Chunk Breakdown Fields","text":"Field Type Description <code>chunk_text</code> <code>str</code> The relevant chunk content <code>is_utilized</code> <code>bool</code> Whether chunk was used in answer"},{"location":"metric-registry/composite/contextual_utilization/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Full Utilization (Score: 1.0) <p>All Relevant Info Used</p> <p>Query:</p> <p>\"What is the boiling point of water?\"</p> <p>Retrieved Context:</p> <ol> <li>\"Water boils at 100\u00b0C at sea level.\" \u2705 Relevant</li> <li>\"This is equivalent to 212\u00b0F.\" \u2705 Relevant</li> <li>\"Ice cream is a popular dessert.\" \u274c Not relevant</li> </ol> <p>Generated Answer:</p> <p>\"Water boils at 100\u00b0C (212\u00b0F) at sea level.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Boils at 100\u00b0C \u2705 Used Equivalent to 212\u00b0F \u2705 Used <p>Final Score: <code>2 / 2 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Utilization (Score: 0.5) <p>Relevant Info Ignored</p> <p>Query:</p> <p>\"What are the benefits of exercise?\"</p> <p>Retrieved Context:</p> <ol> <li>\"Exercise improves cardiovascular health.\" \u2705 Relevant</li> <li>\"Regular exercise boosts mood and energy.\" \u2705 Relevant</li> <li>\"Exercise helps with weight management.\" \u2705 Relevant</li> <li>\"Gyms offer various equipment.\" \u274c Not relevant</li> </ol> <p>Generated Answer:</p> <p>\"Exercise improves cardiovascular health and boosts mood.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Cardiovascular health \u2705 Used Mood and energy \u2705 Used (partial) Weight management \u274c Not used <p>Final Score: <code>2 / 3 = 0.67</code> </p> <p>Weight management benefit was retrieved but not mentioned.</p> \u274c Scenario 3: Poor Utilization (Score: 0.25) <p>Most Relevant Info Wasted</p> <p>Query:</p> <p>\"Explain the causes of World War I.\"</p> <p>Retrieved Context:</p> <ol> <li>\"Assassination of Archduke Franz Ferdinand triggered WWI.\" \u2705 Relevant</li> <li>\"Alliance systems escalated regional conflicts.\" \u2705 Relevant</li> <li>\"Nationalism and imperialism created tensions.\" \u2705 Relevant</li> <li>\"The war lasted from 1914 to 1918.\" \u2705 Relevant</li> </ol> <p>Generated Answer:</p> <p>\"World War I began after the assassination of Archduke Franz Ferdinand.\"</p> <p>Analysis:</p> Relevant Chunk Utilized Assassination \u2705 Used Alliance systems \u274c Not used Nationalism/imperialism \u274c Not used Duration \u274c Not used <p>Final Score: <code>1 / 4 = 0.25</code> </p> <p>Only one cause mentioned despite retrieving multiple.</p>"},{"location":"metric-registry/composite/contextual_utilization/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Answer Completeness <p>Low utilization often indicates incomplete answers that miss relevant information.</p> \ud83d\udcb0 Efficiency <p>Retrieved content costs tokens. Low utilization = wasted context window space.</p> \ud83d\udd27 Debug Generation <p>If retrieval is good but utilization is low, the problem is in generation, not retrieval.</p>"},{"location":"metric-registry/composite/contextual_utilization/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contextual Utilization = Was the relevant retrieved context actually used in the answer?</p> <ul> <li>Use it when: Debugging incomplete answers or optimizing context usage</li> <li>Score interpretation: Higher = more efficient use of retrieved information</li> <li>Key insight: Measures generation efficiency, not retrieval quality</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContextualUtilization</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Contextual Relevancy \u00b7 Answer Completeness</p> </li> </ul>"},{"location":"metric-registry/composite/factual_accuracy/","title":"Factual Accuracy","text":"Verify AI responses against ground truth statements LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/factual_accuracy/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Ratio of supported statements \u26a1 Default Threshold <code>0.8</code> Higher bar for accuracy \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>expected_output</code> Ground truth required <p>What It Measures</p> <p>Factual Accuracy calculates the percentage of statements in the AI's response that are factually supported by the ground truth (expected_output). Unlike Faithfulness (which checks against retrieved context), this metric verifies against a known-correct answer.</p> Score Interpretation 1.0  Every statement matches ground truth 0.8+  Most statements accurate, minor gaps 0.5  Half the statements are unsupported &lt; 0.5  Significant factual errors \u2705 Use When <ul> <li>You have ground truth answers</li> <li>Testing against known-correct responses</li> <li>Evaluating factual Q&amp;A systems</li> <li>Regression testing AI outputs</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Multiple valid answers exist</li> <li>Testing creative/generative tasks</li> <li>Ground truth may be incomplete</li> </ul> <p>See Also: Faithfulness</p> <p>Factual Accuracy verifies against ground truth (expected_output). Faithfulness verifies against retrieved context.</p> <p>Use Factual Accuracy when you have known-correct answers; use Faithfulness for RAG systems.</p> How It Works  Computation Verdict System <p>The metric extracts statements from the AI response and checks each against the ground truth.</p> <p>Each statement receives a binary verdict\u2014either supported or not supported by the ground truth.</p> <p> \u2705 SUPPORTED 1 Statement is factually consistent with the ground truth. </p> <p> \u274c NOT SUPPORTED 0 Statement is not found or contradicts the ground truth. </p> <p>Score Formula</p> <pre><code>score = supported_statements / total_statements\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Response]\n        C[Expected Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Statement Extraction\"]\n        D[Extract Statements from Response]\n        E[\"Atomic Statements\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 2: Ground Truth Check\"]\n        F[Compare to Expected Output]\n        G[\"Supported / Not Supported\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Count Supported\"]\n        I[\"Calculate Ratio\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    C --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VERIFY stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Simple Configuration</p> <p>Factual Accuracy has minimal configuration\u2014it focuses on binary correctness against ground truth.</p>"},{"location":"metric-registry/composite/factual_accuracy/#code-examples","title":"Code Examples","text":"Basic Usage With Runner <pre><code>from axion.metrics import FactualAccuracy\nfrom axion.dataset import DatasetItem\n\nmetric = FactualAccuracy()\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"Paris is the capital of France. It has a population of about 2 million.\",\n    expected_output=\"Paris is the capital of France. The city has approximately 2.1 million inhabitants.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import FactualAccuracy\nfrom axion.runners import MetricRunner\n\nmetric = FactualAccuracy()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    for verdict in item_result.signals.verdicts:\n        status = \"\u2705\" if verdict.is_supported else \"\u274c\"\n        print(f\"  {status} {verdict.statement}\")\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca FactualityReport Structure <pre><code>FactualityReport(\n{\n    \"verdicts\": [\n        {\n            \"statement\": \"Paris is the capital of France.\",\n            \"is_supported\": 1,\n            \"reason\": \"The ground truth confirms Paris is the capital of France.\"\n        },\n        {\n            \"statement\": \"It has a population of about 2 million.\",\n            \"is_supported\": 1,\n            \"reason\": \"The ground truth states approximately 2.1 million, which aligns with 'about 2 million'.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/factual_accuracy/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>verdicts</code> <code>List[StatementVerdict]</code> Per-statement verdicts"},{"location":"metric-registry/composite/factual_accuracy/#statement-verdict-fields","title":"Statement Verdict Fields","text":"Field Type Description <code>statement</code> <code>str</code> The extracted statement <code>is_supported</code> <code>int</code> 1 = supported, 0 = not supported <code>reason</code> <code>str</code> Explanation for the verdict"},{"location":"metric-registry/composite/factual_accuracy/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Accuracy (Score: 1.0) <p>All Statements Supported</p> <p>Query:</p> <p>\"What year did World War II end?\"</p> <p>Expected Output:</p> <p>\"World War II ended in 1945. Germany surrendered in May, and Japan in September.\"</p> <p>AI Response:</p> <p>\"World War II ended in 1945. Germany surrendered in May, Japan in September.\"</p> <p>Analysis:</p> Statement Verdict Score World War II ended in 1945 SUPPORTED 1 Germany surrendered in May SUPPORTED 1 Japan surrendered in September SUPPORTED 1 <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Accuracy (Score: 0.67) <p>Mixed Verdicts</p> <p>Query:</p> <p>\"What is the speed of light?\"</p> <p>Expected Output:</p> <p>\"The speed of light is approximately 299,792 km/s in a vacuum.\"</p> <p>AI Response:</p> <p>\"The speed of light is about 300,000 km/s. It travels slower through water. Light is the fastest thing in the universe.\"</p> <p>Analysis:</p> Statement Verdict Score Speed of light is about 300,000 km/s SUPPORTED 1 It travels slower through water NOT SUPPORTED 0 Light is the fastest thing in the universe NOT SUPPORTED 0 <p>Final Score: <code>1 / 3 = 0.33</code> </p> <p>The ground truth only mentions vacuum speed\u2014other claims are unsupported.</p> \u274c Scenario 3: Poor Accuracy (Score: 0.0) <p>No Statements Supported</p> <p>Query:</p> <p>\"Who wrote Romeo and Juliet?\"</p> <p>Expected Output:</p> <p>\"Romeo and Juliet was written by William Shakespeare in the 1590s.\"</p> <p>AI Response:</p> <p>\"Romeo and Juliet was written by Christopher Marlowe in 1610.\"</p> <p>Analysis:</p> Statement Verdict Score Written by Christopher Marlowe NOT SUPPORTED 0 Written in 1610 NOT SUPPORTED 0 <p>Final Score: <code>0 / 2 = 0.0</code> </p> <p>Both claims contradict the ground truth.</p>"},{"location":"metric-registry/composite/factual_accuracy/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfaf Ground Truth Validation <p>When you have known-correct answers, this metric tells you exactly how well your AI matches reality.</p> \ud83e\uddea Regression Testing <p>Track factual accuracy over time as you update models or prompts. Catch regressions before deployment.</p> \ud83d\udcca Benchmark Evaluation <p>Compare different models or configurations using the same ground truth dataset.</p>"},{"location":"metric-registry/composite/factual_accuracy/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Factual Accuracy = Does the AI's response match the known-correct answer?</p> <ul> <li>Use it when: You have ground truth (expected_output) to compare against</li> <li>Score interpretation: Higher = more statements verified against ground truth</li> <li>Key difference: Compares to expected_output, not retrieved_content</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.FactualAccuracy</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Completeness \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/composite/faithfulness/","title":"Faithfulness","text":"Measure factual consistency between AI responses and source documents LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/faithfulness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Clamped from weighted average \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> <code>retrieved_content</code> Three fields needed <p>What It Measures</p> <p>Faithfulness evaluates whether every claim in the AI's response can be directly inferred from the provided source material. It acts as your primary defense against hallucinations\u2014ensuring the AI summarizes existing knowledge rather than inventing facts.</p> Score Interpretation 1.0  Every claim is fully supported by context 0.7+  Most claims supported, minor gaps 0.5  Threshold\u2014mixture of supported and unsupported &lt; 0.5  Significant hallucinations or contradictions \u2705 Use When <ul> <li>RAG systems &amp; document Q&amp;A</li> <li>Knowledge base assistants</li> <li>Summarization tasks</li> <li>Any system with retrieved context</li> </ul> \u274c Don't Use When <ul> <li>Creative writing / brainstorming</li> <li>Opinion or preference questions</li> <li>No retrieved context available</li> <li>Open-ended generation tasks</li> </ul> <p>See Also: Answer Relevancy</p> <p>Faithfulness checks if claims are grounded in the source context (factual accuracy). Answer Relevancy checks if statements address the user's query (topical alignment).</p> <p>Use both together for comprehensive RAG evaluation.</p> How It Works  Computation Verdict System <p>The metric uses an Evaluator LLM to decompose the response into atomic claims, then verify each against the retrieved context.</p> <p>Each extracted claim receives a verdict with a corresponding weight. The final score is the weighted average, clamped to <code>[0, 1]</code>.</p> <p> \u2705 FULLY_SUPPORTED +1.0 The claim is explicitly stated in the context. Direct evidence exists. </p> <p> \u26a0\ufe0f PARTIALLY_SUPPORTED +0.5 Core subject is correct but claim exaggerates certainty or has minor inaccuracies. </p> <p> \u2753 NO_EVIDENCE 0.0 Context doesn't contain information to verify the claim. Hallucination. </p> <p> \u274c CONTRADICTORY -1.0 Evidence directly contradicts the claim. Critical factual error. </p> <p>Score Formula</p> <pre><code>score = max(0.0, min(1.0, sum(verdict_weights) / total_claims))\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[Retrieved Context]\n        C[AI Response]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Claim Extraction\"]\n        D[StatementExtractor LLM]\n        E[\"Atomic Claims&lt;br/&gt;&lt;small&gt;Self-contained, verifiable&lt;/small&gt;\"]\n    end\n\n    subgraph VERIFY[\"\u2696\ufe0f Step 2: Verification\"]\n        F[FaithfulnessJudge LLM]\n        G[\"Verdict per Claim\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        H[\"Sum Weighted Verdicts\"]\n        I[\"Clamp to [0, 1]\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    B --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style VERIFY stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/faithfulness/#configuration","title":"Configuration","text":"Parameters Custom Weights Parameter Type Default Description <code>strict_mode</code> <code>bool</code> <code>False</code> When <code>True</code>, <code>NO_EVIDENCE</code> verdicts receive -1.0 (same as contradictions), heavily penalizing hallucinations <code>verdict_scores</code> <code>Dict[str, float]</code> <code>None</code> Custom override for verdict weights. Takes precedence over <code>strict_mode</code> <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level (<code>GRANULAR</code> or <code>HOLISTIC</code>) <p>Strict Mode</p> <p>Enable <code>strict_mode=True</code> for high-stakes domains (legal, medical, financial) where any uncited claim is unacceptable\u2014even if not directly contradicted.</p> <p>Override default verdict weights for domain-specific calibration:</p> <pre><code>from axion.metrics import Faithfulness\n\n# Extra penalty for contradictions, higher partial credit\nmetric = Faithfulness(\n    verdict_scores={\n        'FULLY_SUPPORTED': 1.0,\n        'PARTIALLY_SUPPORTED': 0.75,  # More generous\n        'NO_EVIDENCE': -0.5,          # Moderate penalty\n        'CONTRADICTORY': -2.0,        # Severe penalty\n    }\n)\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#code-examples","title":"Code Examples","text":"Basic Usage Strict Mode With Runner <pre><code>from axion.metrics import Faithfulness\nfrom axion.dataset import DatasetItem\n\n# Initialize with defaults\nmetric = Faithfulness()\n\nitem = DatasetItem(\n    query=\"What is the infield fly rule in baseball?\",\n    actual_output=\"The infield fly rule prevents the defense from intentionally dropping a fly ball to turn a double play.\",\n    retrieved_content=[\n        \"The infield fly rule prevents unfair advantage.\",\n        \"Applies with runners on first and second.\",\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n</code></pre> <pre><code>from axion.metrics import Faithfulness\n\n# Zero tolerance for hallucinations\nmetric = Faithfulness(strict_mode=True)\n\n# Any NO_EVIDENCE claim now scores -1.0 instead of 0.0\n# This dramatically lowers scores for responses with uncited claims\n</code></pre> <pre><code>from axion.metrics import Faithfulness\nfrom axion.runners import MetricRunner\n\n# Initialize with strict mode\nfaithfulness = Faithfulness(strict_mode=True)\n\nrunner = MetricRunner(metrics=[faithfulness])\nresults = await runner.run(dataset)\n\n# Access detailed breakdown\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Claims analyzed: {item_result.data.total_claims}\")\n    for claim in item_result.data.judged_claims:\n        print(f\"  - {claim.verdict}: {claim.text}\")\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca FaithfulnessResult Structure <pre><code>FaithfulnessResult(\n{\n    \"overall_score\": 0.5,\n    \"total_claims\": 2,\n    \"verdict_counts\": {\n        \"fully_supported\": 1,\n        \"partially_supported\": 0,\n        \"no_evidence\": 1,\n        \"contradictory\": 0\n    },\n    \"judged_claims\": [\n        {\n            \"claim_text\": \"The infield fly rule prevents the defense from intentionally dropping a fly ball.\",\n            \"faithfulness_verdict\": \"Fully Supported\",\n            \"reason\": \"The evidence states that the infield fly rule prevents the defense from intentionally dropping a catchable fly ball.\"\n        },\n        {\n            \"claim_text\": \"The infield fly rule is designed to prevent an easy double play when runners are on base.\",\n            \"faithfulness_verdict\": \"No Evidence\",\n            \"reason\": \"The evidence does not mention anything about preventing an easy double play when runners are on base.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/faithfulness/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>overall_score</code> <code>float</code> The 0-1 faithfulness score <code>total_claims</code> <code>int</code> Total claims extracted from the response <code>verdict_counts</code> <code>Dict</code> Breakdown by verdict type (<code>fully_supported</code>, <code>partially_supported</code>, <code>no_evidence</code>, <code>contradictory</code>) <code>judged_claims</code> <code>List</code> Per-claim verdict details"},{"location":"metric-registry/composite/faithfulness/#judged-claim-fields","title":"Judged Claim Fields","text":"Field Type Description <code>claim_text</code> <code>str</code> The extracted claim text <code>faithfulness_verdict</code> <code>str</code> Verdict: <code>Fully Supported</code>, <code>Partially Supported</code>, <code>No Evidence</code>, or <code>Contradictory</code> <code>reason</code> <code>str</code> Human-readable explanation for the verdict"},{"location":"metric-registry/composite/faithfulness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Faithfulness (Score: 1.0) <p>FULLY_SUPPORTED</p> <p>Context:</p> <p>\"The Apollo 11 mission launched on July 16, 1969. Neil Armstrong was the mission commander. The lunar module was named Eagle.\"</p> <p>AI Response:</p> <p>\"Apollo 11 launched in July 1969 with Neil Armstrong as commander. The lunar module was called Eagle.\"</p> <p>Analysis:</p> Claim Verdict Weight Apollo 11 launched in July 1969 FULLY_SUPPORTED +1.0 Neil Armstrong was commander FULLY_SUPPORTED +1.0 Lunar module was called Eagle FULLY_SUPPORTED +1.0 <p>Final Score: <code>3.0 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial Support (Score: 0.5) <p>Mixed Verdicts</p> <p>Context:</p> <p>\"Our refund policy allows returns within 30 days. Items must be unused and in original packaging.\"</p> <p>AI Response:</p> <p>\"You can return items within 30 days if unused. Refunds are processed within 24 hours.\"</p> <p>Analysis:</p> Claim Verdict Weight Returns within 30 days if unused FULLY_SUPPORTED +1.0 Refunds processed within 24 hours NO_EVIDENCE 0.0 <p>Final Score: <code>1.0 / 2 = 0.5</code> </p> <p>In strict mode: <code>(1.0 + -1.0) / 2 = 0.0</code></p> \u274c Scenario 3: Contradiction (Score: 0.0) <p>CONTRADICTORY</p> <p>Context:</p> <p>\"The maximum dosage is 500mg per day. Do not exceed this limit.\"</p> <p>AI Response:</p> <p>\"You can safely take up to 1000mg daily.\"</p> <p>Analysis:</p> Claim Verdict Weight Safe to take up to 1000mg daily CONTRADICTORY -1.0 <p>Final Score: <code>max(0, -1.0 / 1) = 0.0</code> </p> <p>Critical: This response could cause patient harm.</p>"},{"location":"metric-registry/composite/faithfulness/#why-it-matters","title":"Why It Matters","text":"\ud83d\udee1\ufe0f Risk Mitigation <p>Primary guardrail against hallucinations. Protects your brand from legal and reputational liability caused by invented facts.</p> \u2705 User Trust <p>Essential for high-stakes domains (legal, financial, medical) where users must trust the AI is summarizing, not creating.</p> \ud83d\udd0d Debug Isolation <p>Distinguishes retrieval errors (wrong docs found) from generation errors (right docs ignored).</p>"},{"location":"metric-registry/composite/faithfulness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Faithfulness = Does the AI's response stick to the facts in the retrieved documents?</p> <ul> <li>Use it when: You need to ensure AI responses don't contain hallucinations</li> <li>Score interpretation: Higher = more grounded in source material</li> <li>Key config: Enable <code>strict_mode</code> for zero-tolerance on uncited claims</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.Faithfulness</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Relevancy \u00b7 Context Precision \u00b7 Factual Accuracy</p> </li> </ul>"},{"location":"metric-registry/composite/pii_leakage/","title":"PII Leakage","text":"Detect personally identifiable information and privacy violations in AI outputs LLM-Powered Security Single Turn"},{"location":"metric-registry/composite/pii_leakage/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> 1.0 = no PII found (safe) \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Optional: <code>expected_output</code> <p>What It Measures</p> <p>PII Leakage evaluates whether AI outputs contain personally identifiable information or privacy violations. It detects names, addresses, contact info, financial data, medical records, government IDs, and confidential business information. Higher scores mean safer outputs.</p> Score Interpretation 1.0  No PII detected\u2014completely safe 0.7+  Minor potential PII, low risk 0.5  Some PII detected\u2014review needed &lt; 0.5  Significant PII leakage\u2014unsafe \u2705 Use When <ul> <li>Processing user data</li> <li>Healthcare or financial systems</li> <li>Customer service applications</li> <li>Any system with privacy requirements</li> </ul> \u274c Don't Use When <ul> <li>PII is expected/required in output</li> <li>Internal tools with no privacy concerns</li> <li>Synthetic data generation</li> <li>Testing environments with fake data</li> </ul> <p>Privacy &amp; Compliance</p> <p>This metric helps identify potential GDPR, HIPAA, CCPA, and other regulatory violations. Use it as part of a comprehensive privacy strategy\u2014not as a sole compliance mechanism.</p> How It Works  Computation PII Categories Verdict System <p>The metric uses a 3-step LLM-based process to identify and evaluate potential PII.</p> <p>The metric detects multiple categories of personally identifiable information:</p> <p> \ud83d\udc64 Personal Identity Full names, dates of birth, age, gender </p> <p> \ud83d\udccd Location Data Home addresses, work addresses, GPS coordinates </p> <p> \ud83d\udcde Contact Info Phone numbers, email addresses, social handles </p> <p> \ud83d\udcb3 Financial Data Credit cards, bank accounts, income details </p> <p> \ud83c\udfe5 Medical Info Health conditions, medications, medical records </p> <p> \ud83c\udd94 Government IDs SSN, passport numbers, driver's license </p> <p>Each statement receives a binary privacy verdict.</p> <p> \u2705 CLEAN 1 Statement does not contain personally identifiable information. </p> <p> \u26a0\ufe0f PII DETECTED 0 Statement contains personally identifiable information. </p> <p>Score Formula</p> <p><pre><code>score = clean_statements / total_statements\n</code></pre> Higher scores = safer outputs (1.0 = no PII found)</p>"},{"location":"metric-registry/composite/pii_leakage/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Query]\n        B[AI Output]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: PII Extraction\"]\n        C[Extract Potential PII Statements]\n        D[\"Candidate Statements\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Privacy Evaluation\"]\n        E[Evaluate Each Statement]\n        F[\"PII / Clean Verdicts\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        G[\"Count Clean Statements\"]\n        H[\"Calculate Safety Ratio\"]\n        I[\"Final Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Simple Configuration</p> <p>PII Leakage has minimal configuration\u2014it focuses on comprehensive PII detection across all categories.</p>"},{"location":"metric-registry/composite/pii_leakage/#code-examples","title":"Code Examples","text":"Basic Usage Safe Output With Runner <pre><code>from axion.metrics import PIILeakage\nfrom axion.dataset import DatasetItem\n\nmetric = PIILeakage()\n\nitem = DatasetItem(\n    query=\"Tell me about the customer's order\",\n    actual_output=\"The order for John Smith at 123 Main St was shipped yesterday. His phone is 555-1234.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.0 (all statements contain PII)\n</code></pre> <pre><code>from axion.metrics import PIILeakage\n\nmetric = PIILeakage()\n\nitem = DatasetItem(\n    query=\"What's the status of order #12345?\",\n    actual_output=\"Order #12345 was shipped on January 15th and is expected to arrive within 3-5 business days.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 1.0 (no PII detected)\n</code></pre> <pre><code>from axion.metrics import PIILeakage\nfrom axion.runners import MetricRunner\n\nmetric = PIILeakage()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    if item_result.score &lt; 1.0:\n        print(f\"\u26a0\ufe0f PII detected! Score: {item_result.score}\")\n        for stmt in item_result.signals.statement_breakdown:\n            if stmt.pii_verdict == \"yes\":\n                print(f\"  - {stmt.statement_text}\")\n                print(f\"    Reason: {stmt.reasoning}\")\n</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca PIILeakageResult Structure <pre><code>PIILeakageResult(\n{\n    \"final_score\": 0.33,\n    \"total_statements\": 3,\n    \"violation_count\": 2,\n    \"clean_statements\": 1,\n    \"score_calculation\": \"clean_statements / total_statements = 1 / 3\",\n    \"statement_breakdown\": [\n        {\n            \"statement_text\": \"The order for John Smith was shipped\",\n            \"pii_verdict\": \"yes\",\n            \"reasoning\": \"Contains a person's full name (John Smith)\"\n        },\n        {\n            \"statement_text\": \"shipped to 123 Main St\",\n            \"pii_verdict\": \"yes\",\n            \"reasoning\": \"Contains a physical address\"\n        },\n        {\n            \"statement_text\": \"shipped yesterday\",\n            \"pii_verdict\": \"no\",\n            \"reasoning\": \"General shipping information without PII\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/pii_leakage/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Safety score (1.0 = no violations) <code>total_statements</code> <code>int</code> Potentially sensitive statements extracted <code>violation_count</code> <code>int</code> Statements identified as containing PII <code>clean_statements</code> <code>int</code> Statements without PII <code>score_calculation</code> <code>str</code> Formula explanation <code>statement_breakdown</code> <code>List</code> Per-statement verdict details"},{"location":"metric-registry/composite/pii_leakage/#statement-breakdown-fields","title":"Statement Breakdown Fields","text":"Field Type Description <code>statement_text</code> <code>str</code> The extracted statement <code>pii_verdict</code> <code>str</code> <code>\"yes\"</code> (PII) or <code>\"no\"</code> (clean) <code>reasoning</code> <code>str</code> Explanation for the verdict"},{"location":"metric-registry/composite/pii_leakage/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: No PII (Score: 1.0) <p>Privacy-Safe Response</p> <p>Query:</p> <p>\"What are your business hours?\"</p> <p>AI Response:</p> <p>\"Our store is open Monday through Friday from 9 AM to 6 PM, and Saturday from 10 AM to 4 PM. We're closed on Sundays.\"</p> <p>Analysis:</p> Statement Verdict Reason Open Monday-Friday 9 AM to 6 PM Clean General business info Saturday 10 AM to 4 PM Clean General business info Closed on Sundays Clean General business info <p>Final Score: <code>3 / 3 = 1.0</code> </p> \u26a0\ufe0f Scenario 2: Partial PII (Score: 0.5) <p>Some PII Detected</p> <p>Query:</p> <p>\"Can you help me with my account?\"</p> <p>AI Response:</p> <p>\"I can see your account was created in 2020. The email on file is john.doe@email.com. Your subscription is active.\"</p> <p>Analysis:</p> Statement Verdict Reason Account created in 2020 Clean General account info Email is john.doe@email.com PII Contains email address Subscription is active Clean General status info <p>Final Score: <code>2 / 3 = 0.67</code> </p> \u274c Scenario 3: Severe PII Leakage (Score: 0.0) <p>Critical Privacy Violation</p> <p>Query:</p> <p>\"Show me customer details\"</p> <p>AI Response:</p> <p>\"Customer: Sarah Johnson, SSN: 123-45-6789. Address: 456 Oak Ave, Boston, MA. Credit card ending in 4242.\"</p> <p>Analysis:</p> Statement Verdict Reason Sarah Johnson PII Full name SSN: 123-45-6789 PII Social Security Number 456 Oak Ave, Boston, MA PII Physical address Credit card ending 4242 PII Financial information <p>Final Score: <code>0 / 4 = 0.0</code> </p> <p>Critical: Multiple categories of sensitive PII exposed.</p>"},{"location":"metric-registry/composite/pii_leakage/#why-it-matters","title":"Why It Matters","text":"\ud83d\udee1\ufe0f Privacy Protection <p>Prevents accidental exposure of sensitive personal information in AI responses.</p> \u2696\ufe0f Regulatory Compliance <p>Helps maintain compliance with GDPR, HIPAA, CCPA, and other privacy regulations.</p> \ud83d\udd12 Trust &amp; Security <p>Protects user trust by ensuring AI systems don't inadvertently leak personal data.</p>"},{"location":"metric-registry/composite/pii_leakage/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>PII Leakage = Does the AI output contain personally identifiable information?</p> <ul> <li>Use it when: Processing user data or building privacy-sensitive applications</li> <li>Score interpretation: Higher = safer (1.0 = no PII found)</li> <li>Key difference: Detects PII in outputs, not inputs</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.PIILeakage</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Answer Relevancy \u00b7 Tone &amp; Style Consistency</p> </li> </ul>"},{"location":"metric-registry/composite/tone_style_consistency/","title":"Tone &amp; Style Consistency","text":"Evaluate if responses match the expected tone, persona, and formatting style LLM-Powered Knowledge Single Turn"},{"location":"metric-registry/composite/tone_style_consistency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Style alignment score \u26a1 Default Threshold <code>0.8</code> Higher bar for consistency \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Optional: <code>persona_description</code> <p>What It Measures</p> <p>Tone &amp; Style Consistency evaluates whether a response matches the emotional tone and writing style of an expected answer. For customer service agents, \"Voice\" is as important as \"Fact\"\u2014this metric ensures your AI maintains the right persona.</p> Score Interpretation 1.0  Perfect match\u2014exact emotion, enthusiasm, formatting 0.8  Minor drift\u2014generally correct but slightly off 0.5  Significant mismatch\u2014wrong tone or style 0.0  Complete failure\u2014robotic, rude, or ignores persona \u2705 Use When <ul> <li>Building customer service agents</li> <li>Persona consistency matters</li> <li>Brand voice guidelines exist</li> <li>Comparing against reference responses</li> </ul> \u274c Don't Use When <ul> <li>No expected_output available</li> <li>Tone flexibility is acceptable</li> <li>Only factual accuracy matters</li> <li>Creative writing tasks</li> </ul> <p>See Also: Answer Completeness</p> <p>Tone &amp; Style Consistency evaluates how something is said (voice, formatting). Answer Completeness evaluates what is said (content coverage).</p> <p>Use both together for comprehensive response quality evaluation.</p> How It Works  Computation Scoring Rubric <p>The metric uses an LLM-based judge to evaluate both emotional tone and writing style.</p> <p>The metric evaluates responses on a detailed rubric with clear benchmarks.</p> <p> \u2705 PERFECT MATCH 1.0 Exact emotion, enthusiasm level, and formatting style. </p> <p> \ud83d\udcca MINOR DRIFT 0.8 Generally correct but slightly less enthusiastic or formal. </p> <p> \u26a0\ufe0f SIGNIFICANT MISMATCH 0.5 Neutral when should be excited, or style completely different. </p> <p> \u274c COMPLETE FAILURE 0.0 Robotic, rude, or completely ignores persona. </p> <p>Two Dimensions</p> <ul> <li>Tone Match: Emotional alignment (enthusiasm, empathy, formality)</li> <li>Style Match: Formatting, length, vocabulary, structure</li> </ul>"},{"location":"metric-registry/composite/tone_style_consistency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n        C[Persona Description]\n    end\n\n    subgraph ANALYZE[\"\ud83d\udd0d Step 1: Style Analysis\"]\n        D[ToneJudge LLM]\n        E[\"Tone &amp; Style Comparison\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Dimension Scoring\"]\n        F[Evaluate Tone Match]\n        G[Evaluate Style Match]\n        H[\"Identify Differences\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Final Score\"]\n        I[\"Combine Dimensions\"]\n        J[\"Final Score\"]\n    end\n\n    A &amp; B &amp; C --&gt; D\n    D --&gt; E\n    E --&gt; F &amp; G\n    F &amp; G --&gt; H\n    H --&gt; I\n    I --&gt; J\n\n    style INPUT stroke:#1E3A5F,stroke-width:2px\n    style ANALYZE stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style J fill:#1E3A5F,stroke:#0F2440,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>persona_description</code> <code>str</code> <code>None</code> Optional persona to enforce (e.g., \"Helpful, excited, professional\") <code>mode</code> <code>EvaluationMode</code> <code>GRANULAR</code> Evaluation detail level <p>Persona Description</p> <p>When provided, the persona description guides the judge on expected tone characteristics, making evaluation more precise for specific brand voices.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#code-examples","title":"Code Examples","text":"Basic Usage With Persona With Runner <pre><code>from axion.metrics import ToneStyleConsistency\nfrom axion.dataset import DatasetItem\n\nmetric = ToneStyleConsistency()\n\nitem = DatasetItem(\n    actual_output=\"Your order has been shipped. It will arrive in 3-5 business days.\",\n    expected_output=\"Great news! \ud83c\udf89 Your order is on its way! You can expect it within 3-5 business days. We're so excited for you!\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: 0.5 (tone mismatch - neutral vs enthusiastic)\n</code></pre> <pre><code>from axion.metrics import ToneStyleConsistency\n\n# Define expected persona\nmetric = ToneStyleConsistency()\n\nitem = DatasetItem(\n    actual_output=\"I apologize for the inconvenience. Let me help resolve this.\",\n    expected_output=\"I'm truly sorry this happened. I completely understand your frustration, and I'm here to make things right!\",\n    persona_description=\"Empathetic, warm, solution-oriented customer service agent\",\n)\n\nresult = await metric.execute(item)\n</code></pre> <pre><code>from axion.metrics import ToneStyleConsistency\nfrom axion.runners import MetricRunner\n\nmetric = ToneStyleConsistency()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score}\")\n    print(f\"Tone Match: {item_result.signals.tone_match}\")\n    print(f\"Style Match: {item_result.signals.style_match}\")\n    for diff in item_result.signals.differences:\n        print(f\"  - {diff.dimension}: {diff.description}\")\n</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly why a score was given\u2014no black boxes.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca ToneStyleResult Structure <pre><code>ToneStyleResult(\n{\n    \"final_score\": 0.5,\n    \"tone_match\": false,\n    \"style_match\": true,\n    \"differences\": [\n        {\n            \"dimension\": \"Enthusiasm Level\",\n            \"expected\": \"Excited, celebratory with emoji\",\n            \"actual\": \"Neutral, matter-of-fact\",\n            \"impact\": \"Major - missed opportunity to delight customer\"\n        },\n        {\n            \"dimension\": \"Emotional Warmth\",\n            \"expected\": \"Personal, caring language\",\n            \"actual\": \"Formal, impersonal\",\n            \"impact\": \"Moderate - feels robotic\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/composite/tone_style_consistency/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Overall tone &amp; style alignment score <code>tone_match</code> <code>bool</code> Whether emotional tone matches expected <code>style_match</code> <code>bool</code> Whether formatting/writing style matches <code>differences</code> <code>List</code> Specific differences identified"},{"location":"metric-registry/composite/tone_style_consistency/#difference-fields","title":"Difference Fields","text":"Field Type Description <code>dimension</code> <code>str</code> Aspect that differs (e.g., \"Enthusiasm Level\") <code>expected</code> <code>str</code> What was expected <code>actual</code> <code>str</code> What was observed <code>impact</code> <code>str</code> Severity of the mismatch"},{"location":"metric-registry/composite/tone_style_consistency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>Tone &amp; Style Aligned</p> <p>Expected Output:</p> <p>\"Hi there! \ud83d\udc4b Thanks for reaching out! I'd be happy to help you with your question about returns. Our policy allows full refunds within 30 days!\"</p> <p>AI Response:</p> <p>\"Hello! \ud83d\ude0a Thanks so much for contacting us! I'm thrilled to assist with your returns question. You can get a full refund within 30 days\u2014no problem at all!\"</p> <p>Analysis:</p> Dimension Match Enthusiasm \u2705 Both excited and welcoming Emoji usage \u2705 Appropriate friendly emoji Formality \u2705 Casual, approachable Helpfulness \u2705 Eager to assist <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Style Drift (Score: 0.5) <p>Tone Mismatch</p> <p>Expected Output:</p> <p>\"Great news! \ud83c\udf89 Your order is on its way! You can expect delivery within 3-5 business days. We're so excited for you!\"</p> <p>AI Response:</p> <p>\"Your order has been shipped. Estimated delivery: 3-5 business days.\"</p> <p>Analysis:</p> Dimension Match Information \u2705 Same facts conveyed Enthusiasm \u274c Neutral vs celebratory Emoji usage \u274c None vs appropriate celebration Warmth \u274c Impersonal vs personal <p>Final Score: <code>0.5</code> </p> <p>Content is correct but voice is completely wrong.</p> \u274c Scenario 3: Complete Mismatch (Score: 0.0) <p>Persona Ignored</p> <p>Expected Output:</p> <p>\"I'm so sorry to hear about this issue! \ud83d\ude14 That's definitely not the experience we want for you. Let me personally look into this right away and make it right!\"</p> <p>AI Response:</p> <p>\"Your complaint has been logged. Reference number: #12345. Allow 5-7 business days for review.\"</p> <p>Analysis:</p> Dimension Match Empathy \u274c None vs deeply apologetic Tone \u274c Cold/bureaucratic vs warm Personal touch \u274c Ticket number vs personal commitment Resolution focus \u274c Process vs solution <p>Final Score: <code>0.0</code> </p> <p>Response is robotic when empathy was expected.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#why-it-matters","title":"Why It Matters","text":"\ud83c\udfad Brand Voice <p>Ensures AI maintains your brand's personality across all interactions. Inconsistent tone damages brand perception.</p> \ud83d\udcac Customer Experience <p>Customers expect warmth and empathy, not robotic responses. Tone directly impacts satisfaction and loyalty.</p> \ud83d\udd04 Consistency <p>Maintain uniform voice across all AI-generated responses, regardless of the underlying model or prompt.</p>"},{"location":"metric-registry/composite/tone_style_consistency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Tone &amp; Style Consistency = Does the AI response sound like it should?</p> <ul> <li>Use it when: Brand voice and persona consistency matter</li> <li>Score interpretation: Higher = better alignment with expected tone</li> <li>Key difference: Measures how something is said, not what is said</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ToneStyleConsistency</code></p> </li> <li> <p> Related Metrics</p> <p> Answer Completeness \u00b7 Answer Relevancy \u00b7 Answer Criteria</p> </li> </ul>"},{"location":"metric-registry/conversational/","title":"Conversational Metrics","text":"<p>Conversational metrics evaluate multi-turn interactions and dialogue quality. These metrics assess how well AI agents maintain context, flow, and achieve user goals across conversation turns.</p>"},{"location":"metric-registry/conversational/#available-metrics","title":"Available Metrics","text":"<p>Coming soon...</p>"},{"location":"metric-registry/heuristic/","title":"Heuristic Metrics","text":"Fast, deterministic evaluation metrics using rule-based and statistical methods 13 Metrics No LLM Required <p>Heuristic metrics are rule-based evaluation metrics that don't require LLM calls. They're ideal for production environments where speed, cost efficiency, and deterministic results are critical. These metrics use pattern matching, statistical analysis, and algorithmic comparisons.</p>"},{"location":"metric-registry/heuristic/#string-matching-metrics","title":"String Matching Metrics","text":"<p>Compare actual outputs against expected outputs using various matching strategies.</p> Exact String Match <p>Check for identical strings</p> <code>actual_output</code> <code>expected_output</code> Contains Match <p>Check if output contains expected text</p> <code>actual_output</code> <code>expected_output</code> Levenshtein Ratio <p>Character-level string similarity</p> <code>actual_output</code> <code>expected_output</code> Sentence BLEU <p>N-gram precision similarity</p> <code>actual_output</code> <code>expected_output</code>"},{"location":"metric-registry/heuristic/#safety-compliance-metrics","title":"Safety &amp; Compliance Metrics","text":"<p>Evaluate outputs for privacy, citations, and policy compliance.</p> PII Leakage (Heuristic) <p>Detect PII using regex patterns</p> <code>query</code> <code>actual_output</code> Citation Presence <p>Verify responses include citations</p> <code>actual_output</code>"},{"location":"metric-registry/heuristic/#performance-metrics","title":"Performance Metrics","text":"<p>Monitor execution time and operational performance.</p> Latency <p>Measure and evaluate execution time</p> <code>latency</code>"},{"location":"metric-registry/heuristic/#output-constraints","title":"Output Constraints","text":"<p>Enforce length and format requirements on outputs.</p> Length Constraint <p>Verify character and sentence limits</p> <code>actual_output</code>"},{"location":"metric-registry/heuristic/#retrieval-metrics-ir","title":"Retrieval Metrics (IR)","text":"<p>Standard information retrieval metrics for evaluating search and ranking quality.</p> Hit Rate @ K <p>Any relevant result in top K?</p> <code>actual_ranking</code> <code>expected_reference</code> MRR <p>Rank of first relevant result</p> <code>actual_ranking</code> <code>expected_reference</code> NDCG @ K <p>Graded relevance with discounting</p> <code>actual_ranking</code> <code>expected_reference</code> Precision @ K <p>Fraction of top K that's relevant</p> <code>actual_ranking</code> <code>expected_reference</code> Recall @ K <p>Coverage of relevant documents</p> <code>actual_ranking</code> <code>expected_reference</code>"},{"location":"metric-registry/heuristic/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Exact String Match 0.0 or 1.0 0.5 Are strings identical? Contains Match 0.0 or 1.0 0.5 Is expected text in output? Levenshtein Ratio 0.0 \u2013 1.0 0.2 How similar are the strings? Sentence BLEU 0.0 \u2013 1.0 0.5 How much n-gram overlap? PII Leakage (Heuristic) 0.0 \u2013 1.0 0.8 Is output privacy-safe? (1.0 = safe) Citation Presence 0.0 or 1.0 0.5 Are citations included? Latency 0.0 \u2013 \u221e 5.0s How fast was the response? Length Constraint 0.0 or 1.0 1.0 Within char/sentence limits? Hit Rate @ K 0.0 or 1.0 - Any relevant in top K? MRR 0.0 \u2013 1.0 - How early is first relevant? NDCG @ K 0.0 \u2013 1.0 - Is ranking optimal? Precision @ K 0.0 \u2013 1.0 - Are results mostly relevant? Recall @ K 0.0 \u2013 1.0 - Did we find all relevant?"},{"location":"metric-registry/heuristic/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import (\n    ExactStringMatch,\n    LevenshteinRatio,\n    LengthConstraint,\n    PIILeakageHeuristic,\n    HitRateAtK,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import Dataset\n\n# Initialize metrics\nmetrics = [\n    ExactStringMatch(),\n    LevenshteinRatio(case_sensitive=False),\n    LengthConstraint(max_chars=1000, sentence_range=(1, 5)),\n    PIILeakageHeuristic(confidence_threshold=0.7),\n    HitRateAtK(k=10),\n]\n\n# Run evaluation\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run(dataset)\n\n# Analyze results\nfor item in results:\n    print(f\"Exact Match: {item.scores.get('exact_string_match', 'N/A')}\")\n    print(f\"Similarity: {item.scores.get('levenshtein_ratio', 'N/A'):.2f}\")\n    print(f\"Privacy Safe: {item.scores.get('pii_leakage_heuristic', 'N/A'):.2f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/#choosing-the-right-metrics","title":"Choosing the Right Metrics","text":"<p>Evaluation Strategy</p> <p>For Exact Outputs (Code, JSON, IDs):</p> <ul> <li>Use Exact String Match for strict equality</li> <li>Add Contains Match for partial verification</li> </ul> <p>For Natural Language:</p> <ul> <li>Use Levenshtein Ratio for typo/variation tolerance</li> <li>Use Sentence BLEU for paraphrase comparison</li> </ul> <p>For Privacy &amp; Compliance:</p> <ul> <li>Use PII Leakage (Heuristic) for fast screening</li> <li>Add Citation Presence for source attribution</li> </ul> <p>For Output Length/Format:</p> <ul> <li>Use Length Constraint for character and sentence limits</li> </ul> <p>For Search/Retrieval:</p> <ul> <li>Use Hit Rate for quick sanity checks</li> <li>Use NDCG for comprehensive ranking evaluation</li> <li>Use Precision/Recall for classic IR metrics</li> </ul>"},{"location":"metric-registry/heuristic/#why-heuristic-metrics","title":"Why Heuristic Metrics?","text":"\u26a1 Instant Results <p>No LLM calls needed\u2014microsecond latency.</p> \ud83d\udcb0 Zero Cost <p>No API costs or token usage.</p> \ud83d\udd04 Deterministic <p>Same input always produces same output.</p> \ud83d\udcc8 Scalable <p>Evaluate millions of items without limits.</p>"},{"location":"metric-registry/heuristic/citation_presence/","title":"Citation Presence","text":"Verify responses include properly formatted citations Heuristic Knowledge Multi-Turn"},{"location":"metric-registry/heuristic/citation_presence/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> Optional: conversation <p>What It Measures</p> <p>Citation Presence evaluates whether AI responses include properly formatted citations\u2014URLs, DOIs, or academic references. It supports both single-turn responses and multi-turn conversations.</p> Score Interpretation 1.0  Citations present (in at least one message) 0.0  No citations found \u2705 Use When <ul> <li>Requiring sourced responses</li> <li>Building research assistants</li> <li>Enforcing citation policies</li> <li>Validating knowledge retrieval</li> </ul> \u274c Don't Use When <ul> <li>Citations aren't required</li> <li>Checking citation accuracy (use Faithfulness)</li> <li>Creative/generative tasks</li> <li>Simple Q&amp;A without sources</li> </ul> <p>Citation Presence vs Faithfulness</p> <p>Citation Presence checks: \"Are citations included?\" Faithfulness checks: \"Is the content accurate to the source?\"</p> <p>Use Citation Presence for format compliance; use Faithfulness for content verification.</p> How It Works  Computation Detected Citation Formats Evaluation Modes <p>The metric extracts citations using regex patterns and evaluates based on the configured mode.</p> Format Pattern Example HTTP/HTTPS URLs <code>https?://...</code> <code>https://docs.python.org/3/</code> WWW URLs <code>www.domain.com</code> <code>www.wikipedia.org</code> DOI References <code>doi:10.xxxx/...</code> <code>doi:10.1000/xyz123</code> Academic <code>(Author, Year)</code> <code>(Smith et al., 2023)</code> <p> any_citation (default) Pass if any citation appears anywhere in the response. </p> <p> resource_section Pass only if citations appear in a dedicated Resources/References section. </p>"},{"location":"metric-registry/heuristic/citation_presence/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Response Text]\n        B[Mode Setting]\n    end\n\n    subgraph EXTRACT[\"\ud83d\udd0d Step 1: Extract Citations\"]\n        C[Run citation patterns]\n        D1[\"HTTP/HTTPS URLs\"]\n        D2[\"DOI references\"]\n        D3[\"Academic citations\"]\n    end\n\n    subgraph EVALUATE[\"\u2696\ufe0f Step 2: Mode-Based Evaluation\"]\n        E{Mode?}\n        F[\"any_citation: Any URL/DOI found?\"]\n        G[\"resource_section: Section with citations?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        H[\"1.0 = Pass\"]\n        I[\"0.0 = Fail\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3\n    D1 &amp; D2 &amp; D3 --&gt; E\n    E --&gt;|any_citation| F\n    E --&gt;|resource_section| G\n    F &amp; G --&gt;|Yes| H\n    F &amp; G --&gt;|No| I\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style EXTRACT stroke:#3b82f6,stroke-width:2px\n    style EVALUATE stroke:#8b5cf6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>mode</code> <code>str</code> <code>any_citation</code> Evaluation mode: <code>any_citation</code> or <code>resource_section</code> <code>strict</code> <code>bool</code> <code>False</code> If True, validates URLs are live <code>use_semantic_search</code> <code>bool</code> <code>False</code> Use embeddings for fallback detection <code>embed_model</code> <code>EmbeddingRunnable</code> <code>None</code> Embedding model (required if semantic search enabled) <code>resource_similarity_threshold</code> <code>float</code> <code>0.8</code> Threshold for semantic matching <code>custom_resource_phrases</code> <code>List[str]</code> <code>None</code> Custom phrases to identify resource sections <p>Strict Mode</p> <p>When <code>strict=True</code>, the metric validates that URLs are live by making HEAD requests. This ensures citations point to actual resources but adds latency.</p>"},{"location":"metric-registry/heuristic/citation_presence/#code-examples","title":"Code Examples","text":"Basic Usage No Citations Resource Section Mode Multi-Turn Conversation <pre><code>from axion.metrics import CitationPresence\nfrom axion.dataset import DatasetItem\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"Python is a programming language. Learn more at https://python.org\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - URL citation found\n</code></pre> <pre><code>from axion.metrics import CitationPresence\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"Python is a great programming language for beginners.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - no citations\nprint(result.explanation)\n# \"Mode: any_citation. FAILURE: No assistant message satisfied the citation requirement.\"\n</code></pre> <pre><code>from axion.metrics import CitationPresence\n\n# Require citations in a dedicated section\nmetric = CitationPresence(mode='resource_section')\n\nitem = DatasetItem(\n    actual_output=\"\"\"\n    Python is versatile and beginner-friendly.\n\n    For More Information:\n    - https://docs.python.org/3/\n    - https://realpython.com/\n    \"\"\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - resource section with citations\n</code></pre> <pre><code>from axion.metrics import CitationPresence\nfrom axion._core.schema import Conversation, HumanMessage, AIMessage\n\nmetric = CitationPresence()\n\nitem = DatasetItem(\n    actual_output=\"\",  # Will check conversation instead\n    conversation=Conversation(messages=[\n        HumanMessage(content=\"What is Python?\"),\n        AIMessage(content=\"Python is a programming language.\"),\n        HumanMessage(content=\"Where can I learn more?\"),\n        AIMessage(content=\"Check out https://python.org and https://realpython.com\"),\n    ]),\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - citation in second AI message\nprint(result.signals.messages_with_citations)  # [3] (index of 2nd AI message)\n</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code>.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca CitationPresenceResult Structure <pre><code>CitationPresenceResult(\n{\n    \"passes_presence_check\": True,\n    \"total_assistant_messages\": 2,\n    \"messages_with_citations\": [3]  # 0-indexed message positions\n}\n)\n</code></pre>"},{"location":"metric-registry/heuristic/citation_presence/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>passes_presence_check</code> <code>bool</code> Whether citation requirement was met <code>total_assistant_messages</code> <code>int</code> Number of AI messages evaluated <code>messages_with_citations</code> <code>List[int]</code> Indices of messages with valid citations"},{"location":"metric-registry/heuristic/citation_presence/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: URL Citation (Score: 1.0) <p>HTTP URL Found</p> <p>Output:</p> <p>\"Machine learning is a subset of AI. See https://scikit-learn.org for tutorials.\"</p> <p>Citations Detected: <code>https://scikit-learn.org</code></p> <p>Final Score: <code>1.0</code> </p> \u2705 Scenario 2: Academic Citation (Score: 1.0) <p>Author-Year Format</p> <p>Output:</p> <p>\"Attention mechanisms transformed NLP (Vaswani et al., 2017).\"</p> <p>Citations Detected: <code>(Vaswani et al., 2017)</code></p> <p>Final Score: <code>1.0</code> </p> \u274c Scenario 3: No Citations (Score: 0.0) <p>Missing Citations</p> <p>Output:</p> <p>\"Deep learning uses neural networks with multiple layers to process data.\"</p> <p>Citations Detected: None</p> <p>Final Score: <code>0.0</code> </p> \u26a0\ufe0f Scenario 4: Resource Section Required <p>Wrong Mode</p> <p>Mode: <code>resource_section</code></p> <p>Output:</p> <p>\"Python documentation is at https://python.org which explains everything.\"</p> <p>Analysis: URL exists but not in a resource section.</p> <p>Final Score: <code>0.0</code> </p> <p>Switch to <code>any_citation</code> mode or add a Resources section.</p>"},{"location":"metric-registry/heuristic/citation_presence/#why-it-matters","title":"Why It Matters","text":"\ud83d\udcda Source Attribution <p>Ensure AI outputs provide proper attribution to sources.</p> \ud83c\udf93 Research Quality <p>Enforce citation standards for academic or research applications.</p> \u2705 Policy Compliance <p>Verify responses meet organizational citation requirements.</p>"},{"location":"metric-registry/heuristic/citation_presence/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Citation Presence = Does the response include citations?</p> <ul> <li>Use it when: Requiring sourced responses or research assistants</li> <li>Score interpretation: 1.0 = citations found, 0.0 = none</li> <li>Key config: <code>mode</code> determines where citations must appear</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.CitationPresence</code></p> </li> <li> <p> Related Metrics</p> <p> Faithfulness \u00b7 Contextual Relevancy \u00b7 Answer Relevancy</p> </li> </ul>"},{"location":"metric-registry/heuristic/contains_match/","title":"Contains Match","text":"Check if output contains the expected text as a substring Heuristic Single Turn Binary"},{"location":"metric-registry/heuristic/contains_match/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Substring to find <p>What It Measures</p> <p>Contains Match checks whether the expected output appears as a substring within the actual output. This is the simplest form of text matching\u2014does the response include the required content anywhere?</p> Score Interpretation 1.0  Expected text found in output 0.0  Expected text not found \u2705 Use When <ul> <li>Checking for required keywords/phrases</li> <li>Validating specific content inclusion</li> <li>Simple pass/fail tests</li> <li>Fast sanity checks</li> </ul> \u274c Don't Use When <ul> <li>Exact match required (use Exact String Match)</li> <li>Similarity scoring needed (use BLEU/Levenshtein)</li> <li>Case variations matter</li> <li>Semantic matching needed</li> </ul> <p>See Also: Exact String Match</p> <p>Contains Match checks if expected is a substring of actual. Exact String Match checks if they are identical.</p> How It Works  Computation Logic <p>Simple substring check after stripping whitespace.</p> <pre><code>score = 1.0 if expected.strip() in actual.strip() else 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Strip whitespace]\n        D[\"Check: expected in actual?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        E[\"1.0 = Found\"]\n        F[\"0.0 = Not Found\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt;|Yes| E\n    D --&gt;|No| F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description (none) - - No configuration options <p>Simple by Design</p> <p>Contains Match is intentionally simple with no configuration. For case-insensitive matching or fuzzy matching, use other metrics.</p>"},{"location":"metric-registry/heuristic/contains_match/#code-examples","title":"Code Examples","text":"Basic Usage No Match Example With Runner <pre><code>from axion.metrics import ContainsMatch\nfrom axion.dataset import DatasetItem\n\nmetric = ContainsMatch()\n\nitem = DatasetItem(\n    actual_output=\"The capital of France is Paris, a beautiful city.\",\n    expected_output=\"Paris\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - \"Paris\" is in the output\n</code></pre> <pre><code>from axion.metrics import ContainsMatch\n\nmetric = ContainsMatch()\n\nitem = DatasetItem(\n    actual_output=\"The capital of France is a beautiful city.\",\n    expected_output=\"Paris\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - \"Paris\" not found\n</code></pre> <pre><code>from axion.metrics import ContainsMatch\nfrom axion.runners import MetricRunner\n\nmetric = ContainsMatch()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\npassed = sum(1 for r in results if r.score == 1.0)\nprint(f\"Passed: {passed}/{len(results)}\")\n</code></pre>"},{"location":"metric-registry/heuristic/contains_match/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Substring Found (Score: 1.0) <p>Match Found</p> <p>Expected: <code>\"42\"</code></p> <p>Actual: <code>\"The answer to life, the universe, and everything is 42.\"</code></p> <p>Result: <code>1.0</code> </p> <p>The substring \"42\" exists in the output.</p> \u274c Scenario 2: Substring Not Found (Score: 0.0) <p>No Match</p> <p>Expected: <code>\"Python\"</code></p> <p>Actual: <code>\"JavaScript is a popular programming language.\"</code></p> <p>Result: <code>0.0</code> </p> <p>\"Python\" does not appear in the output.</p> \u26a0\ufe0f Scenario 3: Case Sensitivity <p>Case Matters</p> <p>Expected: <code>\"PARIS\"</code></p> <p>Actual: <code>\"The capital is Paris.\"</code></p> <p>Result: <code>0.0</code> </p> <p>\"PARIS\" (uppercase) does not match \"Paris\" (title case).</p>"},{"location":"metric-registry/heuristic/contains_match/#why-it-matters","title":"Why It Matters","text":"\u26a1 Instant Results <p>O(n) substring search. No external dependencies or LLM calls.</p> \u2705 Sanity Checks <p>Quickly verify required content is present in responses.</p> \ud83d\udd0d Keyword Validation <p>Ensure specific terms, codes, or phrases appear in output.</p>"},{"location":"metric-registry/heuristic/contains_match/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Contains Match = Does the output contain the expected text?</p> <ul> <li>Use it when: Checking for required keywords or phrases</li> <li>Score interpretation: 1.0 = found, 0.0 = not found</li> <li>Key behavior: Case-sensitive substring match</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ContainsMatch</code></p> </li> <li> <p> Related Metrics</p> <p> Exact String Match \u00b7 Sentence BLEU \u00b7 Levenshtein Ratio</p> </li> </ul>"},{"location":"metric-registry/heuristic/exact_string_match/","title":"Exact String Match","text":"Check if output exactly matches expected text Heuristic Single Turn Binary"},{"location":"metric-registry/heuristic/exact_string_match/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Full text comparison <p>What It Measures</p> <p>Exact String Match checks whether the actual output is identical to the expected output (after stripping whitespace). This is the strictest form of text matching\u2014the response must be exactly what was expected.</p> Score Interpretation 1.0  Perfect match\u2014strings are identical 0.0  No match\u2014any difference fails \u2705 Use When <ul> <li>Exact output format is required</li> <li>Testing deterministic transformations</li> <li>Validating code generation</li> <li>Comparing structured outputs (JSON, XML)</li> </ul> \u274c Don't Use When <ul> <li>Minor variations are acceptable</li> <li>Semantic equivalence matters more</li> <li>Paraphrasing is allowed</li> <li>Case differences should be ignored</li> </ul> <p>See Also: Contains Match</p> <p>Exact String Match checks if actual equals expected. Contains Match checks if expected is a substring of actual.</p> <p>Use Exact Match when precision matters; use Contains Match for keyword validation.</p> How It Works  Computation Logic <p>Simple string equality check after stripping whitespace.</p> <pre><code>score = 1.0 if actual.strip() == expected.strip() else 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Strip whitespace]\n        D[\"Check: actual == expected?\"]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        E[\"1.0 = Match\"]\n        F[\"0.0 = No Match\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt;|Yes| E\n    D --&gt;|No| F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description (none) - - No configuration options <p>Simple by Design</p> <p>Exact String Match is intentionally simple with no configuration. For case-insensitive or fuzzy matching, use Levenshtein Ratio.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#code-examples","title":"Code Examples","text":"Basic Usage No Match Example With Runner <pre><code>from axion.metrics import ExactStringMatch\nfrom axion.dataset import DatasetItem\n\nmetric = ExactStringMatch()\n\nitem = DatasetItem(\n    actual_output=\"Hello, World!\",\n    expected_output=\"Hello, World!\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - exact match\n</code></pre> <pre><code>from axion.metrics import ExactStringMatch\n\nmetric = ExactStringMatch()\n\nitem = DatasetItem(\n    actual_output=\"Hello, world!\",  # lowercase 'w'\n    expected_output=\"Hello, World!\",  # uppercase 'W'\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - case mismatch\n</code></pre> <pre><code>from axion.metrics import ExactStringMatch\nfrom axion.runners import MetricRunner\n\nmetric = ExactStringMatch()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nexact_matches = sum(1 for r in results if r.score == 1.0)\nprint(f\"Exact matches: {exact_matches}/{len(results)}\")\n</code></pre>"},{"location":"metric-registry/heuristic/exact_string_match/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>Identical Strings</p> <p>Expected: <code>\"The answer is 42.\"</code></p> <p>Actual: <code>\"The answer is 42.\"</code></p> <p>Result: <code>1.0</code> </p> <p>Strings are character-for-character identical.</p> \u274c Scenario 2: Case Mismatch (Score: 0.0) <p>Different Case</p> <p>Expected: <code>\"PASS\"</code></p> <p>Actual: <code>\"Pass\"</code></p> <p>Result: <code>0.0</code> </p> <p>Case sensitivity causes failure.</p> \u26a0\ufe0f Scenario 3: Whitespace Handling <p>Leading/Trailing Whitespace</p> <p>Expected: <code>\"  Hello  \"</code></p> <p>Actual: <code>\"Hello\"</code></p> <p>Result: <code>1.0</code> </p> <p>Whitespace is stripped before comparison, so these match.</p> \u274c Scenario 4: Extra Content (Score: 0.0) <p>Additional Text</p> <p>Expected: <code>\"Paris\"</code></p> <p>Actual: <code>\"The answer is Paris.\"</code></p> <p>Result: <code>0.0</code> </p> <p>Even containing the expected text, the strings aren't equal.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#why-it-matters","title":"Why It Matters","text":"\u26a1 Instant Results <p>O(n) string comparison. No external dependencies or LLM calls.</p> \ud83c\udfaf Maximum Precision <p>No false positives\u2014only identical strings pass.</p> \ud83d\udd27 Deterministic Testing <p>Perfect for testing deterministic transformations and code generation.</p>"},{"location":"metric-registry/heuristic/exact_string_match/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Exact String Match = Is the output identical to the expected text?</p> <ul> <li>Use it when: Exact output format is required (code, JSON, IDs)</li> <li>Score interpretation: 1.0 = identical, 0.0 = any difference</li> <li>Key behavior: Case-sensitive, whitespace-trimmed</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ExactStringMatch</code></p> </li> <li> <p> Related Metrics</p> <p> Contains Match \u00b7 Levenshtein Ratio \u00b7 Sentence BLEU</p> </li> </ul>"},{"location":"metric-registry/heuristic/latency/","title":"Latency","text":"Measure and evaluate execution time performance Heuristic Single Turn Performance"},{"location":"metric-registry/heuristic/latency/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>\u221e</code> Seconds (or normalized 0-1) \u26a1 Default Threshold <code>5.0s</code> Target latency \ud83d\udccb Required Inputs <code>latency</code> Execution time in seconds <p>What It Measures</p> <p>The Latency metric evaluates execution time performance. It can return raw latency values or normalize them to a 0-1 scale using various decay functions.</p> Mode Score Interpretation Raw Actual latency in seconds (lower is better) Normalized 0.0-1.0 where 1.0 = instant, 0.0 = very slow \u2705 Use When <ul> <li>Monitoring response times</li> <li>SLA compliance checking</li> <li>Performance regression testing</li> <li>Comparing model latencies</li> </ul> \u274c Don't Use When <ul> <li>Quality metrics are more important</li> <li>Latency isn't being tracked</li> <li>Network conditions are highly variable</li> <li>Cold start effects dominate</li> </ul> <p>Inverse Scoring</p> <p>Unlike most metrics where higher is better, lower latency is better. The metric is marked as <code>inverse_scoring_metric = True</code> for proper aggregation.</p> How It Works  Computation Normalization Methods <p>The metric reads the latency value and optionally normalizes it.</p> <p>Four normalization methods convert raw latency to a 0-1 score:</p> Method Formula Characteristics exponential <code>exp(-latency/threshold)</code> Smooth decay, never reaches 0 sigmoid <code>1/(1 + exp((latency-threshold)/scale))</code> S-curve centered at threshold reciprocal <code>threshold/(threshold + latency)</code> Hyperbolic decay linear <code>max(0, 1 - latency/threshold)</code> Linear drop to 0 <p> \ud83d\udcc8 Exponential Smooth decay. At threshold: ~0.37 </p> <p> \ud83d\udcc9 Sigmoid S-curve. At threshold: 0.5 </p> <p> \ud83d\udcca Reciprocal Hyperbolic. At threshold: 0.5 </p> <p> \ud83d\udcd0 Linear Simple. At threshold: 0.0 </p>"},{"location":"metric-registry/heuristic/latency/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Latency Value]\n        B[Threshold Setting]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C{Normalize?}\n        D[Return raw latency]\n        E[Apply normalization function]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"Raw: seconds\"]\n        G[\"Normalized: 0.0-1.0\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt;|No| D\n    C --&gt;|Yes| E\n    D --&gt; F\n    E --&gt; G\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/latency/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>threshold</code> <code>float</code> <code>5.0</code> Target latency in seconds <code>normalize</code> <code>bool</code> <code>False</code> Whether to normalize to 0-1 range <code>normalization_method</code> <code>str</code> <code>exponential</code> Method: exponential, sigmoid, reciprocal, linear <p>Choosing a Normalization Method</p> <ul> <li>exponential: Good default, smooth decay</li> <li>sigmoid: Hard cutoff around threshold</li> <li>reciprocal: Balanced decay, never hits 0</li> <li>linear: Simple, goes to 0 at threshold</li> </ul>"},{"location":"metric-registry/heuristic/latency/#code-examples","title":"Code Examples","text":"Basic Usage (Raw) Normalized Scoring With Runner <pre><code>from axion.metrics import Latency\nfrom axion.dataset import DatasetItem\n\nmetric = Latency(threshold=2.0)\n\nitem = DatasetItem(\n    query=\"What is the capital of France?\",\n    actual_output=\"Paris\",\n    latency=1.5,  # 1.5 seconds\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.5 (raw latency)\nprint(result.explanation)\n# \"Raw latency: 1.500s, below threshold (2.0s).\"\n</code></pre> <pre><code>from axion.metrics import Latency\n\n# Exponential normalization\nmetric = Latency(\n    threshold=2.0,\n    normalize=True,\n    normalization_method='exponential'\n)\n\nitem = DatasetItem(latency=1.0)  # 1 second\nresult = await metric.execute(item)\nprint(f\"{result.score:.3f}\")  # ~0.607 (exp(-1/2))\n\n# Linear normalization\nmetric_linear = Latency(\n    threshold=2.0,\n    normalize=True,\n    normalization_method='linear'\n)\nresult_linear = await metric_linear.execute(item)\nprint(f\"{result_linear.score:.3f}\")  # 0.5 (1 - 1/2)\n</code></pre> <pre><code>from axion.metrics import Latency\nfrom axion.runners import MetricRunner\n\nmetric = Latency(threshold=3.0, normalize=True)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Latency score: {item_result.score:.2f}\")\n    print(f\"  {item_result.explanation}\")\n</code></pre>"},{"location":"metric-registry/heuristic/latency/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Excellent Performance <p>Below Half Threshold</p> <p>Threshold: 5.0s</p> <p>Latency: 2.0s (40% of threshold)</p> <p>Raw Score: <code>2.0</code></p> <p>Normalized (exponential): <code>~0.67</code> </p> <p>Explanation: \"Latency: 2.000s. Normalized score: 0.670 (threshold: 5.0s, method: exponential). Performance: excellent.\"</p> \u26a0\ufe0f Scenario 2: At Threshold <p>Exactly at Target</p> <p>Threshold: 5.0s</p> <p>Latency: 5.0s</p> <p>Raw Score: <code>5.0</code></p> <p>Normalized Scores:</p> Method Score exponential 0.37 sigmoid 0.50 reciprocal 0.50 linear 0.00 \u274c Scenario 3: Poor Performance <p>Above Threshold</p> <p>Threshold: 2.0s</p> <p>Latency: 8.0s (4x threshold)</p> <p>Raw Score: <code>8.0</code></p> <p>Normalized (exponential): <code>~0.02</code> </p> <p>Explanation: \"Latency: 8.000s. Normalized score: 0.018 (threshold: 2.0s, method: exponential). Performance: poor.\"</p>"},{"location":"metric-registry/heuristic/latency/#why-it-matters","title":"Why It Matters","text":"\u23f1\ufe0f SLA Compliance <p>Track response times against service level agreements.</p> \ud83d\udcc8 Performance Monitoring <p>Detect regressions and optimize slow endpoints.</p> \u2696\ufe0f Quality vs Speed <p>Balance model quality against response time requirements.</p>"},{"location":"metric-registry/heuristic/latency/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Latency = How fast was the response?</p> <ul> <li>Use it when: Monitoring performance or SLA compliance</li> <li>Score interpretation: Raw (seconds) or normalized (0-1, higher = faster)</li> <li>Key config: <code>threshold</code> sets target, <code>normalize</code> enables 0-1 scoring</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.Latency</code></p> </li> <li> <p> Related Concepts</p> <p> MetricRunner \u00b7 Evaluation Strategies</p> </li> </ul>"},{"location":"metric-registry/heuristic/length_constraint/","title":"Length Constraint","text":"Verify response length meets character and sentence constraints Heuristic Single Turn Binary"},{"location":"metric-registry/heuristic/length_constraint/#at-a-glance","title":"At a Glance","text":"\ud83d\udccf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \u26a1 Default Threshold <code>1.0</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> Text to evaluate <p>What It Measures</p> <p>Length Constraint verifies that the response meets character and/or sentence count constraints. This is useful for enforcing output limits in chatbots, summaries, or any application with length requirements.</p> Score Interpretation 1.0  Passed\u2014within all constraints 0.0  Failed\u2014exceeded character limit or outside sentence range Use When <ul> <li>Enforcing token/character limits for LLM outputs</li> <li>Validating summary length requirements</li> <li>Ensuring concise responses in chatbots</li> <li>Meeting UI display constraints</li> </ul> Don't Use When <ul> <li>Quality matters more than length</li> <li>Variable-length outputs are acceptable</li> <li>Sentence structure is irregular (code, lists)</li> </ul> How It Works  Computation Logic <p>Checks character count against <code>max_chars</code> and sentence count against <code>sentence_range</code>.</p> <pre><code># Character check\nchar_passed = len(text) &lt;= max_chars\n\n# Sentence check (splits on .!? followed by space or end)\nsentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\nsentence_count = len(sentences)\n\nmin_s, max_s = sentence_range\nsentence_passed = (min_s is None or sentence_count &gt;= min_s) and \\\n                  (max_s is None or sentence_count &lt;= max_s)\n\nscore = 1.0 if (char_passed and sentence_passed) else 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/length_constraint/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"Input\"]\n        A[Actual Output]\n    end\n\n    subgraph PROCESS[\"Processing\"]\n        B[Count characters]\n        C[Split into sentences]\n        D[\"Check: chars &lt;= max_chars?\"]\n        E[\"Check: sentence_count in range?\"]\n    end\n\n    subgraph OUTPUT[\"Result\"]\n        F[\"1.0 = All constraints met\"]\n        G[\"0.0 = Constraint violated\"]\n    end\n\n    A --&gt; B\n    A --&gt; C\n    B --&gt; D\n    C --&gt; E\n    D &amp; E --&gt;|All pass| F\n    D &amp; E --&gt;|Any fail| G\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/length_constraint/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>max_chars</code> <code>int</code> <code>2800</code> Maximum allowed characters. Set to <code>None</code> to disable. <code>sentence_range</code> <code>tuple</code> <code>None</code> <code>(min, max)</code> sentence bounds. Use <code>None</code> for open ends. <code>field_mapping</code> <code>dict</code> <code>None</code> Remap <code>actual_output</code> to another field path. <p>Flexible Sentence Ranges</p> <ul> <li><code>(3, 5)</code> \u2014 Between 3 and 5 sentences (inclusive)</li> <li><code>(None, 5)</code> \u2014 At most 5 sentences</li> <li><code>(3, None)</code> \u2014 At least 3 sentences</li> </ul>"},{"location":"metric-registry/heuristic/length_constraint/#code-examples","title":"Code Examples","text":"Basic Usage With Sentence Range Constraint Violation Field Mapping With Runner <pre><code>from axion.metrics import LengthConstraint\nfrom axion.dataset import DatasetItem\n\n# Default: max 2800 characters, no sentence constraint\nmetric = LengthConstraint()\n\nitem = DatasetItem(\n    actual_output=\"This is a short response.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - within limits\nprint(result.signals.char_count)  # 26\n</code></pre> <pre><code>from axion.metrics import LengthConstraint\n\n# Require 2-4 sentences, max 500 characters\nmetric = LengthConstraint(\n    max_chars=500,\n    sentence_range=(2, 4),\n)\n\nitem = DatasetItem(\n    actual_output=\"First sentence. Second sentence. Third sentence.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - 3 sentences, 48 chars\nprint(result.signals.sentence_count)  # 3\n</code></pre> <pre><code>from axion.metrics import LengthConstraint\n\nmetric = LengthConstraint(\n    max_chars=50,\n    sentence_range=(None, 2),  # max 2 sentences\n)\n\nitem = DatasetItem(\n    actual_output=\"First. Second. Third. Fourth. Fifth sentence here.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 0.0 - too many sentences\nprint(result.explanation)\n# \"FAILED. Sentence count 5 outside (Range: 0-2).\"\n</code></pre> <pre><code>from axion.metrics import LengthConstraint\n\n# Evaluate a nested field instead of actual_output\nmetric = LengthConstraint(\n    max_chars=1000,\n    field_mapping={'actual_output': 'additional_output.summary'},\n)\n\nitem = DatasetItem(\n    additional_output={'summary': 'A brief summary of the document.'},\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0\n</code></pre> <pre><code>from axion.metrics import LengthConstraint\nfrom axion.runners import MetricRunner\n\nmetric = LengthConstraint(max_chars=1000, sentence_range=(1, 5))\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\npassed = sum(1 for r in results if r.score == 1.0)\nprint(f\"Passed length constraints: {passed}/{len(results)}\")\n</code></pre>"},{"location":"metric-registry/heuristic/length_constraint/#example-scenarios","title":"Example Scenarios","text":"Scenario 1: Within All Constraints (Score: 1.0) <p>Passed</p> <p>Config: <code>max_chars=100</code>, <code>sentence_range=(1, 3)</code></p> <p>Actual Output: <code>\"Hello there. How can I help you today?\"</code></p> <p>Result: <code>1.0</code> </p> <ul> <li>Characters: 39 (within 100)</li> <li>Sentences: 2 (within 1-3 range)</li> </ul> Scenario 2: Character Limit Exceeded (Score: 0.0) <p>Failed - Too Long</p> <p>Config: <code>max_chars=50</code></p> <p>Actual Output: <code>\"This response is definitely going to exceed the maximum character limit that was set.\"</code></p> <p>Result: <code>0.0</code> </p> <p>Explanation: <code>\"FAILED. Exceeded chars (85/50).\"</code></p> Scenario 3: Too Few Sentences (Score: 0.0) <p>Failed - Below Minimum</p> <p>Config: <code>sentence_range=(3, None)</code> (at least 3 sentences)</p> <p>Actual Output: <code>\"Just one sentence.\"</code></p> <p>Result: <code>0.0</code> </p> <p>Explanation: <code>\"FAILED. Sentence count 1 outside (Range: 3-inf).\"</code></p> Scenario 4: Too Many Sentences (Score: 0.0) <p>Failed - Above Maximum</p> <p>Config: <code>sentence_range=(None, 2)</code> (at most 2 sentences)</p> <p>Actual Output: <code>\"First point. Second point. Third point. And more!\"</code></p> <p>Result: <code>0.0</code> </p> <p>Explanation: <code>\"FAILED. Sentence count 4 outside (Range: 0-2).\"</code></p>"},{"location":"metric-registry/heuristic/length_constraint/#signal-details","title":"Signal Details","text":"<p>The metric returns a <code>LengthResult</code> object with detailed information:</p> Signal Type Description <code>char_count</code> <code>int</code> Total characters in the output <code>max_chars_allowed</code> <code>int</code> Configured maximum (or <code>None</code> if disabled) <code>sentence_count</code> <code>int</code> Number of sentences detected <code>sentence_range</code> <code>tuple</code> Configured <code>(min, max)</code> range <code>passed</code> <code>bool</code> Whether all constraints were met"},{"location":"metric-registry/heuristic/length_constraint/#why-it-matters","title":"Why It Matters","text":"\u26a1 Instant Results <p>O(n) computation. No LLM calls required.</p> \ud83d\udcf1 UI Compliance <p>Ensure outputs fit display constraints and character limits.</p> \ud83d\udcb0 Cost Control <p>Prevent overly verbose outputs that increase token costs.</p> \ud83c\udfaf Conciseness <p>Enforce brevity requirements for summaries and chatbots.</p>"},{"location":"metric-registry/heuristic/length_constraint/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Length Constraint = Does the output meet character and sentence limits?</p> <ul> <li>Use it when: Enforcing length limits for UI, cost, or conciseness</li> <li>Score interpretation: 1.0 = all constraints met, 0.0 = any violation</li> <li>Key behavior: Character counting + regex-based sentence detection</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.LengthConstraint</code></p> </li> <li> <p> Related Metrics</p> <p> Latency - Performance metrics</p> </li> </ul>"},{"location":"metric-registry/heuristic/levenshtein_ratio/","title":"Levenshtein Ratio","text":"Calculate character-level string similarity Heuristic Single Turn Fast"},{"location":"metric-registry/heuristic/levenshtein_ratio/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Character-level similarity \u26a1 Default Threshold <code>0.2</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Text comparison <p>What It Measures</p> <p>Levenshtein Ratio calculates the character-level similarity between two strings using the SequenceMatcher algorithm. It measures how many edits (insertions, deletions, substitutions) are needed to transform one string into another.</p> Score Interpretation 1.0  Identical strings 0.8+  Very similar, minor typos 0.5-0.8  Moderate similarity &lt; 0.5  Significant differences \u2705 Use When <ul> <li>Checking for typos or small variations</li> <li>Fuzzy string matching needed</li> <li>Comparing names or identifiers</li> <li>Near-match detection</li> </ul> \u274c Don't Use When <ul> <li>Semantic similarity matters</li> <li>Word-level comparison preferred (use BLEU)</li> <li>Long texts with different structures</li> <li>Exact match required</li> </ul> <p>See Also: Sentence BLEU</p> <p>Levenshtein Ratio measures character-level edit distance. Sentence BLEU measures word-level n-gram precision.</p> <p>Use Levenshtein for typo detection; use BLEU for paraphrase comparison.</p> How It Works  Computation Formula <p>Uses Python's <code>SequenceMatcher</code> to calculate the ratio of matching characters.</p> <p>The SequenceMatcher ratio is calculated as:</p> <pre><code>ratio = 2.0 * M / T\n\nwhere:\nM = number of matches (characters in common)\nT = total number of characters in both strings\n</code></pre> <p>Example: <pre><code>String 1: \"hello\"\nString 2: \"hallo\"\n\nMatches: h, l, l, o = 4 characters match\nTotal: 5 + 5 = 10 characters\nRatio: 2.0 * 4 / 10 = 0.8\n</code></pre></p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Actual Output]\n        B[Expected Output]\n    end\n\n    subgraph PROCESS[\"\ud83d\udd0d Processing\"]\n        C[Optional: Convert to lowercase]\n        D[Find matching subsequences]\n        E[Calculate similarity ratio]\n    end\n\n    subgraph OUTPUT[\"\ud83d\udcca Result\"]\n        F[\"Score: 0.0 to 1.0\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style PROCESS stroke:#3b82f6,stroke-width:2px\n    style OUTPUT stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>case_sensitive</code> <code>bool</code> <code>False</code> Whether comparison is case-sensitive <p>Case Sensitivity</p> <p>By default, comparison is case-insensitive (both strings converted to lowercase). Set <code>case_sensitive=True</code> for strict character matching.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#code-examples","title":"Code Examples","text":"Basic Usage Case Sensitive With Runner <pre><code>from axion.metrics import LevenshteinRatio\nfrom axion.dataset import DatasetItem\n\nmetric = LevenshteinRatio()\n\nitem = DatasetItem(\n    actual_output=\"The quick brown fox\",\n    expected_output=\"The quick brown fox\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - identical strings\n</code></pre> <pre><code>from axion.metrics import LevenshteinRatio\n\n# Case insensitive (default)\nmetric = LevenshteinRatio(case_sensitive=False)\n\nitem = DatasetItem(\n    actual_output=\"HELLO\",\n    expected_output=\"hello\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - case ignored\n\n# Case sensitive\nmetric_strict = LevenshteinRatio(case_sensitive=True)\nresult_strict = await metric_strict.execute(item)\nprint(result_strict.score)  # 0.0 - case matters\n</code></pre> <pre><code>from axion.metrics import LevenshteinRatio\nfrom axion.runners import MetricRunner\n\nmetric = LevenshteinRatio()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\navg_similarity = sum(r.score for r in results) / len(results)\nprint(f\"Average similarity: {avg_similarity:.2%}\")\n</code></pre>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High Similarity (Score: 0.95) <p>Minor Typo</p> <p>Expected: <code>\"accommodation\"</code></p> <p>Actual: <code>\"accomodation\"</code> (missing 'm')</p> <p>Result: <code>~0.92</code> </p> <p>Single character difference results in high similarity.</p> \u26a0\ufe0f Scenario 2: Moderate Similarity (Score: 0.67) <p>Multiple Differences</p> <p>Expected: <code>\"Hello World\"</code></p> <p>Actual: <code>\"Helo Wrld\"</code> (missing letters)</p> <p>Result: <code>~0.67</code> </p> <p>Several missing characters reduce similarity.</p> \u274c Scenario 3: Low Similarity (Score: 0.2) <p>Very Different Strings</p> <p>Expected: <code>\"The quick brown fox\"</code></p> <p>Actual: <code>\"A lazy dog sleeps\"</code></p> <p>Result: <code>~0.2</code> </p> <p>Completely different content results in low similarity.</p> \u2705 Scenario 4: Case Handling <p>Case Insensitive Match</p> <p>Expected: <code>\"OpenAI\"</code></p> <p>Actual: <code>\"openai\"</code></p> <p>Result (default): <code>1.0</code> </p> <p>Result (case_sensitive=True): <code>~0.67</code> </p> <p>Case sensitivity significantly affects scoring.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Deterministic <p>No LLM calls needed. Instant, reproducible results.</p> \ud83d\udd24 Typo Detection <p>Perfect for detecting spelling errors and near-matches.</p> \ud83d\udcca Gradual Scoring <p>Unlike binary metrics, provides nuanced similarity scores.</p>"},{"location":"metric-registry/heuristic/levenshtein_ratio/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Levenshtein Ratio = How similar are the strings at the character level?</p> <ul> <li>Use it when: Checking for typos or fuzzy matching</li> <li>Score interpretation: Higher = more similar characters</li> <li>Key config: <code>case_sensitive</code> controls case handling</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.LevenshteinRatio</code></p> </li> <li> <p> Related Metrics</p> <p> Sentence BLEU \u00b7 Exact String Match \u00b7 Contains Match</p> </li> </ul>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/","title":"PII Leakage (Heuristic)","text":"Detect personally identifiable information using regex patterns Heuristic Single Turn Safety"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Privacy score (1.0 = safe) \u26a1 Default Threshold <code>0.8</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>query</code> <code>actual_output</code> Response to analyze <p>What It Measures</p> <p>PII Leakage (Heuristic) detects personally identifiable information in model outputs using regex patterns and validation rules. It identifies emails, phone numbers, SSNs, credit cards, addresses, and more\u2014without requiring LLM calls.</p> Score Interpretation 1.0  No PII detected\u2014output is safe 0.7-0.9  Low-risk PII (names, zip codes) 0.3-0.7  Medium-risk PII (emails, phones) &lt; 0.3  High-risk PII (SSN, credit cards) \u2705 Use When <ul> <li>Fast, deterministic PII detection needed</li> <li>Production monitoring at scale</li> <li>CI/CD safety gates</li> <li>High-throughput screening</li> </ul> \u274c Don't Use When <ul> <li>Context-aware detection required</li> <li>Non-standard PII formats exist</li> <li>Need semantic understanding</li> <li>International formats dominate</li> </ul> <p>Heuristic vs LLM-based PII Detection</p> <p>PII Leakage (Heuristic) uses regex patterns\u2014fast and deterministic. PII Leakage (LLM) uses language models\u2014slower but more context-aware.</p> <p>Use heuristic for high-throughput screening; use LLM-based for nuanced analysis.</p> How It Works  Computation Detected PII Types Score Calculation <p>The metric scans text using regex patterns, validates matches, and calculates a privacy score.</p> <p> \ud83d\udd34 High Risk <ul> <li>Social Security Numbers (SSN)</li> <li>Credit Card Numbers</li> <li>Passport Numbers</li> </ul> </p> <p> \ud83d\udfe1 Medium Risk <ul> <li>Email Addresses</li> <li>Phone Numbers</li> <li>Street Addresses</li> <li>Date of Birth</li> <li>Driver's License</li> </ul> </p> <p> \ud83d\udfe2 Low Risk <ul> <li>Person Names</li> <li>IP Addresses</li> <li>ZIP Codes</li> </ul> </p> <pre><code>penalty = \u03a3(severity \u00d7 confidence) for each detection\nscore = 1.0 - min(1.0, penalty)\n</code></pre> <p>Severity Weights:</p> PII Type Severity SSN 1.0 Credit Card 1.0 Passport 0.9 Date of Birth 0.8 Email 0.7 Phone 0.7 Street Address 0.6 Driver's License 0.6 Person Name 0.5 IP Address 0.3 ZIP Code 0.2"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Input\"]\n        A[Actual Output Text]\n    end\n\n    subgraph DETECT[\"\ud83d\udd0d Step 1: Pattern Detection\"]\n        B[Run regex patterns]\n        C1[\"Email patterns\"]\n        C2[\"Phone patterns\"]\n        C3[\"SSN patterns\"]\n        C4[\"Credit card patterns\"]\n        CN[\"More patterns...\"]\n    end\n\n    subgraph VALIDATE[\"\u2705 Step 2: Validation\"]\n        D[Validate matches]\n        E1[\"Luhn check for CC\"]\n        E2[\"SSN format check\"]\n        E3[\"IP address validation\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Scoring\"]\n        F[Apply severity weights]\n        G[Calculate penalty]\n        H[\"Privacy Score: 1.0 - penalty\"]\n    end\n\n    A --&gt; B\n    B --&gt; C1 &amp; C2 &amp; C3 &amp; C4 &amp; CN\n    C1 &amp; C2 &amp; C3 &amp; C4 &amp; CN --&gt; D\n    D --&gt; E1 &amp; E2 &amp; E3\n    E1 &amp; E2 &amp; E3 --&gt; F\n    F --&gt; G\n    G --&gt; H\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style DETECT stroke:#3b82f6,stroke-width:2px\n    style VALIDATE stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>confidence_threshold</code> <code>float</code> <code>0.6</code> Minimum confidence to count detection <p>Confidence Filtering</p> <p>Detections below the confidence threshold are ignored when calculating the final score. Higher thresholds reduce false positives but may miss some PII.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#code-examples","title":"Code Examples","text":"Basic Usage Detection Example Custom Threshold With Runner <pre><code>from axion.metrics import PIILeakageHeuristic\nfrom axion.dataset import DatasetItem\n\nmetric = PIILeakageHeuristic()\n\nitem = DatasetItem(\n    query=\"What's the weather today?\",\n    actual_output=\"The weather in New York is sunny and 72\u00b0F.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - no PII detected\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\n\nmetric = PIILeakageHeuristic()\n\nitem = DatasetItem(\n    query=\"Contact info?\",\n    actual_output=\"You can reach John Smith at john.smith@email.com or 555-123-4567.\",\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # ~0.3 - email and phone detected\nprint(result.explanation)\n# \"Detected 2 potential PII instances of types: email, phone_us.\"\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\n\n# Higher confidence threshold - fewer false positives\nmetric = PIILeakageHeuristic(confidence_threshold=0.8)\n\nitem = DatasetItem(\n    query=\"What is 123-45-6789?\",\n    actual_output=\"That looks like it could be a social security number format.\",\n)\n\nresult = await metric.execute(item)\n# Only high-confidence SSN detections will affect score\n</code></pre> <pre><code>from axion.metrics import PIILeakageHeuristic\nfrom axion.runners import MetricRunner\n\nmetric = PIILeakageHeuristic()\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\n# Flag outputs with potential PII\nfor item_result in results:\n    if item_result.score &lt; 0.8:\n        print(f\"PII detected: {item_result.explanation}\")\n        # Access detailed breakdown\n        if item_result.signals:\n            print(f\"High-risk: {item_result.signals.categorized_counts['high_risk']}\")\n            print(f\"Medium-risk: {item_result.signals.categorized_counts['medium_risk']}\")\n</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#metric-diagnostics","title":"Metric Diagnostics","text":"<p>Every evaluation is fully interpretable. Access detailed diagnostic results via <code>result.signals</code> to understand exactly what was detected.</p> <pre><code>result = await metric.execute(item)\nprint(result.pretty())      # Human-readable summary\nresult.signals              # Full diagnostic breakdown\n</code></pre> \ud83d\udcca PIIHeuristicResult Structure <pre><code>PIIHeuristicResult(\n{\n    \"final_score\": 0.3,\n    \"total_detections\": 3,\n    \"significant_detections_count\": 2,\n    \"confidence_threshold\": 0.6,\n    \"categorized_counts\": {\n        \"high_risk\": 0,\n        \"medium_risk\": 2,\n        \"low_risk\": 0\n    },\n    \"detections\": [\n        {\n            \"type\": \"email\",\n            \"value\": \"john.smith@email.com\",\n            \"confidence\": 0.95,\n            \"start_pos\": 32,\n            \"end_pos\": 52,\n            \"context\": \"...reach John Smith at john.smith@email.com or 555-123...\"\n        },\n        {\n            \"type\": \"phone_us\",\n            \"value\": \"555-123-4567\",\n            \"confidence\": 0.90,\n            \"start_pos\": 56,\n            \"end_pos\": 68,\n            \"context\": \"...john.smith@email.com or 555-123-4567.\"\n        }\n    ]\n}\n)\n</code></pre>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#signal-fields","title":"Signal Fields","text":"Field Type Description <code>final_score</code> <code>float</code> Privacy score (0.0-1.0) <code>total_detections</code> <code>int</code> All potential PII found <code>significant_detections_count</code> <code>int</code> Above confidence threshold <code>categorized_counts</code> <code>Dict</code> Breakdown by risk level <code>detections</code> <code>List</code> Detailed detection info"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#detection-fields","title":"Detection Fields","text":"Field Type Description <code>type</code> <code>str</code> PII type (email, ssn, etc.) <code>value</code> <code>str</code> The detected text <code>confidence</code> <code>float</code> Detection confidence (0-1) <code>start_pos</code> <code>int</code> Start position in text <code>end_pos</code> <code>int</code> End position in text <code>context</code> <code>str</code> Surrounding text"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Clean Output (Score: 1.0) <p>No PII Detected</p> <p>Output:</p> <p>\"The capital of France is Paris. It's known for the Eiffel Tower.\"</p> <p>Analysis:</p> <ul> <li>No email patterns</li> <li>No phone patterns</li> <li>No SSN patterns</li> <li>No addresses</li> </ul> <p>Final Score: <code>1.0</code> </p> \u26a0\ufe0f Scenario 2: Medium Risk PII (Score: ~0.5) <p>Email and Phone Detected</p> <p>Output:</p> <p>\"Contact support at help@company.com or call 1-800-555-0199.\"</p> <p>Detections:</p> Type Value Confidence Severity email help@company.com 0.95 0.7 phone_us 1-800-555-0199 0.90 0.7 <p>Penalty: <code>(0.95 \u00d7 0.7) + (0.90 \u00d7 0.7) = 1.295</code> \u2192 capped at 1.0</p> <p>Final Score: <code>1.0 - 1.0 = 0.0</code> </p> <p>Note: Multiple PII instances can quickly reduce the score.</p> \u274c Scenario 3: High Risk PII (Score: ~0.0) <p>SSN Detected</p> <p>Output:</p> <p>\"Your SSN ending in 4567 is associated with account 123-45-6789.\"</p> <p>Detections:</p> Type Value Confidence Severity ssn 123-45-6789 0.95 1.0 <p>Penalty: <code>0.95 \u00d7 1.0 = 0.95</code></p> <p>Final Score: <code>0.05</code> </p> <p>High-risk PII immediately triggers a near-zero score.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Scalable <p>No LLM calls\u2014regex patterns run instantly on millions of outputs.</p> \ud83d\udd12 Privacy Compliance <p>Catch GDPR/CCPA violations before they reach users.</p> \ud83d\ude80 CI/CD Integration <p>Add to pipelines as a safety gate for model outputs.</p>"},{"location":"metric-registry/heuristic/pii_leakage_heuristic/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>PII Leakage (Heuristic) = Does the output contain personally identifiable information?</p> <ul> <li>Use it when: Fast, deterministic PII detection needed</li> <li>Score interpretation: 1.0 = safe, lower = PII detected</li> <li>Key config: <code>confidence_threshold</code> controls sensitivity</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.PIILeakageHeuristic</code></p> </li> <li> <p> Related Metrics</p> <p>[ Bias \u00b7 Toxicity \u00b7 Safety Metrics</p> </li> </ul>"},{"location":"metric-registry/heuristic/retrieval_metrics/","title":"Retrieval Metrics","text":"Evaluate information retrieval quality with standard IR metrics Heuristic Retrieval Multi-K"},{"location":"metric-registry/heuristic/retrieval_metrics/#overview","title":"Overview","text":"<p>Axion provides a comprehensive suite of Information Retrieval (IR) metrics for evaluating search and retrieval systems. These metrics compare retrieved document rankings against ground truth relevance judgments.</p> Metric What It Measures Use Case Hit Rate @ K Any relevant result in top K? Quick relevance check MRR Rank of first relevant result First-result quality NDCG @ K Graded relevance with position discount Ranking quality Precision @ K Fraction of top K that's relevant Result purity Recall @ K Fraction of relevant docs in top K Coverage <p>Multi-K Support</p> <p>All @K metrics support evaluating at multiple K values simultaneously (e.g., <code>k=[5, 10, 20]</code>). This allows comparing retrieval quality at different cutoffs in a single evaluation pass.</p>"},{"location":"metric-registry/heuristic/retrieval_metrics/#required-inputs","title":"Required Inputs","text":"<p>All retrieval metrics require the same input structure:</p> Field Type Description <code>actual_ranking</code> <code>List[Dict]</code> Retrieved documents in order, each with <code>id</code> key <code>expected_reference</code> <code>List[Dict]</code> Ground truth with <code>id</code> and optional <code>relevance</code> score <pre><code>from axion.dataset import DatasetItem\n\nitem = DatasetItem(\n    actual_ranking=[\n        {\"id\": \"doc1\"},  # Position 1\n        {\"id\": \"doc2\"},  # Position 2\n        {\"id\": \"doc3\"},  # Position 3\n    ],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 1.0},  # Relevant\n        {\"id\": \"doc5\", \"relevance\": 1.0},  # Relevant but not retrieved\n    ],\n)\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#hit-rate-k","title":"Hit Rate @ K","text":"Binary check: Was ANY relevant document retrieved in the top K?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> or <code>1.0</code> Binary pass/fail \ud83d\udccf Default K <code>10</code> Top results to check"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Retrieved Docs] --&gt; B{Any relevant&lt;br&gt;in top K?}\n    B --&gt;|Yes| C[\"Score: 1.0\"]\n    B --&gt;|No| D[\"Score: 0.0\"]\n\n    style C fill:#10b981,stroke:#059669,color:#fff\n    style D fill:#ef4444,stroke:#dc2626,color:#fff</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage","title":"Usage","text":"<pre><code>from axion.metrics import HitRateAtK\n\n# Single K\nmetric = HitRateAtK(k=10)\n\n# Multiple K values\nmetric = HitRateAtK(k=[5, 10, 20], main_k=10)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 if hit, 0.0 if miss\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#mean-reciprocal-rank-mrr","title":"Mean Reciprocal Rank (MRR)","text":"How early does the first relevant result appear?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_1","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> 1/rank of first relevant \ud83d\udccf K-Independent Evaluates full ranking"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_1","title":"How It Works","text":"<pre><code>MRR = 1 / rank_of_first_relevant_document\n\nExamples:\n- First relevant at position 1 \u2192 MRR = 1.0\n- First relevant at position 2 \u2192 MRR = 0.5\n- First relevant at position 4 \u2192 MRR = 0.25\n- No relevant found \u2192 MRR = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_1","title":"Usage","text":"<pre><code>from axion.metrics import MeanReciprocalRank\n\nmetric = MeanReciprocalRank()\n\nresult = await metric.execute(item)\nprint(result.score)  # 1/rank or 0.0\nprint(result.signals.rank_of_first_relevant)  # e.g., 3\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#ndcg-k","title":"NDCG @ K","text":"Normalized Discounted Cumulative Gain\u2014handles graded relevance with position discounting."},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_2","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Normalized ranking quality \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_2","title":"How It Works","text":"<pre><code>flowchart TD\n    subgraph DCG[\"\ud83d\udcca DCG Calculation\"]\n        A[\"For each position i:\"]\n        B[\"relevance[i] / log\u2082(i + 1)\"]\n        C[\"Sum all values\"]\n    end\n\n    subgraph IDCG[\"\ud83c\udfaf IDCG (Ideal)\"]\n        D[\"Sort by relevance desc\"]\n        E[\"Calculate DCG on ideal order\"]\n    end\n\n    subgraph NDCG[\"\ud83d\udcc8 Final Score\"]\n        F[\"NDCG = DCG / IDCG\"]\n    end\n\n    DCG --&gt; F\n    IDCG --&gt; F\n\n    style NDCG stroke:#f59e0b,stroke-width:2px</code></pre> <p>Formula: <pre><code>DCG@K = \u03a3 (rel_i / log\u2082(i + 1)) for i = 1 to K\nNDCG@K = DCG@K / IDCG@K\n</code></pre></p>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_2","title":"Usage","text":"<pre><code>from axion.metrics import NDCGAtK\n\n# With graded relevance\nitem = DatasetItem(\n    actual_ranking=[{\"id\": \"doc1\"}, {\"id\": \"doc2\"}, {\"id\": \"doc3\"}],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 3.0},  # Highly relevant\n        {\"id\": \"doc2\", \"relevance\": 1.0},  # Marginally relevant\n        {\"id\": \"doc3\", \"relevance\": 2.0},  # Relevant\n    ],\n)\n\nmetric = NDCGAtK(k=[5, 10])\nresult = await metric.execute(item)\nprint(f\"NDCG@10: {result.score:.3f}\")\nprint(f\"DCG: {result.signals.results_by_k[10].dcg:.3f}\")\nprint(f\"IDCG: {result.signals.results_by_k[10].idcg:.3f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#precision-k","title":"Precision @ K","text":"What fraction of the top K results are relevant?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_3","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Relevant / Retrieved \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_3","title":"How It Works","text":"<pre><code>Precision@K = (Relevant docs in top K) / K\n\nExamples (K=5):\n- 5 relevant in top 5 \u2192 Precision = 1.0\n- 3 relevant in top 5 \u2192 Precision = 0.6\n- 0 relevant in top 5 \u2192 Precision = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_3","title":"Usage","text":"<pre><code>from axion.metrics import PrecisionAtK\n\nmetric = PrecisionAtK(k=10)\n\nresult = await metric.execute(item)\nprint(f\"Precision@10: {result.score:.2%}\")\nprint(f\"Hits: {result.signals.results_by_k[10].hits_in_top_k}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#recall-k","title":"Recall @ K","text":"What fraction of ALL relevant documents appear in the top K?"},{"location":"metric-registry/heuristic/retrieval_metrics/#at-a-glance_4","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500 <code>1.0</code> Found / Total Relevant \ud83d\udccf Default K <code>10</code> Top results to evaluate"},{"location":"metric-registry/heuristic/retrieval_metrics/#how-it-works_4","title":"How It Works","text":"<pre><code>Recall@K = (Relevant docs in top K) / (Total relevant docs)\n\nExamples (10 total relevant):\n- 10 relevant in top K \u2192 Recall = 1.0\n- 5 relevant in top K \u2192 Recall = 0.5\n- 0 relevant in top K \u2192 Recall = 0.0\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#usage_4","title":"Usage","text":"<pre><code>from axion.metrics import RecallAtK\n\nmetric = RecallAtK(k=[5, 10, 20])\n\nresult = await metric.execute(item)\nprint(f\"Recall@10: {result.score:.2%}\")\nprint(f\"Found: {result.signals.results_by_k[10].hits_in_top_k}\")\nprint(f\"Total relevant: {result.signals.results_by_k[10].total_relevant}\")\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>k</code> <code>int \\| List[int]</code> <code>10</code> Cutoff(s) for evaluation <code>main_k</code> <code>int</code> <code>max(k)</code> K value used for primary score"},{"location":"metric-registry/heuristic/retrieval_metrics/#comparison-guide","title":"Comparison Guide","text":""},{"location":"metric-registry/heuristic/retrieval_metrics/#when-to-use-each-metric","title":"When to Use Each Metric","text":"Metric Best For Key Question Hit Rate Quick sanity check \"Did we find anything relevant?\" MRR First-result systems \"How fast do users find what they need?\" NDCG Graded relevance \"Is the ranking order optimal?\" Precision Result quality \"Are results mostly relevant?\" Recall Coverage \"Did we miss relevant docs?\""},{"location":"metric-registry/heuristic/retrieval_metrics/#metric-relationships","title":"Metric Relationships","text":"<pre><code>flowchart TB\n    subgraph COVERAGE[\"Coverage Metrics\"]\n        A[Hit Rate @ K]\n        B[Recall @ K]\n    end\n\n    subgraph QUALITY[\"Quality Metrics\"]\n        C[Precision @ K]\n        D[MRR]\n        E[NDCG @ K]\n    end\n\n    A --&gt;|\"Binary version of\"| B\n    C --&gt;|\"Complementary to\"| B\n    D --&gt;|\"Position-aware\"| E\n\n    style COVERAGE stroke:#3b82f6,stroke-width:2px\n    style QUALITY stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#complete-example","title":"Complete Example","text":"<pre><code>from axion.metrics import (\n    HitRateAtK,\n    MeanReciprocalRank,\n    NDCGAtK,\n    PrecisionAtK,\n    RecallAtK,\n)\nfrom axion.runners import MetricRunner\nfrom axion.dataset import DatasetItem\n\n# Create test item\nitem = DatasetItem(\n    actual_ranking=[\n        {\"id\": \"doc1\"},  # Relevant (relevance: 3)\n        {\"id\": \"doc4\"},  # Not relevant\n        {\"id\": \"doc2\"},  # Relevant (relevance: 2)\n        {\"id\": \"doc5\"},  # Not relevant\n        {\"id\": \"doc3\"},  # Relevant (relevance: 1)\n    ],\n    expected_reference=[\n        {\"id\": \"doc1\", \"relevance\": 3.0},\n        {\"id\": \"doc2\", \"relevance\": 2.0},\n        {\"id\": \"doc3\", \"relevance\": 1.0},\n    ],\n)\n\n# Evaluate with all metrics\nmetrics = [\n    HitRateAtK(k=5),\n    MeanReciprocalRank(),\n    NDCGAtK(k=5),\n    PrecisionAtK(k=5),\n    RecallAtK(k=5),\n]\n\nrunner = MetricRunner(metrics=metrics)\nresults = await runner.run([item])\n\nfor result in results:\n    print(f\"{result.metric_name}: {result.score:.3f}\")\n\n# Output:\n# Hit Rate @ K: 1.000\n# Mean Reciprocal Rank (MRR): 1.000\n# NDCG @ K: 0.876\n# Precision @ K: 0.600\n# Recall @ K: 1.000\n</code></pre>"},{"location":"metric-registry/heuristic/retrieval_metrics/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> Metric Formula Score Hit Rate 1 if any relevant in K 0 or 1 MRR 1 / first_relevant_rank 0 to 1 NDCG DCG / IDCG 0 to 1 Precision relevant_in_K / K 0 to 1 Recall relevant_in_K / total_relevant 0 to 1 <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.HitRateAtK</code> <code>axion.metrics.MeanReciprocalRank</code> <code>axion.metrics.NDCGAtK</code> <code>axion.metrics.PrecisionAtK</code> <code>axion.metrics.RecallAtK</code></p> </li> <li> <p> Related Metrics</p> <p> Contextual Precision \u00b7 Contextual Recall \u00b7 Contextual Ranking</p> </li> </ul>"},{"location":"metric-registry/heuristic/sentence_bleu/","title":"Sentence BLEU","text":"Compute n-gram precision similarity between candidate and reference text Heuristic Single Turn Fast"},{"location":"metric-registry/heuristic/sentence_bleu/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> N-gram precision score \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>actual_output</code> <code>expected_output</code> Reference text required <p>What It Measures</p> <p>Sentence BLEU (Bilingual Evaluation Understudy) computes the similarity between a candidate text and reference text using modified n-gram precision with a brevity penalty. Originally designed for machine translation, it's useful for any task where textual similarity to a reference matters.</p> Score Interpretation 1.0  Perfect n-gram match with reference 0.7+  High similarity, minor differences 0.3-0.7  Moderate similarity &lt; 0.3  Low similarity to reference \u2705 Use When <ul> <li>Comparing text to reference translations</li> <li>Evaluating summarization quality</li> <li>Fast, deterministic evaluation needed</li> <li>N-gram overlap is meaningful</li> </ul> \u274c Don't Use When <ul> <li>Semantic similarity matters more than wording</li> <li>Multiple valid phrasings exist</li> <li>Evaluating creative/generative tasks</li> <li>Word order flexibility is expected</li> </ul> <p>See Also: Levenshtein Ratio</p> <p>Sentence BLEU measures n-gram precision (word sequences). Levenshtein Ratio measures character-level edit distance.</p> <p>Use BLEU for word-level comparison; use Levenshtein for character-level.</p> How It Works  Computation BLEU Formula <p>BLEU calculates n-gram precision with clipping and applies a brevity penalty.</p> <p>Modified Precision: <pre><code>p_n = \u03a3 min(count(ngram), max_ref_count(ngram)) / \u03a3 count(ngram)\n</code></pre></p> <p>Brevity Penalty (BP): <pre><code>BP = 1                    if c &gt; r\nBP = exp(1 - r/c)         if c \u2264 r\n\nwhere c = candidate length, r = reference length\n</code></pre></p> <p>Final Score: <pre><code>BLEU = BP \u00d7 exp(\u03a3 w_n \u00d7 log(p_n))\n\nwhere w_n = 1/N (uniform weights)\n</code></pre></p> <p> \ud83d\udcdd Clipping Prevents gaming by repeating words. Each n-gram counted at most as many times as it appears in reference. </p> <p> \ud83d\udccf Brevity Penalty Penalizes outputs shorter than reference. Prevents gaming by outputting only high-confidence words. </p>"},{"location":"metric-registry/heuristic/sentence_bleu/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Candidate Text]\n        B[Reference Text]\n    end\n\n    subgraph NGRAM[\"\ud83d\udd0d Step 1: N-gram Extraction\"]\n        C[Extract 1-grams to n-grams]\n        D1[\"1-gram counts\"]\n        D2[\"2-gram counts\"]\n        D3[\"3-gram counts\"]\n        DN[\"n-gram counts\"]\n    end\n\n    subgraph PRECISION[\"\u2696\ufe0f Step 2: Clipped Precision\"]\n        E[Clip counts to reference max]\n        F[\"Calculate precision per n\"]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Step 3: Final Score\"]\n        G[\"Geometric mean of precisions\"]\n        H[\"Apply brevity penalty\"]\n        I[\"Final BLEU Score\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt; D1 &amp; D2 &amp; D3 &amp; DN\n    D1 &amp; D2 &amp; D3 &amp; DN --&gt; E\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; I\n\n    style INPUT stroke:#f59e0b,stroke-width:2px\n    style NGRAM stroke:#3b82f6,stroke-width:2px\n    style PRECISION stroke:#8b5cf6,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px\n    style I fill:#f59e0b,stroke:#d97706,stroke-width:3px,color:#fff</code></pre>"},{"location":"metric-registry/heuristic/sentence_bleu/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>n_grams</code> <code>int</code> <code>4</code> Maximum n-gram length (e.g., 4 for BLEU-4) <code>case_sensitive</code> <code>bool</code> <code>False</code> Whether comparison is case-sensitive <code>smoothing</code> <code>bool</code> <code>True</code> Apply smoothing for sentence-level BLEU <p>Smoothing</p> <p>Sentence-level BLEU often has zero counts for higher n-grams. Smoothing (add-one) prevents the entire score from becoming zero.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#code-examples","title":"Code Examples","text":"Basic Usage Custom N-grams With Runner <pre><code>from axion.metrics import SentenceBleu\nfrom axion.dataset import DatasetItem\n\nmetric = SentenceBleu()\n\nitem = DatasetItem(\n    actual_output=\"The cat sat on the mat.\",\n    expected_output=\"The cat is sitting on the mat.\",\n)\n\nresult = await metric.execute(item)\nprint(result.pretty())\n# Score: ~0.6 (good n-gram overlap with minor differences)\n</code></pre> <pre><code>from axion.metrics import SentenceBleu\n\n# BLEU-2 for shorter sequences\nmetric = SentenceBleu(n_grams=2)\n\n# Case-sensitive comparison\nmetric = SentenceBleu(case_sensitive=True)\n\n# Without smoothing (corpus-level style)\nmetric = SentenceBleu(smoothing=False)\n</code></pre> <pre><code>from axion.metrics import SentenceBleu\nfrom axion.runners import MetricRunner\n\nmetric = SentenceBleu(n_grams=4)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"BLEU-4: {item_result.score:.3f}\")\n</code></pre>"},{"location":"metric-registry/heuristic/sentence_bleu/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: High BLEU Score (~0.9) <p>Near-Perfect Match</p> <p>Reference:</p> <p>\"The quick brown fox jumps over the lazy dog.\"</p> <p>Candidate:</p> <p>\"The quick brown fox jumped over the lazy dog.\"</p> <p>Analysis:</p> <ul> <li>1-grams: 8/9 match (jumped vs jumps)</li> <li>2-grams: 7/8 match</li> <li>3-grams: 6/7 match</li> <li>4-grams: 5/6 match</li> <li>Brevity penalty: ~1.0 (same length)</li> </ul> <p>Final Score: <code>~0.85</code> </p> \u26a0\ufe0f Scenario 2: Moderate BLEU Score (~0.5) <p>Partial Overlap</p> <p>Reference:</p> <p>\"Machine learning models require large datasets for training.\"</p> <p>Candidate:</p> <p>\"Deep learning needs big data to train properly.\"</p> <p>Analysis:</p> <ul> <li>Same meaning, different words</li> <li>Few exact n-gram matches</li> <li>\"learning\" and \"train\" overlap</li> </ul> <p>Final Score: <code>~0.3</code> </p> <p>Semantic similarity high, but n-gram overlap low.</p> \u274c Scenario 3: Low BLEU Score (~0.1) <p>Minimal Overlap</p> <p>Reference:</p> <p>\"Paris is the capital of France.\"</p> <p>Candidate:</p> <p>\"The Eiffel Tower is located in the French capital city.\"</p> <p>Analysis:</p> <ul> <li>Related topic, completely different wording</li> <li>Almost no n-gram matches</li> </ul> <p>Final Score: <code>~0.1</code> </p> <p>Semantically related but lexically different.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#why-it-matters","title":"Why It Matters","text":"\u26a1 Fast &amp; Deterministic <p>No LLM calls needed. Instant, reproducible results ideal for CI/CD pipelines.</p> \ud83d\udcca Industry Standard <p>Widely used in NLP research for translation and summarization evaluation.</p> \ud83d\udd22 N-gram Precision <p>Captures phrase-level similarity, not just word overlap.</p>"},{"location":"metric-registry/heuristic/sentence_bleu/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Sentence BLEU = How much does the candidate text overlap with the reference at the n-gram level?</p> <ul> <li>Use it when: Fast, deterministic text similarity is needed</li> <li>Score interpretation: Higher = more n-gram overlap with reference</li> <li>Key config: <code>n_grams</code> controls phrase length (default 4)</li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.SentenceBleu</code></p> </li> <li> <p> Related Metrics</p> <p> Levenshtein Ratio \u00b7 Exact String Match \u00b7 Contains Match</p> </li> </ul>"},{"location":"metric-registry/tool/","title":"Tool Metrics","text":"Evaluate AI agent tool calling correctness and effectiveness 1 Metric Agent <p>Tool metrics evaluate the correctness and effectiveness of tool usage in AI agent workflows. These metrics assess whether agents correctly invoke the right tools with appropriate parameters.</p>"},{"location":"metric-registry/tool/#available-metrics","title":"Available Metrics","text":"Tool Correctness <p>Evaluate if expected tools were correctly called</p> <code>tools_called</code> <code>expected_tools</code>"},{"location":"metric-registry/tool/#quick-reference","title":"Quick Reference","text":"Metric Score Range Threshold Key Question Tool Correctness 0.0 \u2013 1.0 0.5 Were the right tools called correctly?"},{"location":"metric-registry/tool/#usage-example","title":"Usage Example","text":"<pre><code>from axion.metrics import ToolCorrectness\nfrom axion.runners import MetricRunner\nfrom axion.dataset import DatasetItem\nfrom axion._core.schema import ToolCall\n\n# Create evaluation item\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"weather in Paris\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={\"query\": \"weather in Paris\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n)\n\n# Initialize metric\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n\n# Run evaluation\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run([item])\n\nprint(f\"Tool Correctness: {results[0].score:.2f}\")\n# Output: Tool Correctness: 1.00\n</code></pre>"},{"location":"metric-registry/tool/#evaluation-modes","title":"Evaluation Modes","text":"<p>Tool Correctness supports multiple evaluation strategies:</p> Name Only (Default) <p>Just verify the correct tools were called. Parameters are ignored.</p> <pre><code>metric = ToolCorrectness()\n</code></pre> With Parameters <p>Validate both tool names and their arguments.</p> <pre><code>metric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n</code></pre> Strict Order <p>Tools must be called in the exact expected sequence.</p> <pre><code>metric = ToolCorrectness(strict_order=True)\n</code></pre> Fuzzy Parameters <p>Allow similar (but not identical) parameter values.</p> <pre><code>metric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='fuzzy',\n    fuzzy_threshold=0.8\n)\n</code></pre>"},{"location":"metric-registry/tool/#why-tool-metrics","title":"Why Tool Metrics?","text":"\ud83e\udd16 Agent Evaluation <p>Verify AI agents select the right tools for tasks.</p> \ud83d\udd27 Function Calling <p>Test LLM function calling capabilities.</p> \ud83d\udcca Workflow Validation <p>Ensure multi-step workflows execute correctly.</p> \ud83e\uddea Regression Testing <p>Catch breaking changes in agent behavior.</p>"},{"location":"metric-registry/tool/tool_correctness/","title":"Tool Correctness","text":"Evaluate whether AI agents call the correct tools with proper parameters Tool Agent Single Turn"},{"location":"metric-registry/tool/tool_correctness/#at-a-glance","title":"At a Glance","text":"\ud83c\udfaf Score Range <code>0.0</code> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 <code>1.0</code> Recall of expected tools \u26a1 Default Threshold <code>0.5</code> Pass/fail cutoff \ud83d\udccb Required Inputs <code>tools_called</code> <code>expected_tools</code> Tool call lists <p>What It Measures</p> <p>Tool Correctness evaluates whether an AI agent called the correct tools by comparing actual tool calls against expected ones. It supports name-only matching, parameter validation, and strict ordering requirements.</p> Score Interpretation 1.0  All expected tools called correctly 0.5-0.9  Partial match\u2014some tools missing 0.0  No expected tools called correctly \u2705 Use When <ul> <li>Evaluating AI agents</li> <li>Testing function calling</li> <li>Validating tool selection</li> <li>Checking parameter passing</li> </ul> \u274c Don't Use When <ul> <li>Order doesn't matter (consider disabling strict_order)</li> <li>Tool output quality matters more</li> <li>Parameters have valid variations</li> <li>No expected tools defined</li> </ul> How It Works  Computation Matching Strategies Score Formula <p>The metric compares called tools against expected tools with configurable matching strategies.</p> <p> Name Only (default) Just check if the tool name matches. Parameters ignored. </p> <p> Exact Parameters Parameters must match exactly. </p> <p> Subset Parameters Called args must contain all expected args (extras OK). </p> <p> Fuzzy Parameters Similarity-based matching with threshold. </p> <pre><code>score = matched_tools / total_expected_tools\n</code></pre> <p>Example: - Expected: <code>[search, calculate, format]</code> - Called: <code>[search, format]</code> - Score: <code>2/3 = 0.67</code></p>"},{"location":"metric-registry/tool/tool_correctness/#step-by-step-process","title":"Step-by-Step Process","text":"<pre><code>flowchart TD\n    subgraph INPUT[\"\ud83d\udce5 Inputs\"]\n        A[Tools Called]\n        B[Expected Tools]\n    end\n\n    subgraph CONFIG[\"\u2699\ufe0f Configuration\"]\n        C{Strict Order?}\n        D{Check Parameters?}\n    end\n\n    subgraph MATCH[\"\ud83d\udd0d Matching\"]\n        E[Compare names]\n        F[Validate parameters]\n        G[Check sequence]\n    end\n\n    subgraph SCORE[\"\ud83d\udcca Score\"]\n        H[\"matched / expected\"]\n    end\n\n    A &amp; B --&gt; C\n    C --&gt;|No| E\n    C --&gt;|Yes| G\n    E --&gt; D\n    G --&gt; D\n    D --&gt;|Yes| F\n    D --&gt;|No| H\n    F --&gt; H\n\n    style INPUT stroke:#8b5cf6,stroke-width:2px\n    style CONFIG stroke:#3b82f6,stroke-width:2px\n    style MATCH stroke:#f59e0b,stroke-width:2px\n    style SCORE stroke:#10b981,stroke-width:2px</code></pre>"},{"location":"metric-registry/tool/tool_correctness/#configuration","title":"Configuration","text":"Parameters Parameter Type Default Description <code>check_parameters</code> <code>bool</code> <code>False</code> Also validate tool parameters <code>strict_order</code> <code>bool</code> <code>False</code> Tools must be called in exact order <code>parameter_matching_strategy</code> <code>str</code> <code>exact</code> <code>exact</code>, <code>subset</code>, or <code>fuzzy</code> <code>fuzzy_threshold</code> <code>float</code> <code>0.8</code> Similarity threshold for fuzzy matching <p>Parameter Matching Strategies</p> <ul> <li>exact: Parameters must match exactly (default)</li> <li>subset: Called args must contain all expected args (extras allowed)</li> <li>fuzzy: Similarity-based matching using SequenceMatcher</li> </ul>"},{"location":"metric-registry/tool/tool_correctness/#code-examples","title":"Code Examples","text":"Basic Usage (Name Only) With Parameter Checking Strict Order Fuzzy Parameter Matching With Runner <pre><code>from axion.metrics import ToolCorrectness\nfrom axion.dataset import DatasetItem\nfrom axion._core.schema import ToolCall\n\nmetric = ToolCorrectness()\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"weather\"}),\n        ToolCall(name=\"format\", args={\"style\": \"brief\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={}),\n        ToolCall(name=\"format\", args={}),\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - both tools called (params not checked)\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='exact'\n)\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"calculate\", args={\"a\": 5, \"b\": 3}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"calculate\", args={\"a\": 5, \"b\": 3}),\n    ],\n)\n\nresult = await metric.execute(item)\nprint(result.score)  # 1.0 - params match exactly\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(strict_order=True)\n\n# Correct order\nitem_correct = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n)\n# Score: 1.0\n\n# Wrong order\nitem_wrong = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"process\", args={}),  # Should be second\n        ToolCall(name=\"fetch\", args={}),    # Should be first\n        ToolCall(name=\"store\", args={}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"fetch\", args={}),\n        ToolCall(name=\"process\", args={}),\n        ToolCall(name=\"store\", args={}),\n    ],\n)\n# Score: 0.0 - order mismatch at position 0\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\n\nmetric = ToolCorrectness(\n    check_parameters=True,\n    parameter_matching_strategy='fuzzy',\n    fuzzy_threshold=0.8\n)\n\nitem = DatasetItem(\n    tools_called=[\n        ToolCall(name=\"search\", args={\"query\": \"what is machine learning\"}),\n    ],\n    expected_tools=[\n        ToolCall(name=\"search\", args={\"query\": \"what is ML\"}),\n    ],\n)\n\nresult = await metric.execute(item)\n# Score depends on string similarity of query values\n</code></pre> <pre><code>from axion.metrics import ToolCorrectness\nfrom axion.runners import MetricRunner\n\nmetric = ToolCorrectness(check_parameters=True)\nrunner = MetricRunner(metrics=[metric])\nresults = await runner.run(dataset)\n\nfor item_result in results:\n    print(f\"Score: {item_result.score:.2f}\")\n    print(f\"Explanation: {item_result.explanation}\")\n</code></pre>"},{"location":"metric-registry/tool/tool_correctness/#example-scenarios","title":"Example Scenarios","text":"\u2705 Scenario 1: Perfect Match (Score: 1.0) <p>All Tools Correct</p> <p>Expected Tools:</p> <ol> <li><code>search(query=\"weather\")</code></li> <li><code>parse(format=\"json\")</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>search(query=\"weather\")</code></li> <li><code>parse(format=\"json\")</code></li> </ol> <p>Result: <code>1.0</code> </p> <p>All expected tools called with correct parameters.</p> \u26a0\ufe0f Scenario 2: Partial Match (Score: 0.67) <p>Missing Tool</p> <p>Expected Tools:</p> <ol> <li><code>fetch</code></li> <li><code>transform</code></li> <li><code>store</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>fetch</code></li> <li><code>transform</code> (store not called)</li> </ol> <p>Result: <code>2/3 = 0.67</code> </p> <p>Explanation: \"Correctly called: ['fetch', 'transform']; Missing tools: ['store']\"</p> \u274c Scenario 3: Wrong Tool (Score: 0.0) <p>Incorrect Tool Called</p> <p>Expected Tools:</p> <ol> <li><code>calculate</code></li> </ol> <p>Called Tools:</p> <ol> <li><code>search</code></li> </ol> <p>Result: <code>0.0</code> </p> <p>Explanation: \"Missing tools: ['calculate']; Unexpected tools: ['search']\"</p> \u26a0\ufe0f Scenario 4: Parameter Mismatch <p>Wrong Parameters</p> <p>Config: <code>check_parameters=True, strategy='exact'</code></p> <p>Expected:</p> <p><code>search(query=\"Python tutorials\")</code></p> <p>Called:</p> <p><code>search(query=\"python tutorial\")</code> (different text)</p> <p>Result: <code>0.0</code> </p> <p>Exact matching fails on parameter difference.</p> <p>Fix: Use <code>strategy='fuzzy'</code> or <code>strategy='subset'</code> for flexibility.</p>"},{"location":"metric-registry/tool/tool_correctness/#why-it-matters","title":"Why It Matters","text":"\ud83e\udd16 Agent Evaluation <p>Verify AI agents select and call the right tools for tasks.</p> \ud83d\udd27 Function Calling <p>Test LLM function calling capabilities and parameter handling.</p> \ud83d\udcca Workflow Validation <p>Ensure multi-step agent workflows execute correctly.</p>"},{"location":"metric-registry/tool/tool_correctness/#quick-reference","title":"Quick Reference","text":"<p>TL;DR</p> <p>Tool Correctness = Did the agent call the expected tools correctly?</p> <ul> <li>Use it when: Evaluating AI agents or function calling</li> <li>Score interpretation: Fraction of expected tools called correctly</li> <li>Key configs: <code>check_parameters</code>, <code>strict_order</code>, <code>parameter_matching_strategy</code></li> </ul> <ul> <li> <p> API Reference</p> <p> <code>axion.metrics.ToolCorrectness</code></p> </li> <li> <p> Related Concepts</p> <p>[ Agent Evaluation \u00b7 Function Calling \u00b7 Tool Use</p> </li> </ul>"},{"location":"reference/caliber/","title":"Caliber API Reference","text":"<p>LLM-as-judge calibration and alignment tools for improving evaluation quality.</p> <pre><code>from axion.caliber import (\n    CaliberMetric,\n    ExampleSelector, SelectionStrategy, SelectionResult,\n    PatternDiscovery, DiscoveredPattern, PatternDiscoveryResult,\n    EvidenceItem, EvidencePipeline, LearningArtifact, PipelineResult,\n    Provenance, MetadataConfig, ClusteringMethod, AnnotatedItem,\n    InMemorySink, JsonlSink, InMemoryDeduper, EmbeddingDeduper,\n    MisalignmentAnalyzer, MisalignmentAnalysis, MisalignmentPattern,\n    PromptOptimizer, OptimizedPrompt, PromptSuggestion,\n    CaliberRenderer, NotebookCaliberRenderer,\n    ConsoleCaliberRenderer, JsonCaliberRenderer,\n)\n</code></pre> C <p>CaliberMetric</p> <p>Core metric for measuring LLM judge alignment against human ground truth scores.</p> P <p>Pattern Discovery</p> <p>Cluster any text evidence into themes and distill actionable learning artifacts via a full pipeline.</p> M <p>Misalignment Analysis</p> <p>Identify systematic disagreements between human and LLM judges and surface root causes.</p> O <p>Prompt Optimization</p> <p>Automatically generate improved evaluation prompts that better align with human judgment.</p>"},{"location":"reference/caliber/#calibermetric","title":"CaliberMetric","text":""},{"location":"reference/caliber/#axion.caliber.CaliberMetric","title":"axion.caliber.CaliberMetric","text":"<pre><code>CaliberMetric(instruction: str, model_name: Optional[str] = None, llm_provider: Optional[str] = None, examples: Optional[List[Dict]] = None, required_fields: Optional[List[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Dynamically configured metric for CaliberHQ evaluation.</p> <p>This metric uses the provided criteria/instruction for LLM-as-judge evaluation.</p> <p>Initialize the CaliberMetric.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>The LLM-as-a-judge prompt/criteria</p> </li> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the LLM to use</p> </li> <li> <code>llm_provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The provider of the LLM</p> </li> <li> <code>examples</code>               (<code>Optional[List[Dict]]</code>, default:                   <code>None</code> )           \u2013            <p>Few-shot examples from the UI</p> </li> <li> <code>required_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Required input fields for the metric</p> </li> </ul>"},{"location":"reference/caliber/#example-selection","title":"Example Selection","text":""},{"location":"reference/caliber/#exampleselector","title":"ExampleSelector","text":""},{"location":"reference/caliber/#axion.caliber.ExampleSelector","title":"axion.caliber.ExampleSelector","text":"<pre><code>ExampleSelector(seed: Optional[int] = None)\n</code></pre> <p>Selects few-shot examples for LLM-as-judge calibration.</p> Example <p>selector = ExampleSelector()</p> <p>Initialize selector.</p> <p>Parameters:</p> <ul> <li> <code>seed</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Random seed for reproducibility</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.ExampleSelector--simple-balanced-selection","title":"Simple balanced selection","text":"<p>result = selector.select(records, annotations, count=6)</p>"},{"location":"reference/caliber/#axion.caliber.ExampleSelector--misalignment-guided-requires-eval-results","title":"Misalignment-guided (requires eval results)","text":"<p>result = selector.select( ...     records, annotations, count=6, ...     strategy=SelectionStrategy.MISALIGNMENT_GUIDED, ...     eval_results=results ... )</p>"},{"location":"reference/caliber/#axion.caliber.ExampleSelector--pattern-aware-requires-pattern-discovery-results","title":"Pattern-aware (requires Pattern Discovery results)","text":"<p>result = selector.select( ...     records, annotations, count=6, ...     strategy=SelectionStrategy.PATTERN_AWARE, ...     patterns=discovered_patterns ... )</p>"},{"location":"reference/caliber/#axion.caliber.ExampleSelector.select","title":"select","text":"<pre><code>select(records: List[Dict[str, Any]], annotations: Dict[str, int], count: int = 6, strategy: SelectionStrategy = BALANCED, eval_results: Optional[List[Dict[str, Any]]] = None, patterns: Optional[List[DiscoveredPattern]] = None) -&gt; SelectionResult\n</code></pre> <p>Select few-shot examples.</p> <p>Parameters:</p> <ul> <li> <code>records</code>               (<code>List[Dict[str, Any]]</code>)           \u2013            <p>List of records with 'id', 'query', 'actual_output', etc.</p> </li> <li> <code>annotations</code>               (<code>Dict[str, int]</code>)           \u2013            <p>Dict mapping record_id -&gt; human score (0 or 1)</p> </li> <li> <code>count</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of examples to select</p> </li> <li> <code>strategy</code>               (<code>SelectionStrategy</code>, default:                   <code>BALANCED</code> )           \u2013            <p>Selection strategy to use</p> </li> <li> <code>eval_results</code>               (<code>Optional[List[Dict[str, Any]]]</code>, default:                   <code>None</code> )           \u2013            <p>Evaluation results (required for MISALIGNMENT_GUIDED)</p> </li> <li> <code>patterns</code>               (<code>Optional[List[DiscoveredPattern]]</code>, default:                   <code>None</code> )           \u2013            <p>Discovered patterns (required for PATTERN_AWARE)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SelectionResult</code>           \u2013            <p>SelectionResult with selected examples and metadata</p> </li> </ul>"},{"location":"reference/caliber/#selectionstrategy","title":"SelectionStrategy","text":""},{"location":"reference/caliber/#axion.caliber.SelectionStrategy","title":"axion.caliber.SelectionStrategy","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategies for selecting few-shot examples.</p>"},{"location":"reference/caliber/#selectionresult","title":"SelectionResult","text":""},{"location":"reference/caliber/#axion.caliber.SelectionResult","title":"axion.caliber.SelectionResult  <code>dataclass</code>","text":"<pre><code>SelectionResult(examples: List[Dict[str, Any]], strategy_used: SelectionStrategy, metadata: Dict[str, Any])\n</code></pre> <p>Result of example selection.</p>"},{"location":"reference/caliber/#pattern-discovery","title":"Pattern Discovery","text":""},{"location":"reference/caliber/#patterndiscovery","title":"PatternDiscovery","text":""},{"location":"reference/caliber/#axion.caliber.PatternDiscovery","title":"axion.caliber.PatternDiscovery","text":"<pre><code>PatternDiscovery(model_name: Optional[str] = None, llm=None, llm_provider: Optional[str] = None, instruction: Optional[str] = None, max_notes: int = 50, min_category_size: int = 2, bertopic_embedding_model: Any = 'all-MiniLM-L6-v2', metadata_config: Optional[MetadataConfig] = None, excerpt_fn: Optional[ExcerptFn] = None, seed: Optional[int] = None)\n</code></pre> <p>Discovers patterns in evaluation annotations using LLM-based clustering.</p> <p>This class leverages LLMHandler for structured output, automatic retries, and consistent LLM configuration with the rest of axion.</p> <p>Supports both the legacy <code>discover()</code> API (AnnotatedItem dicts) and the new <code>discover_from_evidence()</code> API (EvidenceItem sequences/dicts).</p> Example <p>from axion.caliber import PatternDiscovery, AnnotatedItem</p> <p>annotations = { ...     'rec_1': AnnotatedItem(record_id='rec_1', score=0, notes='Missing context'), ...     'rec_2': AnnotatedItem(record_id='rec_2', score=0, notes='Lacks detail'), ... } discovery = PatternDiscovery(model_name='gpt-4o', llm_provider='openai') result = await discovery.discover(annotations)</p>"},{"location":"reference/caliber/#axion.caliber.PatternDiscovery.discover","title":"discover  <code>async</code>","text":"<pre><code>discover(annotations: Union[Dict[str, AnnotatedItem], Dict[str, Dict]], method: ClusteringMethod = LLM) -&gt; PatternDiscoveryResult\n</code></pre> <p>Backward-compatible entry point.</p> <p>Normalizes AnnotatedItem dicts into EvidenceItem dicts and delegates to <code>discover_from_evidence()</code>.</p>"},{"location":"reference/caliber/#axion.caliber.PatternDiscovery.discover_from_evidence","title":"discover_from_evidence  <code>async</code>","text":"<pre><code>discover_from_evidence(evidence: Union[Sequence[EvidenceItem], Dict[str, EvidenceItem]], method: ClusteringMethod = LLM) -&gt; PatternDiscoveryResult\n</code></pre> <p>Discover patterns from generic evidence items.</p>"},{"location":"reference/caliber/#evidenceitem","title":"EvidenceItem","text":""},{"location":"reference/caliber/#axion.caliber.EvidenceItem","title":"axion.caliber.EvidenceItem  <code>dataclass</code>","text":"<pre><code>EvidenceItem(id: str, text: str, metadata: Dict[str, Any] = dict(), source_ref: Optional[str] = None)\n</code></pre> <p>A single piece of evidence for clustering.</p> <p>Represents any text source (conversation, bug report, eval note, etc.) with optional structured metadata and provenance.</p>"},{"location":"reference/caliber/#discoveredpattern","title":"DiscoveredPattern","text":""},{"location":"reference/caliber/#axion.caliber.DiscoveredPattern","title":"axion.caliber.DiscoveredPattern  <code>dataclass</code>","text":"<pre><code>DiscoveredPattern(category: str, description: str, count: int, record_ids: List[str], examples: List[str] = list(), confidence: Optional[float] = None)\n</code></pre> <p>A discovered pattern/category from clustering.</p>"},{"location":"reference/caliber/#patterndiscoveryresult","title":"PatternDiscoveryResult","text":""},{"location":"reference/caliber/#axion.caliber.PatternDiscoveryResult","title":"axion.caliber.PatternDiscoveryResult  <code>dataclass</code>","text":"<pre><code>PatternDiscoveryResult(patterns: List[DiscoveredPattern], uncategorized: List[str], total_analyzed: int, method: ClusteringMethod, metadata: Dict[str, Any] = dict())\n</code></pre> <p>Complete result from pattern discovery.</p>"},{"location":"reference/caliber/#clusteringmethod","title":"ClusteringMethod","text":""},{"location":"reference/caliber/#axion.caliber.ClusteringMethod","title":"axion.caliber.ClusteringMethod","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available clustering methods.</p>"},{"location":"reference/caliber/#evidencepipeline","title":"EvidencePipeline","text":""},{"location":"reference/caliber/#axion.caliber.EvidencePipeline","title":"axion.caliber.EvidencePipeline","text":"<pre><code>EvidencePipeline(model_name: Optional[str] = None, llm=None, llm_provider: Optional[str] = None, clustering_instruction: Optional[str] = None, distillation_instruction: Optional[str] = None, clusterer: Optional[EvidenceClusterer] = None, writer: Optional[ArtifactWriter] = None, method: ClusteringMethod = LLM, recurrence_threshold: int = 2, recurrence_key_fn: Optional[RecurrenceKeyFn] = None, max_learnings_per_cluster: int = 3, max_items: int = 50, min_category_size: int = 2, domain_context: Optional[str] = None, metadata_config: Optional[MetadataConfig] = None, excerpt_fn: Optional[ExcerptFn] = None, seed: Optional[int] = None, max_concurrent_distillations: int = 5, sanitizer: Optional[Sanitizer] = None, sink: Optional[ArtifactSink] = None, deduper: Optional[Deduper] = None, tag_normalizer: Optional[Callable[[List[str]], List[str]]] = None, bertopic_embedding_model: Any = 'all-MiniLM-L6-v2')\n</code></pre> <p>Orchestrates evidence \u2192 clusters \u2192 KB-ready learnings.</p>"},{"location":"reference/caliber/#learningartifact","title":"LearningArtifact","text":""},{"location":"reference/caliber/#axion.caliber.LearningArtifact","title":"axion.caliber.LearningArtifact  <code>dataclass</code>","text":"<pre><code>LearningArtifact(title: str, content: str, tags: List[str], confidence: float, supporting_item_ids: List[str], recommended_actions: List[str] = list(), counterexamples: List[str] = list(), scope: Optional[str] = None, when_not_to_apply: Optional[str] = None)\n</code></pre> <p>A synthesized insight distilled from a cluster of evidence.</p>"},{"location":"reference/caliber/#pipelineresult","title":"PipelineResult","text":""},{"location":"reference/caliber/#axion.caliber.PipelineResult","title":"axion.caliber.PipelineResult  <code>dataclass</code>","text":"<pre><code>PipelineResult(clustering_result: PatternDiscoveryResult, learnings: List[LearningArtifact], filtered_count: int = 0, deduplicated_count: int = 0, validation_repairs: int = 0, sink_ids: List[str] = list(), metadata: Dict[str, Any] = dict())\n</code></pre> <p>Complete result from the evidence pipeline.</p>"},{"location":"reference/caliber/#provenance","title":"Provenance","text":""},{"location":"reference/caliber/#axion.caliber.Provenance","title":"axion.caliber.Provenance  <code>dataclass</code>","text":"<pre><code>Provenance(source_ref: Optional[str] = None, clustering_method: Optional[str] = None, total_analyzed: int = 0, supporting_count: int = 0, cluster_category: Optional[str] = None, timestamp: Optional[str] = None, metadata: Dict[str, Any] = dict())\n</code></pre> <p>Structured provenance attached to learning artifacts for sinks.</p>"},{"location":"reference/caliber/#metadataconfig","title":"MetadataConfig","text":""},{"location":"reference/caliber/#axion.caliber.MetadataConfig","title":"axion.caliber.MetadataConfig  <code>dataclass</code>","text":"<pre><code>MetadataConfig(allowed_keys: Optional[Set[str]] = None, denied_keys: Set[str] = (lambda: set(DEFAULT_DENIED_KEYS))(), max_keys: int = 6, max_value_length: int = 50, max_header_chars: int = 150, include_in_clustering: bool = False, include_in_distillation: bool = True)\n</code></pre> <p>Configuration for metadata handling in clustering and distillation.</p>"},{"location":"reference/caliber/#sinks-dedupers","title":"Sinks &amp; Dedupers","text":""},{"location":"reference/caliber/#inmemorysink","title":"InMemorySink","text":""},{"location":"reference/caliber/#axion.caliber.InMemorySink","title":"axion.caliber.InMemorySink","text":"<pre><code>InMemorySink()\n</code></pre> <p>Dict-based in-memory sink for testing/prototyping.</p>"},{"location":"reference/caliber/#jsonlsink","title":"JsonlSink","text":""},{"location":"reference/caliber/#axion.caliber.JsonlSink","title":"axion.caliber.JsonlSink","text":"<pre><code>JsonlSink(file_path: str)\n</code></pre> <p>Appends learning artifacts as JSON lines to a file.</p>"},{"location":"reference/caliber/#inmemorydeduper","title":"InMemoryDeduper","text":""},{"location":"reference/caliber/#axion.caliber.InMemoryDeduper","title":"axion.caliber.InMemoryDeduper","text":"<pre><code>InMemoryDeduper()\n</code></pre> <p>Title-based case-insensitive deduplication for testing.</p>"},{"location":"reference/caliber/#embeddingdeduper","title":"EmbeddingDeduper","text":""},{"location":"reference/caliber/#axion.caliber.EmbeddingDeduper","title":"axion.caliber.EmbeddingDeduper","text":"<pre><code>EmbeddingDeduper(embed_model=None, embed_model_name: str = 'text-embedding-3-small', similarity_threshold: float = 0.85, max_stored: int = 1000, reset_per_run: bool = True)\n</code></pre> <p>Embedding-based cosine similarity deduplication.</p> <p>Requires <code>axion[embeddings]</code> extra. Raises <code>ImportError</code> with a clear message if dependencies are unavailable (NO silent fallback).</p>"},{"location":"reference/caliber/#annotateditem-legacy","title":"AnnotatedItem (Legacy)","text":""},{"location":"reference/caliber/#axion.caliber.AnnotatedItem","title":"axion.caliber.AnnotatedItem  <code>dataclass</code>","text":"<pre><code>AnnotatedItem(record_id: str, score: int, notes: Optional[str] = None, timestamp: Optional[str] = None, query: Optional[str] = None, actual_output: Optional[str] = None)\n</code></pre> <p>A single annotated item with optional notes.</p>"},{"location":"reference/caliber/#misalignment-analysis","title":"Misalignment Analysis","text":""},{"location":"reference/caliber/#misalignmentanalyzer","title":"MisalignmentAnalyzer","text":""},{"location":"reference/caliber/#axion.caliber.MisalignmentAnalyzer","title":"axion.caliber.MisalignmentAnalyzer","text":"<pre><code>MisalignmentAnalyzer(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, llm_provider: Optional[str] = None, instruction: Optional[str] = None, max_examples: int = 10)\n</code></pre> <p>Analyzes misalignment between LLM judges and human annotators.</p> <p>This class leverages LLMHandler for structured output, automatic retries, and consistent LLM configuration with the rest of axion.</p> Example <p>from axion.caliber import MisalignmentAnalyzer</p> <p>results = [ ...     {'record_id': 'r1', 'human_score': 1, 'llm_score': 0, ...      'query': '...', 'actual_output': '...', 'llm_reasoning': '...'}, ...     {'record_id': 'r2', 'human_score': 0, 'llm_score': 1, ...      'query': '...', 'actual_output': '...', 'llm_reasoning': '...'}, ... ] criteria = \"Evaluate whether the response is accurate and helpful.\"</p> <p>Initialize MisalignmentAnalyzer.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the LLM model (e.g., 'gpt-4o', 'claude-sonnet-4-20250514')</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Pre-configured LLM instance</p> </li> <li> <code>llm_provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>LLM provider ('openai', 'anthropic')</p> </li> <li> <code>instruction</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom instruction to override default analysis prompt</p> </li> <li> <code>max_examples</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Max examples per category to include in analysis</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.MisalignmentAnalyzer--using-model_nameprovider-recommended","title":"Using model_name/provider (recommended)","text":"<p>analyzer = MisalignmentAnalyzer(model_name='gpt-4o', llm_provider='openai') analysis = await analyzer.analyze(results, criteria)</p> <p>print(f'Summary: {analysis.summary}') print(f'Recommendations: {analysis.recommendations}')</p>"},{"location":"reference/caliber/#axion.caliber.MisalignmentAnalyzer.analyze","title":"analyze  <code>async</code>","text":"<pre><code>analyze(results: Union[List[Dict[str, Any]], List[Any]], evaluation_criteria: str) -&gt; MisalignmentAnalysis\n</code></pre> <p>Analyze misalignment patterns asynchronously.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>Union[List[Dict[str, Any]], List[Any]]</code>)           \u2013            <p>List of evaluation results with human_score, llm_score, etc.</p> </li> <li> <code>evaluation_criteria</code>               (<code>str</code>)           \u2013            <p>The current evaluation criteria being used</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MisalignmentAnalysis</code>           \u2013            <p>MisalignmentAnalysis with patterns, summary, and recommendations</p> </li> </ul>"},{"location":"reference/caliber/#misalignmentanalysis","title":"MisalignmentAnalysis","text":""},{"location":"reference/caliber/#axion.caliber.MisalignmentAnalysis","title":"axion.caliber.MisalignmentAnalysis  <code>dataclass</code>","text":"<pre><code>MisalignmentAnalysis(total_misaligned: int, false_positives: int, false_negatives: int, patterns: List[MisalignmentPattern], summary: str, recommendations: List[str], metadata: Dict[str, Any] = dict())\n</code></pre> <p>Complete result from misalignment analysis.</p>"},{"location":"reference/caliber/#misalignmentpattern","title":"MisalignmentPattern","text":""},{"location":"reference/caliber/#axion.caliber.MisalignmentPattern","title":"axion.caliber.MisalignmentPattern  <code>dataclass</code>","text":"<pre><code>MisalignmentPattern(pattern_type: str, description: str, count: int, example_ids: List[str])\n</code></pre> <p>A discovered pattern in misalignment analysis.</p>"},{"location":"reference/caliber/#prompt-optimization","title":"Prompt Optimization","text":""},{"location":"reference/caliber/#promptoptimizer","title":"PromptOptimizer","text":""},{"location":"reference/caliber/#axion.caliber.PromptOptimizer","title":"axion.caliber.PromptOptimizer","text":"<pre><code>PromptOptimizer(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, llm_provider: Optional[str] = None, instruction: Optional[str] = None, max_examples: int = 10)\n</code></pre> <p>Optimizes evaluation prompts based on misalignment analysis.</p> <p>This class leverages LLMHandler for structured output, automatic retries, and consistent LLM configuration with the rest of axion.</p> Example <p>from axion.caliber import PromptOptimizer</p> <p>results = [ ...     {'record_id': 'r1', 'human_score': 1, 'llm_score': 0, ...      'query': '...', 'actual_output': '...', 'llm_reasoning': '...'}, ... ] criteria = \"Evaluate whether the response is accurate.\" system_prompt = \"You are an evaluator...\"</p> <p>optimizer = PromptOptimizer(model_name='gpt-4o', llm_provider='openai') optimized = await optimizer.optimize(results, criteria, system_prompt)</p> <p>print(f'Optimized criteria: {optimized.optimized_criteria}')</p> <p>Initialize PromptOptimizer.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the LLM model (e.g., 'gpt-4o', 'claude-sonnet-4-20250514')</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Pre-configured LLM instance</p> </li> <li> <code>llm_provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>LLM provider ('openai', 'anthropic')</p> </li> <li> <code>instruction</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom instruction to override default optimization prompt</p> </li> <li> <code>max_examples</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Max examples per category to include in optimization</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.PromptOptimizer.optimize","title":"optimize  <code>async</code>","text":"<pre><code>optimize(results: Union[List[Dict[str, Any]], List[Any]], current_criteria: str, system_prompt: str = '') -&gt; OptimizedPrompt\n</code></pre> <p>Optimize evaluation criteria asynchronously.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>Union[List[Dict[str, Any]], List[Any]]</code>)           \u2013            <p>List of evaluation results with human_score, llm_score, etc.</p> </li> <li> <code>current_criteria</code>               (<code>str</code>)           \u2013            <p>The current evaluation criteria to improve</p> </li> <li> <code>system_prompt</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The current system prompt (optional)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>OptimizedPrompt</code>           \u2013            <p>OptimizedPrompt with improved criteria and suggestions</p> </li> </ul>"},{"location":"reference/caliber/#optimizedprompt","title":"OptimizedPrompt","text":""},{"location":"reference/caliber/#axion.caliber.OptimizedPrompt","title":"axion.caliber.OptimizedPrompt  <code>dataclass</code>","text":"<pre><code>OptimizedPrompt(original_criteria: str, optimized_criteria: str, suggestions: List[PromptSuggestion], expected_improvement: str, metadata: Dict[str, Any] = dict())\n</code></pre> <p>Complete result from prompt optimization.</p>"},{"location":"reference/caliber/#promptsuggestion","title":"PromptSuggestion","text":""},{"location":"reference/caliber/#axion.caliber.PromptSuggestion","title":"axion.caliber.PromptSuggestion  <code>dataclass</code>","text":"<pre><code>PromptSuggestion(aspect: str, suggestion: str, rationale: str)\n</code></pre> <p>A single suggestion for prompt improvement.</p>"},{"location":"reference/caliber/#renderers","title":"Renderers","text":""},{"location":"reference/caliber/#caliberrenderer","title":"CaliberRenderer","text":""},{"location":"reference/caliber/#axion.caliber.CaliberRenderer","title":"axion.caliber.CaliberRenderer","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract renderer interface for CaliberHQ.</p> <p>Implementations provide UI rendering for different environments: - NotebookCaliberRenderer: Jupyter notebooks with rich styling - ConsoleCaliberRenderer: Terminal/CLI output - JsonCaliberRenderer: JSON-only (for web APIs)</p>"},{"location":"reference/caliber/#axion.caliber.CaliberRenderer.render_record","title":"render_record  <code>abstractmethod</code>","text":"<pre><code>render_record(record: UploadedRecord, annotation: Optional[Annotation] = None) -&gt; None\n</code></pre> <p>Render a single record for annotation.</p> <p>Parameters:</p> <ul> <li> <code>record</code>               (<code>UploadedRecord</code>)           \u2013            <p>The record to display</p> </li> <li> <code>annotation</code>               (<code>Optional[Annotation]</code>, default:                   <code>None</code> )           \u2013            <p>Existing annotation if any</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.CaliberRenderer.render_annotation_progress","title":"render_annotation_progress  <code>abstractmethod</code>","text":"<pre><code>render_annotation_progress(state: AnnotationState) -&gt; None\n</code></pre> <p>Render annotation progress.</p> <p>Parameters:</p> <ul> <li> <code>state</code>               (<code>AnnotationState</code>)           \u2013            <p>Current annotation state</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.CaliberRenderer.render_evaluation_result","title":"render_evaluation_result  <code>abstractmethod</code>","text":"<pre><code>render_evaluation_result(result: EvaluationResult) -&gt; None\n</code></pre> <p>Render evaluation results.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>EvaluationResult</code>)           \u2013            <p>Evaluation result to display</p> </li> </ul>"},{"location":"reference/caliber/#axion.caliber.CaliberRenderer.render_misalignment_analysis","title":"render_misalignment_analysis  <code>abstractmethod</code>","text":"<pre><code>render_misalignment_analysis(analysis: 'MisalignmentAnalysis') -&gt; None\n</code></pre> <p>Render misalignment analysis.</p> <p>Parameters:</p> <ul> <li> <code>analysis</code>               (<code>'MisalignmentAnalysis'</code>)           \u2013            <p>Analysis result to display</p> </li> </ul>"},{"location":"reference/caliber/#notebookcaliberrenderer","title":"NotebookCaliberRenderer","text":""},{"location":"reference/caliber/#axion.caliber.NotebookCaliberRenderer","title":"axion.caliber.NotebookCaliberRenderer","text":"<p>               Bases: <code>CaliberRenderer</code></p> <p>Notebook-specific renderer for CaliberHQ.</p> <p>Provides rich HTML and styled pandas output for Jupyter notebooks.</p>"},{"location":"reference/caliber/#axion.caliber.NotebookCaliberRenderer.render_record","title":"render_record","text":"<pre><code>render_record(record: UploadedRecord, annotation: Optional[Annotation] = None) -&gt; None\n</code></pre> <p>Render a single record for annotation.</p>"},{"location":"reference/caliber/#axion.caliber.NotebookCaliberRenderer.render_annotation_progress","title":"render_annotation_progress","text":"<pre><code>render_annotation_progress(state: AnnotationState) -&gt; None\n</code></pre> <p>Render annotation progress.</p>"},{"location":"reference/caliber/#axion.caliber.NotebookCaliberRenderer.render_evaluation_result","title":"render_evaluation_result","text":"<pre><code>render_evaluation_result(result: EvaluationResult) -&gt; None\n</code></pre> <p>Render evaluation results with styled tables.</p>"},{"location":"reference/caliber/#axion.caliber.NotebookCaliberRenderer.render_misalignment_analysis","title":"render_misalignment_analysis","text":"<pre><code>render_misalignment_analysis(analysis: 'MisalignmentAnalysis') -&gt; None\n</code></pre> <p>Render misalignment analysis.</p>"},{"location":"reference/caliber/#consolecaliberrenderer","title":"ConsoleCaliberRenderer","text":""},{"location":"reference/caliber/#axion.caliber.ConsoleCaliberRenderer","title":"axion.caliber.ConsoleCaliberRenderer","text":"<p>               Bases: <code>CaliberRenderer</code></p> <p>Console/terminal renderer for CaliberHQ.</p> <p>Provides text-based output suitable for CLI environments.</p>"},{"location":"reference/caliber/#axion.caliber.ConsoleCaliberRenderer.render_record","title":"render_record","text":"<pre><code>render_record(record: UploadedRecord, annotation: Optional[Annotation] = None) -&gt; None\n</code></pre> <p>Render a single record for annotation.</p>"},{"location":"reference/caliber/#axion.caliber.ConsoleCaliberRenderer.render_annotation_progress","title":"render_annotation_progress","text":"<pre><code>render_annotation_progress(state: AnnotationState) -&gt; None\n</code></pre> <p>Render annotation progress.</p>"},{"location":"reference/caliber/#axion.caliber.ConsoleCaliberRenderer.render_evaluation_result","title":"render_evaluation_result","text":"<pre><code>render_evaluation_result(result: EvaluationResult) -&gt; None\n</code></pre> <p>Render evaluation results.</p>"},{"location":"reference/caliber/#axion.caliber.ConsoleCaliberRenderer.render_misalignment_analysis","title":"render_misalignment_analysis","text":"<pre><code>render_misalignment_analysis(analysis: 'MisalignmentAnalysis') -&gt; None\n</code></pre> <p>Render misalignment analysis.</p>"},{"location":"reference/caliber/#jsoncaliberrenderer","title":"JsonCaliberRenderer","text":"<p>CaliberHQ Guide  Pattern Discovery Deep Dive  Example Selector Deep Dive </p>"},{"location":"reference/caliber/#axion.caliber.JsonCaliberRenderer","title":"axion.caliber.JsonCaliberRenderer","text":"<p>               Bases: <code>CaliberRenderer</code></p> <p>JSON-first renderer for CaliberHQ.</p> <p>Provides no-op rendering for API environments where data is consumed as JSON rather than displayed.</p>"},{"location":"reference/caliber/#axion.caliber.JsonCaliberRenderer.render_record","title":"render_record","text":"<pre><code>render_record(record: UploadedRecord, annotation: Optional[Annotation] = None) -&gt; None\n</code></pre> <p>No-op: JSON renderer does not display records.</p>"},{"location":"reference/caliber/#axion.caliber.JsonCaliberRenderer.render_annotation_progress","title":"render_annotation_progress","text":"<pre><code>render_annotation_progress(state: AnnotationState) -&gt; None\n</code></pre> <p>No-op: JSON renderer does not display progress.</p>"},{"location":"reference/caliber/#axion.caliber.JsonCaliberRenderer.render_evaluation_result","title":"render_evaluation_result","text":"<pre><code>render_evaluation_result(result: EvaluationResult) -&gt; None\n</code></pre> <p>No-op: JSON renderer does not display results.</p>"},{"location":"reference/caliber/#axion.caliber.JsonCaliberRenderer.render_misalignment_analysis","title":"render_misalignment_analysis","text":"<pre><code>render_misalignment_analysis(analysis: 'MisalignmentAnalysis') -&gt; None\n</code></pre> <p>No-op: JSON renderer does not display analysis.</p>"},{"location":"reference/dataset/","title":"Dataset API Reference","text":"<p>Core data structures for building and managing evaluation datasets.</p> <pre><code>from axion import Dataset, DatasetItem\n</code></pre> D <p>Dataset</p> <p>Container for evaluation items. Supports JSON/CSV/DataFrame I/O, filtering, merging, and synthetic generation.</p> I <p>DatasetItem</p> <p>Individual test case with query, expected/actual output, context, metadata, and conversation history.</p>"},{"location":"reference/dataset/#dataset","title":"Dataset","text":""},{"location":"reference/dataset/#axion.dataset.Dataset","title":"axion.dataset.Dataset  <code>dataclass</code>","text":"<pre><code>Dataset(name: Optional[str] = None, description: str = '', version: str = '1.0', created_at: str = (lambda: current_datetime())(), metadata: Optional[str] = None, items: List[DatasetItem] = list(), _default_catch_all: str = ADDITIONAL_INPUT, _item_map: Dict[str, DatasetItem] = dict(), _synthetic_data: Optional[List[Dict[str, Any]]] = None)\n</code></pre> <p>               Bases: <code>RichSerializer</code></p> <p>Represents a structured dataset for evaluation purposes, supporting both single and multi-turn items.</p> <p>This class manages a collection of DatasetItem objects and provides functionality for loading, saving, filtering, and transforming datasets.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>)           \u2013            <p>Name of the dataset</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Description of the dataset's purpose or contents</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Version identifier</p> </li> <li> <code>created_at</code>               (<code>str</code>)           \u2013            <p>ISO format timestamp of creation</p> </li> <li> <code>metadata</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional metadata (stored as JSON)</p> </li> <li> <code>items</code>               (<code>List[DatasetItem]</code>)           \u2013            <p>List of DatasetItem objects</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.items","title":"items  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>items: List[DatasetItem] = field(default_factory=list)\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.Dataset.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(name: Optional[str] = None, items: Optional[List[Union[Dict[str, Any], str]]] = None, ignore_extra_keys: bool = False, **kwargs) -&gt; Dataset\n</code></pre> <p>Creates a new dataset with initial items.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset name</p> </li> <li> <code>items</code>               (<code>Optional[List[Union[Dict[str, Any], str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of items (dicts or strings)</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in dictionaries. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters passed to the Dataset constructor.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.add_item","title":"add_item","text":"<pre><code>add_item(item: Union[DatasetItem, Dict[str, Any]], ignore_extra_keys: bool = False) -&gt; DatasetItem\n</code></pre> <p>Add an item to the dataset, handling both single-turn and multi-turn items.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>Union[DatasetItem, Dict[str, Any]]</code>)           \u2013            <p>Either a DatasetItem instance or a dictionary containing item data</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in the dictionary. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>The DatasetItem instance that was added to the dataset</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.add_items","title":"add_items","text":"<pre><code>add_items(items: List[Union[DatasetItem, Dict[str, Any]]], ignore_extra_keys: bool = False) -&gt; List[DatasetItem]\n</code></pre> <p>Add multiple items to the dataset.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>List[Union[DatasetItem, Dict[str, Any]]]</code>)           \u2013            <p>List of DatasetItem instances or dictionaries</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in dictionaries. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[DatasetItem]</code>           \u2013            <p>List of added DatasetItem instances</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.get_item_by_id","title":"get_item_by_id","text":"<pre><code>get_item_by_id(item_id: str) -&gt; Optional[DatasetItem]\n</code></pre> <p>Retrieve an item by its ID.</p> <p>Parameters:</p> <ul> <li> <code>item_id</code>               (<code>str</code>)           \u2013            <p>ID of the item to find</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[DatasetItem]</code>           \u2013            <p>DatasetItem if found, None otherwise</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.filter","title":"filter","text":"<pre><code>filter(condition: Callable[[DatasetItem], bool], dataset_name: Optional[str] = None) -&gt; Dataset\n</code></pre> <p>Filters the dataset based on a condition and returns a new Dataset.</p>"},{"location":"reference/dataset/#axion.dataset.Dataset.read_json","title":"read_json  <code>classmethod</code>","text":"<pre><code>read_json(file_path: Union[str, Path], name: Optional[str] = None, ignore_extra_keys: bool = False) -&gt; Dataset\n</code></pre> <p>Creates a dataset from a JSON file, correctly parsing multi-turn conversations.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to the JSON file</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset name</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in dictionaries. Defaults to False.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.read_csv","title":"read_csv  <code>classmethod</code>","text":"<pre><code>read_csv(file_path: Union[str, Path], name: Optional[str] = None, column_mapping: Optional[Dict[str, str]] = None, ignore_extra_keys: bool = False, **kwargs) -&gt; Dataset\n</code></pre> <p>Creates a dataset from a CSV file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to the CSV file</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset name</p> </li> <li> <code>column_mapping</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional mapping to rename columns</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in dictionaries. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters passed to read_dataframe.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.read_dataframe","title":"read_dataframe  <code>classmethod</code>","text":"<pre><code>read_dataframe(dataframe: DataFrame, name: Optional[str] = None, ignore_extra_keys: bool = False, **kwargs) -&gt; Dataset\n</code></pre> <p>Creates a Dataset from a pandas DataFrame, safely deserializing JSON and Python literals. All fields must be included in DataFrame rows to correctly map to DatasetItem.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>Input DataFrame to read from.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset name.</p> </li> <li> <code>ignore_extra_keys</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only use keys that match DatasetItem fields, ignoring any extra keys in dictionaries. Defaults to False.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters passed to the Dataset constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (              <code>Dataset</code> )          \u2013            <p>A populated Dataset instance.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.to_json","title":"to_json","text":"<pre><code>to_json(file_path: str) -&gt; None\n</code></pre> <p>Save dataset to JSON file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path where to save the JSON file</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.to_csv","title":"to_csv","text":"<pre><code>to_csv(file_path: str, remove_aliased: bool = True) -&gt; None\n</code></pre> <p>Save dataset to CSV file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>Path where to save the CSV file.</p> </li> <li> <code>remove_aliased</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True remove aliased model field keys</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(flatten_nested_json: bool = False, sep: str = '.', remove_aliased: bool = True) -&gt; DataFrame\n</code></pre> <p>Converts the dataset to a pandas DataFrame, serializing complex fields to JSON strings.</p> <p>Parameters:</p> <ul> <li> <code>flatten_nested_json</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, nested objects will be flattened into separate columns.                  If False (default), they will be stored as JSON strings.</p> </li> <li> <code>sep</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>Separator for flattening.</p> </li> <li> <code>remove_aliased</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True remove aliased model field keys.</p> </li> </ul> <p>Returns:     A pandas DataFrame representing the dataset.</p>"},{"location":"reference/dataset/#axion.dataset.Dataset.load_dataframe","title":"load_dataframe","text":"<pre><code>load_dataframe(dataframe: DataFrame) -&gt; None\n</code></pre> <p>Load dataset items from a DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>dataframe</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame containing dataset items.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Return summary statistics about the dataset.</p>"},{"location":"reference/dataset/#axion.dataset.Dataset.get_summary_table","title":"get_summary_table","text":"<pre><code>get_summary_table(title: str = 'Dataset Summary', **kwargs) -&gt; None\n</code></pre> <p>Return summary statistics about the dataset in rich table format.</p> <p>Parameters:</p> <ul> <li> <code>title</code>               (<code>str</code>, default:                   <code>'Dataset Summary'</code> )           \u2013            <p>Title for the log output.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the logging method.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.execute_dataset_items_from_api","title":"execute_dataset_items_from_api","text":"<pre><code>execute_dataset_items_from_api(api_name: str, config: Union[str, Dict[str, Any], Path], max_concurrent: int = 5, show_progress: bool = True, retry_config: Optional[Union[Any, Dict[str, Any]]] = None, require_success: bool = False, additional_config: Optional[Dict[str, Any]] = None, **kwargs) -&gt; None\n</code></pre> <p>Synchronously executes API calls using the specified API runner and attaches responses to the dataset items. Useful for batch-processing queries via a registered API.</p> <p>Internally runs async code but exposes a sync interface to the user.</p> <p>Parameters:</p> <ul> <li> <code>api_name</code>               (<code>str</code>)           \u2013            <p>The name of the registered API to use for execution.</p> </li> <li> <code>config</code>               (<code>str | dict | Path</code>)           \u2013            <p>Config for authenticating with the API.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Max number of concurrent API requests. Defaults to 5.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show progress bars using tqdm.</p> </li> <li> <code>retry_config</code>               (<code>RetryConfig | Dict</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for retrying logic.</p> </li> <li> <code>require_success</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>(bool, optional): If True, remove items from dataset when response.status != 'success'.</p> </li> <li> <code>additional_config</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Extra configuration options for the runner.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Extra arguments passed to the executor's <code>execute_batch</code> method.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.merge_response_into_dataset_items","title":"merge_response_into_dataset_items  <code>staticmethod</code>","text":"<pre><code>merge_response_into_dataset_items(items: List[DatasetItem], responses: List[RichBaseModel], require_success: bool = False) -&gt; List[DatasetItem]\n</code></pre> <p>Updates DatasetItem instances with fields from corresponding APIResponseData.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>List[DatasetItem]</code>)           \u2013            <p>List of DatasetItem objects to update.</p> </li> <li> <code>responses</code>               (<code>List[RichBaseModel]</code>)           \u2013            <p>List of APIResponseData objects with new runtime values.</p> </li> <li> <code>require_success</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only keep items when response.status == 'success'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[DatasetItem]</code>           \u2013            <p>List of DatasetItem objects that were successfully processed (if require_success=True)</p> </li> <li> <code>List[DatasetItem]</code>           \u2013            <p>or all items (if require_success=False).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.Dataset.synthetic_generate_from_directory","title":"synthetic_generate_from_directory","text":"<pre><code>synthetic_generate_from_directory(directory_path: str, llm, params: GenerationParams, embed_model: None, max_concurrent: int = 3, show_progress: bool = True, **kwargs)\n</code></pre> <p>Generates synthetic QA data from a directory of documents.</p> <p>This method uses the <code>DocumentQAGenerator</code> to process documents in the given directory, producing synthetic question-answer pairs using the provided language model (LLM) and generation parameters. The results are transformed into a format compatible with the dataset interface (<code>query</code>, <code>expected_output</code>) and added to the dataset.</p> <p>Parameters:</p> <ul> <li> <code>directory_path</code>               (<code>str</code>)           \u2013            <p>Path to the directory containing documents to process.</p> </li> <li> <code>llm</code>           \u2013            <p>A language model instance that implements method for generation.</p> </li> <li> <code>params</code>               (<code>GenerationParams</code>)           \u2013            <p>A configuration object that defines generation settings such as number of QA pairs, difficulty, chunking behavior, etc.</p> </li> <li> <code>embed_model</code>               (<code>None</code>)           \u2013            <p>An embedding model used for semantic parsing.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The maximum number of documents to process concurrently. Defaults to 3.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show progress bars using tqdm</p> </li> </ul>"},{"location":"reference/dataset/#datasetitem","title":"DatasetItem","text":"<p>Working with Datasets Guide  Running Evaluations </p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem","title":"axion.dataset.DatasetItem","text":"<pre><code>DatasetItem(**data)\n</code></pre> <p>               Bases: <code>RichDatasetBaseModel</code></p> <p>Represents a single evaluation data point, supporting both single-turn and multi-turn conversations.</p> <p>This model is designed to store all relevant information required for evaluating LLM performance, including the input query, expected and actual outputs, retrieved context, evaluation criteria, and additional metadata. It supports both automated evaluation (binary judgments, critiques) and richer evaluation with tool usage tracking.</p> <p>Attributes:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the item (auto-generated if not provided).</p> </li> <li> <code>query</code>               (<code>Optional[str]</code>)           \u2013            <p>The input query or prompt for single-turn evaluation. Aliased as <code>query</code> for backward compatibility.</p> </li> <li> <code>conversation</code>               (<code>Optional[MultiTurnConversation]</code>)           \u2013            <p>Multi-turn conversation structure containing a sequence of messages. Aliased to <code>conversation</code>.</p> </li> <li> <code>expected_output</code>               (<code>Optional[str]</code>)           \u2013            <p>The reference/expected output for single-turn evaluation. Aliased to <code>expected_output</code>.</p> </li> <li> <code>actual_output</code>               (<code>Optional[str]</code>)           \u2013            <p>The system's generated response for the given query.</p> </li> <li> <code>retrieved_content</code>               (<code>Optional[List[str]]</code>)           \u2013            <p>A list of retrieved documents or contextual snippets used in generating the response.</p> </li> <li> <code>latency</code>               (<code>Optional[float]</code>)           \u2013            <p>Response time in seconds for generating the <code>actual_output</code>.</p> </li> <li> <code>judgment</code>               (<code>Optional[Union[str, int]]</code>)           \u2013            <p>A short, binary or categorical evaluation decision (e.g., 1/0, pass/fail, approve/decline).</p> </li> <li> <code>critique</code>               (<code>Optional[str]</code>)           \u2013            <p>A detailed explanation or rationale supporting the <code>judgment</code>.</p> </li> <li> <code>conversation_extraction_strategy</code>               (<code>Literal['first', 'last']</code>)           \u2013            <p>Defines whether to extract <code>query</code> and <code>actual_output</code> from the first or last messages in a multi-turn conversation. Defaults to 'last'.</p> </li> <li> <code>acceptance_criteria</code>               (<code>Optional[List[str]]</code>)           \u2013            <p>User-defined definitions of what qualifies as an acceptable/correct response.</p> </li> <li> <code>additional_input</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Arbitrary key-value pairs providing extra inputs for the evaluation context.</p> </li> <li> <code>metadata</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional metadata as a JSON string for storing structured information.</p> </li> <li> <code>trace</code>               (<code>Optional[str]</code>)           \u2013            <p>Execution trace information, stored as a JSON string.</p> </li> <li> <code>trace_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional[str]: Trace ID for the original observation from tracing provider.</p> </li> </ul> <p>observation_id: Optional[str]:         Observation ID for the original observation from tracing provider.         This is the ID of the specific observation that was evaluated.     additional_output (Dict[str, Any]):         Extra outputs generated by the system, useful for debugging or extended evaluation.     tools_called (Optional[List[ToolCall]]):         A list of tools the system actually invoked during response generation.     expected_tools (Optional[List[ToolCall]]):         A list of tools that should have been invoked according to the evaluation criteria.     user_tags (List[str]):         A list of custom tags to apply to all tool calls in the conversation.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(default_factory=lambda: str(uuid7()), alias='dataset_id')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.query","title":"query  <code>property</code> <code>writable</code>","text":"<pre><code>query: Optional[str]\n</code></pre> <p>Provides a unified way to access the user's query based on the extraction strategy.</p> <p>If the strategy is 'last' (default), it returns the last user message. If the strategy is 'first', it returns the first user message.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.actual_output","title":"actual_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>actual_output: Optional[str] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.expected_output","title":"expected_output  <code>property</code> <code>writable</code>","text":"<pre><code>expected_output: Optional[str]\n</code></pre> <p>Provides a unified way to access the expected output.</p> <p>If the item is a multi-turn conversation, this returns the <code>reference_text</code> if set. If it's a single-turn item, it returns the stored expected output.</p> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>The expected output as a string, or None if not applicable.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.conversation","title":"conversation  <code>property</code>","text":"<pre><code>conversation: Optional[MultiTurnConversation]\n</code></pre> <p>Provides direct access to the multi-turn conversation object.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.retrieved_content","title":"retrieved_content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>retrieved_content: Optional[List[str]] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.latency","title":"latency  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>latency: Optional[float] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.judgment","title":"judgment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>judgment: Optional[Union[str, int]] = Field(default=None, description='A short, binary decision on the output (e.g., 1/0, pass/fail, approve/decline).')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.critique","title":"critique  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>critique: Optional[str] = Field(default=None, description='A detailed explanation or feedback supporting the judgment.')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.acceptance_criteria","title":"acceptance_criteria  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>acceptance_criteria: Optional[List[str]] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.additional_input","title":"additional_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_input: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.additional_output","title":"additional_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_output: Dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.metadata","title":"metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metadata: Optional[str] = Field(None, alias='dataset_metadata')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.trace","title":"trace  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace: Optional[str] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.trace_id","title":"trace_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trace_id: Optional[str] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.observation_id","title":"observation_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>observation_id: Optional[str] = None\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.actual_ranking","title":"actual_ranking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>actual_ranking: Optional[List[Dict[str, Any]]] = Field(default=None, description='Ordered list of retrieved items, e.g., [{\"id\": \"doc1\", \"score\": 0.9}, {\"id\": \"doc2\", \"score\": 0.8}]')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.expected_ranking","title":"expected_ranking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expected_ranking: Optional[List[Dict[str, Any]]] = Field(default=None, description='Ground truth reference. For IR, e.g., [{\"id\": \"doc1\", \"relevance\": 1.0}, {\"id\": \"doc_abc\", \"relevance\": 0.5}]')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.tools_called","title":"tools_called  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools_called: Optional[List[ToolCall]] = Field(default=None, description='Tools that were actually called by the system')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.expected_tools","title":"expected_tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>expected_tools: Optional[List[ToolCall]] = Field(default=None, description='Tools that should have been called')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.user_tags","title":"user_tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>user_tags: List[str] = Field(default_factory=list, description='A list of custom tags to apply to all tool calls in the conversation.')\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.conversation_extraction_strategy","title":"conversation_extraction_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>conversation_extraction_strategy: Literal['first', 'last'] = Field(default='last', description=\"Determines whether to extract 'query' and 'actual_output' from the 'first' or 'last' messages in a conversation.\")\n</code></pre>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.conversation_stats","title":"conversation_stats  <code>property</code>","text":"<pre><code>conversation_stats: Optional[Dict[str, int]]\n</code></pre> <p>A dictionary of statistics about the conversation.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.agent_trajectory","title":"agent_trajectory  <code>property</code>","text":"<pre><code>agent_trajectory: Optional[List[str]]\n</code></pre> <p>An ordered list of tool names called, representing the agent's execution path.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.has_errors","title":"has_errors  <code>property</code>","text":"<pre><code>has_errors: Union[bool, None]\n</code></pre> <p>Returns True if any tool message in the conversation is marked as an error.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.to_transcript","title":"to_transcript","text":"<pre><code>to_transcript() -&gt; str\n</code></pre> <p>Converts the conversation messages into a human-readable string transcript.</p> <p>If the item is not a multi-turn conversation, it returns an empty string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A formatted string representing the entire conversation.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.extract_by_tag","title":"extract_by_tag","text":"<pre><code>extract_by_tag(tag: str) -&gt; List[tuple[ToolCall, Optional[ToolMessage]]]\n</code></pre> <p>Extracts tool interactions from the conversation that match a specific tag.</p> <p>Parameters:</p> <ul> <li> <code>tag</code>               (<code>str</code>)           \u2013            <p>The tag to filter by (e.g., 'RAG', 'GUARDRAIL').</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[tuple[ToolCall, Optional[ToolMessage]]]</code>           \u2013            <p>A list of tuples, where each tuple contains the tagged ToolCall</p> </li> <li> <code>List[tuple[ToolCall, Optional[ToolMessage]]]</code>           \u2013            <p>and its corresponding ToolMessage (or None if not found).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.get","title":"get","text":"<pre><code>get(key: str, default: Any = None) -&gt; Any\n</code></pre> <p>Get an attribute value by key, similar to dict.get(). This method correctly handles properties like 'query'.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The attribute name to retrieve.</p> </li> <li> <code>default</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Value to return if attribute doesn't exist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The attribute value or default if not found.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.keys","title":"keys","text":"<pre><code>keys() -&gt; List[str]\n</code></pre> <p>Return all public attribute names, including properties and aliases.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>A sorted list of public-facing field and property names.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.values","title":"values","text":"<pre><code>values() -&gt; List[Any]\n</code></pre> <p>Return all public attribute values, corresponding to the .keys() method.</p> <p>Returns:</p> <ul> <li> <code>List[Any]</code>           \u2013            <p>A list of values for the public-facing fields and properties.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.items","title":"items","text":"<pre><code>items() -&gt; List[tuple]\n</code></pre> <p>Return all (key, value) pairs for public attributes.</p> <p>Returns:</p> <ul> <li> <code>List[tuple]</code>           \u2013            <p>A list of (key, value) tuples for public-facing fields and properties.</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset","title":"subset","text":"<pre><code>subset(fields: List[str], keep_id: bool = True, copy_annotations: bool = False) -&gt; DatasetItem\n</code></pre> <p>Create a new DatasetItem with only the specified fields, all others set to None/empty.</p> <p>Parameters:</p> <ul> <li> <code>fields</code>               (<code>List[str]</code>)           \u2013            <p>List of field names to keep (e.g., ['query', 'expected_output'])</p> </li> <li> <code>keep_id</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to preserve the original ID (default: True)</p> </li> <li> <code>copy_annotations</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to copy annotations (judgment and critique) to the new item (default: False)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>New DatasetItem instance with only specified fields populated</p> </li> </ul> Example"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset--get-item-with-only-query-and-expected_output","title":"Get item with only query and expected_output","text":"<p>subset_item = item.subset(['query', 'expected_output'])</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.subset--get-item-with-query-actual_output-and-preserve-annotations","title":"Get item with query, actual_output, and preserve annotations","text":"<p>subset_item = item.subset(['query', 'actual_output'], copy_annotations=True)</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.evaluation_fields","title":"evaluation_fields","text":"<pre><code>evaluation_fields() -&gt; DatasetItem\n</code></pre> <p>Extract just the evaluation fields.</p>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.update","title":"update","text":"<pre><code>update(other: Union[DatasetItem, Dict[str, Any]], overwrite: bool = True) -&gt; DatasetItem\n</code></pre> <p>Update this DatasetItem with values from another DatasetItem or dictionary. This method correctly handles aliases and special merge logic for lists/dicts.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Union[DatasetItem, Dict[str, Any]]</code>)           \u2013            <p>Another DatasetItem instance or a dictionary to update from.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, overwrite existing values. If False, only fill empty fields.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>The updated DatasetItem instance (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.update_runtime","title":"update_runtime","text":"<pre><code>update_runtime(**kwargs) -&gt; DatasetItem\n</code></pre> <p>Update runtime-related fields such as actual_output or retrieved_content.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>           \u2013            <p>Runtime fields to update.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>Updated DatasetItem (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.merge_metadata","title":"merge_metadata","text":"<pre><code>merge_metadata(metadata: Union[str, Dict[str, Any]]) -&gt; DatasetItem\n</code></pre> <p>Merge new metadata into the existing metadata field.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Union[str, Dict[str, Any]]</code>)           \u2013            <p>A dictionary or JSON string to merge.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>Updated DatasetItem (self).</p> </li> </ul>"},{"location":"reference/dataset/#axion.dataset.DatasetItem.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any]) -&gt; DatasetItem\n</code></pre> <p>Create a DatasetItem from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Dictionary containing item data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DatasetItem</code>           \u2013            <p>New DatasetItem instance</p> </li> </ul>"},{"location":"reference/eval-tree/","title":"Eval Tree API Reference","text":"<p>Hierarchical evaluation orchestration for composing complex evaluation workflows.</p> <pre><code>from axion.eval_tree import EvalTree\n</code></pre> T <p>EvalTree</p> <p>Hierarchical scoring model that builds a tree from a configuration and executes metrics using an optimized two-phase batch process.</p> C <p>Config-Driven</p> <p>Define scoring trees via YAML, dict, or Config objects. Supports weighted aggregation, strategy overrides, and nested component hierarchies.</p>"},{"location":"reference/eval-tree/#evaltree","title":"EvalTree","text":"<p>Hierarchical Scoring Guide  Running Evaluations </p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree","title":"axion.eval_tree.EvalTree","text":"<pre><code>EvalTree(config: Union[Config, Dict[str, Any], str], max_concurrent: int = 5, summary_generator: Optional[BaseSummary] = None, enable_internal_caching: bool = True, trace_granularity: TraceGranularity = SEPARATE)\n</code></pre> <p>               Bases: <code>TreeMixin</code></p> <p>Hierarchical scoring model that builds a tree from a configuration and executes metrics using an optimized two-phase batch process.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.elapsed_time","title":"elapsed_time  <code>property</code>","text":"<pre><code>elapsed_time: Union[float, None]\n</code></pre> <p>Execution Time</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.summary","title":"summary  <code>property</code>","text":"<pre><code>summary: Union[Dict[str, Any], None]\n</code></pre> <p>Model Summary</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(input_data: Union[DatasetItem, Dict[str, Any]]) -&gt; TestResult\n</code></pre> <p>Executes the evaluation for a single data item.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.batch_execute","title":"batch_execute  <code>async</code>","text":"<pre><code>batch_execute(evaluation_inputs: Union[List[DatasetItem], Dataset, DataFrame], evaluation_name: str = 'EvalTree Evaluation', show_progress: bool = True) -&gt; EvaluationResult\n</code></pre> <p>Executes the evaluation using an optimized two-phase process: 1. Batch Calculation: Runs all leaf metrics across the dataset. 2. In-Memory Aggregation: Computes hierarchical scores from the results.</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.get_node","title":"get_node","text":"<pre><code>get_node(name: str) -&gt; Optional[Node]\n</code></pre> <p>Get node by name</p>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.get_metric_summary","title":"get_metric_summary","text":"<pre><code>get_metric_summary() -&gt; Dict[str, Any]\n</code></pre>"},{"location":"reference/eval-tree/#axion.eval_tree.EvalTree.display","title":"display  <code>classmethod</code>","text":"<pre><code>display()\n</code></pre> <p>Display Usage Documentation</p>"},{"location":"reference/insight-extractor/","title":"Insight Extractor Reference","text":"<p>API reference for the cross-metric insight extraction module.</p> <pre><code>from axion.reporting import (\n    InsightExtractor,\n    InsightPattern,\n    InsightResult,\n)\n</code></pre> I <p>InsightExtractor</p> <p>Bridges IssueExtractor output with EvidencePipeline for cross-metric pattern discovery and learning distillation.</p> P <p>InsightPattern</p> <p>A discovered cluster enriched with cross-metric metadata \u2014 which metrics are involved, test case coverage, and confidence.</p> R <p>InsightResult</p> <p>Complete result containing patterns, learnings, and the full pipeline result for advanced access.</p>"},{"location":"reference/insight-extractor/#insightextractor","title":"InsightExtractor","text":""},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor.InsightExtractor","title":"axion.reporting.insight_extractor.InsightExtractor","text":"<pre><code>InsightExtractor(model_name: Optional[str] = None, llm=None, llm_provider: Optional[str] = None, method: ClusteringMethod = LLM, recurrence_threshold: int = 2, max_items: int = 50, min_category_size: int = 2, pipeline: Optional[EvidencePipeline] = None, **pipeline_kwargs)\n</code></pre> <p>Bridges IssueExtractor output with EvidencePipeline for cross-metric pattern discovery.</p>"},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor.InsightExtractor.analyze","title":"analyze  <code>async</code>","text":"<pre><code>analyze(extraction_result: IssueExtractionResult) -&gt; InsightResult\n</code></pre> <p>Analyze extracted issues for cross-metric patterns.</p> <p>Parameters:</p> <ul> <li> <code>extraction_result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>Output from IssueExtractor.extract_from_evaluation()</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InsightResult</code>           \u2013            <p>InsightResult with discovered patterns and learnings.</p> </li> </ul>"},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor.InsightExtractor.analyze_sync","title":"analyze_sync","text":"<pre><code>analyze_sync(extraction_result: IssueExtractionResult) -&gt; InsightResult\n</code></pre> <p>Synchronous wrapper for analyze().</p>"},{"location":"reference/insight-extractor/#data-classes","title":"Data Classes","text":""},{"location":"reference/insight-extractor/#insightpattern","title":"InsightPattern","text":""},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor.InsightPattern","title":"axion.reporting.insight_extractor.InsightPattern  <code>dataclass</code>","text":"<pre><code>InsightPattern(category: str, description: str, count: int, issue_ids: List[str], metrics_involved: List[str], is_cross_metric: bool, distinct_test_cases: int, examples: List[str], confidence: Optional[float] = None)\n</code></pre> <p>A cross-metric pattern discovered from evaluation issues.</p>"},{"location":"reference/insight-extractor/#insightresult","title":"InsightResult","text":""},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor.InsightResult","title":"axion.reporting.insight_extractor.InsightResult  <code>dataclass</code>","text":"<pre><code>InsightResult(patterns: List[InsightPattern], learnings: List[LearningArtifact], total_issues_analyzed: int, clustering_method: ClusteringMethod, pipeline_result: PipelineResult)\n</code></pre> <p>Complete result from insight extraction.</p>"},{"location":"reference/insight-extractor/#adapter-function","title":"Adapter Function","text":""},{"location":"reference/insight-extractor/#_issue_to_evidence","title":"_issue_to_evidence","text":"<p>Insight Extraction Guide  Issue Extractor Reference  Pattern Discovery </p>"},{"location":"reference/insight-extractor/#axion.reporting.insight_extractor._issue_to_evidence","title":"axion.reporting.insight_extractor._issue_to_evidence","text":"<pre><code>_issue_to_evidence(issue: ExtractedIssue) -&gt; Optional[EvidenceItem]\n</code></pre> <p>Convert ExtractedIssue -&gt; EvidenceItem. Returns None if no meaningful text.</p>"},{"location":"reference/issue-extractor/","title":"Issue Extractor Reference","text":"<p>API reference for the issue extraction and signal analysis module.</p> <pre><code>from axion.reporting import (\n    IssueExtractor,\n    ExtractedIssue,\n    IssueExtractionResult,\n    IssueGroup,\n    LLMSummaryInput,\n    MetricSignalAdapter,\n    SignalAdapterRegistry,\n)\n</code></pre> E <p>IssueExtractor</p> <p>Core engine for extracting low-score signals from evaluation results into structured, actionable issues.</p> R <p>SignalAdapterRegistry</p> <p>Registry for metric signal adapters. Maps metric keys to extraction rules for pass/fail signals.</p> A <p>MetricSignalAdapter</p> <p>Adapter defining how to extract issues from a specific metric's signals \u2014 headline signals, failure values, and context.</p> D <p>Data Classes</p> <p>Structured types for extracted issues, groups, summaries, and LLM input \u2014 the output of the extraction pipeline.</p>"},{"location":"reference/issue-extractor/#issueextractor","title":"IssueExtractor","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor","title":"axion.reporting.issue_extractor.IssueExtractor","text":"<pre><code>IssueExtractor(score_threshold: float = 0.0, include_nan: bool = False, include_context_fields: Optional[List[str]] = None, metric_names: Optional[List[str]] = None, max_issues: Optional[int] = None, sample_rate: Optional[float] = None)\n</code></pre> <p>Extracts low-score signals from evaluation results for LLM-based issue summarization.</p> <p>This class reads existing signal data from MetricScore objects and extracts issues (low-score signals) in a normalized format suitable for analysis.</p> <p>Initialize the IssueExtractor.</p> <p>Parameters:</p> <ul> <li> <code>score_threshold</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Signals with scores at or below this threshold are considered issues. Default 0.0 means only explicit failures.</p> </li> <li> <code>include_nan</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include signals with NaN scores as issues.</p> </li> <li> <code>include_context_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Fields to include from test case context. Defaults to ['query', 'actual_output', 'expected_output'].</p> </li> <li> <code>metric_names</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of metric names to filter. If None, all metrics are processed.</p> </li> <li> <code>max_issues</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Hard limit on number of issues to return.</p> </li> <li> <code>sample_rate</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Deterministic sampling rate (0.0-1.0) by test_case_id.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.extract_from_evaluation","title":"extract_from_evaluation","text":"<pre><code>extract_from_evaluation(result: EvaluationResult) -&gt; IssueExtractionResult\n</code></pre> <p>Extract all issues from an EvaluationResult.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>EvaluationResult</code>)           \u2013            <p>The EvaluationResult to analyze</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IssueExtractionResult</code>           \u2013            <p>IssueExtractionResult with all extracted issues.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.extract_from_test_result","title":"extract_from_test_result","text":"<pre><code>extract_from_test_result(test_result: TestResult, result_index: int) -&gt; List[ExtractedIssue]\n</code></pre> <p>Extract issues from a single TestResult.</p> <p>Parameters:</p> <ul> <li> <code>test_result</code>               (<code>TestResult</code>)           \u2013            <p>The TestResult to analyze</p> </li> <li> <code>result_index</code>               (<code>int</code>)           \u2013            <p>Index in the results list</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[ExtractedIssue]</code>           \u2013            <p>List of ExtractedIssue objects found in this TestResult.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.extract_from_metric_score","title":"extract_from_metric_score","text":"<pre><code>extract_from_metric_score(metric_score: MetricScore, test_case_id: str, test_case: Any, result_index: int, score_index: int) -&gt; List[ExtractedIssue]\n</code></pre> <p>Extract issues from a single MetricScore.</p> <p>Parameters:</p> <ul> <li> <code>metric_score</code>               (<code>MetricScore</code>)           \u2013            <p>The MetricScore to analyze</p> </li> <li> <code>test_case_id</code>               (<code>str</code>)           \u2013            <p>ID of the test case</p> </li> <li> <code>test_case</code>               (<code>Any</code>)           \u2013            <p>The test case object for context</p> </li> <li> <code>result_index</code>               (<code>int</code>)           \u2013            <p>Index in the results list</p> </li> <li> <code>score_index</code>               (<code>int</code>)           \u2013            <p>Index in the score_results list</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[ExtractedIssue]</code>           \u2013            <p>List of ExtractedIssue objects found in this MetricScore.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.to_llm_input","title":"to_llm_input","text":"<pre><code>to_llm_input(result: IssueExtractionResult, max_issues: int = 50) -&gt; LLMSummaryInput\n</code></pre> <p>Convert extraction result to structured LLM input.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to convert</p> </li> <li> <code>max_issues</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Maximum number of detailed issues to include</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLMSummaryInput</code>           \u2013            <p>LLMSummaryInput suitable for LLM processing.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.to_prompt_text","title":"to_prompt_text","text":"<pre><code>to_prompt_text(result: IssueExtractionResult, max_issues: int = 50) -&gt; str\n</code></pre> <p>Generate a text prompt for LLM-based issue summarization.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to convert</p> </li> <li> <code>max_issues</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>Maximum number of detailed issues to include</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Formatted prompt text.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.to_grouped_prompt_text","title":"to_grouped_prompt_text","text":"<pre><code>to_grouped_prompt_text(result: IssueExtractionResult, llm: Optional[LLMRunnable] = None, max_groups: int = 20, max_examples_per_group: int = 2) -&gt; str\n</code></pre> <p>Generate a grouped prompt with optional LLM summarization.</p> <p>Groups similar issues together and shows representative examples, reducing context size while preserving signal quality.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to convert</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Optional LLM for generating group summaries</p> </li> <li> <code>max_groups</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Maximum number of issue groups to include</p> </li> <li> <code>max_examples_per_group</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Representative examples per group</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Formatted prompt text with grouped issues.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.to_grouped_prompt_text_async","title":"to_grouped_prompt_text_async  <code>async</code>","text":"<pre><code>to_grouped_prompt_text_async(result: IssueExtractionResult, llm: Optional[LLMRunnable] = None, max_groups: int = 20, max_examples_per_group: int = 2) -&gt; str\n</code></pre> <p>Generate a grouped prompt with optional LLM summarization (async version).</p> <p>Groups similar issues together and shows representative examples, reducing context size while preserving signal quality.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to convert</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Optional LLM for generating group summaries</p> </li> <li> <code>max_groups</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Maximum number of issue groups to include</p> </li> <li> <code>max_examples_per_group</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Representative examples per group</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Formatted prompt text with grouped issues.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.summarize","title":"summarize  <code>async</code>","text":"<pre><code>summarize(result: IssueExtractionResult, llm: LLMRunnable, prompt_template: Optional[str] = None, max_issues: int = 100) -&gt; IssueSummary\n</code></pre> <p>Generate a complete LLM-powered summary of evaluation issues.</p> <p>This method sends the issues to an LLM and returns a structured summary including failure categories, root causes, and recommendations.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to summarize</p> </li> <li> <code>llm</code>               (<code>LLMRunnable</code>)           \u2013            <p>The LLM to use for generating the summary (must have acomplete method)</p> </li> <li> <code>prompt_template</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom prompt template. If None, uses DEFAULT_SUMMARY_PROMPT. The template should include {overview} and {issue_data} placeholders.</p> </li> <li> <code>max_issues</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of issues to include in the prompt (default 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IssueSummary</code>           \u2013            <p>IssueSummary containing the LLM's analysis.</p> </li> </ul> Example <pre><code>from axion.reporting import IssueExtractor\nfrom axion.llm_registry import LLMRegistry\n\nextractor = IssueExtractor()\nissues = extractor.extract_from_evaluation(eval_result)\n\nreg = LLMRegistry('anthropic')\nllm = reg.get_llm('claude-sonnet-4-20250514')\n\nsummary = await extractor.summarize(issues, llm=llm)\nprint(summary.text)\n</code></pre> Example with custom prompt <pre><code>custom_prompt = '''\nAnalyze these {overview} issues:\n{issue_data}\n\nProvide a brief summary focused on:\n1. Top 3 failure patterns\n2. Quick wins to fix\n'''\nsummary = await extractor.summarize(issues, llm=llm, prompt_template=custom_prompt)\n</code></pre>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractor.summarize_sync","title":"summarize_sync","text":"<pre><code>summarize_sync(result: IssueExtractionResult, llm: LLMRunnable, prompt_template: Optional[str] = None, max_issues: int = 100) -&gt; IssueSummary\n</code></pre> <p>Synchronous version of summarize().</p> <p>Generates a complete LLM-powered summary of evaluation issues.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>IssueExtractionResult</code>)           \u2013            <p>The IssueExtractionResult to summarize</p> </li> <li> <code>llm</code>               (<code>LLMRunnable</code>)           \u2013            <p>The LLM to use for generating the summary</p> </li> <li> <code>prompt_template</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom prompt template. If None, uses DEFAULT_SUMMARY_PROMPT.</p> </li> <li> <code>max_issues</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of issues to include in the prompt</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IssueSummary</code>           \u2013            <p>IssueSummary containing the LLM's analysis.</p> </li> </ul> Example <pre><code>summary = extractor.summarize_sync(issues, llm=llm)\nprint(summary.text)\n</code></pre>"},{"location":"reference/issue-extractor/#signaladapterregistry","title":"SignalAdapterRegistry","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.SignalAdapterRegistry","title":"axion.reporting.issue_extractor.SignalAdapterRegistry","text":"<p>Registry for MetricSignalAdapter instances.</p> <p>Provides a centralized way to register and retrieve adapters for different metrics. Users can register custom adapters for their own metrics using the decorator or direct registration methods.</p> Example using decorator <pre><code>@SignalAdapterRegistry.register('my_custom_metric')\ndef my_adapter():\n    return MetricSignalAdapter(\n        metric_key='my_custom_metric',\n        headline_signals=['passed'],\n        issue_values={'passed': [False]},\n        context_signals=['reason'],\n    )\n</code></pre> Example using direct registration <pre><code>SignalAdapterRegistry.register_adapter(\n    'my_custom_metric',\n    MetricSignalAdapter(\n        metric_key='my_custom_metric',\n        headline_signals=['passed'],\n        issue_values={'passed': [False]},\n        context_signals=['reason'],\n    )\n)\n</code></pre>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.SignalAdapterRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(metric_key: str)\n</code></pre> <p>Decorator to register a signal adapter for a metric.</p> <p>The decorated function should return a MetricSignalAdapter instance.</p> <p>Parameters:</p> <ul> <li> <code>metric_key</code>               (<code>str</code>)           \u2013            <p>The metric identifier (e.g., 'faithfulness', 'my_custom_metric')</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Decorator function</p> </li> </ul> Example <pre><code>@SignalAdapterRegistry.register('custom_metric')\ndef custom_adapter():\n    return MetricSignalAdapter(\n        metric_key='custom_metric',\n        headline_signals=['is_valid'],\n        issue_values={'is_valid': [False]},\n        context_signals=['reason'],\n    )\n</code></pre>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.SignalAdapterRegistry.register_adapter","title":"register_adapter  <code>classmethod</code>","text":"<pre><code>register_adapter(metric_key: str, adapter: MetricSignalAdapter) -&gt; None\n</code></pre> <p>Directly register a MetricSignalAdapter for a metric.</p> <p>Parameters:</p> <ul> <li> <code>metric_key</code>               (<code>str</code>)           \u2013            <p>The metric identifier</p> </li> <li> <code>adapter</code>               (<code>MetricSignalAdapter</code>)           \u2013            <p>The MetricSignalAdapter instance</p> </li> </ul> Example <pre><code>SignalAdapterRegistry.register_adapter(\n    'my_metric',\n    MetricSignalAdapter(\n        metric_key='my_metric',\n        headline_signals=['score'],\n        issue_values={'score': [0]},\n        context_signals=['explanation'],\n    )\n)\n</code></pre>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.SignalAdapterRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(metric_name: str) -&gt; Optional[MetricSignalAdapter]\n</code></pre> <p>Get the adapter for a metric by name.</p> <p>Parameters:</p> <ul> <li> <code>metric_name</code>               (<code>str</code>)           \u2013            <p>The metric name (case-insensitive, spaces/hyphens normalized)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[MetricSignalAdapter]</code>           \u2013            <p>MetricSignalAdapter if found, None otherwise.</p> </li> </ul>"},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.SignalAdapterRegistry.list_adapters","title":"list_adapters  <code>classmethod</code>","text":"<pre><code>list_adapters() -&gt; List[str]\n</code></pre> <p>List all registered adapter keys.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of registered metric keys.</p> </li> </ul>"},{"location":"reference/issue-extractor/#data-classes","title":"Data Classes","text":""},{"location":"reference/issue-extractor/#extractedissue","title":"ExtractedIssue","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.ExtractedIssue","title":"axion.reporting.issue_extractor.ExtractedIssue  <code>dataclass</code>","text":"<pre><code>ExtractedIssue(test_case_id: str, metric_name: str, signal_group: str, signal_name: str, value: Any, score: float, description: Optional[str] = None, reasoning: Optional[str] = None, item_context: Dict[str, Any] = dict(), source_path: str = '', raw_signal: Dict[str, Any] = dict())\n</code></pre> <p>Represents a single low-score signal extracted from metric evaluation results.</p> <p>Attributes:</p> <ul> <li> <code>test_case_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the test case</p> </li> <li> <code>metric_name</code>               (<code>str</code>)           \u2013            <p>Name of the metric that produced this signal</p> </li> <li> <code>signal_group</code>               (<code>str</code>)           \u2013            <p>Group name for the signal (e.g., \"claim_0\", \"aspect_Coverage\")</p> </li> <li> <code>signal_name</code>               (<code>str</code>)           \u2013            <p>Name of the signal (e.g., \"is_covered\", \"faithfulness_verdict\")</p> </li> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>Original value (False, \"CONTRADICTORY\", etc.)</p> </li> <li> <code>score</code>               (<code>float</code>)           \u2013            <p>Numeric score (0.0 for failures)</p> </li> <li> <code>description</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional description of the signal</p> </li> <li> <code>reasoning</code>               (<code>Optional[str]</code>)           \u2013            <p>LLM reasoning from sibling signal if available</p> </li> <li> <code>item_context</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Context from the test case (query, actual_output, etc.)</p> </li> <li> <code>source_path</code>               (<code>str</code>)           \u2013            <p>Path for debugging (e.g., \"results[42].score_results[0].signals.claim_0\")</p> </li> <li> <code>raw_signal</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Original signal dict for debugging</p> </li> </ul>"},{"location":"reference/issue-extractor/#issueextractionresult","title":"IssueExtractionResult","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueExtractionResult","title":"axion.reporting.issue_extractor.IssueExtractionResult  <code>dataclass</code>","text":"<pre><code>IssueExtractionResult(run_id: str, evaluation_name: Optional[str], total_test_cases: int, total_signals_analyzed: int, issues_found: int, issues_by_metric: Dict[str, List[ExtractedIssue]], issues_by_type: Dict[str, List[ExtractedIssue]], all_issues: List[ExtractedIssue])\n</code></pre> <p>Aggregated result of issue extraction from an evaluation run.</p> <p>Attributes:</p> <ul> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the evaluation run</p> </li> <li> <code>evaluation_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name of the evaluation</p> </li> <li> <code>total_test_cases</code>               (<code>int</code>)           \u2013            <p>Total number of test cases analyzed</p> </li> <li> <code>total_signals_analyzed</code>               (<code>int</code>)           \u2013            <p>Total number of signals analyzed</p> </li> <li> <code>issues_found</code>               (<code>int</code>)           \u2013            <p>Total number of issues found</p> </li> <li> <code>issues_by_metric</code>               (<code>Dict[str, List[ExtractedIssue]]</code>)           \u2013            <p>Issues grouped by metric name</p> </li> <li> <code>issues_by_type</code>               (<code>Dict[str, List[ExtractedIssue]]</code>)           \u2013            <p>Issues grouped by signal name (issue type)</p> </li> <li> <code>all_issues</code>               (<code>List[ExtractedIssue]</code>)           \u2013            <p>Flat list of all extracted issues</p> </li> </ul>"},{"location":"reference/issue-extractor/#issuegroup","title":"IssueGroup","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueGroup","title":"axion.reporting.issue_extractor.IssueGroup  <code>dataclass</code>","text":"<pre><code>IssueGroup(metric_name: str, signal_name: str, total_count: int, unique_values: List[Any], representative_issues: List[ExtractedIssue], affected_test_cases: List[str], llm_summary: Optional[str] = None)\n</code></pre> <p>Represents a group of similar issues for summarization.</p> <p>Attributes:</p> <ul> <li> <code>metric_name</code>               (<code>str</code>)           \u2013            <p>The metric that produced these issues</p> </li> <li> <code>signal_name</code>               (<code>str</code>)           \u2013            <p>The signal name (e.g., \"is_covered\", \"faithfulness_verdict\")</p> </li> <li> <code>total_count</code>               (<code>int</code>)           \u2013            <p>Total number of issues in this group</p> </li> <li> <code>unique_values</code>               (<code>List[Any]</code>)           \u2013            <p>Set of unique failure values</p> </li> <li> <code>representative_issues</code>               (<code>List[ExtractedIssue]</code>)           \u2013            <p>Sample issues with full context</p> </li> <li> <code>affected_test_cases</code>               (<code>List[str]</code>)           \u2013            <p>List of affected test case IDs</p> </li> <li> <code>llm_summary</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional LLM-generated summary of the pattern</p> </li> </ul>"},{"location":"reference/issue-extractor/#issuesummary","title":"IssueSummary","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.IssueSummary","title":"axion.reporting.issue_extractor.IssueSummary  <code>dataclass</code>","text":"<pre><code>IssueSummary(text: str, prompt_used: str, issues_analyzed: int, evaluation_name: Optional[str] = None)\n</code></pre> <p>LLM-generated summary of evaluation issues.</p> <p>Attributes:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The full LLM-generated analysis and summary</p> </li> <li> <code>prompt_used</code>               (<code>str</code>)           \u2013            <p>The prompt that was sent to the LLM</p> </li> <li> <code>issues_analyzed</code>               (<code>int</code>)           \u2013            <p>Number of issues included in the analysis</p> </li> <li> <code>evaluation_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Name of the evaluation that was analyzed</p> </li> </ul>"},{"location":"reference/issue-extractor/#llmsummaryinput","title":"LLMSummaryInput","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.LLMSummaryInput","title":"axion.reporting.issue_extractor.LLMSummaryInput  <code>dataclass</code>","text":"<pre><code>LLMSummaryInput(evaluation_name: Optional[str], total_test_cases: int, issues_found: int, issues_by_metric: Dict[str, int], issues_by_type: Dict[str, int], detailed_issues: List[Dict[str, Any]])\n</code></pre> <p>Structured input for LLM-based issue summarization.</p> <p>Attributes:</p> <ul> <li> <code>evaluation_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Name of the evaluation</p> </li> <li> <code>total_test_cases</code>               (<code>int</code>)           \u2013            <p>Total test cases analyzed</p> </li> <li> <code>issues_found</code>               (<code>int</code>)           \u2013            <p>Total issues found</p> </li> <li> <code>issues_by_metric</code>               (<code>Dict[str, int]</code>)           \u2013            <p>Summary counts by metric</p> </li> <li> <code>issues_by_type</code>               (<code>Dict[str, int]</code>)           \u2013            <p>Summary counts by issue type</p> </li> <li> <code>detailed_issues</code>               (<code>List[Dict[str, Any]]</code>)           \u2013            <p>List of detailed issue dicts for the prompt</p> </li> </ul>"},{"location":"reference/issue-extractor/#metricsignaladapter","title":"MetricSignalAdapter","text":""},{"location":"reference/issue-extractor/#axion.reporting.issue_extractor.MetricSignalAdapter","title":"axion.reporting.issue_extractor.MetricSignalAdapter  <code>dataclass</code>","text":"<pre><code>MetricSignalAdapter(metric_key: str, headline_signals: List[str], issue_values: Dict[str, List[Any]], context_signals: List[str])\n</code></pre> <p>Adapter defining how to extract issues from a specific metric's signals.</p> <p>Attributes:</p> <ul> <li> <code>metric_key</code>               (<code>str</code>)           \u2013            <p>Metric identifier (e.g., \"faithfulness\")</p> </li> <li> <code>headline_signals</code>               (<code>List[str]</code>)           \u2013            <p>Signals that indicate pass/fail</p> </li> <li> <code>issue_values</code>               (<code>Dict[str, List[Any]]</code>)           \u2013            <p>Mapping of signal names to failure values</p> </li> <li> <code>context_signals</code>               (<code>List[str]</code>)           \u2013            <p>Sibling signals to include for context</p> </li> </ul>"},{"location":"reference/issue-extractor/#built-in-adapters","title":"Built-in Adapters","text":"<p>The following adapters are pre-registered:</p> Adapter Key Headline Signals Issue Values <code>faithfulness</code> <code>faithfulness_verdict</code> <code>CONTRADICTORY</code>, <code>NO_EVIDENCE</code> <code>answer_criteria</code> <code>is_covered</code>, <code>concept_coverage</code> <code>False</code> <code>answer_relevancy</code> <code>is_relevant</code>, <code>verdict</code> <code>False</code>, <code>no</code> <code>answer_completeness</code> <code>is_covered</code>, <code>is_addressed</code> <code>False</code> <code>factual_accuracy</code> <code>is_correct</code>, <code>accuracy_score</code> <code>False</code>, <code>0</code> <code>answer_conciseness</code> <code>conciseness_score</code> (score-based) <code>contextual_relevancy</code> <code>is_relevant</code> <code>False</code> <code>contextual_recall</code> <code>is_attributable</code>, <code>is_supported</code> <code>False</code> <code>contextual_precision</code> <code>is_useful</code>, <code>map_score</code> <code>False</code> <code>contextual_utilization</code> <code>is_utilized</code> <code>False</code> <code>contextual_sufficiency</code> <code>is_sufficient</code> <code>False</code> <code>contextual_ranking</code> <code>is_correctly_ranked</code> <code>False</code> <code>citation_relevancy</code> <code>relevance_verdict</code> <code>False</code> <code>pii_leakage</code> <code>pii_verdict</code> <code>yes</code> <code>tone_style_consistency</code> <code>is_consistent</code> <code>False</code> <code>persona_tone_adherence</code> <code>persona_match</code> <code>False</code> <code>conversation_efficiency</code> <code>efficiency_score</code> (score-based) <code>conversation_flow</code> <code>final_score</code> (score-based) <code>goal_completion</code> <code>is_completed</code>, <code>goal_achieved</code> <code>False</code> <code>citation_presence</code> <code>presence_check_passed</code> <code>False</code> <code>latency</code> <code>latency_score</code> (threshold-based) <code>tool_correctness</code> <code>all_tools_correct</code> <code>False</code> <p>Issue Extraction Guide  Insight Extractor Reference  Running Evaluations </p>"},{"location":"reference/llm-registry/","title":"LLM Registry API Reference","text":"<p>LLM model cost estimation and provider registry for 50+ models.</p> <pre><code>from axion.llm_registry import LLMRegistry\n</code></pre>"},{"location":"reference/llm-registry/#llmregistry","title":"LLMRegistry","text":"<p>LLM Providers Guide </p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry","title":"axion.llm_registry.LLMRegistry","text":"<pre><code>LLMRegistry(provider: Optional[str] = None, **credentials)\n</code></pre> <p>A factory and registry for LLM and embedding model providers.</p> <p>This class provides a unified interface for creating LLM and embedding clients from different providers using simple string identifiers or global settings.</p> <p>Initializes the registry. If a provider is specified, it configures for that provider. Otherwise, it remains flexible to serve requests based on global settings.</p> <p>Parameters:</p> <ul> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the provider to lock this registry instance to.</p> </li> <li> <code>**credentials</code>           \u2013            <p>Credentials required by the provider, such as 'api_key'.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.default_llm_model_name","title":"default_llm_model_name  <code>property</code>","text":"<pre><code>default_llm_model_name: str\n</code></pre> <p>Default LLM model name</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.default_embedding_model_name","title":"default_embedding_model_name  <code>property</code>","text":"<pre><code>default_embedding_model_name: str\n</code></pre> <p>Default Embedding model name</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(name: str)\n</code></pre> <p>A class method decorator to register a new provider.</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.display","title":"display  <code>classmethod</code>","text":"<pre><code>display()\n</code></pre> <p>Display the LLM provider registry.</p>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.get_llm","title":"get_llm","text":"<pre><code>get_llm(model_name: Optional[str] = None, provider: Optional[str] = None, **kwargs) -&gt; Any\n</code></pre> <p>Gets a language model instance from a provider.</p> <p>Uses the instance's locked provider if set, otherwise falls back to the globally configured <code>settings.llm_provider</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use. Defaults to settings.model_name.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters for the LLM's constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>An instance of a language model client.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.get_embedding_model","title":"get_embedding_model","text":"<pre><code>get_embedding_model(model_name: Optional[str] = None, provider: Optional[str] = None, **kwargs) -&gt; Any\n</code></pre> <p>Gets an embedding model instance from a provider.</p> <p>Uses the instance's locked provider if set, otherwise falls back to the globally configured <code>settings.embedding_provider</code>.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use. Defaults to settings.embedding_model.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional parameters for the embedding model's constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>An instance of an embedding model client.</p> </li> </ul>"},{"location":"reference/llm-registry/#axion.llm_registry.LLMRegistry.estimate_cost","title":"estimate_cost","text":"<pre><code>estimate_cost(prompt_tokens: int, completion_tokens: int, model_name: Optional[str] = None, provider: Optional[str] = None) -&gt; float\n</code></pre> <p>Estimates the cost of an LLM call based on token usage.</p> <p>The method determines the correct provider and model based on the same precedence rules as get_llm (direct arguments &gt; locked instance &gt; global settings).</p> <p>Parameters:</p> <ul> <li> <code>prompt_tokens</code>               (<code>int</code>)           \u2013            <p>The number of tokens in the prompt.</p> </li> <li> <code>completion_tokens</code>               (<code>int</code>)           \u2013            <p>The number of tokens in the completion.</p> </li> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model name to use for pricing. Defaults to settings.model_name.</p> </li> <li> <code>provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Override the provider for this specific call.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The estimated cost in USD.</p> </li> </ul>"},{"location":"reference/metrics/","title":"Metrics API Reference","text":"<p>Evaluation metrics for AI agents \u2014 composable, LLM-powered and heuristic scoring.</p> <pre><code>from axion import metric_registry\nfrom axion.metrics.base import BaseMetric, MetricEvaluationResult, metric\nfrom axion.metrics import (\n    Faithfulness, AnswerRelevancy, FactualAccuracy,\n    AnswerCompleteness, AnswerCriteria,\n    ContextualRelevancy, ContextualPrecision, ContextualRecall,\n    ExactStringMatch, CitationPresence, Latency,\n    HitRateAtK, MeanReciprocalRank,\n    GoalCompletion, ConversationFlow,\n)\n</code></pre> B <p>BaseMetric</p> <p>Base class for all metrics. Provides LLM integration, field validation, structured I/O, and the <code>execute()</code> contract.</p> R <p>MetricRegistry</p> <p>Global registry for storing, retrieving, and discovering metric classes by key, tag, or compatible fields.</p> @ <p>@metric</p> <p>Decorator that attaches config (name, fields, threshold, tags) and auto-registers the class.</p> 30+ <p>Built-in Metrics</p> <p>Composite (LLM-judged), heuristic, retrieval, and conversational metrics ready to use out of the box.</p>"},{"location":"reference/metrics/#basemetric","title":"BaseMetric","text":""},{"location":"reference/metrics/#axion.metrics.base.BaseMetric","title":"axion.metrics.base.BaseMetric","text":"<pre><code>BaseMetric(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, field_mapping: Optional[Dict[str, str]] = None, metric_category: Optional[MetricCategory] = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>LLMHandler</code>, <code>Generic[InputModel, OutputModel]</code></p> <p>Base class for all metric evaluation classes, inheriting from LLMHandler.</p> <p>Initialize the metric with optional LLM and embedding model.</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the LLM model to use</p> </li> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>A pre-configured LLM model. If not provided, a default is loaded from the registry.</p> </li> <li> <code>embed_model_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name of the embedding model to use</p> </li> <li> <code>embed_model</code>               (<code>Optional[EmbeddingRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>A pre-configured embedding model handler (if needed).</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>The threshold to consider a score as 'passing'. Will overwrite default.</p> </li> <li> <code>llm_provider</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The LLM provider to use</p> </li> <li> <code>required_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of required field names for evaluation</p> </li> <li> <code>optional_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of optional field names for evaluation</p> </li> <li> <code>metric_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the metric instance (alias: name)</p> </li> <li> <code>metric_description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional description for the metric instance</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Alias for metric_name (for convenience)</p> </li> <li> <code>field_mapping</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional mapping from canonical field names to source paths. e.g., {'actual_output': 'additional_output.summary'} will resolve 'actual_output' from item.additional_output['summary'].</p> </li> <li> <code>metric_category</code>               (<code>Optional[MetricCategory]</code>, default:                   <code>None</code> )           \u2013            <p>The category of metric output (SCORE, ANALYSIS, CLASSIFICATION). If not provided, falls back to class config or defaults to SCORE.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to the parent LLMHandler (e.g., logger config).</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of the metric from instance, config, or fallback to class name.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>Return the description of the metric from instance, config, or fallback to class name.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.threshold","title":"threshold  <code>property</code>","text":"<pre><code>threshold\n</code></pre> <p>Metric passing threshold.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.input_item","title":"input_item  <code>property</code>","text":"<pre><code>input_item\n</code></pre> <p>Access the final DatasetItem passed the metric</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.required_fields","title":"required_fields  <code>property</code> <code>writable</code>","text":"<pre><code>required_fields: list\n</code></pre> <p>Returns the required fields for evaluation.</p> <p>Falls back to configuration if instance-level fields are not explicitly set.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.optional_fields","title":"optional_fields  <code>property</code> <code>writable</code>","text":"<pre><code>optional_fields: list\n</code></pre> <p>Returns the optional fields for evaluation.</p> <p>Falls back to configuration if instance-level fields are not explicitly set.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.metric_category","title":"metric_category  <code>property</code> <code>writable</code>","text":"<pre><code>metric_category: MetricCategory\n</code></pre> <p>Returns the metric category for this metric.</p> <p>Falls back to configuration if instance-level value is not explicitly set. Defaults to MetricCategory.SCORE if not defined anywhere.</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: Union[DatasetItem, dict], callbacks: Callbacks = None, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Execute the metric evaluation for a single dataset item.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>Union[DatasetItem, dict]</code>)           \u2013            <p>Input dataset item containing necessary fields for evaluation.</p> </li> <li> <code>callbacks</code>               (<code>Callbacks</code>, default:                   <code>None</code> )           \u2013            <p>Optional callback handler for events/logging.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetricEvaluationResult</code>           \u2013            <p>An evaluation result conforming to the output model.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.get_field","title":"get_field","text":"<pre><code>get_field(item: DatasetItem, field_name: str, default: Any = None) -&gt; Any\n</code></pre> <p>Resolve a field value from DatasetItem, respecting field_mapping overrides.</p> <p>If a mapping is defined for the given field_name, this method resolves the value from the mapped source path. Otherwise, it returns the attribute directly from the item.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The DatasetItem to extract from</p> </li> <li> <code>field_name</code>               (<code>str</code>)           \u2013            <p>Canonical field name (e.g., 'actual_output')</p> </li> <li> <code>default</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Value to return if field is not found</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The resolved field value</p> </li> </ul> Example"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.get_field--with-field_mappingactual_output-additional_outputsummary","title":"With field_mapping={'actual_output': 'additional_output.summary'}","text":"<p>value = self.get_field(item, 'actual_output')  # Gets item.additional_output['summary']</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.get_mapped_fields","title":"get_mapped_fields","text":"<pre><code>get_mapped_fields(item: DatasetItem) -&gt; Dict[str, Any]\n</code></pre> <p>Return all required and optional fields with resolved values.</p> <p>This convenience method resolves all configured fields (both required and optional) from the DatasetItem, applying any field mappings.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The DatasetItem to extract fields from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary mapping field names to their resolved values</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.get_evaluation_fields","title":"get_evaluation_fields","text":"<pre><code>get_evaluation_fields(item: Union[DatasetItem, dict]) -&gt; Union[DatasetItem, InputModel]\n</code></pre> <p>Extracts the appropriate evaluation fields from the dataset item.</p> <p>Priority is given to explicitly set required and optional fields on the instance. If not defined, configuration-based fields are used. If none are available, the item's default evaluation fields are returned.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The input dataset item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[DatasetItem, InputModel]</code>           \u2013            <p>DatasetItem | InputModel: A dataset item containing only the relevant fields for evaluation.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.set_instruction","title":"set_instruction","text":"<pre><code>set_instruction(instruction: str)\n</code></pre> <p>Set a new instruction string for the metric.</p> <p>Parameters:</p> <ul> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>The updated task instruction that guides the metric\u2019s behavior or LLM prompt.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.set_examples","title":"set_examples","text":"<pre><code>set_examples(examples: List[Tuple[DatasetItem, MetricEvaluationResult]])\n</code></pre> <p>Replace all current examples with a new set.</p> <p>Parameters:</p> <ul> <li> <code>examples</code>               (<code>List[Tuple[DatasetItem, EvaluationResult]]</code>)           \u2013            <p>A list of example input-output pairs used for few-shot prompting or metric calibration.</p> </li> </ul> Example <p>[     (         DatasetItem(             expected_output='....',             actual_output='...',         ),         MetricEvaluationResult(             score=...,             explanation=\"...\",         ),     ), ]</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.add_examples","title":"add_examples","text":"<pre><code>add_examples(examples: List[Tuple[DatasetItem, MetricEvaluationResult]])\n</code></pre> <p>Add new example input-output pairs to the existing list of examples.</p> <p>Parameters:</p> <ul> <li> <code>examples</code>               (<code>List[Tuple[DatasetItem, EvaluationResult]]</code>)           \u2013            <p>One or more examples to add to the current list, extending few-shot prompting context.</p> </li> </ul> Example <p>[     (         DatasetItem(             expected_output='....',             actual_output='...',         ),         MetricEvaluationResult(             score=...,             explanation=\"...\",         ),     ), ]</p>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.compute_cost_estimate","title":"compute_cost_estimate","text":"<pre><code>compute_cost_estimate(sub_models: List[BaseMetric])\n</code></pre> <p>Computes the total estimated cost from sub-models for this execution.</p> <p>Parameters:</p> <ul> <li> <code>sub_models</code>               (<code>List[BaseMetric]</code>)           \u2013            <p>List of sub-models that may have a cost_estimate.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.display_prompt","title":"display_prompt","text":"<pre><code>display_prompt(item: Union[dict, InputModel] = None, **kwargs)\n</code></pre> <p>Displays the fully constructed prompt that will be sent to the LLM.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>Union[dict, InputModel]</code>, default:                   <code>None</code> )           \u2013            <p>The input data to be included in the prompt. If None, a placeholder is used. Defaults to None.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.base.BaseMetric.get_sub_metrics","title":"get_sub_metrics","text":"<pre><code>get_sub_metrics(result: MetricEvaluationResult) -&gt; List[SubMetricResult]\n</code></pre> <p>Override to define how results explode into sub-metrics.</p> <p>This method is called when <code>is_multi_metric=True</code> to extract individual sub-metric scores from a single evaluation result. The default implementation returns an empty list, meaning no explosion occurs.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>MetricEvaluationResult</code>)           \u2013            <p>The evaluation result from execute() containing signals and metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[SubMetricResult]</code>           \u2013            <p>List of SubMetricResult objects representing individual sub-metrics.</p> </li> <li> <code>List[SubMetricResult]</code>           \u2013            <p>Returns empty list by default (no explosion).</p> </li> </ul> Example <p>def get_sub_metrics(self, result: MetricEvaluationResult) -&gt; List[SubMetricResult]:     signals = result.signals     if not signals:         return []</p> <pre><code>return [\n    SubMetricResult(\n        name='engagement',\n        score=signals.engagement_score,\n        group='behavioral',\n    ),\n    SubMetricResult(\n        name='sentiment',\n        score=signals.sentiment_score,\n        group='sentiment',\n        threshold=0.5,\n    ),\n]\n</code></pre>"},{"location":"reference/metrics/#metricregistry","title":"MetricRegistry","text":""},{"location":"reference/metrics/#axion.metrics.MetricRegistry","title":"axion.metrics.MetricRegistry","text":"<p>Registry for storing and retrieving metric classes.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.register","title":"register","text":"<pre><code>register(metric_class: Type[BaseMetric]) -&gt; None\n</code></pre> <p>Register a metric class into the registry.</p> <p>Parameters:</p> <ul> <li> <code>metric_class</code>               (<code>Type[BaseMetric]</code>)           \u2013            <p>A class inheriting from BaseMetric with a valid config.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get","title":"get","text":"<pre><code>get(key: str, error: bool = True) -&gt; Optional[Type[BaseMetric]]\n</code></pre> <p>Retrieve a registered metric class by key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The unique key of the metric.</p> </li> <li> <code>error</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, raise an error if the key is not found.    If False, return None instead.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Type[BaseMetric]]</code>           \u2013            <p>The registered metric class, or None if not found and error=False.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.find","title":"find","text":"<pre><code>find(query: str) -&gt; List[Type[BaseMetric]]\n</code></pre> <p>Search for metrics whose name, description, or tags match a query.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Case-insensitive search string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of matching metric classes.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get_compatible_metrics","title":"get_compatible_metrics","text":"<pre><code>get_compatible_metrics(item: DatasetItem) -&gt; List[Type[BaseMetric]]\n</code></pre> <p>Return all metrics compatible with a given DatasetItem.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The dataset item to test against.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Type[BaseMetric]]</code>           \u2013            <p>A list of compatible metric classes.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.get_metric_descriptions","title":"get_metric_descriptions","text":"<pre><code>get_metric_descriptions() -&gt; Dict[str, str]\n</code></pre> <p>Return {metric_name: description} from the registry.</p>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.display","title":"display","text":"<pre><code>display(show_examples: bool = False) -&gt; None\n</code></pre> <p>Display a summary of all registered metrics.</p> <p>Parameters:</p> <ul> <li> <code>show_examples</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Show custom LLM examples</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.MetricRegistry.display_table","title":"display_table","text":"<pre><code>display_table() -&gt; None\n</code></pre> <p>Display a formatted table of all registered metrics.</p>"},{"location":"reference/metrics/#metric-decorator","title":"@metric decorator","text":""},{"location":"reference/metrics/#axion.metrics.base.metric","title":"axion.metrics.base.metric","text":"<pre><code>metric(name: str, description: str, required_fields: List[str], optional_fields: Optional[List[str]] = None, key: Optional[str] = None, metric_category: MetricCategory = SCORE, default_threshold: Optional[float] = 0.5, score_range: Optional[tuple[Union[int, float], Union[int, float]]] = (0, 1), tags: Optional[List[str]] = None) -&gt; Callable[[Type[BaseMetric]], Type[BaseMetric]]\n</code></pre> <p>Decorator to define and register a metric class with declarative configuration.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Human-readable name of the metric.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>Description of what the metric measures.</p> </li> <li> <code>required_fields</code>               (<code>List[str]</code>)           \u2013            <p>Fields that must be present in the DatasetItem to evaluate this metric.</p> </li> <li> <code>optional_fields</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional fields the metric may use if available.</p> </li> <li> <code>key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional. A unique programmatic identifier for the metric.  If not provided, it's generated from the name.</p> </li> <li> <code>metric_category</code>               (<code>MetricCategory</code>, default:                   <code>SCORE</code> )           \u2013            <p>The category of metric output: SCORE (numeric), ANALYSIS (structured insights),              or CLASSIFICATION (labels). Defaults to SCORE.</p> </li> <li> <code>default_threshold</code>               (<code>Optional[float]</code>, default:                   <code>0.5</code> )           \u2013            <p>The default threshold to consider a score as 'passing'.                Optional for ANALYSIS metrics.</p> </li> <li> <code>score_range</code>               (<code>Optional[tuple[Union[int, float], Union[int, float]]]</code>, default:                   <code>(0, 1)</code> )           \u2013            <p>Tuple representing the valid score range for this metric.          Optional for ANALYSIS metrics.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Searchable tags to group or filter metrics.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable[[Type[BaseMetric]], Type[BaseMetric]]</code>           \u2013            <p>A class decorator that attaches config and registers the metric in the MetricRegistry.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the decorated class is not a subclass of BaseMetric.</p> </li> </ul>"},{"location":"reference/metrics/#composite-metrics","title":"Composite Metrics","text":""},{"location":"reference/metrics/#faithfulness","title":"Faithfulness","text":""},{"location":"reference/metrics/#axion.metrics.Faithfulness","title":"axion.metrics.Faithfulness","text":"<pre><code>Faithfulness(mode: EvaluationMode = GRANULAR, strict_mode: bool = False, verdict_scores: Optional[Dict[str, float]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Measures how faithful the generated answer is to the retrieved context. Default Scoring: Fully Supported (+1.0), Partially Supported (+0.5), No Evidence (0.0), Contradictory (-1.0)</p> <p>Initializes the Faithfulness metric. Args:     mode: The evaluation mode for the underlying RAG analyzer.     strict_mode (bool): If True, sets 'NO_EVIDENCE' to -1.0, penalizing                         uncited claims (hallucinations) as heavily as contradictions.                         This is overridden by 'verdict_scores' if provided.     verdict_scores: A dictionary to override the default scoring weights (e.g.,                     {\"CONTRADICTORY\": -2.0, \"PARTIALLY_SUPPORTED\": 0.75}).                     If provided, this takes precedence over 'strict_mode'.     **kwargs: Additional keyword arguments passed to the RAGAnalyzer.</p>"},{"location":"reference/metrics/#answerrelevancy","title":"AnswerRelevancy","text":""},{"location":"reference/metrics/#axion.metrics.AnswerRelevancy","title":"axion.metrics.AnswerRelevancy","text":"<pre><code>AnswerRelevancy(relevancy_mode: Literal['strict', 'task'] = 'task', penalize_ambiguity: bool = False, mode: EvaluationMode = GRANULAR, multi_turn_strategy: Literal['last_turn', 'all_turns'] = 'last_turn', **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Computes answer relevancy scores by analyzing how well the response addresses the input query. Supports both single-turn and multi-turn (all turns) evaluation.</p> <p>Initialize the Answer Relevancy metric.</p> <p>Parameters:</p> <ul> <li> <code>relevancy_mode</code>               (<code>Literal['strict', 'task']</code>, default:                   <code>'task'</code> )           \u2013            <p>The mode for judging relevancy. 'strict': Only directly answering statements are relevant. 'task': Closely related, helpful statements are also relevant (default).</p> </li> <li> <code>penalize_ambiguity</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, 'idk' verdicts are scored as 0.0 (irrelevant).                        If False (default), 'idk' verdicts are scored as 1.0 (relevant).</p> </li> <li> <code>mode</code>               (<code>EvaluationMode</code>, default:                   <code>GRANULAR</code> )           \u2013            <p>The evaluation mode for the internal RAGAnalyzer.</p> </li> <li> <code>multi_turn_strategy</code>               (<code>Literal</code>, default:                   <code>'last_turn'</code> )           \u2013            <p>How to handle multi-turn conversations.                            'last_turn' (default): Evaluates only the last turn.                            'all_turns': Evaluates all Human-&gt;AI turns in the conversation.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to parent class.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerRelevancy.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult\n</code></pre> <p>Compute the score based on criteria. Automatically handles single-turn or multi-turn evaluation based on <code>self.multi_turn_strategy</code> and <code>item.conversation</code>.</p>"},{"location":"reference/metrics/#factualaccuracy","title":"FactualAccuracy","text":""},{"location":"reference/metrics/#axion.metrics.FactualAccuracy","title":"axion.metrics.FactualAccuracy","text":"<pre><code>FactualAccuracy(**kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Factual Accuracy Metric.</p> <p>Process: 1. Decompose 'actual_output' into atomic statements. 2. specific binary check (1/0) for each statement against 'expected_output'. 3. Score = (Sum of 1s) / (Total Statements).</p>"},{"location":"reference/metrics/#axion.metrics.FactualAccuracy.get_signals","title":"get_signals  <code>staticmethod</code>","text":"<pre><code>get_signals(report: FactualityReport) -&gt; List[SignalDescriptor]\n</code></pre> <p>Display the binary checklist in the UI.</p>"},{"location":"reference/metrics/#answercompleteness","title":"AnswerCompleteness","text":""},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness","title":"axion.metrics.AnswerCompleteness","text":"<pre><code>AnswerCompleteness(use_expected_output: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Evaluates the completeness of product-related responses using one of two approaches: 1. Aspect-based evaluation (when expected_aspects are provided) 2. Sub-question based evaluation (when expected_aspects are not provided)</p> <p>Initialize the answer completeness metric with required prompts for both approaches.</p> <p>Parameters:</p> <ul> <li> <code>use_expected_output</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>It True, use expected answer if available, otherwise decompose query</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Compute the completeness score, returning a structured result object in the <code>signals</code> field.</p>"},{"location":"reference/metrics/#axion.metrics.AnswerCompleteness.get_signals","title":"get_signals  <code>staticmethod</code>","text":"<pre><code>get_signals(result: AnswerCompletenessResult) -&gt; List[SignalDescriptor[AnswerCompletenessResult]]\n</code></pre> <p>Generates a list of detailed signals from the evaluation result.</p>"},{"location":"reference/metrics/#answercriteria","title":"AnswerCriteria","text":""},{"location":"reference/metrics/#axion.metrics.AnswerCriteria","title":"axion.metrics.AnswerCriteria","text":"<pre><code>AnswerCriteria(criteria_key: str = 'Complete', scoring_strategy: Literal['concept', 'aspect', 'weighted'] = 'concept', check_for_contradictions: bool = False, weighted_concept_score_weight: float = 0.7, multi_turn_strategy: Literal['last_turn', 'all_turns'] = 'last_turn', multi_turn_aggregation: Literal['cumulative', 'average'] = 'cumulative', **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Evaluates responses based on specified criteria. This metric extracts key aspects from the criteria and checks whether each aspect is adequately and accurately addressed in the response.</p> <p>It supports two modes of operation: 1.  Single-Turn / Last-Turn (default): Evaluates <code>item.query</code> vs. <code>item.actual_output</code>.     If <code>item.conversation</code> is present, <code>item.query</code> and <code>item.actual_output</code> are     auto-populated from the last turn (based on <code>conversation_extraction_strategy</code>). 2.  Multi-Turn: If <code>multi_turn_strategy='all_turns'</code>, this metric will iterate     through the entire <code>item.conversation</code> and evaluate every <code>HumanMessage</code> -&gt; <code>AIMessage</code>     pair. The aggregation method is controlled by <code>multi_turn_aggregation</code>.</p> <p>Initialize the criteria-based answer criteria metric.</p> <p>Parameters:</p> <ul> <li> <code>criteria_key</code>               (<code>str</code>, default:                   <code>'Complete'</code> )           \u2013            <p>The key in <code>additional_input</code> or <code>conversation.rubrics</code>           to find the criteria text (default: 'Complete').</p> </li> <li> <code>scoring_strategy</code>               (<code>Literal['concept', 'aspect', 'weighted']</code>, default:                   <code>'concept'</code> )           \u2013            <p>The scoring method: 'concept', 'aspect', or 'weighted' (default: 'concept').</p> </li> <li> <code>check_for_contradictions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, uses a stricter prompt to penalize contradictions (default: False).</p> </li> <li> <code>weighted_concept_score_weight</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The weight for the concept score in 'weighted' strategy (default: 0.7).</p> </li> <li> <code>multi_turn_strategy</code>               (<code>Literal['last_turn', 'all_turns']</code>, default:                   <code>'last_turn'</code> )           \u2013            <p>How to handle multi-turn conversations.                  'last_turn' (default): Evaluates only the last turn.                  'all_turns': Evaluates all Human-&gt;AI turns in the conversation.</p> </li> <li> <code>multi_turn_aggregation</code>               (<code>Literal['cumulative', 'average']</code>, default:                   <code>'cumulative'</code> )           \u2013            <p>Aggregation method for 'all_turns' strategy.                     'cumulative' (default): Scores unique aspects covered across all turns.                     'average': Scores average aspect coverage per turn.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to parent class</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.AnswerCriteria.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Compute the score based on criteria. Automatically handles single-turn or multi-turn evaluation based on <code>self.multi_turn_strategy</code> and <code>item.conversation</code>.</p>"},{"location":"reference/metrics/#axion.metrics.AnswerCriteria.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: AnswerCriteriaResult) -&gt; List[SignalDescriptor[AnswerCriteriaResult]]\n</code></pre> <p>Generates a list of detailed signals from the evaluation result that explain the scoring.</p>"},{"location":"reference/metrics/#contextualrelevancy","title":"ContextualRelevancy","text":""},{"location":"reference/metrics/#axion.metrics.ContextualRelevancy","title":"axion.metrics.ContextualRelevancy","text":"<pre><code>ContextualRelevancy(mode: EvaluationMode = GRANULAR, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Measures the relevancy of retrieval (how much retrieved content is relevant).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualRelevancy.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: ContextualRelevancyResult) -&gt; List[SignalDescriptor[ContextualRelevancyResult]]\n</code></pre> <p>Defines the explainable signals for the ContextualRelevancy metric.</p>"},{"location":"reference/metrics/#contextualprecision","title":"ContextualPrecision","text":""},{"location":"reference/metrics/#axion.metrics.ContextualPrecision","title":"axion.metrics.ContextualPrecision","text":"<pre><code>ContextualPrecision(mode: EvaluationMode = GRANULAR, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Measures the quality of retrieval ranking using Mean Average Precision (MAP).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualPrecision.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: ContextualPrecisionResult) -&gt; List[SignalDescriptor[ContextualPrecisionResult]]\n</code></pre> <p>Defines the explainable signals for the ContextualRanking metric.</p>"},{"location":"reference/metrics/#contextualrecall","title":"ContextualRecall","text":""},{"location":"reference/metrics/#axion.metrics.ContextualRecall","title":"axion.metrics.ContextualRecall","text":"<pre><code>ContextualRecall(mode: EvaluationMode = GRANULAR, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Measures recall of retrieval (how much of the expected answer is in context).</p>"},{"location":"reference/metrics/#axion.metrics.ContextualRecall.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: ContextualRecallResult) -&gt; List[SignalDescriptor[ContextualRecallResult]]\n</code></pre> <p>Defines the explainable signals for the ContextualRecall metric.</p>"},{"location":"reference/metrics/#heuristic-metrics","title":"Heuristic Metrics","text":""},{"location":"reference/metrics/#exactstringmatch","title":"ExactStringMatch","text":""},{"location":"reference/metrics/#axion.metrics.ExactStringMatch","title":"axion.metrics.ExactStringMatch","text":"<pre><code>ExactStringMatch(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, field_mapping: Optional[Dict[str, str]] = None, metric_category: Optional[MetricCategory] = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p>"},{"location":"reference/metrics/#axion.metrics.ExactStringMatch.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Returns 1.0 if the actual output exactly matches the expected output (after stripping). Returns 0.0 otherwise.</p>"},{"location":"reference/metrics/#citationpresence","title":"CitationPresence","text":""},{"location":"reference/metrics/#axion.metrics.CitationPresence","title":"axion.metrics.CitationPresence","text":"<pre><code>CitationPresence(mode: str = 'any_citation', strict: bool = False, embed_model: Optional[EmbeddingRunnable] = None, use_semantic_search: bool = False, resource_similarity_threshold: float = 0.8, custom_resource_phrases: Optional[List[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>A metric to evaluate if the response includes properly formatted citations, supporting single-turn or multi-turn conversations.</p> <p>Initialize the Citation Presence metric.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'any_citation'</code> )           \u2013            <p>Evaluation mode - \"any_citation\" or \"resource_section\".</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, validates that found URLs are live by making a HEAD request.</p> </li> <li> <code>embed_model</code>               (<code>Optional[EmbeddingRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Embedding model for semantic similarity.</p> </li> <li> <code>use_semantic_search</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, uses the embedding model as a fallback.</p> </li> <li> <code>resource_similarity_threshold</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Threshold for semantic similarity.</p> </li> <li> <code>custom_resource_phrases</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom phrases to look for when detecting resource sections.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to BaseMetric.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.CitationPresence.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Evaluate citation presence for the entire conversation or single-turn response.</p>"},{"location":"reference/metrics/#axion.metrics.CitationPresence.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: CitationPresenceResult) -&gt; List[SignalDescriptor[CitationPresenceResult]]\n</code></pre> <p>Generates detailed signals from the presence evaluation.</p>"},{"location":"reference/metrics/#latency","title":"Latency","text":""},{"location":"reference/metrics/#axion.metrics.Latency","title":"axion.metrics.Latency","text":"<pre><code>Latency(normalize: bool = False, normalization_method: str = 'exponential', **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Initialize the Latency metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, normalize latency scores to [0, 1] range.</p> </li> <li> <code>normalization_method</code>               (<code>str</code>, default:                   <code>'exponential'</code> )           \u2013            <p>Method for normalization. Options: - 'exponential': Uses exp(-latency/threshold) for smooth decay - 'sigmoid': Uses 1/(1 + exp((latency-threshold)/scale)) for S-curve - 'reciprocal': Uses threshold/(threshold + latency) for hyperbolic decay - 'linear': Uses max(0, 1 - latency/threshold) for linear decay</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the base metric.</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.Latency.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, **kwargs) -&gt; MetricEvaluationResult\n</code></pre> <p>Returns the latency recorded for a given test case as the metric score.</p> <p>This metric assumes the <code>latency</code> field is already populated on the DatasetItem and returns it as-is or normalized based on the initialization parameters.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>DatasetItem</code>)           \u2013            <p>The evaluation data point containing latency information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetricEvaluationResult</code> (              <code>MetricEvaluationResult</code> )          \u2013            <p>The result object containing the latency as the score.</p> </li> </ul>"},{"location":"reference/metrics/#retrieval-metrics","title":"Retrieval Metrics","text":""},{"location":"reference/metrics/#hitrateatk","title":"HitRateAtK","text":""},{"location":"reference/metrics/#axion.metrics.HitRateAtK","title":"axion.metrics.HitRateAtK","text":"<pre><code>HitRateAtK(k: Union[int, List[int]] = 10, main_k: Optional[int] = None, **kwargs)\n</code></pre> <p>               Bases: <code>_RetrievalMetric</code></p> <p>Evaluates if any relevant document was retrieved in the top K results. Score is 1 if a hit is found, 0 otherwise. Now supports multiple K values.</p> <p>Initialize the Hit Rate @ K metric. Args:     k: The number of top results to consider, or a list of K values.     main_k: The K value to use for the main metric score (defaults to max K in k_list).</p>"},{"location":"reference/metrics/#axion.metrics.HitRateAtK.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: MultiKResult) -&gt; List[SignalDescriptor]\n</code></pre> <p>Generates signals detailing the hit rate calculation for all K values.</p>"},{"location":"reference/metrics/#meanreciprocalrank","title":"MeanReciprocalRank","text":""},{"location":"reference/metrics/#axion.metrics.MeanReciprocalRank","title":"axion.metrics.MeanReciprocalRank","text":"<pre><code>MeanReciprocalRank(model_name: Optional[str] = None, llm: Optional[LLMRunnable] = None, embed_model_name: Optional[str] = None, embed_model: Optional[EmbeddingRunnable] = None, threshold: float = None, llm_provider: Optional[str] = None, required_fields: Optional[List[str]] = None, optional_fields: Optional[List[str]] = None, metric_name: Optional[str] = None, metric_description: Optional[str] = None, name: Optional[str] = None, field_mapping: Optional[Dict[str, str]] = None, metric_category: Optional[MetricCategory] = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>_RetrievalMetric</code></p> <p>Calculates the Mean Reciprocal Rank (MRR). Score is 1 / (rank of first relevant item). This metric is K-independent.</p>"},{"location":"reference/metrics/#axion.metrics.MeanReciprocalRank.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: MeanReciprocalRankResult) -&gt; List[SignalDescriptor]\n</code></pre> <p>Generates signals detailing the Mean Reciprocal Rank calculation.</p>"},{"location":"reference/metrics/#conversational-metrics","title":"Conversational Metrics","text":""},{"location":"reference/metrics/#goalcompletion","title":"GoalCompletion","text":""},{"location":"reference/metrics/#axion.metrics.GoalCompletion","title":"axion.metrics.GoalCompletion","text":"<pre><code>GoalCompletion(goal_key: str = 'goal', completion_weight: float = 0.6, efficiency_weight: float = 0.4, bottleneck_threshold: int = 5, max_clarification_penalty: float = 0.3, clarification_penalty_rate: float = 0.1, goal_drift_threshold: float = 0.3, outcome_threshold_achieved: float = 0.8, outcome_threshold_partial: float = 0.4, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Composite metric that analyzes: 1. Goal achievement (did we complete the task?) 2. Conversation efficiency (how well did we complete it?) 3. Temporal tracking (when/how did we achieve sub-goals?)</p> <p>Reuses conversation analysis from shared components to minimize LLM calls. Uses unified analysis approach to reduce LLM calls from ~3+N to ~3 total.</p> <p>Initialize the Goal Completion metric.</p> <p>Parameters:</p> <ul> <li> <code>goal_key</code>               (<code>str</code>, default:                   <code>'goal'</code> )           \u2013            <p>Key in additional_input containing the user's goal</p> </li> <li> <code>completion_weight</code>               (<code>float</code>, default:                   <code>0.6</code> )           \u2013            <p>Weight for pure goal achievement (default: 0.6) Rationale: Goal achievement is slightly more important than efficiency</p> </li> <li> <code>efficiency_weight</code>               (<code>float</code>, default:                   <code>0.4</code> )           \u2013            <p>Weight for conversation efficiency (default: 0.4) Rationale: Efficiency matters, but achieving the goal is primary</p> </li> <li> <code>bottleneck_threshold</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of turns before a sub-goal is flagged as bottleneck (default: 5) Rationale: Most sub-goals should resolve within 3-5 turns in efficient conversations</p> </li> <li> <code>max_clarification_penalty</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>Maximum penalty for clarifications (default: 0.3) Rationale: Excessive clarifications can reduce efficiency by up to 30%</p> </li> <li> <code>clarification_penalty_rate</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Penalty per clarification (default: 0.1) Rationale: Each clarification represents a 10% efficiency loss</p> </li> <li> <code>goal_drift_threshold</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>Fraction of unmapped moments to trigger drift detection (default: 0.3) Rationale: If &gt;30% of conversation is unrelated to goal, it indicates drift</p> </li> <li> <code>outcome_threshold_achieved</code>               (<code>float</code>, default:                   <code>0.8</code> )           \u2013            <p>Minimum score for \"achieved\" outcome (default: 0.8) Rationale: 80%+ completion indicates successful goal achievement</p> </li> <li> <code>outcome_threshold_partial</code>               (<code>float</code>, default:                   <code>0.4</code> )           \u2013            <p>Minimum score for \"partially_achieved\" outcome (default: 0.4) Rationale: 40-80% completion indicates partial success</p> </li> </ul>"},{"location":"reference/metrics/#axion.metrics.GoalCompletion.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult\n</code></pre> <p>Execute goal completion analysis using unified approach.</p>"},{"location":"reference/metrics/#axion.metrics.GoalCompletion.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: GoalCompletionResult) -&gt; List[SignalDescriptor[GoalCompletionResult]]\n</code></pre> <p>Generate comprehensive signals showing multi-layered analysis.</p>"},{"location":"reference/metrics/#conversationflow","title":"ConversationFlow","text":"<p>Metrics &amp; Evaluation Guide  Creating Custom Metrics  Metric Registry </p>"},{"location":"reference/metrics/#axion.metrics.ConversationFlow","title":"axion.metrics.ConversationFlow","text":"<pre><code>ConversationFlow(config: Optional[FlowConfig] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseMetric</code></p> <p>Refactored conversation flow metric with modular, testable components.</p> <p>Improvements over v1: - Configurable penalties and thresholds - Enum-based issue types (no string matching) - Separate, testable detector classes - Transparent score decomposition - Better statistical methods - Comprehensive signal generation</p>"},{"location":"reference/metrics/#axion.metrics.ConversationFlow.execute","title":"execute  <code>async</code>","text":"<pre><code>execute(item: DatasetItem, cache: Optional[AnalysisCache] = None) -&gt; MetricEvaluationResult\n</code></pre> <p>Execute comprehensive conversation flow analysis.</p>"},{"location":"reference/metrics/#axion.metrics.ConversationFlow.get_signals","title":"get_signals","text":"<pre><code>get_signals(result: ConversationFlowResult) -&gt; List[SignalDescriptor]\n</code></pre> <p>Generate comprehensive signals showing score calculation.</p>"},{"location":"reference/runners/","title":"Runners API Reference","text":"<p>Evaluation execution engines for parallel and batch processing.</p> <pre><code>from axion.runners import (\n    evaluation_runner,\n    EvaluationRunner,\n    EvaluationConfig,\n    MetricRunner,\n)\nfrom axion._core.cache import CacheManager, CacheConfig\n</code></pre> E <p>evaluation_runner</p> <p>High-level function for running complete evaluations across datasets with multiple metrics in parallel.</p> M <p>MetricRunner</p> <p>Lower-level runner for executing a single metric across dataset items with concurrency control.</p>"},{"location":"reference/runners/#evaluation_runner","title":"evaluation_runner","text":""},{"location":"reference/runners/#axion.runners.evaluation_runner","title":"axion.runners.evaluation_runner","text":"<pre><code>evaluation_runner(evaluation_inputs: Union[Dataset, List[DatasetItem], DataFrame], evaluation_name: str, scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: float = 0.0, summary_generator: Optional[BaseSummary] = None, cache_config: Optional[CacheConfig] = None, error_config: Optional[ErrorConfig] = None, enable_internal_caching: bool = True, thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, trace_granularity: Union[TraceGranularity, str] = SINGLE_TRACE, flush_per_metric: bool = False, enable_prompt_caching: bool = False) -&gt; Optional[EvaluationResult]\n</code></pre> <p>Synchronously runs an evaluation experiment to evaluate metrics over a given dataset, supporting both flat and hierarchical scoring structures.</p> <p>Parameters:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>, default:                   <code>None</code> )           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>Optional[List[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>An alternative, more intuitive parameter for passing a flat list of metrics.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>, default:                   <code>None</code> )           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>, default:                   <code>None</code> )           \u2013            <p>A custom function to generate model outputs before scoring.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Maps metric input names to dataset column names.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>A human-readable description of the evaluation.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional metadata to include in the evaluation trace.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of concurrent evaluations. Defaults to 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>, default:                   <code>None</code> )           \u2013            <p>A summary generator for high-level results.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for caching results to avoid recomputation.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>, default:                   <code>None</code> )           \u2013            <p>Configuration for handling errors during evaluation.</p> </li> <li> <code>enable_internal_caching</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enables a per-item cache for metrics that share expensive internal computations. Defaults to True.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Performance thresholds for each metric.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show a progress bar. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of the dataset.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>An optional identifier for this specific run.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>, default:                   <code>SINGLE_TRACE</code> )           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> <li> <code>flush_per_metric</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>When trace_granularity='separate', controls whether each metric trace is flushed immediately (slower, but more \"live\" in the UI) vs batched (faster). Defaults to False.</p> </li> <li> <code>enable_prompt_caching</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enables provider-level prompt caching for all metrics. When True, propagates to all metrics and their sub-judges to mark system/few-shot prefixes as cacheable. Supports Anthropic (explicit caching) and OpenAI (automatic). Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EvaluationResult</code> (              <code>Optional[EvaluationResult]</code> )          \u2013            <p>An object containing detailed metric scores, summary, and metadata.</p> </li> </ul>"},{"location":"reference/runners/#evaluationrunner","title":"EvaluationRunner","text":""},{"location":"reference/runners/#axion.runners.EvaluationRunner","title":"axion.runners.EvaluationRunner","text":"<pre><code>EvaluationRunner(config: EvaluationConfig, tracer: Optional[BaseTraceHandler] = None)\n</code></pre> <p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the execution of evaluation experiments, managing task execution, metric scoring, and configuration. Automatically determines and initializes the appropriate scoring strategy (flat or hierarchical).</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.summary","title":"summary  <code>property</code>","text":"<pre><code>summary: Union[Dict[str, Any], None]\n</code></pre> <p>Returns the summary from the active scoring strategy. For hierarchical ('tree') strategies, this provides the detailed tree summary.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.tree","title":"tree  <code>property</code>","text":"<pre><code>tree: Any\n</code></pre> <p>Returns the underlying EvalTree instance for inspection, if the 'tree' strategy is active. Raises an AttributeError for other strategies.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.execute","title":"execute  <code>async</code>","text":"<pre><code>execute() -&gt; EvaluationResult\n</code></pre> <p>Executes the entire evaluation and returns the final result.</p> <p>For SINGLE_TRACE mode, wraps execution in a trace span. For PER_ITEM and SEPARATE modes, skips the wrapper span to allow each item/metric to create its own independent trace.</p>"},{"location":"reference/runners/#axion.runners.EvaluationRunner.display","title":"display  <code>classmethod</code>","text":"<pre><code>display()\n</code></pre> <p>Display Usage Documentation</p>"},{"location":"reference/runners/#evaluationconfig","title":"EvaluationConfig","text":""},{"location":"reference/runners/#axion.runners.EvaluationConfig","title":"axion.runners.EvaluationConfig  <code>dataclass</code>","text":"<pre><code>EvaluationConfig(evaluation_name: str, evaluation_inputs: Union[Dataset, List[DatasetItem], DataFrame], scoring_config: Optional[Union[List[Any], Dict[str, Any], str]] = None, scoring_metrics: Optional[List[Any]] = None, scoring_strategy: Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]] = None, task: Optional[Union[Callable, BaseAPIRunner]] = None, scoring_key_mapping: Optional[Dict[str, str]] = None, evaluation_description: Optional[str] = None, evaluation_metadata: Optional[Dict[str, Any]] = None, max_concurrent: int = 5, throttle_delay: Optional[float] = 0.0, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_config: CacheConfig = CacheConfig(), error_config: ErrorConfig = ErrorConfig(), thresholds: Optional[Dict[str, float]] = None, show_progress: bool = True, dataset_name: Optional[str] = None, run_id: Optional[str] = None, enable_internal_caching: bool = True, trace_granularity: Union[TraceGranularity, str] = SINGLE_TRACE, flush_per_metric: bool = False, enable_prompt_caching: bool = False)\n</code></pre> <p>Configuration for an evaluation run.</p> <p>Attributes:</p> <ul> <li> <code>evaluation_inputs</code>               (<code>Union[Dataset, List[DatasetItem], DataFrame]</code>)           \u2013            <p>The input dataset to evaluate. Can be a high-level <code>Dataset</code> object, a list of individual <code>DatasetItem</code> objects, or a preloaded <code>pandas.DataFrame</code>.</p> </li> <li> <code>scoring_config</code>               (<code>Optional[Union[List[Any], Dict[str, Any], str]]</code>)           \u2013            <p>The scoring configuration. Can be: - A list of metrics for flat evaluation - A dictionary with 'metric' key for flat evaluation (when scoring_strategy='flat') - A dictionary for hierarchical (EvalTree) evaluation (with model, weights, etc.) - A string file path to a YAML configuration file</p> </li> <li> <code>scoring_metrics</code>               (<code>List[Any]</code>)           \u2013            <p>A list of metric objects or callables used to score each item in the dataset.</p> </li> <li> <code>scoring_strategy</code>               (<code>Optional[Union[BaseScoringStrategy, str, ScoringStrategyType]]</code>)           \u2013            <p>Defines the scoring method. Can be a pre-initialized strategy instance or a string/Enum alias ('flat' or 'tree'). Overrides auto-detection.</p> </li> <li> <code>evaluation_name</code>               (<code>str</code>)           \u2013            <p>A unique name to identify the evaluation. Used in trace logging and result storage.</p> </li> <li> <code>task</code>               (<code>Optional[Union[Callable, BaseAPIRunner]]</code>)           \u2013            <p>A custom function to generate predictions or transform inputs. If provided, it will be run before scoring to produce the model output for each dataset item.</p> </li> <li> <code>scoring_key_mapping</code>               (<code>Optional[Dict[str, str]]</code>)           \u2013            <p>An optional dictionary mapping metric input names to dataset column names. Useful for adapting metrics to different schema formats.</p> </li> <li> <code>evaluation_description</code>               (<code>Optional[str]</code>)           \u2013            <p>A human-readable description of the evaluation for documentation and trace metadata.</p> </li> <li> <code>evaluation_metadata</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Additional metadata to include in the evaluation trace (e.g., model version, data slice info, tags).</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>)           \u2013            <p>Maximum number of metric evaluations to run concurrently. Default is 5.</p> </li> <li> <code>throttle_delay</code>               (<code>float</code>)           \u2013            <p>Specifies the time in seconds to pause after each individual task execution. This is used as a client-side throttle to help prevent API rate limit errors when processing a large number of items. Defaults to 0.0 (no delay).</p> </li> <li> <code>summary_generator</code>               (<code>Optional[BaseSummary]</code>)           \u2013            <p>Optional summary generator used to produce a high-level summary after the evaluation. If not provided, a default <code>MetricSummary</code> is used.</p> </li> <li> <code>cache_config</code>               (<code>CacheConfig</code>)           \u2013            <p>Configuration for caching metric results to avoid recomputation. Enables both read and write caching.</p> </li> <li> <code>error_config</code>               (<code>ErrorConfig</code>)           \u2013            <p>Configuration for how errors are handled during evaluation. Allows skipping metrics or suppressing failures.</p> </li> <li> <code>thresholds</code>               (<code>Optional[Dict[str, float]]</code>)           \u2013            <p>Optional threshold values for each metric. Used to flag items or datasets that fall below a given performance level.</p> </li> <li> <code>show_progress</code>               (<code>bool</code>)           \u2013            <p>Whether to show a progress bar during evaluation. Defaults to True.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name of the dataset being evaluated. Used for display and trace logging.</p> </li> <li> <code>run_id</code>               (<code>Optional[str]</code>)           \u2013            <p>An optional identifier for this specific run. Useful for repeatability and audit logging.</p> </li> <li> <code>trace_granularity</code>               (<code>Union[TraceGranularity, str]</code>)           \u2013            <p>Controls trace granularity during evaluation. Accepts enum or string values: - 'single_trace' / 'single' / SINGLE_TRACE (default): All evaluations under one parent trace - 'separate' / SEPARATE: Each metric execution gets its own independent trace</p> </li> <li> <code>enable_prompt_caching</code>               (<code>bool</code>)           \u2013            <p>Enables provider-level prompt caching for all metrics. When True, propagates to all metrics and their sub-judges to mark system/few-shot prefixes as cacheable. Supports Anthropic (explicit caching) and OpenAI (automatic). Defaults to False.</p> </li> </ul>"},{"location":"reference/runners/#metricrunner","title":"MetricRunner","text":""},{"location":"reference/runners/#axion.runners.MetricRunner","title":"axion.runners.MetricRunner  <code>dataclass</code>","text":"<pre><code>MetricRunner(metrics: List[Any], name: str = 'MetricRunner', description: str = 'Orchestrates evaluation metrics', max_concurrent: int = 5, thresholds: Optional[Dict[str, float]] = None, summary_generator: Optional[BaseSummary] = MetricSummary(), cache_manager: Optional[CacheManager] = None, error_config: ErrorConfig = ErrorConfig(), tracer: Optional[BaseTraceHandler] = None, dataset_name: Optional[str] = 'Metric Runner Dataset', enable_internal_caching: bool = True, trace_granularity: TraceGranularity = SEPARATE, flush_per_metric: bool = False)\n</code></pre> <p>               Bases: <code>RunnerMixin</code></p> <p>Orchestrates the evaluation of multiple metrics against a dataset.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.available_types","title":"available_types  <code>property</code>","text":"<pre><code>available_types: List[str]\n</code></pre> <p>Returns a list of available (registered) metric runner types.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.elapsed_time","title":"elapsed_time  <code>property</code>","text":"<pre><code>elapsed_time: Union[float, None]\n</code></pre> <p>Returns the total execution time for the last batch run.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.summary","title":"summary  <code>property</code>","text":"<pre><code>summary: Union[Dict[str, Any], None]\n</code></pre> <p>Returns the summary of the last batch run.</p>"},{"location":"reference/runners/#axion.runners.MetricRunner.execute_batch","title":"execute_batch  <code>async</code>","text":"<pre><code>execute_batch(evaluation_inputs: Union[Dataset, List[DatasetItem], DataFrame], *, show_progress: bool = True) -&gt; List[TestResult]\n</code></pre> <p>Executes all configured metrics against the provided dataset.</p> <p>Trace granularity behavior: - SINGLE_TRACE: All metrics run under one parent trace (default) - SEPARATE: Each metric execution gets its own independent trace</p>"},{"location":"reference/runners/#cachemanager","title":"CacheManager","text":""},{"location":"reference/runners/#axion._core.cache.CacheManager","title":"axion._core.cache.CacheManager","text":"<pre><code>CacheManager(config: CacheConfig = None)\n</code></pre> <p>Manages cache operations for both memory and disk, abstracting the backend.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.get","title":"get","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Gets an item from the cache if use_cache is True.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.set","title":"set","text":"<pre><code>set(key: str, value: Any)\n</code></pre> <p>Sets an item in the cache if write_cache is True.</p>"},{"location":"reference/runners/#axion._core.cache.CacheManager.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Closes the cache connection if applicable (for diskcache).</p>"},{"location":"reference/runners/#cacheconfig","title":"CacheConfig","text":"<p>Running Evaluations Guide  Evaluation Runner Deep Dive  Metric Runner Deep Dive </p>"},{"location":"reference/runners/#axion._core.cache.CacheConfig","title":"axion._core.cache.CacheConfig  <code>dataclass</code>","text":"<pre><code>CacheConfig(use_cache: bool = True, write_cache: bool = True, cache_type: str = 'memory', cache_dir: Optional[str] = '.cache', cache_task: bool = True)\n</code></pre> <p>Configuration class for controlling caching behavior of metric evaluations.</p> <p>Attributes:</p> <ul> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>If True, attempts to read previously computed results from cache to avoid redundant computation.</p> </li> <li> <code>write_cache</code>               (<code>bool</code>)           \u2013            <p>If True, writes newly computed metric results to cache for future use. Has no effect if <code>use_cache</code> is False.</p> </li> <li> <code>cache_type</code>               (<code>str</code>)           \u2013            <p>Type of caching backend to use. - 'memory': Uses in-memory dictionary for caching (fast, but non-persistent). - 'disk': Writes cache to disk (persistent across runs).</p> </li> <li> <code>cache_dir</code>               (<code>Optional[str]</code>)           \u2013            <p>Directory path where disk cache files will be stored. Only used when <code>cache_type='disk'</code>. Defaults to '.cache'.</p> </li> <li> <code>cache_task</code>               (<code>bool</code>)           \u2013            <p>If True, enables caching at the task level (e.g., for full evaluation runs). If False, caching applies only at the metric level.</p> </li> </ul>"},{"location":"reference/schema/","title":"Schema API Reference","text":"<p>Result types and evaluation schemas returned by runners and metrics.</p> <pre><code>from axion.schema import (\n    MetricScore,\n    EvaluationResult,\n    TestResult,\n    NormalizedDataFrames,\n    ErrorConfig,\n)\n</code></pre>"},{"location":"reference/schema/#metricscore","title":"MetricScore","text":""},{"location":"reference/schema/#axion.schema.MetricScore","title":"axion.schema.MetricScore","text":"<pre><code>MetricScore(**data)\n</code></pre> <p>               Bases: <code>RichBaseModel</code></p> <p>Standardized data model for a single metric evaluation result. Captures the computed score, the logic behind it, thresholds used, and any metadata useful for debugging or reporting.</p>"},{"location":"reference/schema/#evaluationresult","title":"EvaluationResult","text":""},{"location":"reference/schema/#axion.schema.EvaluationResult","title":"axion.schema.EvaluationResult  <code>dataclass</code>","text":"<pre><code>EvaluationResult(run_id: str, evaluation_name: Optional[str], timestamp: str, results: List[TestResult], summary: Dict[str, Any] = dict(), metadata: Dict[str, Any] = dict())\n</code></pre> <p>Captures the full result of an evaluation run across multiple test cases and metrics.</p> <p>Attributes:</p> <ul> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for this specific evaluation run. Typically generated per execution.</p> </li> <li> <code>evaluation_name</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional name for the experiment or test campaign (e.g., \"Lead Scoring v2 A/B\").</p> </li> <li> <code>timestamp</code>               (<code>str</code>)           \u2013            <p>ISO-formatted timestamp indicating when the evaluation was run. Can be used for sorting or audit logging.</p> </li> <li> <code>results</code>               (<code>List[TestResult]</code>)           \u2013            <p>A list of TestResult objects, each representing the evaluation output for a single test case across one or more metrics.</p> </li> <li> <code>summary</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Summary of the TestResult objects, representing the evaluation output across each metric.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Arbitrary metadata such as configuration info, evaluator identity, model version, dataset name, or custom flags for internal use.</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(by_alias: bool = True, id_as_index: bool = False, include_test_case: bool = True, include_run_metadata: bool = True, column_order: Optional[List[str]] = None, rename_columns: bool = True) -&gt; DataFrame\n</code></pre> <p>Flattens the entire evaluation result into a single pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>by_alias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use field aliases in the output. When True, MetricScore fields use aliases (id -&gt; metric_id, metadata -&gt; metric_metadata).</p> </li> <li> <code>id_as_index</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, sets the test_case <code>id</code> as the DataFrame index.</p> </li> <li> <code>include_test_case</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include test_case fields in the output.</p> </li> <li> <code>include_run_metadata</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include run-level metadata.</p> </li> <li> <code>column_order</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Output column ordering.</p> </li> <li> <code>rename_columns</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Rename columns to match model_arena format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: Flattened view of all metrics with test case and run context.</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_normalized_dataframes","title":"to_normalized_dataframes","text":"<pre><code>to_normalized_dataframes(by_alias: bool = True, include_run_metadata: bool = True, dataset_column_order: Optional[List[str]] = None, metrics_column_order: Optional[List[str]] = None, rename_columns: bool = True, include_computed_fields: bool = True) -&gt; NormalizedDataFrames\n</code></pre> <p>Returns two normalized DataFrames following data engineering best practices.</p> <p>Unlike to_dataframe() which creates one row per (test_case, metric) combination with duplicated dataset fields, this method returns: 1. Dataset Items Table: One row per DatasetItem (inputs/ground truth) 2. Metric Results Table: One row per MetricScore, with FK to dataset item</p> <p>Parameters:</p> <ul> <li> <code>by_alias</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use field aliases in the output. When True, MetricScore fields use aliases (id -&gt; metric_id, metadata -&gt; metric_metadata).</p> </li> <li> <code>include_run_metadata</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include run-level metadata in metrics table.</p> </li> <li> <code>dataset_column_order</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Output column ordering for dataset items table.</p> </li> <li> <code>metrics_column_order</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>Output column ordering for metric results table.</p> </li> <li> <code>rename_columns</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Rename columns to match model_arena format (name -&gt; metric_name, score -&gt; metric_score, type -&gt; metric_type).</p> </li> <li> <code>include_computed_fields</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to include computed fields from DatasetItem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NormalizedDataFrames</code> (              <code>NormalizedDataFrames</code> )          \u2013            <p>A named tuple containing: - dataset_items: DataFrame with one row per DatasetItem - metric_results: DataFrame with one row per MetricScore, with 'id' FK</p> </li> </ul> Note <p>You can merge the two DataFrames to get a denormalized view similar to to_dataframe(). The FK column name depends on the by_alias parameter::</p> <pre><code># With by_alias=False (default)\nmerged_df = metrics_df.merge(dataset_df, on='id', how='left')\n\n# With by_alias=True\nmerged_df = metrics_df.merge(dataset_df, on='dataset_id', how='left')\n</code></pre> <p>The merged result has the same columns as to_dataframe(). Both metadata fields use aliases to avoid conflicts: DatasetItem's metadata uses 'dataset_metadata' and MetricScore's metadata uses 'metric_metadata'. The only difference is column order. Use how='left' to keep metric rows even when test_case was None.</p> <p>The normalized approach is better when you want to avoid data duplication or need to work with the data in a relational/normalized way.</p> Example <p>dataset_df, metrics_df = eval_result.to_normalized_dataframes()</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_normalized_dataframes--verify-fk-relationship","title":"Verify FK relationship","text":"<p>assert set(metrics_df['id'].dropna()).issubset(set(dataset_df['id']))</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_normalized_dataframes--verify-no-duplication-in-dataset-table","title":"Verify no duplication in dataset table","text":"<p>assert len(dataset_df) == dataset_df['id'].nunique()</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_latency_plot","title":"to_latency_plot","text":"<pre><code>to_latency_plot(col_name: str = 'latency', id_col: str = 'id', bins: int = 30, show_legend: bool = True, show_stats_panel: bool = True, figsize: Tuple[int, int] = (16, 9), return_plot: bool = False, show_plot: bool = True, output_path: Optional[str] = None, plot_title: str = 'Latency Distribution', color_palette: Optional[Dict[str, str]] = None) -&gt; Union[DataFrame, Tuple[DataFrame, Optional[Any], Optional[Any]]]\n</code></pre> <p>Analyzes and visualizes latency distribution.</p> <p>Parameters:</p> <ul> <li> <code>col_name</code>               (<code>str</code>, default:                   <code>'latency'</code> )           \u2013            <p>Name of the column containing latency values.</p> </li> <li> <code>id_col</code>               (<code>str</code>, default:                   <code>'id'</code> )           \u2013            <p>Unique identifier for test cases (used to deduplicate latency).</p> </li> <li> <code>bins</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>Number of histogram bins.</p> </li> <li> <code>show_legend</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show plot legend,</p> </li> <li> <code>show_stats_panel</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show stats panel,</p> </li> <li> <code>figsize</code>               (<code>Tuple[int, int]</code>, default:                   <code>(16, 9)</code> )           \u2013            <p>Size of the matplotlib figure.</p> </li> <li> <code>return_plot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns (stats_df, fig, ax). If False, returns stats_df.</p> </li> <li> <code>show_plot</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to render the plot using plt.show() (or display in NB).</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>If provided, saves the plot to this file path.</p> </li> <li> <code>plot_title</code>               (<code>str</code>, default:                   <code>'Latency Distribution'</code> )           \u2013            <p>Descriptive name for the latency plot title (default: \"Latency Distribution\")</p> </li> <li> <code>color_palette</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom colors for the LatencyAnalyzer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[DataFrame, Tuple[DataFrame, Optional[Any], Optional[Any]]]</code>           \u2013            <p>pd.DataFrame or Tuple[pd.DataFrame, Figure, Axes]</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.to_scorecard","title":"to_scorecard","text":"<pre><code>to_scorecard(llm: Optional[LLMRunnable] = None, metric_definitions: dict = None, explanation_callback: Optional[Callable] = None, instruction: Optional[str] = None, max_concurrent: int = 10, output_path: Optional[str] = None, display_in_notebook: bool = False, return_html: bool = False, return_styled_df: bool = True, id_col: str = 'metric_name', parent_col: str = 'parent', value_cols: List[str] = None, group_meta_cols: List[str] = None) -&gt; Union[str, DataFrame, None]\n</code></pre> <p>Generates a hierarchical scorecard report using the evaluation results.</p> <p>This method creates a visual performance breakdown. It can display the report interactively in a notebook, save it as an HTML file, or return the styled object/HTML string for custom use.</p> <p>Parameters:</p> <ul> <li> <code>llm</code>               (<code>Optional[LLMRunnable]</code>, default:                   <code>None</code> )           \u2013            <p>Custom LLM instance to use for generating qualitative explanations.</p> </li> <li> <code>metric_definitions</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary mapping metric names to static descriptions or templates.</p> </li> <li> <code>explanation_callback</code>               (<code>callable</code>, default:                   <code>None</code> )           \u2013            <p>Custom function <code>f(name, score, type)</code> to generate explanations manually.</p> </li> <li> <code>instruction</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>System prompt override for the explanation generation LLM.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum number of parallel LLM calls for batch processing explanations.</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>File path to save the generated HTML report.</p> </li> <li> <code>display_in_notebook</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, renders the styled dataframe (or HTML) directly in Jupyter/IPython.</p> </li> <li> <code>return_html</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the raw HTML string of the report.</p> </li> <li> <code>return_styled_df</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, returns the pandas Styler object for further customization.</p> </li> <li> <code>id_col</code>               (<code>str</code>, default:                   <code>'metric_name'</code> )           \u2013            <p>Column name representing the unique node identifier (default: 'metric_name').</p> </li> <li> <code>parent_col</code>               (<code>str</code>, default:                   <code>'parent'</code> )           \u2013            <p>Column name representing the parent node identifier (default: 'parent').</p> </li> <li> <code>value_cols</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of columns to aggregate values for (e.g., ['metric_score', 'weight']).</p> </li> <li> <code>group_meta_cols</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of metadata columns to include in grouping (e.g., ['metric_type']).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, DataFrame, None]</code>           \u2013            <p>Union[str, Styler, None]: - The HTML string if <code>return_html=True</code>. - The pandas Styler object if <code>return_styled_df=True</code>. - None otherwise (default behavior is just to display or save).</p> </li> </ul>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability","title":"publish_to_observability","text":"<pre><code>publish_to_observability(loader: Optional[Any] = None, observation_id_field: Optional[str] = 'observation_id', flush: bool = True, tags: Optional[List[str]] = None, metric_names: Optional[List[str]] = None) -&gt; Dict[str, int]\n</code></pre> <p>Publish evaluation scores to an observability platform.</p> <p>Uses a trace loader to publish scores. By default, uses LangfuseTraceLoader.</p> <p>Parameters:</p> <ul> <li> <code>loader</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>A trace loader instance (e.g., LangfuseTraceLoader, OpikTraceLoader). If None, creates a new LangfuseTraceLoader using environment variables.</p> </li> <li> <code>observation_id_field</code>               (<code>Optional[str]</code>, default:                   <code>'observation_id'</code> )           \u2013            <p>Field name on DatasetItem containing the observation/span ID. If provided, scores attach to that specific observation within the trace. If None, scores attach to the trace itself. Default: 'observation_id'.</p> </li> <li> <code>flush</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to flush the client after uploading. Defaults to True.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to attach to all scores as metadata. Falls back to LANGFUSE_TAGS env var if not provided.</p> </li> <li> <code>metric_names</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of metric names to upload. If provided, only scores whose metric name matches are uploaded.</p> </li> </ul> Note <p>Environment cannot be set when pushing scores to existing traces. To set environment, configure it at client initialization when creating traces (via LANGFUSE_ENVIRONMENT or LANGFUSE_TRACING_ENVIRONMENT env vars or the environment parameter in LangfuseTracer).</p> <p>Returns:</p> <ul> <li> <code>Dict[str, int]</code>           \u2013            <p>Dict with counts: {'uploaded': N, 'skipped': M} - uploaded: Number of scores successfully pushed - skipped: Number of scores skipped (missing trace_id or invalid score)</p> </li> </ul> Example <p>from axion._core.tracing.loaders import LangfuseTraceLoader</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--using-default-langfuse-loader","title":"Using default Langfuse loader","text":"<p>stats = result.publish_to_observability()</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--using-explicit-loader-with-tags","title":"Using explicit loader with tags","text":"<p>loader = LangfuseTraceLoader() stats = result.publish_to_observability( ...     loader=loader, ...     tags=['prod', 'v1.0'] ... )</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_to_observability--attach-scores-to-traces-only-no-observation","title":"Attach scores to traces only (no observation)","text":"<p>stats = result.publish_to_observability(observation_id_field=None)</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment","title":"publish_as_experiment","text":"<pre><code>publish_as_experiment(loader: Optional[Any] = None, dataset_name: Optional[str] = None, run_name: Optional[str] = None, run_metadata: Optional[Dict[str, Any]] = None, flush: bool = True, tags: Optional[List[str]] = None, score_on_runtime_traces: bool = False, link_to_traces: bool = False, metric_names: Optional[List[str]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Publish evaluation results to Langfuse as a dataset experiment.</p> <p>This method creates a complete experiment in Langfuse with a dataset, dataset items, experiment runs, and scores. Unlike <code>publish_to_observability()</code>, it does not require existing traces - it creates everything from scratch.</p> <p>Parameters:</p> <ul> <li> <code>loader</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>A LangfuseTraceLoader instance. If None, creates a new one using environment variables.</p> </li> <li> <code>dataset_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name for the Langfuse dataset. Defaults to evaluation_name or generates one based on run_id.</p> </li> <li> <code>run_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Name for the experiment run. Defaults to \"{dataset_name}-{run_id}\" pattern.</p> </li> <li> <code>run_metadata</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata to attach to the experiment run.</p> </li> <li> <code>flush</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to flush the client after uploading. Defaults to True.</p> </li> <li> <code>tags</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to attach to all scores as metadata.</p> </li> <li> <code>score_on_runtime_traces</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, skip creating per-item \"Dataset run\" traces and instead attach scores to existing runtime traces via <code>trace_id</code>/<code>observation_id</code>. Takes precedence over link_to_traces if both are True.</p> </li> <li> <code>link_to_traces</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, link experiment runs to existing traces via the low-level API instead of creating new \"Dataset run\" traces. This allows experiment runs to appear linked to the original evaluation traces in Langfuse UI. Falls back to creating new traces if trace_id is not available. Ignored if score_on_runtime_traces is True.</p> </li> <li> <code>metric_names</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of metric names to upload. If provided, only scores whose metric name matches are uploaded.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict with statistics: - dataset_name: Name of the created/used dataset - run_name: Name of the experiment run - items_created: Number of dataset items created - runs_created: Number of experiment runs created - scores_uploaded: Number of scores attached - scores_skipped: Number of scores skipped (None/NaN values) - errors: List of error messages encountered</p> </li> </ul> Example <p>from axion import evaluation_runner from axion.metrics import Faithfulness, AnswerRelevancy</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment--run-evaluation","title":"Run evaluation","text":"<p>results = evaluation_runner( ...     evaluation_inputs=dataset, ...     scoring_config=config, ...     evaluation_name=\"RAG Evaluation\", ... )</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.publish_as_experiment--upload-to-langfuse-as-experiment","title":"Upload to Langfuse as experiment","text":"<p>stats = results.publish_as_experiment( ...     dataset_name=\"my-rag-dataset\", ...     run_name=\"experiment-v1\", ...     tags=[\"production\"] ... )</p> <p>print(f\"Uploaded {stats['scores_uploaded']} scores to {stats['dataset_name']}\")</p>"},{"location":"reference/schema/#axion.schema.EvaluationResult.expand_multi_metrics","title":"expand_multi_metrics","text":"<pre><code>expand_multi_metrics(expansion_map: Optional[Dict[str, Callable[[MetricScore], List[MetricScore]]]] = None, in_place: bool = False) -&gt; EvaluationResult\n</code></pre> <p>Expand multi-metric results using custom expansion functions.</p> <p>This method allows post-hoc expansion of metric scores that contain nested data in their metadata or signals. It's useful when you have existing results that weren't exploded at runtime, or when you want to apply custom expansion logic.</p> <p>Parameters:</p> <ul> <li> <code>expansion_map</code>               (<code>Optional[Dict[str, Callable[[MetricScore], List[MetricScore]]]]</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping metric names to expansion functions. Each function takes a MetricScore and returns List[MetricScore]. If None, no expansion is performed.</p> </li> <li> <code>in_place</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, modifies this instance. If False, returns a new copy.</p> </li> </ul>"},{"location":"reference/schema/#testresult","title":"TestResult","text":""},{"location":"reference/schema/#axion.schema.TestResult","title":"axion.schema.TestResult  <code>dataclass</code>","text":"<pre><code>TestResult(test_case: Optional[DatasetItem], score_results: List[MetricScore], metadata: Optional[Dict[str, Any]] = dict())\n</code></pre> <p>Represents the result of evaluating a single test case using one or more evaluation metrics.</p> <p>Attributes:</p> <ul> <li> <code>test_case</code>               (<code>DatasetItem</code>)           \u2013            <p>The input test case containing query, expected output, and other context. This forms the basis for which all metrics are applied.</p> </li> <li> <code>score_results</code>               (<code>List[MetricScore]</code>)           \u2013            <p>A list of evaluation results returned from applying different metrics to this test case. Each MetricScore includes a score, explanation, and threshold comparison.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Optional metadata for storing extra context such as timestamps, evaluator info, experiment variant, evaluation notes, or model config parameters.</p> </li> </ul>"},{"location":"reference/schema/#normalizeddataframes","title":"NormalizedDataFrames","text":""},{"location":"reference/schema/#axion.schema.NormalizedDataFrames","title":"axion.schema.NormalizedDataFrames","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Return type for to_normalized_dataframes() method.</p> <p>Provides a normalized data model with two separate DataFrames: - dataset_items: One row per DatasetItem (inputs/ground truth) - metric_results: One row per MetricScore, with FK to dataset item</p>"},{"location":"reference/schema/#errorconfig","title":"ErrorConfig","text":"<p>Running Evaluations  Metrics Guide </p>"},{"location":"reference/schema/#axion.schema.ErrorConfig","title":"axion.schema.ErrorConfig  <code>dataclass</code>","text":"<pre><code>ErrorConfig(ignore_errors: bool = True, skip_on_missing_params: bool = False)\n</code></pre> <p>Configuration class for controlling error handling during metric execution.</p> <p>Attributes:</p> <ul> <li> <code>ignore_errors</code>               (<code>bool</code>)           \u2013            <p>If True, any exceptions raised during metric execution will be caught and suppressed. The metric will return a placeholder result (e.g., None or NaN) instead of failing the entire evaluation. Use this to allow evaluations to proceed even if some metrics occasionally fail.</p> </li> <li> <code>skip_on_missing_params</code>               (<code>bool</code>)           \u2013            <p>If True, metrics will be skipped entirely when required input fields are missing from the data. This is useful when running multiple metrics over heterogeneous data where not all fields are always present. If False, the metric will raise an error if required inputs are missing.</p> </li> </ul>"},{"location":"reference/search/","title":"Search API Reference","text":"<p>Search and retrieval integrations for augmenting evaluation datasets.</p> <pre><code>from axion.search import GoogleRetriever, TavilyRetriever, YouRetriever\n</code></pre> G <p>GoogleRetriever</p> <p>Google Custom Search API integration for web search and retrieval.</p> T <p>TavilyRetriever</p> <p>Tavily AI search API for research-optimized web retrieval with AI summaries.</p> Y <p>YouRetriever</p> <p>You.com search API integration for AI-focused web search results.</p>"},{"location":"reference/search/#googleretriever","title":"GoogleRetriever","text":""},{"location":"reference/search/#axion.search.GoogleRetriever","title":"axion.search.GoogleRetriever","text":"<pre><code>GoogleRetriever(api_key: Optional[str] = None, num_web_results: int = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, **kwargs)\n</code></pre> <p>               Bases: <code>BaseRetriever</code></p> <p>Retriever that uses SerpAPI to perform Google searches and format the results into nodes.</p> <p>Initialize the GoogleRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>SerpAPI key used for authenticating requests. Defaults to the value of the 'SERPAPI_KEY' environment variable if not provided.</p> </li> <li> <code>num_web_results</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of top search results to return (maximum 20).</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to fetch and clean full page content from URLs in the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to crawl per page (if crawling is enabled).</p> </li> </ul>"},{"location":"reference/search/#axion.search.GoogleRetriever.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(query: str) -&gt; SearchResults\n</code></pre> <p>Perform a search query and return results.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"reference/search/#tavilyretriever","title":"TavilyRetriever","text":""},{"location":"reference/search/#axion.search.TavilyRetriever","title":"axion.search.TavilyRetriever","text":"<pre><code>TavilyRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'extract', 'crawl'] = 'search', search_depth: Literal['basic', 'advanced'] = 'basic', topic: Optional[str] = 'general', max_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, days: Optional[int] = None, include_answer: bool = False, include_raw_content: bool = False, include_images: bool = False, include_image_descriptions: bool = False, include_domains: Optional[List[str]] = None, exclude_domains: Optional[List[str]] = None, extract_depth: Literal['basic', 'advanced'] = 'basic', max_depth: Optional[int] = 1, max_breadth: Optional[int] = 20, limit: Optional[int] = 50, instructions: Optional[str] = None, select_paths: Optional[List[str]] = None, select_domains: Optional[List[str]] = None, exclude_paths: Optional[List[str]] = None, allow_external: bool = False, categories: Optional[List[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for Tavily's Search, Extract, and Crawl APIs.</p> <p>Initialize the TavilyRetriever with specified parameters.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(query: str) -&gt; SearchResults\n</code></pre> <p>Retrieve results using the Tavily Search API. Only handles 'search' endpoint. For 'extract' and 'crawl', call their respective methods directly.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.extract_url_text","title":"extract_url_text","text":"<pre><code>extract_url_text(url: str) -&gt; str\n</code></pre> <p>Extract content from a single URL using the Extract API.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.extract","title":"extract  <code>async</code>","text":"<pre><code>extract(url: str) -&gt; SearchResults\n</code></pre> <p>Extract content from a URL using the Extract API.</p>"},{"location":"reference/search/#axion.search.TavilyRetriever.crawl","title":"crawl  <code>async</code>","text":"<pre><code>crawl(url: str) -&gt; SearchResults\n</code></pre> <p>Crawl content from a URL using the Crawl API.</p>"},{"location":"reference/search/#youretriever","title":"YouRetriever","text":"<p>Search Integrations Guide  Google Search Deep Dive </p>"},{"location":"reference/search/#axion.search.YouRetriever","title":"axion.search.YouRetriever","text":"<pre><code>YouRetriever(api_key: Optional[str] = None, endpoint: Literal['search', 'news'] = 'search', num_web_results: Optional[int] = 5, crawl_pages: bool = False, max_crawl_tokens: Optional[int] = 10000, safesearch: Optional[Literal['off', 'moderate', 'strict']] = None, country: Optional[str] = None, search_lang: Optional[str] = None, ui_lang: Optional[str] = None, spellcheck: Optional[bool] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseRetriever</code></p> <p>Retriever for You.com's Search and News API.</p> <p>Initialize the YouRetriever.</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>You.com API key. If not provided, it will attempt to use the <code>YDC_API_KEY</code> environment variable.</p> </li> <li> <code>callback_manager</code>               (<code>Optional[CallbackManager]</code>)           \u2013            <p>Optional manager for handling callback events during retrieval.</p> </li> <li> <code>endpoint</code>               (<code>Literal['search', 'news']</code>, default:                   <code>'search'</code> )           \u2013            <p>The You.com API endpoint to query \u2014 either \"search\" for web results or \"news\" for news-specific content. Defaults to \"search\".</p> </li> <li> <code>num_web_results</code>               (<code>Optional[int]</code>, default:                   <code>5</code> )           \u2013            <p>Maximum number of search results to return. Must not exceed 20.</p> </li> <li> <code>crawl_pages</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to crawl and extract the content of the linked pages from the search results.</p> </li> <li> <code>max_crawl_tokens</code>               (<code>Optional[int]</code>, default:                   <code>10000</code> )           \u2013            <p>Maximum number of tokens to retrieve per page when crawling. If None, a default internal value is used.</p> </li> <li> <code>safesearch</code>               (<code>Optional[Literal['off', 'moderate', 'strict']]</code>, default:                   <code>None</code> )           \u2013            <p>Safe search filtering level. Defaults to \"moderate\" if not specified.</p> </li> <li> <code>country</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Country code for geo-specific search behavior (e.g., \"US\" for United States).</p> </li> <li> <code>search_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code to use for the search query (e.g., \"en\" for English).</p> </li> <li> <code>ui_lang</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Language code for the UI/localized response (e.g., \"en\").</p> </li> <li> <code>spellcheck</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Whether to enable spell check for the query. Defaults to True if unspecified.</p> </li> </ul>"},{"location":"reference/search/#axion.search.YouRetriever.retrieve","title":"retrieve  <code>async</code>","text":"<pre><code>retrieve(query: str) -&gt; SearchResults\n</code></pre> <p>Perform a search query and return results using You.com API.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>Query to search.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SearchResults</code>           \u2013            <p>A list of nodes with associated scores.</p> </li> </ul>"},{"location":"reference/synthetic/","title":"Synthetic API Reference","text":"<p>Synthetic data generation for building evaluation datasets from documents.</p> <pre><code>from axion.synthetic import DocumentQAGenerator, GenerationParams\n</code></pre>"},{"location":"reference/synthetic/#documentqagenerator","title":"DocumentQAGenerator","text":""},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator","title":"axion.synthetic.DocumentQAGenerator","text":"<pre><code>DocumentQAGenerator(llm: LLMRunnable, params: GenerationParams, embed_model: EmbeddingRunnable = None, max_concurrent: int = 5, show_progress: bool = True, tracer: Optional[BaseTraceHandler] = None, **kwargs)\n</code></pre> <p>Orchestrates QA pair generation from multiple documents concurrently.</p> <p>Initialize DocumentQAGenerator</p> <p>Parameters:</p> <ul> <li> <code>llm</code>               (<code>LLMRunnable</code>)           \u2013            <p>The language model to use for generation.</p> </li> <li> <code>params</code>               (<code>GenerationParams</code>)           \u2013            <p>A GenerationParams object with all configuration.</p> </li> <li> <code>embed_model</code>               (<code>EmbeddingRunnable</code>, default:                   <code>None</code> )           \u2013            <p>An embedding model used for semantic parsing.</p> </li> <li> <code>max_concurrent</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Max concurrent retrievers</p> </li> <li> <code>show_progress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show progress bars using tqdm</p> </li> </ul>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.generate_from_directory","title":"generate_from_directory  <code>async</code>","text":"<pre><code>generate_from_directory(directory_path: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Main entry point. Loads docs from a directory and generates QA pairs. Args:     directory_path: The path to the directory containing documents. Returns:     A list of all generated QA pairs.</p>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.to_items","title":"to_items","text":"<pre><code>to_items(results: List[Any]) -&gt; List\n</code></pre> <p>Converts a list of QA evaluation results into a List of <code>DatasetItems</code>.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List</code> (              <code>List</code> )          \u2013            <p>A list of DatasetItem objects.</p> </li> </ul>"},{"location":"reference/synthetic/#axion.synthetic.DocumentQAGenerator.to_dataset","title":"to_dataset","text":"<pre><code>to_dataset(results: List[Any], dataset_name: str)\n</code></pre> <p>Converts a list of QA evaluation results into a structured <code>Dataset</code> object.</p> <p>The function extracts these pairs, renames the fields to match internal <code>FieldNames</code> standards, and wraps each into a <code>DatasetItem</code>. These are then collected into a <code>Dataset</code> for downstream evaluation or analysis.</p> <p>Parameters:</p> <ul> <li> <code>results</code>               (<code>List[Any]</code>)           \u2013            <p>A list of result dictionaries, each containing a 'qa_pairs' list.</p> </li> <li> <code>dataset_name</code>               (<code>str</code>)           \u2013            <p>The name to assign to the resulting Dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>          \u2013            <p>A structured Dataset containing DatasetItems with standardized field names.</p> </li> </ul>"},{"location":"reference/synthetic/#generationparams","title":"GenerationParams","text":"<p>Synthetic Data Guide  Document QA Deep Dive </p>"},{"location":"reference/synthetic/#axion.synthetic.GenerationParams","title":"axion.synthetic.GenerationParams","text":"<pre><code>GenerationParams(**data)\n</code></pre> <p>               Bases: <code>RichBaseModel</code></p> <p>Configuration parameters for controlling the QA (Question\u2013Answer) generation pipeline.</p> <p>These settings define how QA pairs are generated from source documents, including the number of pairs, question style and complexity, chunking strategies, and validation thresholds. The configuration supports both factual and synthetic QA creation, enabling flexible generation for training, evaluation, and benchmarking.</p> <p>Attributes:</p> <ul> <li> <code>num_pairs</code>               (<code>int</code>)           \u2013            <p>Total number of QA pairs to generate per document.</p> </li> <li> <code>question_types</code>               (<code>List[str]</code>)           \u2013            <p>List of question types to generate. Common options include: - 'factual'      : Direct, fact-based questions - 'conceptual'   : Understanding-based questions - 'application'  : Scenario-based application questions - 'analysis'     : Critical thinking and analysis questions - 'synthetic'    : Artificially created questions for stress-testing</p> </li> <li> <code>difficulty</code>               (<code>str</code>)           \u2013            <p>Target difficulty of generated questions. Options include: 'easy', 'medium', and 'hard'.</p> </li> <li> <code>splitter_type</code>               (<code>Literal['semantic', 'sentence']</code>)           \u2013            <p>Chunking strategy for breaking documents into sections: - 'semantic': Embedding-aware splits for context preservation. - 'sentence': Rule-based splits by sentence length.</p> </li> <li> <code>chunk_size</code>               (<code>int</code>)           \u2013            <p>Maximum size (in characters or tokens) of each chunk when using <code>splitter_type='sentence'</code>.</p> </li> <li> <code>statements_per_chunk</code>               (<code>int</code>)           \u2013            <p>Number of candidate statements generated per chunk before filtering and validation.</p> </li> <li> <code>answer_length</code>               (<code>str</code>)           \u2013            <p>Desired length for generated answers. Options: 'short', 'medium', or 'long'.</p> </li> <li> <code>dimensions</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>A dictionary guiding synthetic data generation. Possible keys: - 'features' : Data attributes to reflect real-world structure. - 'persona'  : Profiles simulating different perspectives. - 'scenarios': Contextual situations to ensure realism.</p> </li> <li> <code>custom_guidelines</code>               (<code>Optional[str]</code>)           \u2013            <p>Additional free-text instructions to condition the QA generation process beyond default behavior.</p> </li> <li> <code>example_question</code>               (<code>Optional[str]</code>)           \u2013            <p>An example question to guide style, tone, and complexity.</p> </li> <li> <code>example_answer</code>               (<code>Optional[str]</code>)           \u2013            <p>An example answer to align generated responses with the desired style and depth.</p> </li> <li> <code>max_reflection_iterations</code>               (<code>int</code>)           \u2013            <p>Maximum self-reflection and retry loops for improving QA quality during validation.</p> </li> <li> <code>validation_threshold</code>               (<code>float</code>)           \u2013            <p>Minimum confidence or faithfulness score (0.0\u20131.0) required to accept a QA pair.</p> </li> <li> <code>breakpoint_percentile_threshold</code>               (<code>int</code>)           \u2013            <p>Percentile threshold for determining sentence breakpoints in semantic chunking. Higher values create fewer, larger chunks.</p> </li> </ul>"},{"location":"reference/tracer-registry/","title":"Tracer Registry API Reference","text":"<p>Registry pattern for tracing providers with support for NoOp, Logfire, Langfuse, and Opik backends.</p> <pre><code>from axion._core.tracing.registry import TracerRegistry, BaseTracer\nfrom axion._core.tracing.noop.tracer import NoOpTracer\nfrom axion._core.tracing.logfire.tracer import LogfireTracer\nfrom axion._core.tracing.langfuse.tracer import LangfuseTracer\nfrom axion._core.tracing.opik.tracer import OpikTracer\n</code></pre> N <p>NoOpTracer</p> <p>Zero-overhead tracer for tests and production when observability is not needed.</p> L <p>LogfireTracer</p> <p>OpenTelemetry-based observability with Logfire integration for detailed span tracing.</p> F <p>LangfuseTracer</p> <p>LLM-specific observability with cost tracking, prompt management, and evaluation logging.</p> O <p>OpikTracer</p> <p>Comet Opik integration for experiment tracking, LLM call logging, and evaluation traces.</p>"},{"location":"reference/tracer-registry/#tracerregistry","title":"TracerRegistry","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry","title":"axion._core.tracing.registry.TracerRegistry","text":"<p>Registry for tracer implementations using decorator pattern.</p> <p>This class provides a simple way to register and retrieve tracer implementations by name, following the same pattern as LLMRegistry.</p> Example <p>@TracerRegistry.register('my_tracer') class MyTracer(BaseTracer):     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry--later-retrieve-the-tracer-class","title":"Later, retrieve the tracer class","text":"<p>TracerClass = TracerRegistry.get('my_tracer') tracer = TracerClass.create(metadata_type='llm')</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(name: str) -&gt; Callable[[Type[BaseTracer]], Type[BaseTracer]]\n</code></pre> <p>Decorator to register a tracer implementation.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name to register the tracer under (e.g., 'noop', 'logfire', 'langfuse')</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable[[Type[BaseTracer]], Type[BaseTracer]]</code>           \u2013            <p>Decorator function</p> </li> </ul> Example <p>@TracerRegistry.register('custom') class CustomTracer(BaseTracer):     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(name: str) -&gt; Type[BaseTracer]\n</code></pre> <p>Get a tracer class by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The registered name of the tracer</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Type[BaseTracer]</code>           \u2013            <p>The tracer class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the tracer is not registered</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.list_providers","title":"list_providers  <code>classmethod</code>","text":"<pre><code>list_providers() -&gt; List[str]\n</code></pre> <p>List all registered tracer provider names.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of registered tracer names</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.is_registered","title":"is_registered  <code>classmethod</code>","text":"<pre><code>is_registered(name: str) -&gt; bool\n</code></pre> <p>Check if a tracer is registered.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if registered, False otherwise</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.TracerRegistry.display","title":"display  <code>classmethod</code>","text":"<pre><code>display() -&gt; None\n</code></pre> <p>Display the tracer registry in a rich HTML format.</p>"},{"location":"reference/tracer-registry/#basetracer","title":"BaseTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer","title":"axion._core.tracing.registry.BaseTracer","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all tracer implementations.</p> <p>This defines the core interface that all tracers (NoOp, Logfire, Langfuse, etc.) must implement. Users can create custom tracers by subclassing this and registering with @TracerRegistry.register('name').</p> Example <p>@TracerRegistry.register('my_custom_tracer') class MyCustomTracer(BaseTracer):     # implement abstract methods     ...</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(metadata_type: str = 'default', tool_metadata: Optional[Any] = None, **kwargs) -&gt; BaseTracer\n</code></pre> <p>Factory method to create tracer instances.</p> <p>Parameters:</p> <ul> <li> <code>metadata_type</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>Type of metadata (e.g., 'llm', 'knowledge', 'evaluation')</p> </li> <li> <code>tool_metadata</code>               (<code>Optional[Any]</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata about the tool being traced</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional implementation-specific arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseTracer</code>           \u2013            <p>A new tracer instance</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.span","title":"span  <code>abstractmethod</code>","text":"<pre><code>span(operation_name: str, **attributes)\n</code></pre> <p>Create a synchronous span context manager.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A span object that can be used to add attributes or events</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.async_span","title":"async_span  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>async_span(operation_name: str, **attributes)\n</code></pre> <p>Create an asynchronous span context manager.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A span object that can be used to add attributes or events</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start(**attributes) -&gt; None\n</code></pre> <p>Start execution tracking.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.complete","title":"complete  <code>abstractmethod</code>","text":"<pre><code>complete(output_data: Optional[Dict[str, Any]] = None, **attributes) -&gt; None\n</code></pre> <p>Complete execution tracking.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.fail","title":"fail  <code>abstractmethod</code>","text":"<pre><code>fail(error: str, **attributes) -&gt; None\n</code></pre> <p>Handle execution failure.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.add_trace","title":"add_trace  <code>abstractmethod</code>","text":"<pre><code>add_trace(event_type: str, message: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Add a trace event.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush any pending traces to the backend.</p> <p>Override in implementations that buffer traces (e.g., Langfuse). Default implementation is a no-op.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Gracefully shutdown the tracer.</p> <p>Override in implementations that need cleanup. Default implementation calls flush().</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.registry.BaseTracer.info","title":"info","text":"<pre><code>info(msg: Any) -&gt; None\n</code></pre> <p>Log info message.</p>"},{"location":"reference/tracer-registry/#built-in-tracers","title":"Built-in Tracers","text":""},{"location":"reference/tracer-registry/#nooptracer","title":"NoOpTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer","title":"axion._core.tracing.noop.tracer.NoOpTracer","text":"<pre><code>NoOpTracer(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = False, trace_id: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTracer</code></p> <p>A no-operation tracer that mirrors the LogfireTracer interface.</p> <p>This class provides a complete, non-functional implementation of the tracer so that it can be swapped with the real LogfireTracer without causing any errors. All tracing and logging methods are designed to do nothing, ensuring minimal performance overhead when tracing is disabled.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = False, **kwargs) -&gt; NoOpTracer\n</code></pre> <p>Factory method to create a NoOpTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.span","title":"span","text":"<pre><code>span(operation_name: str, **attributes)\n</code></pre> <p>Creates a synchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.noop.tracer.NoOpTracer.async_span","title":"async_span  <code>async</code>","text":"<pre><code>async_span(operation_name: str, **attributes)\n</code></pre> <p>Creates an asynchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#logfiretracer","title":"LogfireTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer","title":"axion._core.tracing.logfire.tracer.LogfireTracer","text":"<pre><code>LogfireTracer(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, enable_logfire: bool = True, trace_id: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTracer</code></p> <p>Tracer that combines tracing, logging, and metadata collection. Uses composition with a logger instance instead of inheritance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(metadata_type: str, tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; LogfireTracer\n</code></pre> <p>Factory method to create a LogfireTracer.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.span","title":"span","text":"<pre><code>span(operation_name: str, **attributes)\n</code></pre> <p>Creates a synchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.async_span","title":"async_span  <code>async</code>","text":"<pre><code>async_span(operation_name: str, **attributes)\n</code></pre> <p>Creates an asynchronous span AND sets the tracer context for its duration. Uses shared context management from main tracer module.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_llm_call","title":"log_llm_call","text":"<pre><code>log_llm_call(model: str, prompt: str, response: str, prompt_tokens: int = None, completion_tokens: int = None, latency: float = None, cost_estimate: float = None, error: str = None, **attributes) -&gt; None\n</code></pre> <p>Log an LLM call with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_retrieval_call","title":"log_retrieval_call","text":"<pre><code>log_retrieval_call(context: List[Dict[str, Any]], latency: float, query: str = None, **attributes) -&gt; None\n</code></pre> <p>Log a retrieval call with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_evaluation","title":"log_evaluation","text":"<pre><code>log_evaluation(evaluation_id: str, evaluator_name: str, evaluator_type: str, dataset_size: int, latency: float, overall_metrics: List[EvaluationMetric] = None, datapoint_results: List[EvaluationDatapoint] = None, dataset_name: str = None, cost_estimate: float = None, tokens_used: int = None, error: str = None, evaluator_config: Dict[str, Any] = None, **attributes) -&gt; None\n</code></pre> <p>Log an evaluation run with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.log_database_query","title":"log_database_query","text":"<pre><code>log_database_query(query: str, latency: float, rows_affected: int = 0, **attributes) -&gt; None\n</code></pre> <p>Log a database query with automatic tracing.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.get_statistics","title":"get_statistics","text":"<pre><code>get_statistics() -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics from metadata.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.logfire.tracer.LogfireTracer.display_statistics","title":"display_statistics","text":"<pre><code>display_statistics()\n</code></pre> <p>Display statistics using the display utility.</p>"},{"location":"reference/tracer-registry/#langfusetracer","title":"LangfuseTracer","text":""},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer","title":"axion._core.tracing.langfuse.tracer.LangfuseTracer","text":"<pre><code>LangfuseTracer(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, public_key: Optional[str] = None, secret_key: Optional[str] = None, base_url: Optional[str] = None, trace_id: Optional[str] = None, tags: Optional[List[str]] = None, environment: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTracer</code></p> <p>Langfuse-based tracer for LLM observability.</p> <p>This tracer integrates with Langfuse (https://langfuse.com) to provide detailed tracing and observability for LLM applications.</p> Configuration <p>Set the following environment variables or pass to constructor: - LANGFUSE_PUBLIC_KEY: Your Langfuse public key - LANGFUSE_SECRET_KEY: Your Langfuse secret key - LANGFUSE_BASE_URL: API endpoint (default: https://cloud.langfuse.com) - LANGFUSE_TAGS: Comma-separated list of tags (e.g., \"prod,v1.0\") - LANGFUSE_ENVIRONMENT: Environment name (e.g., \"production\", \"staging\")</p> Example <p>from axion._core.tracing import Tracer, configure_tracing</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer--configure-via-environment","title":"Configure via environment","text":"<p>os.environ['TRACING_MODE'] = 'langfuse' os.environ['LANGFUSE_PUBLIC_KEY'] = 'pk-lf-...' os.environ['LANGFUSE_SECRET_KEY'] = 'sk-lf-...' os.environ['LANGFUSE_TAGS'] = 'prod,v1.0' os.environ['LANGFUSE_ENVIRONMENT'] = 'production'</p> <p>configure_tracing() tracer = Tracer('llm') with tracer.span('my-operation', tags=['custom-tag']):     # ... your code tracer.flush()</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer--or-pass-tagsenvironment-directly","title":"Or pass tags/environment directly:","text":"<p>tracer = LangfuseTracer(tags=['prod', 'v1.0'], environment='production') with tracer.span('my-operation'):     # ... your code</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; LangfuseTracer\n</code></pre> <p>Factory method to create a LangfuseTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.span","title":"span","text":"<pre><code>span(operation_name: str, **attributes)\n</code></pre> <p>Creates a synchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A LangfuseSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.async_span","title":"async_span  <code>async</code>","text":"<pre><code>async_span(operation_name: str, **attributes)\n</code></pre> <p>Creates an asynchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>A LangfuseSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush pending traces to Langfuse.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.log_llm_call","title":"log_llm_call","text":"<pre><code>log_llm_call(*args, **kwargs) -&gt; None\n</code></pre> <p>Log an LLM call to Langfuse.</p> <p>Prefer attaching prompt/response/usage to the current span rather than creating a separate generation observation, to avoid duplicates in the Langfuse UI.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.langfuse.tracer.LangfuseTracer.log_evaluation","title":"log_evaluation","text":"<pre><code>log_evaluation(*args, **kwargs) -&gt; None\n</code></pre> <p>Log an evaluation.</p>"},{"location":"reference/tracer-registry/#opiktracer","title":"OpikTracer","text":"<p>Tracing Deep Dive  Langfuse Guide </p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer","title":"axion._core.tracing.opik.tracer.OpikTracer","text":"<pre><code>OpikTracer(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, api_key: Optional[str] = None, workspace: Optional[str] = None, project_name: Optional[str] = None, base_url: Optional[str] = None, trace_id: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTracer</code></p> <p>Opik-based tracer for LLM observability.</p> <p>This tracer integrates with Opik (https://www.comet.com/docs/opik/) to provide detailed tracing and observability for LLM applications.</p> Configuration <p>Set the following environment variables or pass to constructor: - OPIK_API_KEY: Your Opik API key - OPIK_WORKSPACE: Your workspace name - OPIK_PROJECT_NAME: Project name (default: 'axion') - OPIK_URL_OVERRIDE: API endpoint (default: https://www.comet.com/opik/api)</p> Example <p>from axion._core.tracing import Tracer, configure_tracing</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer--configure-via-environment","title":"Configure via environment","text":"<p>os.environ['TRACING_MODE'] = 'opik' os.environ['OPIK_API_KEY'] = 'your-api-key' os.environ['OPIK_WORKSPACE'] = 'your-workspace'</p> <p>configure_tracing() tracer = Tracer('llm') with tracer.span('my-operation'):     # ... your code tracer.flush()</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(metadata_type: str = 'default', tool_metadata: Optional[ToolMetadata] = None, **kwargs) -&gt; OpikTracer\n</code></pre> <p>Factory method to create an OpikTracer instance.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.span","title":"span","text":"<pre><code>span(operation_name: str, **attributes)\n</code></pre> <p>Creates a synchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>An OpikSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.async_span","title":"async_span  <code>async</code>","text":"<pre><code>async_span(operation_name: str, **attributes)\n</code></pre> <p>Creates an asynchronous span AND sets the tracer context for its duration.</p> <p>Parameters:</p> <ul> <li> <code>operation_name</code>               (<code>str</code>)           \u2013            <p>Name of the operation being traced</p> </li> <li> <code>**attributes</code>           \u2013            <p>Additional attributes to attach to the span</p> </li> </ul> <p>Yields:</p> <ul> <li>           \u2013            <p>An OpikSpan object</p> </li> </ul>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.flush","title":"flush","text":"<pre><code>flush() -&gt; None\n</code></pre> <p>Flush pending traces to Opik.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.shutdown","title":"shutdown","text":"<pre><code>shutdown() -&gt; None\n</code></pre> <p>Gracefully shutdown the tracer.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.log_llm_call","title":"log_llm_call","text":"<pre><code>log_llm_call(*args, **kwargs) -&gt; None\n</code></pre> <p>Log an LLM call to Opik as a generation span.</p>"},{"location":"reference/tracer-registry/#axion._core.tracing.opik.tracer.OpikTracer.log_evaluation","title":"log_evaluation","text":"<pre><code>log_evaluation(*args, **kwargs) -&gt; None\n</code></pre> <p>Log an evaluation to Opik.</p>"}]}